\documentclass[10pt]{mypackage}

\usepackage{mlmodern}
%\usepackage{newpxtext,eulerpx,eucal}
%\renewcommand*{\mathbb}[1]{\varmathbb{#1}}

%\usepackage{homework}
\usepackage{notes}

%\usepackage[ backend=bibtex, style = alphabetic, sorting=ynt ]{biblatex}
%\addbibresource{  }

\usepackage{parskip}

\fancyhf{}
\fancyhead[R]{Avinash Iyer}
\fancyhead[L]{Random Matrices: Class Notes}
\fancyfoot[C]{\thepage}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
There are three primary ``themes'' to this course:
\begin{itemize}
  \item Gaussian Unitary Ensemble and Free Probability;
  \item Random Unitary Matrices;
  \item and Random Permutation Matrices and Random Graphs.
\end{itemize}
There are many connections to active areas of research emerging from these main themes.
\section{Gaussian Unitary Ensemble}%
\subsection{Wigner Semicircle Law}%
We start by proving the Wigner semicircle law. Toward this end, we recall some definitions from probability theory.

Generally speaking, when we refer to the Gaussian distribution, we are referring to the standard normal distribution, which has density
\begin{align*}
  d\mu &= \frac{1}{\sqrt{2\pi}} e^{-x^2/2}\:dx.
\end{align*}

\begin{definition}
  If $Z$ is a random variable, we say that the $n$th moment of $Z$ is the value
  \begin{align*}
    \mu_n &= E\left[ Z^{n} \right],
  \end{align*}
  where if $Z$ has probability density function
  \begin{align*}
    d\mu &= f\:dx,
  \end{align*}
  we define
  \begin{align*}
    E\left[ Z^{n} \right] &= \int_{-\infty}^{\infty} x^{n}f(x)\:dx.
  \end{align*}
\end{definition}
\begin{proposition}
  The moments of the standard Gaussian are given by
  \begin{align*}
    E\left[ Z^{2k} \right] &= \left( 2k-1 \right)!!\\
                           &= \left( 2k-1 \right)\left( 2k-3 \right)\cdots \left( 3 \right)\left( 1 \right)\\
    E\left[ Z^{2k-1} \right] &= 0.
  \end{align*}
\end{proposition}
\begin{proof}
  We see that
  \begin{align*}
    E\left[ Z^{m} \right] &= \int_{-\infty}^{\infty} x^{m}e^{-x^2/2}\:dx\\
                          &= -x^{m}e^{-x^2/2}\bigr\vert_{-\infty}^{\infty} + \left( m-1 \right) \int_{-\infty}^{\infty} x^{m-2}e^{-x^2/2}\:dx\\
                          &= \left( m-1 \right)E\left[ Z^{m-2} \right].
  \end{align*}
\end{proof}
There is in fact an underlying combinatorial structure. If $\left[ n \right] = \set{1,2,\dots,n}$, then a partition is a collection of disjoint ``blocks'' with
\begin{align*}
  \left[ n \right] &= \bigcup_{i=1}^{n}V_i.
\end{align*}
We let $P(n)$ denote the set of all partitions of $n$. The pair partitions are the set
\begin{align*}
  P_2(n) &= \set{\pi\in P(n) | \left\vert V \right\vert = 2\text{ for all }V\in \pi}.
\end{align*}
By an induction argument, we observe that the value $P_2\left(2n\right)$ follows the formula
\begin{align*}
  \left\vert P_2\left(2n\right) \right\vert &= \left( 2n-1 \right) \left\vert P_2\left( 2n-2 \right) \right\vert\\
                                            &= \left( 2n-1 \right)!!.
\end{align*}
As a general rule, when we discuss \textit{complex} Gaussians, we refer to the random variable
\begin{align*}
  Z &= \frac{1}{\sqrt{2}} \left( X + i Y \right),
\end{align*}
where $X$ and $Y$ are independent real (standard) Gaussians.
\begin{theorem}[Wick's Formula]
  Let $\left( X_1,\dots,X_n \right)$ be a real random Gaussian vector, and let $ \mathbf{i} = \left( i_1,\dots,i_k \right) $ be a multi index contained in $\left[ n \right]^{k}$. If $k$ is even with $\pi\in P_2(k)$, let
  \begin{align*}
    E_{\pi}\left( X_1,\dots,X_k \right) &= \prod_{\left( r,s \right)\in \pi} E\left( X_rX_s \right).
  \end{align*}
  Then, we have
  \begin{align*}
    E\left[ X_{i_1}\cdots X_{i_k} \right] &= \sum_{\pi\in P_2(k)} E_{\pi}\left[ X_{i_1},\dots,X_{i_k} \right]\\
                                          &= \sum_{\pi\in P_2(k)} \prod_{\left( r,s \right)\in \pi}E\left[ X_{ i_r }X_{ i_s } \right]
  \end{align*}
\end{theorem}
The proof goes by first showing the case when the covariance matrix $\Sigma$ is diagonal, then by diagonalizing $\Sigma$ and using multilinearity in the general case.
\begin{definition}
  Let $A = \left( a_{ij} \right)_{i,j}$ be a $N\times N$ matrix, where
  \begin{enumerate}[(i)]
    \item $a_{ii}$ are independent real Gaussians with variance $\frac{1}{N}$;
    \item $a_{ij}$ for $1\leq i < j \leq N$ are independent complex Gaussians with variance $\frac{1}{N}$;
    \item $a_{ij} = \overline{a_{ji}}$.
  \end{enumerate}
  We always assume the Gaussians have mean zero. Then, we call $A$ the \textit{Gaussian Unitary Ensemble}.
\end{definition}
\begin{definition}
  Let $\left( \Omega,\mathcal{F},P \right)$ be a probability space, and $A = \left( a_{ij} \right)_{i,j}$ a self-adjoint matrix of complex-valued random variables. The random measure
  \begin{align*}
    \nu_A \coloneq \frac{1}{N}\sum_{i=1}^{n} \delta_{\lambda_i},
  \end{align*}
  where the $\lambda_i$ are eigenvalues of $A$ counted with multiplicity, is known as the \textit{empirical spectral distribution} of $A$.

  The \textit{average eigenvalue distribution} of $A$ is the measure $\mu_A$ determined by
  \begin{align*}
    \int_{\R}^{} f\:d\mu_A &= E\left[ \int_{\R}^{} f\:d\nu_A \right]
  \end{align*}
  for all measurable functions $f\colon \R\rightarrow \R$. That is, $\mu_A(S) = E\left[ \nu_A(S) \right]$ for measurable subsets $S\subseteq \Omega$.
\end{definition}
Notice that
\begin{align*}
  \int_{\R}^{} x^{m}\:d\mu_A &= E\left[ \int_{\R}^{} x^{m}\:d\nu_A \right]\\
                             &= E\left[ \frac{1}{N}\sum_{i=1}^{N} \lambda_{i}^{m}\right]\\
                             &= E\left[ \operatorname{tr}\left( A^{m} \right) \right],
\end{align*}
where $\operatorname{tr}$ denotes the normalized trace,
\begin{align*}
  \operatorname{tr}(A) &= \frac{1}{N}\sum_{i=1}^{N}A_{ii}.
\end{align*}

\end{document}
