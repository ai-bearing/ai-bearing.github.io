\documentclass[10pt]{mypackage}

%\usepackage{mlmodern}
%\usepackage{newpxtext,eulerpx,eucal}
%\renewcommand*{\mathbb}[1]{\varmathbb{#1}}

%\usepackage{homework}
\usepackage{notes}

\usepackage[ backend=bibtex, style = alphabetic, sorting=ynt ]{biblatex}
\addbibresource{project_references.bib}

\usepackage{parskip}

\fancyhf{}
\fancyhead[R]{Avinash Iyer}
\fancyhead[L]{Free Entropy}
\fancyfoot[C]{\thepage}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight

\section{Background: Asymptotic Freeness and Large Deviations}%
We start by recalling the basic asymptotic freeness result.
\begin{proposition}
  Let $\left( A_1^{N},\dots,A_r^{N} \right)$ be an independent $r$-tuple of GUE $N\times N$ matrices. Then, the family $A_1^N,\dots,A_r^N$ converge in distribution to $r$ independent semicircular elements, $s_1,\dots,s_r\in B\left( \mathcal{F}\left( \C^r \right) \right)$, in the sense that for all $m\geq 1$ and all $1\leq i_1,\dots,i_m \leq r$, we have
  \begin{align*}
    \lim_{N\rightarrow\infty} E\left[ \tr\left( A_{i_1}^N\cdots A_{i_m}^N \right) \right] &= \varphi\left( s_{i_1}\cdots s_{i_m} \right),
  \end{align*}
  where $\varphi$ is the vacuum state, $\varphi\left( T \right) = \iprod{T\Omega}{\Omega}$.
\end{proposition}
In fact, this collection is \textit{almost surely} asymptotically free, in the following sense. Suppose we have two random matrices $A^N$ and $B^N$ defined on probability spaces $\left( X_N,\mu_N \right)$. Define
\begin{align*}
  X &\coloneq \prod_{N\in \N} X_N\\\
  \mu &\coloneq \prod_{N\in \N}\mu_N,
\end{align*}
where the latter is the product measure on $X$. The matrices $A^N$ and $B^N$ are said to be almost surely asymptotically free if there exists a noncommutative probability space $\left( A,\varphi \right)$ and $a,b\in A$, and for almost all $x = \left( x_N \right)_N\in X$, we have $A^N\left(x_N\right),B^N\left(x_N\right)\in \left( \M_N,\tr \right)$ converge in distribution to $a,b$.

Now, from here, we may ask a seemingly simple question: as $N$ grows large, how likely are we to encounter other distributions? To make this sense more precise, we consider a random $N\times N$ self-adjoint matrix $A$, and let
\begin{align*}
  \mu_A &= \frac{1}{N}\sum_{i=1}^{N}\delta_{\lambda_1}
\end{align*}
be its empirical spectral distribution. This is a random probability measure on $\R$, and as $N\rightarrow \infty$, the semicircle law gives that $\mu_A$ converges weakly to the semicircle distribution; this can be strengthened to almost sure convergence by an application of the argument for asymptotic freeness. The question then becomes, how quickly does the deviation between $\mu_A$ and any other probability distribution $\nu$ decrease as $N$ increases? This is where the theory of large deviations starts to take shape.

Much of this exposition related to the classical notions of entropy will be centered around results discussed in \cite[Ch.\,7]{mingo_and_speicher}.
\subsection{Large Deviations for Random Variables}%
We start with one of the classical examples of convergence of random variables to introduce large deviations. Consider a sequence of independent and identically distributed real-valued random variables $\left( X_i \right)_i$ with common distribution $\mu$. Set
\begin{align*}
  S_n &= \frac{1}{n}\sum_{i=1}^{n}X_i.\\
  m &= E\left[ X_1 \right]\\
  v &= E\left[ X_1^2 \right] - m^2.
\end{align*}
Then, we have that if $E\left[ X_1^2 \right] < \infty$, the central limit theorem says that $S_n \approx m + \frac{\sigma}{\sqrt{n}}N\left( 0,1 \right)$.

If $\mu$ is the standard Gaussian distribution, then this gives that $S_n$ is distributed as $N\left( 0,1/n \right)$; we then get that
\begin{align*}
  P\left( S_n\in [x,x+dx] \right) &\approx \sqrt{\frac{n}{2\pi}}e^{-nx^2}dx.
\end{align*}
Asymptotically, this gives that the probability that $S_n$ is near the value $x\in \R$ decays exponentially in $n$ determined by a rate function $I(x) = x^2/2$.

We will now generalize this result. In particular, if we let $\mu$ be any distribution discussed above (rather than simply the normal distribution), then we will find a rate function $I(x)$ such that 
\begin{align*}
  e^{-nI(x)} &\sim P\left( S_n > x \right)
    \intertext{whenever $x > m$, and whenever $x < m$}
  e^{-nI(x)} &\sim P\left( S_n < x \right).
\end{align*}
For a given distribution $\mu$, we can compute the rate function by using a family of basic manipulations. If $x > m$, then for all $\lambda\geq 0$, we may use Markov's inequality to obtain
\begin{align*}
  P\left( S_n > x \right) &= P\left( nS_n > nx \right)\\
                          &= P\left( e^{\lambda\left( nS_n-nx \right)}\geq 1 \right)\\
                          &\leq E\left[ e^{\lambda\left( nS_n-nx \right)} \right]\\
                          &= e^{-\lambda n x} E\left[ e^{\lambda\left( X_1 + \cdots + X_n \right)} \right]\\
                          &= \left( e^{-\lambda x}E\left[ e^{\lambda X} \right] \right)^{n},
\end{align*}
where $X$ is identically distributed to each of the $X_i$, and we use the fact that the $X_i$ are independent. We may then define
\begin{align*}
  \Lambda\left( \lambda \right) &= \ln E\left[ e^{\lambda X} \right]\label{eq:cumulant_generating_function}\tag{$\ast$}
\end{align*}
to be an extended real-valued function, but we only consider $\mu$ for which $\Lambda\left( \lambda \right)$ is finite for all $\lambda$ in an open neighborhood of $0$. The equation \eqref{eq:cumulant_generating_function} is known as the cumulant generating function for $\mu$.

This gives the inequality
\begin{align*}
  P\left( S_n > x \right) &\leq e^{-n\left( \lambda x - \Lambda\left( \lambda \right) \right)}.
\end{align*}
Since $\ln$ is a concave function, Jensen's inequality gives
\begin{align*}
  \Lambda\left( \lambda \right) &\geq E\left[ \ln\left( e^{\lambda X} \right) \right]\\
                                &= E\left[ \lambda X \right]\\
                                &= \lambda m.
\end{align*}
In particular, for any $\lambda < 0$ and $x > m$, we have $-n\left( \lambda x - \Lambda\left( \lambda \right) \right) \geq 0$, meaning this equation is valid for all $\lambda$. In particular, we have
\begin{align*}
  P\left( S_n > x \right) &\leq \inf_{\lambda\in \R} e^{-n\lambda x - \Lambda\left( \lambda \right)}.
\end{align*}
Now, we observe that $\Lambda$ is convex. This follows from Hölder's inequality
\begin{align*}
  E\left[ e^{\left( 1-t \right)\lambda_1 x + t\lambda_2 x} \right] &\leq E\left[ e^{\lambda_1 x} \right]^{1-t}E\left[ e^{\lambda_2 x} \right]^{t}
\end{align*}
so that
\begin{align*}
  \Lambda\left( \left( 1-t \right)\lambda_1 + t\lambda_2 \right) &\leq \left( 1-t \right)\Lambda\left( \lambda_1 \right) + t\Lambda\left( \lambda_2 \right).
\end{align*}
Defining the \textit{Legendre transform} of $\Lambda$ by
\begin{align*}
  \Lambda^{\ast}\left( x \right) &= \sup_{\lambda\in \R} \left( \lambda x - \Lambda\left( \lambda \right) \right),
\end{align*}
we find that this is a convex function of $x$, as it is a supremum of a family of convex functions of $x$.

Now, since $\Lambda(0) = 0$, it follows that $\Lambda^{\ast}(x)\geq 0$, and has $\Lambda^{\ast}\left( m \right) = 0$. In particular, this gives
\begin{align*}
  P\left( S_n > x \right) &\leq e^{-n\Lambda^{\ast}\left( x \right)}
\end{align*}
whenever $x > m$.

It can also be shown that $e^{-n\Lambda^{\ast}(x)}$ is an asymptotic lower bound, in that
\begin{align*}
  \liminf_{n\rightarrow\infty} \frac{1}{n}\ln P\left( x-\delta < S_n < x + \delta \right) \geq -\Lambda^{\ast}(x)
\end{align*}
for all $x\in \R$ and all $\delta > 0$. The method for doing so is outlined in \cite[Ch. 7, Section 2]{mingo_and_speicher}, and results in Cramér's theorem for real-valued random vectors.
\begin{theorem}[Cramér's Theorem]
  Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed random vectors in $\R^d$ with common distribution $\mu$. Define
  \begin{align*}
    \Lambda(\lambda) &= \ln E\left[ e^{ \iprod{\lambda}{X_i} } \right]\\
    \Lambda^{\ast}\left( x \right) &= \sup_{\Lambda\in \R^d} \left( \iprod{\lambda}{x}-\Lambda\left( \lambda \right) \right),
  \end{align*}
  and assume that $\Lambda\left( \lambda \right) < \infty$ for all $\lambda$. Set $S_n = \frac{1}{n}\left( X_1 + \cdots + X_n \right)$. Then, the distribution $\mu_{S_n}$ satisfies has
  \begin{itemize}
    \item $x\mapsto \Lambda^{\ast}(x)$ is convex;
    \item $\set{x\in \R^d | \Lambda^{\ast}\left( x \right) \leq \alpha}$ is compact for all $\alpha\in \R$;
    \item for any closed $F\subseteq \R^d$,
      \begin{align*}
        \limsup_{n\rightarrow\infty} \frac{1}{n}\ln P\left( S_n\in F \right) &\leq -\inf_{x\in F}\Lambda^{\ast}\left( x \right),
      \end{align*}
    \item and for any open $G\subseteq \R^d$,
      \begin{align*}
        \liminf_{n\rightarrow\infty} \frac{1}{n}\ln P\left( S_n\in G \right) &\geq -\inf_{x\in G}\Lambda^{\ast}\left( x \right).
      \end{align*}
  \end{itemize}
\end{theorem}
This theorem defines precisely the large deviation principle that the partial sums satisfy --- namely, it is the Legendre transform of the cumulant-generating function.
\subsection{Large Deviations for the Empirical Distribution}%
Now, our next step is to develop an analogous large deviation principle for the empirical distribution of the random variables $X_1,X_2,\dots$. This will give us the idea of classical entropy.

We start by considering the case of (independent and identically distributed) random variables $X_i\colon \Omega\rightarrow A$ taking values in a finite set $\set{a_1,\dots,a_d}$, and define $p_k\coloneq P\left( X_i = a_k \right)$. We expect that, as $n\rightarrow\infty$, the empirical distribution of the $X_i$ should converge to the probability measure $\left( p_1,\dots,p_d \right)$ on $A$.

Toward this end, let $Y_i\colon \Omega\rightarrow \R^d$ be defined by
\begin{align*}
  Y_i &\coloneq \left( \chi_{\set{a_1}}\left( X_i \right),\dots,\chi_{\set{a_d}}\left( X_i \right) \right).
\end{align*}
We observe that $p_k$ is the probability that $Y_i$ will have $1$ in position $k$ and $0$ elsewhere, and that $\frac{1}{n}\left( Y_1 + \cdots + Y_n \right)$ gives the relative frequency of $a_1,\dots,a_d$ --- i.e., this has the same information as the empirical distribution of $X_1,\dots,X_n$.

Any probability measure on $A$ is a $d$-tuple of positive real numbers satisfying $q_1 + \cdots + q_d = 1$. By Cramér's theorem and our discussion above, we have
\begin{align*}
  P\left( \frac{1}{n}\left( \delta_{X_1} + \cdots + \delta_{X_n} \right) \approx \left( q_1,\dots,q_d \right) \right) &= P\left( \frac{1}{n}\left( Y_1 + \cdots + Y_n \right) \approx \left( q_1,\dots,q_d \right) \right)\\
                                                                                                                              &\sim e^{-n\Lambda^{\ast}\left( q_1,\dots,q_d \right)}.
\end{align*}
Applying our definitions for $\Lambda$ and $\Lambda^{\ast}$, we have
\begin{align*}
  \Lambda\left( \lambda_1,\dots,\lambda_d \right) &= \ln\left( p_1e^{\lambda_1} + \cdots + p_de^{\lambda_d} \right)\\
  \Lambda^{\ast}\left( q_1,\dots,q_d \right) &= \sup_{\left( \lambda_1,\dots,\lambda_d \right)} \left( \lambda_1q_1 + \cdots + \lambda_dq_d - \ln\left( p_1e^{\lambda_1} + \cdots + p_de^{\lambda_d} \right) \right).
\end{align*}
To compute the supremum over all tuples, we find that the partial derivatives with respect to each $\lambda_i$ are given by
\begin{align*}
  q_i - \frac{1}{p_1e^{\lambda_1} + \cdots + p_de^{\lambda_d}}p_ie^{\lambda_i},
\end{align*}
so by concavity, we get that the maximum value occurs when
\begin{align*}
  \lambda_i &= \ln \left( \frac{q_i}{p_i} \right) + \Lambda\left( \lambda_1 ,\dots,\lambda_d \right).
\end{align*}
We thus get
\begin{align*}
  \Lambda^{\ast}\left( q_1,\dots,q_d \right) &= \sum_{i=1}^{d}q_i \ln\left( \frac{q_i}{p_i} \right).
\end{align*}
The quantity on the right is the relative Shannon entropy $H\left( \left( q_1,\dots,q_d \right) | \left( p_1,\dots,p_d \right) \right)$, which is strictly positive except when $q_1 = p_1,\dots,q_d = p_d$.

In particular, we have that $\left( p_1,\dots,p_d \right)$ admits a large deviation principle with rate function given by the relative Shannon entropy. That this holds for any distribution is known as Sanov's theorem.
\begin{theorem}[Sanov's Theorem]
  Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed real random variables with common distribution $\mu$, and let
  \begin{align*}
    \nu_n &\coloneq \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i}
  \end{align*}
  be the empirical distribution. Then, the family $\set{\nu_n}_{n\geq 1}$ satisfies a large deviation principle given by the rate function
  \begin{align*}
    I\left( \nu \right) &= \begin{cases}
      \int_{}^{} p\ln(p)\:d\mu & d\nu = p\:d\mu\\
      +\infty & \text{else}.
    \end{cases}
  \end{align*}
\end{theorem}
\section{One-Dimensional Free Entropy: A Heuristic Approach}%
The next logical step after defining a large deviation principle for a sequence of random variables is to define such a quantity for single free random variables.

In \cite{voiculescu_analogues_1}, Voiculescu used a heuristic method based on random matrix theory to establish the value $\Sigma(x)$ for some random variable $x$ in a $C^{\ast}$-probability space $\left( \mathcal{A},\varphi \right)$ distributed according to a compactly supported measure $\nu$ on $\R$ (i.e., its spectral measure with respect to the state $\varphi$).

It can be established (as in \cite[Exercise 1.8]{mingo_and_speicher}) that if $X$ is a $N\times N$ GUE matrix, then the density inside the real vector space $\M_N\left( \C \right)_{\sa}$ is given by
\begin{align*}
  dP_N(X) &= K e^{-N^2/2\tr\left( X^2 \right)}\:dm,
\end{align*}
where $m$ is the Lebesgue measure on $\R^{N^2}$ and $\tr$ denotes the normalized trace. It can be shown, as in \cite[Theorem 2.5.2]{anderson_guionnet_zeitouni}, that the joint distribution of the eigenvalues $\lambda_1(X) \leq\cdots\leq \lambda_N(X)$ is absolutely continuous with respect to the Lebesgue measure and has density given by
\begin{align*}
  dQ_N\left(\lambda_1,\dots,\lambda_N\right) &= \frac{N^{N^2/2}}{\left( 2\pi \right)^{N/2}\prod_{j=1}^{N}j!} e^{-\left( N\sum_{i=1}^{N}\lambda_i^2 \right)/2}\prod_{i < j}\left( \lambda_i - \lambda_j \right)^2 \prod_{i=1}^{N}d\lambda_i
\end{align*}
Heuristically, letting $\mu_A$ denote the empirical spectral distribution on $\M_N\left( \C \right)_{\sa}$ given by
\begin{align*}
  \mu_A &= \frac{1}{N}\left( \delta_{\lambda_1(A)} + \cdots + \delta_{\lambda_N(A)} \right),
\end{align*}
we may consider a large deviation principle with $P_N\left( \mu_A\approx \nu \right)\sim e^{-N^2I\left(\nu\right)}$, and write it out as
\begin{align*}
  P_N\left( \mu_A\approx \nu \right) &= Q_N\left( \frac{1}{N}\left( \delta_{\lambda_1} + \cdots + \delta_{\lambda_N} \right) \right)\\
                                     &= \frac{N^{N^2/2}}{\left( 2\pi \right)^{N/2}\prod_{j=1}^{N}j!} \int_{E}^{} e^{-\left( N\sum_{i=1}^{N}\lambda_i^2 \right)/2}\prod_{i < j}\left( \lambda_i-\lambda_j \right)^2\:\prod_{i=1}^{N}d\lambda_i,
\end{align*}
where $E$ is the given set in the $Q_N$. Whenever $\frac{1}{N}\left( \delta_{\lambda_1(A)} + \cdots + \delta_{\lambda_N(A)} \right) \approx \nu$, we have that
\begin{align*}
  -\frac{N}{2}\sum_{i=1^{N}}\lambda_i^2 &= -\frac{N^2}{2}\left( \frac{1}{N}\sum_{i=1}^{N}\lambda_i^2 \right)
\end{align*}
is a Riemann sum for the integral of $t^2$ with respect to $\nu$. Furthermore, we have
\begin{align*}
  \prod_{i < j}\left( \lambda_i-\lambda_j \right)^2 &= e^{\sum_{i\neq j}\ln\left\vert \lambda_i-\lambda_j \right\vert}
\end{align*}
The sum inside the argument of the exponential is a Riemann sum for the quantity $N^2 \iint \ln\left\vert s-t \right\vert\: d\nu(s)d\nu(t)$. All in all, we get the following large deviation principle, which was proven rigorously in \cite{large_deviations_wigner_matrices} for the general case of Gaussian random matrices with Dyson index $\beta$.
\begin{theorem}
  Let
  \begin{align*}
    I\left( \nu \right) &= - \iint_{}^{} \ln\left\vert s-t \right\vert\:d\nu(s)\nu(t) + \frac{1}{2} \int_{}^{} t^2\:d\nu(t) - \frac{3}{4}.
  \end{align*}
  Then, the following hold.
  \begin{enumerate}[(i)]
    \item The function $I\colon \mathcal{M}\rightarrow [0,\infty]$ is a well-defined convex function with compact level sets on the space of probability measures on $\R$ that has minimum value $0$ at Wigner's semicircle law.
    \item The empirical spectral distribution satisfies a large deviation principle with respect to $Q_N$ with rate function $I$. That is,
      \begin{align*}
        \liminf_{N\rightarrow\infty} \frac{1}{N^2} \ln Q_N\left( \frac{1}{N}\left( \delta_{\lambda_1} + \cdots + \delta_{\lambda_N} \right)\in G \right) &\geq -\inf_{\nu\in G} I\left( \nu \right)
      \end{align*}
      for any open $G\subseteq \mathcal{M}$, and
      \begin{align*}
        \limsup_{N\rightarrow\infty} \frac{1}{N^2} \ln Q_N\left( \frac{1}{N}\left( \delta_{\lambda_1} + \cdots + \delta_{\lambda_N} \right)\in F \right) &\leq -\inf_{\nu\in F} I\left( \nu \right)
      \end{align*}
      for any closed $F\subseteq \mathcal{M}$.
  \end{enumerate}
\end{theorem}
\section{Microstates Free Entropy}%
Voiculescu's original definition of free entropy in the case with more than one free random variable can be viewed as a generalization of the microstates approach to entropy for a classical discrete random variable, discussed in the survey \cite{voiculescu2001freeentropy}. For the sections following, I will assume a certain level of proficiency with the theory of von Neumann algebras, as this is the setting where Voiculescu developed the theories around free entropy in \cite{voiculescu_analogues_2}.

Consider a discrete random variable with output values in $\set{1,\dots,n}$ assigned with probabilities $p_1,\dots,p_n$. The microstates of this discrete random variable for a fixed $N$ are then the set \[\set{f | f\colon \set{1,\dots,N}\rightarrow \set{1,\dots,n}},\] and for a fixed $\ve$, the microstates that approximate this distribution are those $f$ with
\begin{align*}
  \left\vert \frac{\left\vert f^{-1}\left( \set{j} \right) \right\vert}{N}-p_j \right\vert &< \ve,
\end{align*}
where $ \left\vert \cdot \right\vert $ denotes the (necessarily finite) cardinality. We denote the collection of such $f$ by $\Gamma\left( p_1,\dots,p_n;\ve,N \right)$. One may find the Shannon entropy by evaluating the limit
\begin{align*}
  \lim_{\ve\rightarrow 0}\lim_{N\rightarrow\infty} \frac{1}{N} \ln \left\vert \Gamma\left( p_1,\dots,p_n;\ve,N \right) \right\vert
\end{align*}
\subsection{Understanding the Microstates Free Entropy}%
Instead of a classical random variable, we will let $\left( M,\tau \right)$ be a tracial von Neumann algebra,\footnote{A von Neumann algebra equipped with a faithful (injective on positive elements), normal ($w^{\ast}$-continuous), tracial ($\tau\left( xy \right) = \tau\left( yx \right)$) state ($\tau(1) = 1$) $\tau\colon M\rightarrow \C$.} and let $\left( x_1,\dots,x_n \right)$ be a tuple of self-adjoint free random variables in $M$.

For a fixed $R$, we let the set of microstates $\Gamma_R\left( X_1,\dots,X_n;m,k,\ve \right)$ admit three degrees of approximation: $m$ denotes the level of approximation of mixed moments, $k$ denotes the size of the (self-adjoint) approximation matrices that have operator norm at most $R$, and $\ve$ denotes the closeness of the approximation. Put into symbols, we select all $n$-tuples $\left( A_1,\dots,A_n \right)\in \left( \M_k\left( \C \right)_{\sa} \right)^{n}$ with each $\norm{A_j} < R$ satisfying
\begin{align*}
  \left\vert \tau\left( x_{i_1}\cdots x_{i_p} \right)- \tr_{k}\left( A_{i_1}\cdots A_{i_p} \right) \right\vert < \ve
\end{align*}
for all $1\leq p \leq m$ and all multi-indices $ \mathbf{i}\colon \set{1,\dots,p}\rightarrow \set{1,\dots,n} $. Here, $\tr_k$ is the normalized trace.

As in the case of entropy for classical random variables, free entropy emerges from a certain large deviation principle. Specifically, recall that the distribution of a family of noncommutative random variables is given by the collection of mixed moments with respect to the trace,
\begin{align*}
  \Delta\left( x_1,\dots,x_n \right) &= \set{\tau\left( x_{i_1}\cdots x_{i_p} \right) | p\in \N, \mathbf{i}\colon \set{1,\dots,p}\rightarrow \set{1,\dots,n}}.
\end{align*}
If we have an $n$-tuple of independent GUE matrices $\left( A_1,\dots,A_n \right)\in \left( \M_k\left( \C \right)_{\sa} \right)^{n}$, we know that as $k\rightarrow\infty$, that there is almost sure convergence in distribution to a family $\left( s_1,\dots,s_n \right)$ of free semicirculars in $M$. The large deviations will then be given by
\begin{align*}
  P_N\left( \Delta\left( A_1,\dots,A_n \right)\approx \Delta\left( x_1,\dots,x_n \right) \right) &\sim e^{-k^2I\left( x_1,\dots,x_n \right)},
\end{align*}
where $I$ is the free entropy.

To compute this value, we let $\lambda$ denote the Lebesgue measure on $\left( \M_k\left( \C \right)_{\sa} \right)^n\cong \R^{nk^2}$, and define
\begin{align*}
  \chi\left( x_1,\dots,x_n \right) &= \sup_{R > 0}\inf_{m\in \N}\inf_{\ve > 0} \limsup_{k\rightarrow\infty} \left( \frac{1}{k^2} \ln \lambda \left( \Gamma_{R}\left( x_1,\dots,x_n; m,k,\ve \right) \right) + \frac{n}{2}\ln k \right).
\end{align*}
It turns out that the value of $R$ has minor influence, and we only need choose a fixed $R$ greater than the norm of each $x_i$.
\begin{proposition}[{\cite[Proposition 2.2]{voiculescu_analogues_2}}]
  Let $C^2 = \tau\left( x_1^2 + \cdots + x_n^2 \right)$. Then,
  \begin{align*}
    \chi\left( x_1,\dots,x_n \right) &\leq 2^{-1}n\ln\left( 2\pi en^{-1}C^2 \right).
  \end{align*}
\end{proposition}
\begin{proof}
  We will instead prove a slightly different inequality. Define
  \begin{align*}
    \chi_R\left( x_1,\dots,x_n;m,k,\ve \right) &= \ln \left( \lambda\left( \Gamma_{R}\left( x_1,\dots,x_n;m,k,\ve \right) \right) \right),
  \end{align*}
  and we will show that
  \begin{align*}
    \chi_R\left( x_1,\dots,x_n;m,k,\ve \right) &\leq 2^{-1}nk^2\left( \ln\left( 2\pi e n^{-1}\left( C^2 + n\ve \right) \right) - \ln k \right),
  \end{align*}
  assuming that $m\geq 2$. Applying the operations $\limsup_{k\rightarrow\infty}$, $\inf_{\ve > 0}$, $\inf_{m\in \N}$, and $\sup_{R > 0}$ in succession will give us the free entropy.

  For this, we use the $p$-dimensional Shannon entropy inequality
  \begin{align*}
    - \int_{}^{} f\ln(f)\:d\lambda_p &\leq 2^{-1}p\ln\left( 2\pi e p^{-1} a^2 \right),
  \end{align*}
  where $f$ is some probability density function on $\R^p$ and
  \begin{align*}
    a^2 &= \int_{}^{} \left( x_1^2 + \cdots + x_p^2 \right)f\:d\lambda_p,
  \end{align*}
  where $\lambda_p$ denotes the Lebesgue measure on $\R^p$.

  We apply this to the special case of the Lebesgue measure on $\left( \M_k\left( \C \right)_{\sa} \right)^{n}$, which is induced by the Hilbert--Schmidt metric
  \begin{align*}
    \iprod{\left( A_1,\dots,A_n \right)}{\left( B_1,\dots,B_n \right)} &= \sum_{i=1}^{n} \tr\left( A_iB_i \right),
  \end{align*}
  and take the indicator function
  \begin{align*}
    f\left( A_1,\dots,A_n \right) &= \begin{cases}
      0 & \left( A_1,\dots,A_n \right)\notin \Gamma_R\left( x_1,\dots,x_n;m,k,\ve \right)\\
      \left( \lambda\left( \Gamma_R\left( x_1,\dots,x_n;m,k,\ve \right) \right) \right)^{-1} & \left( A_1,\dots,A_n \right)\in \Gamma_R\left( x_1,\dots,x_n;m,k,\ve \right),
    \end{cases}
  \end{align*}
  giving
  \begin{align*}
    \chi_R\left( x_1,\dots,x_n;m,k,\ve \right) &\leq 2^{-1}nk^2\left( \ln\left( 2\pi e n^{-1}k^{-2}a^2 \right)-\ln k \right).
  \end{align*}
  Finally, by the definition of the microstate space, we have
  \begin{align*}
    \left\vert \frac{1}{k} \int_{}^{} \Tr\left( \sum_{j=1}^{n}A_j^2 \right)f\:d\lambda - \tau\left( \sum_{j=1}^{n}x_j^2 \right)\right\vert &< n\ve,
  \end{align*}
  meaning that $a^2 \leq C^2 + n\ve$.
\end{proof}
\section{Applications: Structural Properties of Free Group Factors}%

\printbibliography
\end{document}
