\documentclass[10pt]{mypackage}

%\usepackage{mlmodern}
%\usepackage{newpxtext,eulerpx,eucal}
%\renewcommand*{\mathbb}[1]{\varmathbb{#1}}

%\usepackage{homework}
\usepackage{notes}

\usepackage[ backend=bibtex, style = alphabetic, sorting=ynt ]{biblatex}
\addbibresource{project_references.bib}

\usepackage{parskip}

\fancyhf{}
\fancyhead[R]{Avinash Iyer}
\fancyhead[L]{Free Entropy}
\fancyfoot[C]{\thepage}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight

\section{Background: Asymptotic Freeness and Large Deviations}%
We start by recalling the basic asymptotic freeness result discussed in class.
\begin{proposition}
  Let $\left( A_1^{N},\dots,A_r^{N} \right)$ be an independent $r$-tuple of GUE $N\times N$ matrices. Then, the family $A_1^N,\dots,A_r^N$ converge in distribution to $r$ independent semicircular elements, $s_1,\dots,s_r\in B\left( \mathcal{F}\left( \C^r \right) \right)$, in the sense that for all $m\geq 1$ and all $1\leq i_1,\dots,i_m \leq r$, we have
  \begin{align*}
    \lim_{N\rightarrow\infty} E\left[ \tr\left( A_{i_1}^N\cdots A_{i_m}^N \right) \right] &= \varphi\left( s_{i_1}\cdots s_{i_m} \right),
  \end{align*}
  where $\varphi$ is the vacuum state, $\varphi\left( T \right) = \iprod{T\Omega}{\Omega}$.
\end{proposition}
In fact, this collection is \textit{almost surely} asymptotically free, in the following sense. Suppose we have two random matrices $A^N$ and $B^N$ defined on probability spaces $\left( X_N,\mu_N \right)$. Define
\begin{align*}
  X &\coloneq \prod_{N\in \N} X_N\\\
  \mu &\coloneq \prod_{N\in \N}\mu_N,
\end{align*}
where the latter is the product measure on $X$. The matrices $A^N$ and $B^N$ are said to be almost surely asymptotically free if there exists a noncommutative probability space $\left( A,\varphi \right)$ and $a,b\in A$, and for almost all $x = \left( x_N \right)_N\in X$, we have $A^N\left(x_N\right),B^N\left(x_N\right)\in \left( \M_N,\tr \right)$ converge in distribution to $a,b$.

Now, from here, we may ask a seemingly simple question: as $N$ grows large, how likely are we to encounter other distributions? To make this sense more precise, we consider a random $N\times N$ self-adjoint matrix $A$, and let
\begin{align*}
  \mu_A &= \frac{1}{N}\sum_{i=1}^{N}\delta_{\lambda_1}
\end{align*}
be its empirical spectral distribution. This is a random probability measure on $\R$, and as $N\rightarrow \infty$, the semicircle law gives that $\mu_A$ converges weakly to the semicircle distribution; this can be strengthened to almost sure convergence by an application of the argument for asymptotic freeness. The question then becomes, how quickly does the deviation between $\mu_A$ and any other probability distribution $\nu$ decrease as $N$ increases? This is where the theory of large deviations starts to take shape.

Much of this exposition related to the classical notions of entropy will be centered around results discussed in \cite[Ch.\,7]{mingo_and_speicher}.
\subsection{Large Deviations for Random Variables}%
We start with one of the classical examples of convergence of random variables to introduce large deviations. Consider a sequence of independent and identically distributed real-valued random variables $\left( X_i \right)_i$ with common distribution $\mu$. Set
\begin{align*}
  S_n &= \frac{1}{n}\sum_{i=1}^{n}X_i.\\
  m &= E\left[ X_1 \right]\\
  v &= E\left[ X_1^2 \right] - m^2.
\end{align*}
Then, we have that if $E\left[ X_1^2 \right] < \infty$, the central limit theorem says that $S_n \approx m + \frac{\sigma}{\sqrt{n}}N\left( 0,1 \right)$.

If $\mu$ is the standard Gaussian distribution, then this gives that $S_n$ is distributed as $N\left( 0,1/n \right)$; we then get that
\begin{align*}
  P\left( S_n\in [x,x+dx] \right) &\approx \sqrt{\frac{n}{2\pi}}e^{-nx^2}dx.
\end{align*}
Asymptotically, this gives that the probability that $S_n$ is near the value $x\in \R$ decays exponentially in $n$ determined by a rate function $I(x) = x^2/2$.

We will now generalize this result. In particular, if we let $\mu$ be any distribution discussed above (rather than simply the normal distribution), then we will find a rate function $I(x)$ such that 
\begin{align*}
  e^{-nI(x)} &\sim P\left( S_n > x \right)
    \intertext{whenever $x > m$, and whenever $x < m$}
  e^{-nI(x)} &\sim P\left( S_n < x \right).
\end{align*}
For a given distribution $\mu$, we can compute the rate function by using a family of basic manipulations. If $x > m$, then for all $\lambda\geq 0$, we may use Markov's inequality to obtain
\begin{align*}
  P\left( S_n > x \right) &= P\left( nS_n > nx \right)\\
                          &= P\left( e^{\lambda\left( nS_n-nx \right)}\geq 1 \right)\\
                          &leq E\left[ e^{\lambda\left( nS_n-nx \right)} \right]\\
                          &= e^{-\lambda n x} E\left[ e^{\lambda\left( X_1 + \cdots + X_n \right)} \right]\\
                          &= \left( e^{-\lambda x}E\left[ e^{\lambda X} \right] \right)^{n},
\end{align*}
where $X$ is identically distributed to each of the $X_i$, and we use the fact that the $X_i$ are independent. We may then define
\begin{align*}
  \Lambda\left( \lambda \right) &= \ln E\left[ e^{\lambda X} \right]\label{eq:cumulant_generating_function}\tag{$\ast$}
\end{align*}
to be an extended real-valued function, but we only consider $\mu$ for which $\Lambda\left( \lambda \right)$ is finite for all $\lambda$ in an open neighborhood of $0$. The equation \eqref{eq:cumulant_generating_function} is known as the cumulant generating function for $\mu$.

This gives the inequality
\begin{align*}
  P\left( S_n > x \right) &\leq e^{-n\left( \lambda x - \Lambda\left( \lambda \right) \right)}.
\end{align*}
Since $\ln$ is a concave function, Jensen's inequality gives
\begin{align*}
  \Lambda\left( \lambda \right) &\geq E\left[ \ln\left( e^{\lambda X} \right) \right]\\
                                &= E\left[ \lambda X \right]\\
                                &= \lambda m.
\end{align*}
In particular, for any $\lambda < 0$ and $x > m$, we have $-n\left( \lambda x - \Lambda\left( \lambda \right) \right) \geq 0$, meaning this equation is valid for all $\lambda$. In particular, we have
\begin{align*}
  P\left( S_n > x \right) &\leq \inf_{\lambda\in \R} e^{-n\lambda x - \Lambda\left( \lambda \right)}.
\end{align*}
Now, we observe that $\Lambda$ is convex. This follows from Hölder's inequality
\begin{align*}
  E\left[ e^{\left( 1-t \right)\lambda_1 x + t\lambda_2 x} \right] &\leq E\left[ e^{\lambda_1 x} \right]^{1-t}E\left[ e^{\lambda_2 x} \right]^{t}
\end{align*}
so that
\begin{align*}
  \Lambda\left( \left( 1-t \right)\lambda_1 + t\lambda_2 \right) &\leq \left( 1-t \right)\Lambda\left( \lambda_1 \right) + t\Lambda\left( \lambda_2 \right).
\end{align*}
Defining the \textit{Legendre transform} of $\Lambda$ by
\begin{align*}
  \Lambda^{\ast}\left( x \right) &= \sup_{\lambda\in \R} \left( \lambda x - \Lambda\left( \lambda \right) \right),
\end{align*}
we find that this is a convex function of $x$, as it is a supremum of a family of convex functions of $x$.

Now, since $\Lambda(0) = 0$, it follows that $\Lambda^{\ast}(x)\geq 0$, and has $\Lambda^{\ast}\left( m \right) = 0$. In particular, this gives
\begin{align*}
  P\left( S_n > x \right) &\leq e^{-n\Lambda^{\ast}\left( x \right)}
\end{align*}
whenever $x > m$.

It can also be shown that $e^{-n\Lambda^{\ast}(x)}$ is an asymptotic lower bound, in that
\begin{align*}
  \liminf_{n\rightarrow\infty} \frac{1}{n}\ln P\left( x-\delta < S_n < x + \delta \right) \geq -\Lambda^{\ast}(x)
\end{align*}
for all $x\in \R$ and all $\delta > 0$. The method for doing so is outlined in \cite[Ch. 7, Section 2]{mingo_and_speicher}, and results in Cramér's theorem for real-valued random vectors.
\begin{theorem}[Cramér's Theorem]
  Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed random vectors in $\R^d$ with common distribution $\mu$. Define
  \begin{align*}
    \Lambda(\lambda) &= \ln E\left[ e^{ \iprod{\lambda}{X_i} } \right]\\
    \Lambda^{\ast}\left( x \right) &= \sup_{\Lambda\in \R^d} \left( \iprod{\lambda}{x}-\Lambda\left( \lambda \right) \right),
  \end{align*}
  and assume that $\Lambda\left( \lambda \right) < \infty$ for all $\lambda$. Set $S_n = \frac{1}{n}\left( X_1 + \cdots + X_n \right)$. Then, the distribution $\mu_{S_n}$ satisfies has
  \begin{itemize}
    \item $x\mapsto \Lambda^{\ast}(x)$ is convex;
    \item $\set{x\in \R^d | \Lambda^{\ast}\left( x \right) \leq \alpha}$ is compact for all $\alpha\in \R$;
    \item for any closed $F\subseteq \R^d$,
      \begin{align*}
        \limsup_{n\rightarrow\infty} \frac{1}{n}\ln P\left( S_n\in F \right) &\leq -\inf_{x\in F}\Lambda^{\ast}\left( x \right),
      \end{align*}
    \item and for any open $G\subseteq \R^d$,
      \begin{align*}
        \liminf_{n\rightarrow\infty} \frac{1}{n}\ln P\left( S_n\in G \right) &\geq -\inf_{x\in G}\Lambda^{\ast}\left( x \right).
      \end{align*}
  \end{itemize}
\end{theorem}
This theorem defines precisely the large deviation principle that the partial sums satisfy --- namely, it is the Legendre transform of the cumulant-generating function.
\subsection{Large Deviations for the Empirical Distribution}%
Now, our next step is to develop an analogous large deviation principle for the empirical distribution of the random variables $X_1,X_2,\dots$. This will give us the idea of classical entropy.

We start by considering the case of (independent and identically distributed) random variables $X_i\colon \Omega\rightarrow A$ taking values in a finite set $\set{a_1,\dots,a_d}$, and define $p_k\coloneq P\left( X_i = a_k \right)$. We expect that, as $n\rightarrow\infty$, the empirical distribution of the $X_i$ should converge to the probability measure $\left( p_1,\dots,p_d \right)$ on $A$.

Toward this end, let $Y_i\colon \Omega\rightarrow \R^d$ be defined by
\begin{align*}
  Y_i &\coloneq \left( \chi_{\set{a_1}}\left( X_i \right),\dots,\chi_{\set{a_d}}\left( X_i \right) \right).
\end{align*}
We observe that $p_k$ is the probability that $Y_i$ will have $1$ in position $k$ and $0$ elsewhere, and that $\frac{1}{n}\left( Y_1 + \cdots + Y_n \right)$ gives the relative frequency of $a_1,\dots,a_d$ --- i.e., this has the same information as the empirical distribution of $X_1,\dots,X_n$.

Any probability measure on $A$ is a $d$-tuple of positive real numbers satisfying $q_1 + \cdots + q_d = 1$. By Cramér's theorem and our discussion above, we have
\begin{align*}
  P\left( \frac{1}{n}\left( \delta_{X_1} + \cdots + \delta_{X_n} \right) \approx \left( q_1,\dots,q_d \right) \right) &= P\left( \frac{1}{n}\left( Y_1 + \cdots + Y_n \right) \approx \left( q_1,\dots,q_d \right) \right)\\
                                                                                                                              &\sim e^{-n\Lambda^{\ast}\left( q_1,\dots,q_d \right)}.
\end{align*}
Applying our definitions for $\Lambda$ and $\Lambda^{\ast}$, we have
\begin{align*}
  \Lambda\left( \lambda_1,\dots,\lambda_d \right) &= \ln\left( p_1e^{\lambda_1} + \cdots + p_de^{\lambda_d} \right)\\
  \Lambda^{\ast}\left( q_1,\dots,q_d \right) &= \sup_{\left( \lambda_1,\dots,\lambda_d \right)} \left( \lambda_1q_1 + \cdots + \lambda_dq_d - \ln\left( p_1e^{\lambda_1} + \cdots + p_de^{\lambda_d} \right) \right).
\end{align*}
To compute the supremum over all tuples, we find that the partial derivatives with respect to each $\lambda_i$ are given by
\begin{align*}
  q_i - \frac{1}{p_1e^{\lambda_1} + \cdots + p_de^{\lambda_d}}p_ie^{\lambda_i},
\end{align*}
so by concavity, we get that the maximum value occurs when
\begin{align*}
  \lambda_i &= \ln \left( \frac{q_i}{p_i} \right) + \Lambda\left( \lambda_1 ,\dots,\lambda_d \right).
\end{align*}
We thus get
\begin{align*}
  \Lambda^{\ast}\left( q_1,\dots,q_d \right) &= \sum_{i=1}^{d}q_i \ln\left( \frac{q_i}{p_i} \right).
\end{align*}
The quantity on the right is the relative Shannon entropy $H\left( \left( q_1,\dots,q_d \right) | \left( p_1,\dots,p_d \right) \right)$, which is strictly positive except when $q_1 = p_1,\dots,q_d = p_d$.

In particular, we have that $\left( p_1,\dots,p_d \right)$ admits a large deviation principle with rate function given by the relative Shannon entropy. That this holds for any distribution is known as Sanov's theorem.
\begin{theorem}[Sanov's Theorem]
  Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed real random variables with common distribution $\mu$, and let
  \begin{align*}
    \nu_n &\coloneq \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i}
  \end{align*}
  be the empirical distribution. Then, the family $\set{\nu_n}_{n\geq 1}$ satisfies a large deviation principle given by the rate function
  \begin{align*}
    I\left( \nu \right) &= \begin{cases}
      \int_{}^{} p\ln(p)\:d\mu & d\nu = p\:d\mu\\
      +\infty & \text{else}.
    \end{cases}
  \end{align*}
\end{theorem}
\section{One-Dimensional Free Entropy: A Heuristic Approach}%
The next logical step after defining a large deviation principle for a sequence of random variables is to define such a quantity for single free random variables.

In \cite{voiculescu_analogues_1}, Voiculescu used a heuristic method based on random matrix theory to establish the value $\Sigma(x)$ for some random variable $x$ in a $C^{\ast}$-probability space $\left( \mathcal{A},\varphi \right)$ distributed according to a compactly supported measure $\nu$ on $\R$ (i.e., its spectral measure with respect to the state $\varphi$).

It can be established (as in \cite[Exercise 1.8]{mingo_and_speicher}) that if $X$ is a $N\times N$ GUE matrix, then the density inside the real vector space $\M_N\left( \C \right)_{\sa}$ is given by
\begin{align*}
  dP_N(X) &= K e^{-N^2/2\tr\left( X^2 \right)}\:dm,
\end{align*}
where $m$ is the Lebesgue measure on $\R^{N^2}$ and $\tr$ denotes the normalized trace. It can be shown, as in \cite[Theorem 2.5.2]{anderson_guionnet_zeitouni}, that the joint distribution of the eigenvalues $\lambda_1(X) \leq\cdots\leq \lambda_N(X)$ is absolutely continuous with respect to the Lebesgue measure and has density given by
\begin{align*}
  dQ_N\left(\lambda_1,\dots,\lambda_N\right) &= \frac{N^{N^2/2}}{\left( 2\pi \right)^{N/2}\prod_{j=1}^{N}j!} e^{-\left( N\sum_{i=1}^{N}\lambda_i^2 \right)/2}\prod_{i < j}\left( \lambda_i - \lambda_j \right)^2 \prod_{i=1}^{N}d\lambda_i
\end{align*}
Heuristically, letting $\mu_A$ denote the empirical spectral distribution on $\M_N\left( \C \right)_{\sa}$ given by
\begin{align*}
  \mu_A &= \frac{1}{N}\left( \delta_{\lambda_1(A)} + \cdots + \delta_{\lambda_N(A)} \right),
\end{align*}
we may consider a large deviation principle with $P_N\left( \mu_A\approx \nu \right)\sim e^{-N^2I\left(\nu\right)}$, and write it out as
\begin{align*}
  P_N\left( \mu_A\approx \nu \right) &= Q_N\left( \frac{1}{N}\left( \delta_{\lambda_1} + \cdots + \delta_{\lambda_N} \right) \right)\\
                                     &= \frac{N^{N^2/2}}{\left( 2\pi \right)^{N/2}\prod_{j=1}^{N}j!} \int_{E}^{} e&{-\left( N\sum_{i=1}^{N}\lambda_i^2 \right)/2}\prod_{i < j}\left( \lambda_i-\lambda_j \right)^2\:\prod_{i=1}^{N}d\lambda_i,
\end{align*}
where $E$ is the given set in the $Q_N$. Whenever $\frac{1}{N}\left( \delta_{\lambda_1(A)} + \cdots + \delta_{\lambda_N(A)} \right) \approx \nu$, we have that
\begin{align*}
  -\frac{N}{2}\sum_{i=1^{N}}\lambda_i^2 &= -\frac{N^2}{2}\left( \frac{1}{N}\sum_{i=1}^{N}\lambda_i^2 \right)
\end{align*}
is a Riemann sum for the integral of $t^2$ with respect to $\nu$. Furthermore, we have
\begin{align*}
  \prod_{i < j}\left( \lambda_i-\lambda_j \right)^2 &= e^{\sum_{i\neq j}\ln\left\vert \lambda_i-\lambda_j \right\vert}
\end{align*}
The sum inside the argument of the exponential is a Riemann sum for the quantity $N^2 \iint \ln\left\vert s-t \right\vert\: d\nu(s)d\nu(t)$. All in all, we get the following large deviation principle, which was proven rigorously in \cite{large_deviations_wigner_matrices} for the general case of Gaussian random matrices with Dyson index $\beta$.
\begin{theorem}
  Let
  \begin{align*}
    I\left( \nu \right) &= - \iint_{}^{} \ln\left\vert s-t \right\vert\:d\nu(s)\nu(t) + \frac{1}{2} \int_{}^{} t^2\:d\nu(t) - \frac{3}{4}.
  \end{align*}
  Then, the following hold.
  \begin{enumerate}[(i)]
    \item The function $I\colon \mathcal{M}\rightarrow [0,\infty]$ is a well-defined convex function with compact level sets on the space of probability measures on $\R$ that has minimum value $0$ at Wigner's semicircle law.
    \item The empirical spectral distribution satisfies a large deviation principle with respect to $Q_N$ with rate function $I$. That is,
      \begin{align*}
        \liminf_{N\rightarrow\infty} \frac{1}{N^2} \ln Q_N\left( \frac{1}{N}\left( \delta_{\lambda_1} + \cdots + \delta_{\lambda_N} \right)\in G \right) &\geq -\inf_{\nu\in G} I\left( \nu \right)
      \end{align*}
      for any open $G\subseteq \mathcal{M}$, and
      \begin{align*}
        \limsup_{N\rightarrow\infty} \frac{1}{N^2} \ln Q_N\left( \frac{1}{N}\left( \delta_{\lambda_1} + \cdots + \delta_{\lambda_N} \right)\in F \right) &\leq -\inf_{\nu\in F} I\left( \nu \right)
      \end{align*}
      for any closed $F\subseteq \mathcal{M}$.
  \end{enumerate}
\end{theorem}
\section{Microstates Free Entropy}%

\section{Applications: Structural Properties of Free Group Factors}%

\printbibliography
\end{document}
