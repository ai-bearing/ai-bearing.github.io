\documentclass[10pt]{mypackage}

%\usepackage{mlmodern}
%\usepackage{newpxtext,eulerpx,eucal}
%\renewcommand*{\mathbb}[1]{\varmathbb{#1}}

%\usepackage{homework}
%\usepackage{notes}

\usepackage[ backend=bibtex, style = alphabetic, sorting=ynt ]{biblatex}
\addbibresource{project_references.bib}

\usepackage{parskip}

\fancyhf{}
\fancyhead[R]{Avinash Iyer}
\fancyhead[L]{Free Entropy Dimension}
\fancyfoot[C]{\thepage}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\section{Background: Asymptotic Freeness and Large Deviations}%
We start by recalling the basic asymptotic freeness result discussed in class.
\begin{proposition}
  Let $\left( A_1^{N},\dots,A_r^{N} \right)$ be an independent $r$-tuple of GUE $N\times N$ matrices. Then, the family $A_1^N,\dots,A_r^N$ converge in distribution to $r$ independent semicircular elements, $s_1,\dots,s_r\in B\left( \mathcal{F}\left( \C^r \right) \right)$, in the sense that for all $m\geq 1$ and all $1\leq i_1,\dots,i_m \leq r$, we have
  \begin{align*}
    \lim_{N\rightarrow\infty} E\left[ \tr\left( A_{i_1}^N\cdots A_{i_m}^N \right) \right] &= \varphi\left( s_{i_1}\cdots s_{i_m} \right),
  \end{align*}
  where $\varphi$ is the vacuum state, $\varphi\left( T \right) = \iprod{T\Omega}{\Omega}$.
\end{proposition}
In fact, this collection is \textit{almost surely} asymptotically free, in the following sense. Suppose we have two random matrices $A^N$ and $B^N$ defined on probability spaces $\left( X_N,\mu_N \right)$. Define
\begin{align*}
  X &\coloneq \prod_{N\in \N} X_N\\\
  \mu &\coloneq \prod_{N\in \N}\mu_N,
\end{align*}
where the latter is the product measure on $X$. The matrices $A^N$ and $B^N$ are said to be almost surely asymptotically free if there exists a noncommutative probability space $\left( A,\varphi \right)$ and $a,b\in A$, and for almost all $x = \left( x_N \right)_N\in X$, we have $A^N\left(x_N\right),B^N\left(x_N\right)\in \left( \M_N,\tr \right)$ converge in distribution to $a,b$.

Now, from here, we may ask a seemingly simple question: as $N$ grows large, how likely are we to encounter other distributions? To make this sense more precise, we consider a random $N\times N$ self-adjoint matrix $A$, and let
\begin{align*}
  \mu_A &= \frac{1}{N}\sum_{i=1}^{N}\delta_{\lambda_1}
\end{align*}
be its empirical spectral distribution. This is a random probability measure on $\R$, and as $N\rightarrow \infty$, the semicircle law gives that $\mu_A$ converges weakly to the semicircle distribution; this can be strengthened to almost sure convergence by an application of the argument for asymptotic freeness. The question then becomes, how quickly does the deviation between $\mu_A$ and any other probability distribution $\nu$ decrease as $N$ increases? This is where the theory of large deviations starts to take shape.

Much of this exposition related to the classical notions of entropy will be centered around results discussed in \cite[Ch.\,7]{mingo_and_speicher}.
\subsection{Large Deviations}%
We start with one of the classical examples of convergence of random variables to introduce large deviations. Consider a sequence of independent and identically distributed real-valued random variables $\left( X_i \right)_i$ with common distribution $\mu$. Set
\begin{align*}
  S_n &= \frac{1}{n}\sum_{i=1}^{n}X_i.\\
  m &= E\left[ X_1 \right]\\
  v &= E\left[ X_1^2 \right] - m^2.
\end{align*}
Then, we have that if $E\left[ X_1^2 \right] < \infty$, the central limit theorem says that $S_n \approx m + \frac{\sigma}{\sqrt{n}}N\left( 0,1 \right)$.

If $\mu$ is the standard Gaussian distribution, then this gives that $S_n$ is distributed as $N\left( 0,1/n \right)$; we then get that
\begin{align*}
  P\left( S_n\in [x,x+dx] \right) &\approx \sqrt{\frac{n}{2\pi}}e^{-nx^2}dx.
\end{align*}
Asymptotically, this gives that the probability that $S_n$ is near the value $x\in \R$ decays exponentially in $n$ determined by a rate function $I(x) = x^2/2$.

We will now generalize this result. In particular, if we let $\mu$ be any distribution discussed above (rather than simply the normal distribution), then we will find a rate function $I(x)$ such that 
\begin{align*}
  e^{-nI(x)} &\sim P\left( S_n > x \right)
    \intertext{whenever $x > m$, and whenever $x < m$}
  e^{-nI(x)} &\sim P\left( S_n < x \right).
\end{align*}
For a given distribution $\mu$, we can compute the rate function by using a family of basic manipulations. If $x > m$, then for all $\lambda\geq 0$, we may use Markov's inequality to obtain
\begin{align*}
  P\left( S_n > x \right) &= P\left( nS_n > nx \right)\\
                          &= P\left( e^{\lambda\left( nS_n-nx \right)}\geq 1 \right)\\
                          &leq E\left[ e^{\lambda\left( nS_n-nx \right)} \right]\\
                          &= e^{-\lambda n x} E\left[ e^{\lambda\left( X_1 + \cdots + X_n \right)} \right]\\
                          &= \left( e^{-\lambda x}E\left[ e^{\lambda X} \right] \right)^{n},
\end{align*}
where $X$ is identically distributed to each of the $X_i$, and we use the fact that the $X_i$ are independent.
\section{One-Dimensional Free Entropy}%

\section{Microstates Free Entropy}%

\section{Applications: Structural Properties of Free Group Factors}%

\printbibliography
\end{document}
