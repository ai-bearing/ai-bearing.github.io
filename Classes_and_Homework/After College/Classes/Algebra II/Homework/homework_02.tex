\documentclass[10pt]{mypackage}

\usepackage{mlmodern}
\renewcommand{\mathbb}[1]{\mathds{#1}}
%\usepackage{newpxtext,eulerpx,eucal}
%\renewcommand*{\mathbb}[1]{\varmathbb{#1}}

\usepackage{homework}
%\usepackage{notes}

%\usepackage[ backend=bibtex, style = alphabetic, sorting=ynt ]{biblatex}
%\addbibresource{  }

\usepackage{parskip}

\fancyhf{}
\fancyhead[R]{Avinash Iyer}
\fancyhead[L]{Algebra II: Homework 2}
\fancyfoot[C]{\thepage}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\begin{problem}[Problem 1]
  Let $R$ be a Euclidean domain, $n\geq 2$ an integer.
  \begin{enumerate}[(a)]
    \item Use the proof of the Smith Normal Form to show that every matrix $A\in \GL_n\left( R \right)$ can be written as a product of elementary matrices $E_{ij}\left( \lambda \right)$, flip matrices $F_{ij}$, and a diagonal matrix $D$.
    \item Now show that the flip matrices can be eliminated from the product in (a), and one can assume that $D=\operatorname{diag}\left( d,1,\dots,1 \right)$. That is, all diagonal entries of $D$ except possibly the $(1,1)$ entry are equal to $1$.
    \item Deduce from (b) that $\SL_n\left( R \right)$ is generated by the elementary matrices $E_{ij}\left( \lambda \right)$.
  \end{enumerate}
\end{problem}
\begin{solution}\hfill
  \begin{enumerate}[(a)]
    \item Observe that a square matrix is in Smith normal form if and only if it is a diagonal matrix of the form $D = \operatorname{diag}\left( d_1,\dots,d_m,0,\dots,0 \right)$ where $d_1 | d_2 | \cdots | d_m$. By the proof of the Smith normal form, we have that the matrix $UAV$ in Smith normal form is the product of three invertible matrices, so it is invertible, meaning that it is necessarily diagonal with $d_1,\dots,d_n\in R^{\times}$. Since the inverse of any $E_{ij}(\lambda)$ is another matrix of the form $E_{ij}(\lambda)$, and the inverse of $F_{ij}$ is itself, it follows that we may write any $A\in \GL_n\left( R \right)$ as
      \begin{align*}
        A &= U^{-1}DV^{-1},
      \end{align*}
      where $U^{-1}$ and $V^{-1}$ are collections of flips and $E_{ij}(\lambda)$ and $D$ is a diagonal matrix with $d_1,\dots,d_n\in R^{\times}$.
    \item We start by computing $F_{ij}$ in terms of elementary matrices acting on identity. Operating on a $2\times 2$ matrix, we have
      \begin{align*}
        \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix} &\xrightarrow{R_1 \mapsto R_1 + R_2} \begin{pmatrix}1 & 1 \\ 0 & 1\end{pmatrix}\\
                         &\xrightarrow{R_2 \mapsto R_2 - R_1} \begin{pmatrix}1 & 1 \\ -1 & 0\end{pmatrix}\\
                         &\xrightarrow{R_1 \mapsto R_1 + R_2} \begin{pmatrix}0 & 1 \\ -1 & 0\end{pmatrix}\\
                         &\xrightarrow{R_2 \mapsto -R_2} \begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}.
      \end{align*}
      Letting $D_j(\lambda)$ denote the multiplication of the diagonal matrix in row $j$ by $\lambda$, and replacing $1$ and $2$ with $i$ and $j$, we find that this product is equal to $D_j(-1)E_{ij}(1)E_{ji}(-1)E_{ij}(1)$.

      Now, if we let $D = \operatorname{diag}\left( d_1,\dots,d_n \right)$, we observe that
      \begin{align*}
        DE_{ij}(\lambda) &= \begin{cases}
          R_i &\mapsto d_i\left( R_i + \lambda R_j \right)\\
          R_j &\mapsto d_jR_j
        \end{cases}\\
                         &= E_{ij}\left( d_id_j^{-1}\lambda \right) D.
      \end{align*}
      Therefore, by using these procedures, we obtain a matrix of the form
      \begin{align*}
        A &= \prod E_{ij} (\lambda) \operatorname{diag}\left( d_1,\dots,d_n \right).
      \end{align*}
      Next, we will show that this diagonal matrix can be taken to be of the form $\operatorname{diag}\left( d_1d_2\cdots d_n,1,\dots,1 \right)$. We show the $2\times 2$ case.
      \begin{align*}
        \begin{pmatrix}d_1 & 0 \\ 0 & d_2\end{pmatrix} &\xrightarrow{R_1 \mapsto R_1 + d_2^{-1}R_2} \begin{pmatrix}d_1 & 1 \\ 0 & d_2\end{pmatrix}\\
                           &\xrightarrow{R_2 \mapsto R_2 + \left( 1-d_2 \right)R_1} \begin{pmatrix}d_1 & 1 \\ d_1 - d_1d_2 & 1\end{pmatrix}\\
                           &\xrightarrow{C_1 \mapsto \left( d_1d_2-d_1 \right)C_2 + C_1} \begin{pmatrix}d_1d_2 & 1 \\ 0 & 1\end{pmatrix}\\
                           &\xrightarrow{R_1 \mapsto R_1 - R_2} \begin{pmatrix}d_1d_2 & 0 \\ 0 & 1\end{pmatrix}.
      \end{align*}
      Thus, this diagonal matrix is equal to a diagonal matrix of the desired form multiplied by a family of $E_{ij}(\lambda)$. Inductively, we apply this to the entirety of $D$ to obtain our desired result.
    \item Observe that the determinant of $A$ is equal to $d_1\cdots d_n$, since the $E_{ij}(\lambda)$ are all upper-triangular or lower-triangular. In particular, this means that if $A\in \SL_n\left( R \right)$, then $\det(A) = 1$, so that $d_1\cdots d_n = 1$, and the diagonal matrix in part (b) is in fact the identity matrix, so $A$ is the product of $E_{ij}(\lambda)$. Similarly, a matrix that is the product of $E_{ij}(\lambda)$ is in $\SL_n(R)$, so $\SL_n(R)$ is generated by matrices of the form $E_{ij}(\lambda)$.
  \end{enumerate}
\end{solution}
\begin{problem}[Problem 2]
  Let $R$ be a Euclidean domain, let $k,n\in \N$, and let $i\leq \min\left( k,n \right)$. Given a matrix $A\in \Mat_{k,n}(R)$, define $d_i(A)$ to be the greatest common divisor of all $i\times i$ minors of $A$. Prove that $d_i(PAQ) = d_i(A)$ for all $P\in \GL_k(R)$ and $Q\in \GL_n(R)$.
\end{problem}
\begin{solution}
  Since $P$ and $Q$ are invertible $k\times k$ and $n\times n$ matrices respectively, it follows from Problem 1 that we may write $P$ and $Q$ as
  \begin{align*}
    P &= \left( \prod_{\alpha=1}^{\alpha_p} E_{i_{\alpha}j_{\alpha}}\left( \lambda_{\alpha} \right) \right)\left( \operatorname{diag}\left( d_p,1,\dots,1 \right) \right)\\
    Q &= \left( \operatorname{diag}\left( d_q,1,\dots,1 \right) \right) \left( \prod_{\beta=1}^{\beta_q} E_{i_{\beta}j_{\beta}}\left( \lambda_{\beta} \right) \right),
  \end{align*}
  where we used the fact that diagonal matrices commute with all other matrices if $R$ is commutative. Furthermore, since $P$ and $Q$ are commutative, $d_p$ and $d_q$ are units. We observe now that
  \begin{align*}
    PAQ &= \left( \prod_{\alpha=1}^{\alpha_p}E_{i_{\alpha}j_{\alpha}}\left( \lambda_{\alpha} \right) \right) \left( \operatorname{diag}\left( d_p,1,\dots,1 \right)A\operatorname{diag}\left( d_q,1,\dots,1 \right) \right)\left( \prod_{\beta=1}^{\beta_q} E_{i_{\beta}j_{\beta}}\left( \lambda_{\beta} \right) \right).
  \end{align*}
  Focusing on the product in the middle, we find that it multiplies the first column of $A$ by $d_q$ and the first row of $A$ by $d_p$; in particular, it does not affect any of the $i\times i$ minors of $A$ (up to associates). Additionally, since each of the $E_{ij}\left(\lambda\right)$ are simply linear combinations of the columns and rows of $A$ respectively, they do not affect the greatest common divisor of any of the $i\times i$ minors of $A$, meaning that $d_i\left( A \right) = d_i\left( PAQ \right)$.
\end{solution}
\begin{problem}[Problem 3]
  Let $R$ be a commutative ring with $1$.
  \begin{enumerate}[(a)]
    \item Let $C$ be an $R$-algebra, and $A,B\subseteq C$ $R$-subalgebras that commute with each other; that is, $ab = ba$ for any $a\in A$ and $b\in B$. Prove that there is an $R$-algebra homomorphism $\varphi\colon A\otimes B\rightarrow C$ such that $\varphi\left( a\otimes b \right) = ab$ for each $a\in A$ and $b\in B$.
    \item Prove that $\R\otimes_{\Z}\Z[i]\cong \C$ as rings.
    \item Now assume that $R$ is a field, and let $A$ be a finite-dimensional $R$-algebra. Prove that $A\otimes A$ cannot be a field unless $\dim(A) = 1$. 
  \end{enumerate}
\end{problem}
\begin{solution}\hfill
  \begin{enumerate}[(a)]
    \item Let $\phi\colon A\times B \rightarrow C$ be defined by $\left( a,b \right)\mapsto ab$. Then, $\phi$ is an $R$-bilinear map, so it induces a unique linear map on the tensor product $\varphi\colon A\otimes B \rightarrow C$. We claim that this map is compatible with the $R$-algebra structure of $A\otimes B$.

      To see this, observe that if $a_1,a_2\in A$ and $b_1,b_2\in B$, then
      \begin{align*}
        \varphi\left( \left( a_1\otimes b_1 \right)\left( a_2\otimes b_2 \right) \right) &= \varphi\left( a_1a_2\otimes b_1b_2 \right)\\
                                                                                         &= a_1a_2b_1b_2\\
                                                                                         &= a_1b_1a_2b_2\\
                                                                                         &= \varphi\left( a_1\otimes b_1 \right)\varphi\left( a_2\otimes b_2 \right).
      \end{align*}
      This gives our desired $R$-algebra homomorphism.
    \item We observe that both $\R$ and $\Z[i]$ are $\Z$-subalgebras of $\C$. Therefore, from above, we have a $\Z$-algebra homomorphism
      \begin{align*}
        \varphi\colon \R\otimes_{\Z}\Z[i] &\rightarrow \C\\
        t\otimes \left( a+bi \right) &\mapsto ta + tbi.
      \end{align*}
      To see that this map is injective, observe that $ta + tbi = 0$ if and only if $ta = 0$ and $tb = 0$, meaning either that $t = 0$ or $a,b = 0$; in either case, the corresponding element of the tensor product is the zero tensor. As for surjectivity, if we have $x + yi\in \C$, then we may find the element $x\otimes 1 + y\otimes i\in \R\otimes_{\Z}\Z[i]$ that maps to $x + yi$. Since this is a bijective $\Z$-algebra homomorphism, it follows that $\R\otimes_{\Z}\Z[i]\cong \C$ as $\Z$-algebras, hence as rings.
    \item Suppose $A$ is an $R$-algebra such that $A\otimes_{R}A$ is a field. Then, $A\otimes_{R}A$ is generated by $1\otimes 1$. Now, consider the subalgebra $N = \set{\lambda1 | \lambda\in R}$. Then, we see that $N\otimes_{R}A$ is also generated by $1\otimes 1$, so it has the same dimension as $A$, and $N$ commutes with $A$ since it consists of scalar multiples of $1$. This means that $N\otimes_{R}A$ admits a homomorphism of $R$-algebras
      \begin{align*}
        \varphi\colon N\otimes_{R}A &\rightarrow A\\
        \lambda1 \otimes a &\mapsto \lambda a.
      \end{align*}
      This homomorphism is surjective, though, meaning that $\Dim_{R}(A)\leq 1$, so $\Dim_{R}(A) = 1$.
  \end{enumerate}
\end{solution}
\begin{problem}[Problem 4]\hfill
  \begin{enumerate}[(a)]
    \item Prove that $A = \C\otimes_{\R}\C$ and $B = \C\times \C$ are isomorphic as $\C$-algebras.
    \item Explain why $\set{1\otimes 1,1\otimes i}$ is a basis for $A$ over $\C$. Compute $\varphi\left( 1\otimes 1 \right)$ and $\varphi\left( 1\otimes i \right)$, where $\varphi\colon A\rightarrow B$ is the isomorphism from (a).
    \item Prove that there exist precisely $2$ $\C$-algebra isomorphisms from $A$ to $B$.
  \end{enumerate}
\end{problem}
\begin{solution}\hfill
  \begin{enumerate}[(a)]
    \item We identify $\C \cong \R[x]/\left\langle x^2 + 1 \right\rangle$, where we use angle brackets instead of parentheses because we will be using a lot of them. To see this, we observe that by Euclidean division, we have, for any $p(x)\in \R[x]$,
      \begin{align*}
        p(x) &= q(x)\left( x^2 + 1 \right) + bx + a.
      \end{align*}
      In particular, we may then define the map $\varphi\colon \R[x]/\left\langle x^2 + 1 \right\rangle\rightarrow \C$ taking
      \begin{align*}
        bx + a + \left\langle x^2 + 1 \right\rangle &\mapsto a + bi.
      \end{align*}
      This is a linear map, and we observe then that
      \begin{align*}
        \left( bx + a + \left\langle x^2 + 1 \right\rangle\right)\left( dx + c + \left\langle x^2 + 1 \right\rangle\right) &= bdx^2 + \left( bc + ad \right)x + ac + \left\langle x^2 + 1 \right\rangle\\
                                                                                                                                                 &= \left( bc + ad \right)x + \left( ac-bd \right) + \left\langle x^2 + 1 \right\rangle,
      \end{align*}
      so this is an algebra homomorphism.

      By the Chinese remainder theorem, we then have
      \begin{align*}
        \C\otimes_{\R} \C &\cong \C\otimes_{\R} \left( \R[x]/\left\langle x^2 + 1 \right\rangle \right)\\
                          &\cong \C\left[ x \right]/\left\langle x^2 + 1 \right\rangle\\
                          &\cong \frac{\C\left[ x \right]}{\left\langle x-i \right\rangle} \times \frac{\C\left[ x \right]}{\left\langle x+i \right\rangle}\\
                          &\cong \C\times \C
      \end{align*}
      are isomorphic as rings, where the isomorphism $\C[x]/\left\langle x\pm i \right\rangle\cong \C$ is given by evaluation at $x = \mp i$ respectively. In particular, we observe that this is a homomorphism of $\C$-algebras, since the evaluation homomorphism is an algebra homomorphism.
    \item Identifying $i \leftrightarrow x + \left\langle x^2 + 1 \right\rangle$ in $\R[x]/\left( x^2 + 1 \right)$, we observe that $1\otimes 1$ and $1\otimes i$ are necessarily linearly independent, and since $\C$ is a dimension $2$ $\R$-algebra, it follows that $\C\otimes_{\R}\C$ is a dimension $2$ $\C$-algebra. Computing from the definitions, we have
      \begin{align*}
        \varphi\left( 1\otimes 1 \right) &= \left( 1,1 \right)\\
        \varphi\left( 1\otimes i \right) &= \left( i,-i \right).
      \end{align*}
      We observe that the right-hand-side constitutes a basis, since we recover the standard basis for $\C\times \C$ by taking
      \begin{align*}
        \left( 1,0 \right) &= \frac{1}{2}\left( \left( 1,1 \right) - i\left( i,-i \right) \right)\\
        \left( 0,1 \right) &= \frac{1}{2}\left( \left( 1,1 \right) + i\left( i,-i \right) \right).
      \end{align*}
    \item We observe that $\C\times \C$ has a nontrivial automorphism given by $\left( 1,0 \right) \leftrightarrow \left( 0,1 \right)$, so there are at least two automorphisms (including the trivial automorphism). Additionally, the restrictions on $1\otimes 1$ and $1\otimes i$ are such that $1\otimes 1$ must map to the multiplicative identity of $\C\times \C$ as $1\otimes 1$ is the multiplicative identity of $\C\otimes_{\R}\C$, and $\varphi\left( 1\otimes i \right)$ must have (exponential) order $4$ in $\C\times \C$ and map to an element that is linearly independent from $\left( 1,1 \right)$. The set of order $4$ elements are $\left( i,i \right), \left( i,-i \right), \left( -i,i \right), \left( -i,-i \right)$. Of these, only $\left( i,-i \right)$ and $\left( -i,i \right)$ are $\C$-linearly independent from $\left( 1,1 \right)$, so there are at most $2$ choices for where $1\otimes i$ can be mapped to in $\C\times \C$.
  \end{enumerate}
\end{solution}
\begin{problem}[Problem 5]
  Let $V$ and $W$ be finite-dimensional vector spaces over $F$, with $\set{v_1,\dots,v_n}$ a basis for $V$ and $\set{w_1,\dots,w_m}$ a basis for $W$. Let $\varphi\colon V\otimes W\rightarrow \Mat_{n,m}(F)$ be given by $\varphi\left( v_i\otimes w_j \right) = e_{ij}$, where $e_{ij}$ is the matrix unit whose $\left( i,j \right)$ entry is $1$ and all other entries are $0$.
  \begin{enumerate}[(a)]
    \item Prove that for a matrix $A\in \Mat_{n,m}(F)$, the following are equivalent:
      \begin{enumerate}[(i)]
        \item $A = \varphi\left( v\otimes w \right)$ for some elements $v\in V$ and $w\in W$;
        \item $\operatorname{rk}(A) \leq 1$.
      \end{enumerate}
    \item Let $A\in \Mat_{n,m}(F)$. Prove that $\operatorname{rk}(A)$ is the smallest $d$ such that $\varphi^{-1}(A)$ can be written as a sum of $d$ simple tensors.
  \end{enumerate}
\end{problem}
\begin{solution}\hfill
  \begin{enumerate}[(a)]
    \item Suppose that $A = \varphi\left( v\otimes w \right)$ for some $v\in V$ and $w\in W$. We may write
      \begin{align*}
        v\otimes w &= \sum_{i=1}^{n}\sum_{j=1}^{m} s_it_j e_i\otimes e_j\\
        A &= \sum_{i=1}^{n}\sum_{j=1}^{m} s_it_je_{ij}.
      \end{align*}
      Then, using the identity
      \begin{align*}
        e_{ij}\left( e_k \right) &= \delta_{jk}e_i,
      \end{align*}
      where $\delta_{jk}$ denotes the Kronecker delta, we get that for an arbitrary vector
      \begin{align*}
        x &= \sum_{k=1}^{m} r_ke_k
      \end{align*}
      in $F^m$, we have
      \begin{align*}
        Ax &= \left( \sum_{i=1}^{n}\sum_{j=1}^{m}s_it_je_{ij} \right)\left( \sum_{k=1}^{m}r_ke_k \right)\\
           &= \sum_{i=1}^{n} \sum_{j=1}^{m}\sum_{k=1}^{m}s_it_jr_ke_{ij}\left( e_k \right)\\
           &= \sum_{i=1}^{n}\sum_{j=1}^{m}\sum_{k=1}^{m}s_it_jr_k \delta_{jk}e_i\\
           &= \sum_{i=1}^{n}\sum_{j=1}^{m}t_jr_j s_ie_i\\
           &= \sum_{j=1}^{m} t_jr_j\left( \sum_{i=1}^{n}s_ie_i \right)\\
           &\in \Span\set{\sum_{i=1}^{n}s_ie_i}.
      \end{align*}
      Therefore, $\operatorname{rk}(A)\leq 1$.

      If $\operatorname{rk}(A) = 0$, then $v\otimes w$ is the zero tensor since $\varphi$ is an isomorphism. Else, we assume $\operatorname{rk}(A) = 1$. Then, there are some coefficients $s_1,\dots,s_n$ such that
      \begin{align*}
        \img(A) &= \Span\set{\sum_{i=1}^{n}s_ie_i}.
      \end{align*}
      Now, let
      \begin{align*}
        x &= \sum_{k=1}^{m}r_ke_k.
      \end{align*}
      We may then define
      \begin{align*}
        w &= \sum_{j=1}^{m} t_je_j
      \end{align*}
      to be such that
      \begin{align*}
        \sum_{j=1}^{m}t_jr_j &= c,
      \end{align*}
      so that
      \begin{align*}
        \varphi\left( \left( \sum_{i=1}^{n}s_ie_i \right)\otimes \left( \sum_{j=1}^{m}t_je_j \right) \right) \left( \sum_{k=1}^{m}r_ke_k \right) &= c\sum_{i=1}^{n}s_ie_i.
      \end{align*}
      Thus, we find $v\otimes w$ such that $A = \varphi\left( v\otimes w \right)$.
    \item From the Smith normal form, we may write
      \begin{align*}
        \operatorname{rk}(A) &= \operatorname{rk}\left( PAQ \right)
      \end{align*}
      where $Q$ is a change of basis matrix for $V$ taking $v_i\mapsto v_i'$ and $P$ is a change of basis matrix for $W$ taking $w_j \mapsto w_j'$. Define $PAQ = D$, where $D$ is a diagonal matrix with the number of entries along the diagonal equal to its rank. We observe then that a redefined map $\psi\colon V\otimes W \rightarrow \Mat_{m,n}(F)$ taking $v_i'\otimes w_j' \mapsto e_{ij}$ thus has
      \begin{align*}
        \varphi^{-1}(A) &= \psi^{-1}(D)\\
                        &= \sum_{k=1}^{\operatorname{rk}(D)} v_i'\otimes w_i'\\
                        &= \sum_{k=1}^{\operatorname{rk}(A)} v_i'\otimes w_i'.
      \end{align*}
      Thus, we see that, since $\operatorname{rk}(A) = \operatorname{rk}(D)$, we have that $\operatorname{rk}(A)$ is the minimum number of elements of $V\otimes W$ necessary to write $\varphi^{-1}(A)$ as a sum of simple tensors.
  \end{enumerate}
\end{solution}
\begin{problem}[Problem 6]
  Let $R$ be a ring with $1$, and let $M$ be a left $R$-module, $N$ a submodule. Prove that $M$ is Noetherian if and only if $N$ and $M/N$ are both Noetherian.
\end{problem}
\begin{solution}
  Suppose $M$ is a Noetherian module. Then, any submodule of $M$ is finitely generated, so since any submodule of $N$ is a submodule of $M$, $N$ is Noetherian. Similarly, since any submodule of $M/N$ corresponds to a submodule of $M$ that contains $N$ by the Fourth Isomorphism Theorem, it follows that $M/N$ is also Noetherian.

  Now, suppose $M$ is a module such that $M/N$ and $N$ are Noetherian. Let $P_1\leq P_2\leq\cdots$ be an ascending chain of submodules for $M$. Then, $P_1\cap N \leq P_2\cap N\leq \cdots$ is an ascending chain of submodules of $N$, so there is some index $k_1$ such that $P_{k_1 + i} = P_{k_1}$ for all $i\in \N$. Similarly, the set of submodules $P_1 + N\leq P_2 + N \leq \cdots$ is an ascending chain of submodules that contains $N$, so the submodules $\left( P_1+N \right)/N\leq \left( P_2+N \right)/N\leq\cdots$ forms an ascending chain of submodules in $M/N$, so there is some index $k_2$ such that $P_{k_2 + i} = P_{k_2}$ for all $i\in \N$. In particular, this means that for all $i\in \N$, $P_{k+i} = P_k$, where $k = \max\left( k_1,k_2 \right)$, so $M$ is Noetherian.
\end{solution}
\end{document}
