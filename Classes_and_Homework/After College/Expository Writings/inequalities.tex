\documentclass[12pt]{mypackage}

% sans serif font:
%\usepackage{cmbright,sfmath,bbold}
%\renewcommand{\mathcal}{\mathtt}

%Euler:
%\usepackage{newpxtext,eulerpx,eucal,eufrak}
%\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
%\renewcommand*{\hbar}{\hslash}
%\usepackage{homework}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Inequalities and the $L_p$-Spaces}
\usepackage{microtype}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\begin{abstract}
  \noindent We introduce some of the most important inequalities that are used frequently in real and functional analysis. These inequalities include Jensen's inequality and Young's inequality (concerning convex functions), which are then used to prove Hölder's inequality and Minkowski's inequality (concerning $p$-norms). Afterwards, we define the $L_p$-spaces and show some basic properties such as completeness and the $L_p$/$L_q$ duality.
\end{abstract}
\tableofcontents
%\section{Introduction}%
%This is the first of a series of expository writings, primarily focused on analysis (though they will certainly branch out), designed to complement my own knowledge and understanding of the various facets of the field. Ideally these will be shorter in length than my traditional \href{https://ai.avinash-iyer.com/classnotes.html}{notes documents}, and they will focus on some short, coherent theme.\newline
%
%Here, we introduce and discuss some of the most important inequalities in analysis, primarily focusing on the context of function spaces such as $L_p\left( \Omega,\mu \right)$. Afterwards, we will establish the completeness of the $L_p\left( \Omega,\mu \right)$ spaces, and show that $L_p\left( \Omega,\mu \right)^{\ast} = L_q\left( \Omega,\mu \right)$ whenever $1 \leq p < \infty$. Much of this document will be a fleshed out version of certain results and theorems discussed in Walter Rudin's \textit{Real and Complex Analysis}, primarily from Chapter 3, as well as the discussion of duality from Royden and Fitzpatrick's \textit{Real Analysis}.\newline
%
%We will assume that the reader has an understanding of measures, measurable functions, and basic integration theory (such as the Monotone Convergence Theorem, Fatou's Lemma, and the Dominated Convergence Theorem). My \href{https://ai.avinash-iyer.com/Classes_and_Homework/College/Y3/Y3S2,\%20Real\%20II/real_2_notes.pdf}{Real Analysis II} notes should be sufficient as a background.
\section{Convex Functions}%
\begin{definition}
  A function $\varphi\colon (a,b) \rightarrow \R$ is called \textit{convex} if, for all $x,y\in (a,b)$,
  \begin{align*}
    \varphi\left( \left( 1-t \right)x + ty\right) &\leq \left( 1-t \right)\varphi\left( x \right) + t\varphi\left( y \right).
  \end{align*}
\end{definition}
\begin{remark}
A \textit{differentiable} function $\varphi\colon \Omega \rightarrow \R$ is convex if and only if its second derivative is positive along its domain.\newline

Note here that we define convexity along an open interval. This is because it is convenient to do so.
\end{remark}
Two major examples of convex functions are
\begin{align*}
  \varphi_1(x) &= e^{x}\\
  \varphi_2(x) &= x^{p}.\tag*{$1\leq p < \infty$}
\end{align*}
Both of these functions are convex along their domain.\newline

Convex functions defined over an interval are useful precisely because they are continuous --- and, thus, measurable.
\begin{theorem}
  Let $\varphi\colon (a,b)\rightarrow \R$ be convex. Then, $\varphi$ is continuous.
\end{theorem}
We follow the proof from \href{https://unapologetic.wordpress.com/2008/04/15/convex-functions-are-continuous/}{this website}.
\begin{proof}
  We begin by an observation. If $a < x_1 < x_2 < x_3 < b$, then convexity gives
  \begin{align*}
    \frac{f\left( x_2 \right)- f\left( x_1 \right)}{x_2 - x_1} &\leq \frac{f\left( x_3 \right)-f\left( x_2 \right)}{x_3 - x_2}. \label{eq:convexity_fraction_inequality}\tag{\textasteriskcentered}
  \end{align*}
  By the characterization of an interval, for $s , t\in \left( a,b \right)$ with $s < t$, we have $\left[ s,t \right]\subseteq \left( a,b \right)$. Now, note that since $\left( a,b \right)$ is an open interval, there are $s',t'\in \left( a,b \right)$ with $s' < s$ and $t < t'$. In particular, this means that for any $x_1,x_2\in \left[ s,t \right]$ with $x_1 < x_2$, we have
  \begin{align*}
    \frac{f\left( s \right) - f\left( s' \right)}{s-s'} &\leq \frac{f\left( x_1 \right) - f\left( s \right)}{x_1 - s}\\
                                                        &\leq \frac{f\left( x_2 \right) - f\left( x_1 \right)}{x_2 - x_1}\\
                                                        &\leq  \frac{f\left( t \right) - f\left( x_2 \right)}{t-x_2}\\
                                                        &\leq \frac{f\left( t' \right)-f\left( t \right)}{t'-t}.
  \end{align*}
  Setting $C\coloneq \max\set{\frac{f\left( t' \right)-f\left( t \right)}{t'-t},\frac{f\left( s \right)-f\left( s' \right)}{s-s'}}$, we see that for any $x_1,x_2\in \left[ s,t \right]$,
  \begin{align*}
    \left\vert f\left( x_2 \right)-f\left( x_1 \right) \right\vert &\leq C \left\vert x_2 - x_1 \right\vert.
  \end{align*}
  Thus, $f$ is Lipschitz on $\left[ s,t \right]$, so $f$ is continuous on $\left[ s,t \right]$.\newline

  Since, for any $x\in \left( a,b \right)$, there is some closed interval containing $x$, and $f$ is continuous on said closed interval, we have that $f$ is continuous on $\left( a,b \right)$.
\end{proof}
\begin{remark}
  The fact that $\left( a,b \right)$ is an open interval is indeed load-bearing. Consider the function defined by
  \begin{align*}
    f\left( x \right) &= \begin{cases}
      x & x > 0\\
      1 & x = 0
    \end{cases}.
  \end{align*}
  Then, $f$ is convex, but $f$ is not continuous.
\end{remark}
The most famous inequality regarding convex functions is Jensen's inequality, which effectively provides a generalization of the definition of a convex function.
\begin{theorem}[Jensen's Inequality]
  Let $\left( \Omega,\mathcal{M},\mu \right)$ be a probability space, and let $f\in L_1\left( \Omega,\mu \right)$ be such that $a < f(x) < b$ for all $x\in \Omega$. Then, if $\varphi\colon (a,b)\rightarrow \R$ is convex,
  \begin{align*}
    \varphi\left( \int_{\Omega}^{} f\:d\mu \right) &\leq \int_{\Omega}^{} \varphi\circ f\:d\mu
  \end{align*}
\end{theorem}
\begin{proof}
  Set
  \begin{align*}
    t\coloneq \int_{\Omega}^{} f\:d\mu,
  \end{align*}
  and note that $a < t < b$. Note that, by a restatement of \eqref{eq:convexity_fraction_inequality}, if $a < s < t < u < b$, then
  \begin{align*}
    \frac{\varphi\left( t \right) - \varphi\left( s \right)}{t-s} &\leq \frac{\varphi\left( u \right)- \varphi\left( t \right)}{u-t}.
  \end{align*}
  Setting
  \begin{align*}
    \beta\coloneq \sup_{s\in (a,t)} \frac{\varphi\left( t \right) - \varphi\left( s \right)}{t-s},
  \end{align*}
  it follows that
  \begin{align*}
    \beta \leq \frac{\varphi\left( u \right)-\varphi\left( t \right)}{u-t}
  \end{align*}
  for all $u\in (t,b)$. Thus, for all $a < s < b$, we have
  \begin{align*}
    \varphi\left( s \right) &\geq \varphi\left( t \right) + \beta\left( s-t \right).
  \end{align*}
  In particular, this holds for all $s = f(x)$, where $x\in \Omega$, so that
  \begin{align*}
    \varphi\left( f\left( x \right) \right) &\geq \varphi\left( t \right) + \beta f(x) - \beta t.
  \end{align*}
  Integrating, and using the fact that $t$ is a constant, we get
  \begin{align*}
    \int_{\Omega}^{} \varphi\circ f\:d\mu &\geq \varphi\left( \int_{\Omega}^{} f\:d\mu \right) + \underbrace{\beta \int_{\Omega}^{} f\:d\mu - \beta t\mu\left( \Omega \right)}_{=0}.
  \end{align*}
  Thus, we obtain
  \begin{align*}
    \varphi\left( \int_{\Omega}^{} f\:d\mu \right) &\leq \int_{\Omega}^{} \varphi\circ f\:d\mu.
  \end{align*}
\end{proof}
Jensen's inequality is incredibly powerful, as it allows us to establish a variety of other classic inequalities. For instance, if we set $\varphi(x) = e^x$, then Jensen's inequality becomes
\begin{align*}
  e^{\int_{\Omega}^{} f\:d\mu} &\leq \int_{\Omega}^{} e^{f}\:d\mu.
\end{align*}
If $\Omega = \set{p_1,\dots,p_n}$, where $\mu\left( \set{p_i} \right) = \frac{1}{n}$ with $f\left(p_i\right) = x_i$, then this gives
\begin{align*}
  e^{\frac{1}{n}\left( x_1 + \cdots + x_n \right)} &\leq \frac{1}{n}\left( e^{x_1} + \cdots + e^{x_n} \right).
\end{align*}
Setting $y_i\coloneq e^{x_i}$, we recover the AM-GM inequality,
\begin{align*}
  \left( \prod_{i=1}^{n}y_i \right)^{1/n} &\leq \frac{1}{n}\sum_{i=1}^{n}y_i.
\end{align*}
More generally, if $\mu\left( \set{p_i} \right) = \alpha_i > 0$, and $\sum_{i=1}^{n}\alpha_i = 1$, we obtain
\begin{align*}
  \prod_{i=1}^{n}y_i^{\alpha_i} &\leq \sum_{i=1}^{n}\alpha_iy_i.
\end{align*}
\begin{definition}
  If $1 \leq p,q \leq \infty$ are such that
  \begin{align*}
    \frac{1}{p} + \frac{1}{q} &= 1,
  \end{align*}
  then we call $p$ and $q$ \textit{conjugate exponents}. We use the convention that $\frac{1}{\infty} = 0$, so that $p=1,q=\infty$ is a pair of conjugate exponents.
\end{definition}
\begin{theorem}[Young's Inequality]
  If $p$ and $q$ are conjugate exponents, then for any positive $a,b$, we have
  \begin{align*}
    ab &\leq \frac{1}{p}a^p + \frac{1}{q}b^q
  \end{align*}
\end{theorem}
\begin{proof}
  Note that $\frac{1}{p} = 1 - \frac{1}{q}$. Thus, since $\ln$ is a concave function,
  \begin{align*}
    \ln\left( \frac{1}{p}a^p + \frac{1}{q}b^{q} \right) &\geq \frac{1}{p}\ln\left( a^{p} \right) + \frac{1}{q}\ln\left( b^{q} \right)\\
                                                        &= \ln\left( a \right) + \ln\left( b \right)\\
                                                        &= \ln\left( ab \right).
  \end{align*}
  Now, since $e^{x}$ preserves order, we obtain Young's inequality by taking exponentials.
\end{proof}
In \href{https://ai.avinash-iyer.com/Classes_and_Homework/College/Y3/Y3S2,\%20Real\%20II/real_2_notes.pdf}{Real Analysis II}, we used Young's Inequality to prove Hölder's Inequality and Minkowski's Inequality for the case of $x,y\in \C^{n}$.
\begin{theorem}[Hölder's Inequality for $\C^n$]
  Let $x,y\in \C^{n}$. Then, if $p$ and $q$ are conjugate exponents,
  \begin{align*}
    \sum_{j=1}^{n}\left\vert x_jy_j \right\vert &\leq \left( \sum_{j=1}^{n}\left\vert x_j \right\vert^{p} \right)^{1/p}\left( \sum_{j=1}^{n}\left\vert y_j \right\vert^{q} \right)^{1/q}.
  \end{align*}
\end{theorem}
\begin{theorem}[Minkowski's Inequality for $\C^n$]
  Let $x,y\in \C^n$. Then, for any $p \geq 1$,
  \begin{align*}
    \left( \sum_{j=1}^{n}\left\vert x_j + y_j \right\vert^{p} \right)^{1/p} &\leq \left( \sum_{j=1}^{n}\left\vert x_j \right\vert^{p} \right)^{1/p} + \left( \sum_{j=1}^{n}\left\vert y_j \right\vert^{p} \right)^{1/p}.
  \end{align*}
\end{theorem}
We will prove these inequalities in the most general case --- i.e., with integrals.
\section{Hölder's Inequality}%
\begin{theorem}[Hölder's Inequality]
  Let $p$ and $q$ be conjugate exponents with $1 < p < \infty$, and let $\left( X,\mathcal{M},\mu \right)$ be a measure space. Let $f,g\colon X\rightarrow [0,\infty]$ be measurable functions. Then,
  \begin{align*}
    \int_{X}^{} fg\:d\mu &\leq \left( \int_{ X }^{} f^{p}\:d\mu \right)^{1/p}\left( \int_{X}^{} g^{q}\:d\mu \right)^{1/q}.
  \end{align*}
\end{theorem}
\begin{proof}
  Set
  \begin{align*}
    A &\coloneq \left( \int_{X}^{} f^{p}\:d\mu \right)^{1/p}\\
    B &\coloneq \left( \int_{X}^{} g^{q}\:d\mu \right)^{1/q}.
  \end{align*}
  We may safely assume that $0 < A,B < \infty$. Set
  \begin{align*}
    F &= \frac{f}{A}\\
    G &= \frac{g}{B},
  \end{align*}
  giving
  \begin{align*}
    \int_{X}^{} F^{p}\:d\mu &= 1\\
    \int_{X}^{} G^{q}\:d\mu &= 1.
  \end{align*}
  Now, if $x$ is such that $0 < F(x) < \infty$ and $0 < G(x) < \infty$, then there exist $s,t$ such that $F(x) = e^{s/p}$ and $G(x) = e^{t/q}$, as $e^{x}\colon \R\rightarrow (0,\infty)$ is surjective. Since $\frac{1}{p} + \frac{1}{q} = 1$, and the exponential function is convex, we get, from Jensen's Inequality,
  \begin{align*}
    e^{s/p}e^{t/q} &= e^{s/p + t/q}\\
                   &\leq \frac{1}{p}e^{s} + \frac{1}{q}e^{t},
  \end{align*}
  and substituting, we have
  \begin{align*}
    F(x)G(x) &\leq \frac{1}{p}F(x)^{p} + \frac{1}{q}G(x)^{q}
  \end{align*}
  for all $x\in X$. Integrating, we have
  \begin{align*}
    \int_{X}^{} FG\:d\mu &\leq 1.
  \end{align*}
  Substituting our definition for $F$ and $G$, we get
  \begin{align*}
    \int_{X}^{} fg\:d\mu &\leq \left( \int_{X}^{} f^p\:d\mu \right)^{1/p} \left( \int_{X}^{} g^{q}\:d\mu \right)^{1/q}.
  \end{align*}
\end{proof}
\section{Minkowski's Inequality}%
\begin{theorem}[Minkowski's Inequality]
  Let $\left( X,\mathcal{M},\mu \right)$ be a measure space, and let $f,g\colon X\rightarrow [0,\infty]$ be such that
  \begin{align*}
    \int_{X}^{} f^{p}\:d\mu &< \infty\\
    \int_{X}^{} g^{p}\:d\mu &< \infty.
  \end{align*}
  Then, for all $1 \leq p \leq \infty$,
  \begin{align*}
    \left( \int_{X}^{} \left( f + g \right)^{p}\:d\mu \right)^{1/p} &\leq \left( \int_{X}^{} f^{p}\:d\mu \right)^{1/p} + \left( \int_{X}^{} g^{p}\:d\mu \right)^{1/p}.
  \end{align*}
\end{theorem}
\begin{proof}
  Write 
  \begin{align*}
    \left( f + g \right)^{p} &= f\left( f + g \right)^{p-1} + g\left( f + g \right)^{p-1}.
  \end{align*}
  Then, by Hölder's inequality, we have
  \begin{align*}
    \int_{X}^{} f \left( f + g \right)^{p-1}\:d\mu &\leq \left( \int_{X}^{} f^{p}\:d\mu \right)^{1/p}\left( \int_{X}^{} \left( f + g \right)^{\left( p-1 \right)q}\:d\mu \right) ^{1/q}\\
    \int_{X}^{} g\left( f + g \right)^{p-1}\:d\mu &\leq \left( \int_{X}^{} g^{p}\:d\mu \right)^{1/p}\left( \int_{X}^{} \left( f + g \right)^{\left( p-1 \right)q}\:d\mu \right)^{1/q}.
  \end{align*}
  Adding, and noting that $\left( p-1 \right)q = p$, we have
  \begin{align*}
    \int_{X}^{} \left( f + g \right)^{p}\:d\mu &\leq \left( \int_{X}^{} \left( f + g \right)^{p}\:d\mu \right)^{1/q}\left( \left( \int_{X}^{} f^{p}\:d\mu \right)^{1/p} + \left( \int_{X}^{} g^{p}\:d\mu \right)^{1/p} \right)\label{eq:pre_minkowski_inequality}\tag{\textasteriskcentered}
  \end{align*}
  By the convexity of $t^{p}$, for $0 < t < \infty$, we have
  \begin{align*}
    \left( \frac{1}{2}\left( f + g \right) \right)^{p} &\leq \frac{1}{2}\left( f^{p} + g^{p} \right),
  \end{align*}
  so the left side of \eqref{eq:pre_minkowski_inequality} is finite. Dividing, we have
  \begin{align*}
    \left( \int_{X}^{} \left( f + g \right)^{p}\:d\mu \right)^{1/p} &\leq \left( \int_{X}^{} f^{p}\:d\mu \right)^{1/p} + \left( \int_{X}^{} g^{p}\:d\mu \right)^{1/p}.
  \end{align*}
  In the case of $p = 1$ or $p = \infty$, Minkowski's inequality follows from the triangle inequality for $\left\vert \cdot \right\vert$.
\end{proof}
\section{The $L_p$-Spaces}%
Inspired by Minkowski's inequality, we consider a special class of normed space --- the $L_p$-spaces --- and show some of its important properties.
\begin{definition}
  Let $\left( \Omega,\mathcal{M},\mu \right)$ be a measure space, and let $1 \leq p < \infty$. We define the space $L_p\left( \Omega,\mu \right)$ to be
  \begin{align*}
    L_p\left( \Omega,\mu \right) &\coloneq \set{f\colon \Omega\rightarrow \C | \int_{\Omega}^{} \left\vert f \right\vert^{p}\:d\mu < \infty}.
  \end{align*}
  The norm on $L_p\left( \Omega,\mu \right)$ is defined by
  \begin{align*}
    \norm{f}_{L_p} &= \left( \int_{\Omega}^{} \left\vert f \right\vert^{p}\:d\mu \right)^{1/p}.
  \end{align*}
\end{definition}
\begin{definition}
  If $f\colon \Omega\rightarrow \C$ is a measurable function, then an \textit{essential bound} for $f$ is a $C > 0$ such that
  \begin{align*}
    \mu\left( \set{x | \left\vert f(x) \right\vert > C} \right) &= 0.
  \end{align*}
  The \textit{essential supremum} of $f$ is the infimum of all such essential bounds.\newline

  We define the space $L_{\infty}\left( \Omega,\mu \right)$ to be the space of all essentially bounded functions. The norm on $L_{\infty}\left( \Omega,\mu \right)$ is defined by
  \begin{align*}
    \norm{f}_{L_{\infty}} &\coloneq \esssup\left( \left\vert f \right\vert \right).
  \end{align*}
\end{definition}

Note that by Minkowski's inequality, the norm on $L_p\left( \Omega,\mu \right)$ is a bona fide norm.\newline

In this section, we will establish the two major properties of the $L_p$-spaces --- first, their completeness, and second, the duality between $L_p$ and $L_q$.
\begin{theorem}
  The space $L_p\left( \Omega,\mu \right)$ is complete.
\end{theorem}
\begin{proof}
  Let $1 \leq p < \infty$, and let $\left( f_n \right)_n$ be a Cauchy sequence in $L_p\left( \Omega,\mu \right)$. Then, there exists a subsequence such that $\norm{f_{n_i + 1} - f_{n_i}}_{L_p} < 2^{-i}$ for each $i$. We set
  \begin{align*}
    g_k &= \sum_{i=1}^{k}\left\vert f_{n_i + 1} - f_{n_i} \right\vert\\
    g &= \sum_{i=1}^{\infty}\left\vert f_{n_i + 1} - f_{n_i} \right\vert.
  \end{align*}
  By Minkowski's inequality, we know that $\norm{g_k}_{L_p} < 1$ for each $k$, meaning that by Dominated Convergence, $\norm{g}_{L_p} \leq 1$, meaning that $g(x) < \infty$ $\mu$-almost everywhere. Thus, the series 
  \begin{align*}
    f(x) &\coloneq f_{n_1}(x) + \sum_{i=1}^{\infty}\left( f_{n_i + 1}\left( x \right) -  f_{n_i}\left( x \right)\right)
  \end{align*}
  converges absolutely for almost every $x\in \Omega$. Set $f(x) = 0$ where the series does not converge. Now, since
  \begin{align*}
    f_{n_k} &= f_{n_1} + \sum_{i=1}^{k-1}\left( f_{n_i + 1} - f_{n_i} \right),
  \end{align*}
  we have that
  \begin{align*}
    f(x) &= \lim_{i\rightarrow\infty}f_{n_i}(x)
  \end{align*}
  almost everywhere.\newline

  Now, we prove that $\left( f_n \right)_n$ converges to $f$ in $L_p$. Let $\ve > 0$. Then, there is some $N$ such that $\norm{f_n - f_m}_{L_p} < \ve$ for all $n,m \geq N$. By Fatou's Lemma, we have
  \begin{align*}
    \int_{\Omega}^{} \left\vert f-f_m \right\vert^{p}\:d\mu &= \int_{\Omega}^{} \liminf_{i\rightarrow\infty}\left\vert f_{n_i} - f_m \right\vert^{p}\:d\mu\\
                                                            &\leq \liminf_{i\rightarrow\infty}\int_{\Omega}^{} \left\vert f_{n_i} - f_m \right\vert^{p}\:d\mu\\
                                                            &\leq \ve^{p}.
  \end{align*}
  Thus, $f-f_m\in L_p\left( \Omega,\mu \right)$, so $f\in L_p\left( \Omega,\mu \right)$, and $\norm{f - f_m}_{L_p}\xrightarrow{m\rightarrow\infty} 0$.
\end{proof}

\end{document}
