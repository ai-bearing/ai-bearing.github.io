\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright,sfmath,bbold}
%\renewcommand{\mathcal}{\mathtt}

%Euler:
\usepackage{newpxtext,eulerpx,eucal,eufrak}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\renewcommand*{\hbar}{\hslash}

%\renewcommand{\mathbb}{\mathds}
%\usepackage{homework}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Assignment 6}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\begin{solution}[29.5]\hfill
  \begin{enumerate}[(a)]
    \item We have
      \begin{align*}
        \left( \vec{w}\cdot \doublevec{T} \right)_{k} &= \sum_{i,j,k}w_{i}T_{jk}\delta_{ij}\\
                                   &= \sum_{i,k}w_iT_{ik},
      \end{align*}
      which is a first-rank tensor.
    \item Since $\vec{w}\cdot \doublevec{T}$ is a first-rank tensor, and we are taking the dot product of two first rank tensors the expression $\vec{w}\cdot \doublevec{T}\cdot \vec{v}$ is a scalar (or rank zero tensor).
    \item We have
      \begin{align*}
        \doublevec{T}\cdot \doublevec{U} &= \left( \sum_{i,j}T_{ij}e_i\otimes e_j \right)\cdot \left( \sum_{k,\ell}U_{k\ell} e_{k}\otimes e_{\ell} \right)\\
                                         &= \sum_{i,j,k,\ell}T_{ij}U_{k \ell} \left( e_k\cdot e_i \right)\left( e_j\cdot e_{\ell} \right),
      \end{align*}
      which is a scalar.
    \item The expression $\doublevec{T}\vec{v}$ expresses the operation of
      \begin{align*}
        \doublevec{T} &= \sum_{i,j}T_{ij}e_i\otimes e_j
      \end{align*}
      on
      \begin{align*}
        \vec{v} &= \sum_{i}v_ie_i,
      \end{align*}
      meaning that $\doublevec{T}\vec{v}$ is a vector.
    \item The expression $\doublevec{T}\doublevec{U}$ is a composition of two linear maps on $V\otimes V$, so it is a rank $2$ tensor (or another linear map on $V\otimes V$).
  \end{enumerate}
\end{solution}
\begin{solution}[29.7]
  We have $2^{4}$ or $16$ components in $A_{ijkl}$.
\end{solution}
\begin{solution}[29.10]

\end{solution}
\begin{solution}[29.11]\hfill
\end{solution}
\begin{enumerate}[(a)]
  \item We may write $T_{ij}$ as $T = \frac{1}{2}\left( T + T^{T} \right) + \frac{1}{2}\left( T - T^{T} \right)$, which are the symmetric and antisymmetric components.
  \item Taking
    \begin{align*}
      S_{ij} &= \sum_{k,\ell}R_{ik}R_{j\ell}S_{k\ell},
    \end{align*}
    we have
    \begin{align*}
      S_{ji} &= \sum_{k,\ell}R_{jk}R_{i\ell}S_{k\ell}\\
             &= \sum_{k,\ell}R_{j\ell}R_{ik}S_{\ell k}\\
             &= \sum_{k,\ell}R_{ik}R_{j\ell}S_{\ell k}\\
             &= \sum_{k,\ell}R_{ik}R_{j \ell}S_{k \ell}\\
             &= S_{ij}.
    \end{align*}
    Similarly,
    \begin{align*}
      A_{ij} &= \sum_{k,\ell}R_{ik}R_{j\ell}A_{k \ell}\\
      A_{ji} &= \sum_{k,\ell}R_{jk}R_{i\ell}A_{k \ell}\\
             &= -\sum_{k,\ell}R_{ik}R_{j\ell}A_{k\ell}\\
             &= -A_{ij}.
    \end{align*}
    In matrix form, we have
    \begin{align*}
      S_{ji} &= S_{ij}^{T}\\
             &= \left( RS_{k\ell} R^{T} \right)^{T}\\
             &= RS_{k\ell}R^{T}.
    \end{align*}
    and similarly,
    \begin{align*}
      -A_{ji} &= \left( A_{ij} \right)^{T}\\
              &= \left( RA_{k\ell}R^{T} \right)^{T}\\
              &= RA_{k\ell}R^{T}.
    \end{align*}
\end{enumerate}
\begin{solution}[29.12]\hfill
  \begin{enumerate}[(a)]
    \item Let $\mathbf{f}\colon \R^2\rightarrow \R^2$ be a function. Then,
      \begin{align*}
        \nabla\cdot \mathbf{f} &= \pd{f}{x} + \pd{f}{y}.
      \end{align*}
      If we rotate $\nabla\mapsto R\nabla$, then
      \begin{align*}
        R\nabla \cdot \mathbf{f} &= \pd{f}{x'}\pd{x'}{x} + \pd{f}{x'}\pd{x'}{y} + \pd{f}{y'}\pd{y'}{x} + \pd{f}{y'}\pd{y'}{y},
      \end{align*}
      which equals $\nabla\cdot \mathbf{f}$ by chain rule.
    \item We take
      \begin{align*}
        \left( R\nabla \right)_j &= \sum_{j}R_{ij}\pd{}{x_j},
      \end{align*}
      meaning we have
      \begin{align*}
        \left( \left( R\nabla \right)\cdot \mathbf{f} \right)_{j} &= \sum_{j,\ell}\left( R_{ij}\pd{}{x_j} \right)f_{\ell}\delta_{j\ell}\\
                                               &= \sum_{j,\ell}R_{ij} \pd{f_{\ell}}{x_j}\delta_{j\ell}\\
                                               &= \sum_{i} \pd{f_{i}}{x_i}
      \end{align*}
      We use the fact that $R$ is independent of $x_i$ in the switch from line (2) to line (3).
  \end{enumerate}
\end{solution}
\begin{solution}[29.14]
  We have
  \begin{align*}
    \mathbf{u}\left( \mathbf{r} + d\mathbf{r} \right) &= \mathbf{u}\left( \mathbf{r} \right) + \doublevec{\ve}\cdot d\mathbf{r} + \doublevec{\phi}\cdot d\mathbf{r}.
  \end{align*}
  Applying Hooke's Law, and using the fact that $k$ is described in different degrees of freedom than $L$ and $A$, we have
  \begin{align*}
    \sigma_{ij} &= \left( Y\ve \right)_{ij}
                &= \left( \frac{k_{i,j}L_{k,\ell}}{A_{k,\ell}}\ve_{i,j} \right)_{ij}\\
                &= \sum_{k,\ell}Y_{ijk\ell}\ve_{k\ell}.
  \end{align*}
\end{solution}
\begin{solution}[29.23]
  We have
  \begin{align*}
    T_{ij} &= \sum_{k,\ell}R_{i k}R_{j\ell}\left( \frac{1}{3}\tr\left( T \right)\delta_{k \ell} + \frac{1}{2}\left( T_{k \ell} - T_{\ell k}  \right) + \frac{1}{2}\left( T_{k\ell} + T_{\ell k} - \frac{2}{3}\tr\left( T \right)\delta_{ij} \right) \right)\\
           &= \frac{1}{3}\delta_{ij} + \frac{1}{2}\left( T_{ij} - T_{ji} \right) + \frac{1}{2}\left( T_{ij} + T_{ji} - \frac{2}{3}\delta_{ij} \right).
  \end{align*}
\end{solution}
\begin{solution}[29.24]

\end{solution}
\begin{solution}[29.25]

\end{solution}

\end{document}
