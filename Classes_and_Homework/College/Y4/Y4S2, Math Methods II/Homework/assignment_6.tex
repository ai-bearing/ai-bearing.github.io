\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright,sfmath,bbold}
%\renewcommand{\mathcal}{\mathtt}

%Euler:
\usepackage{newpxtext,eulerpx,eucal,eufrak}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\renewcommand*{\hbar}{\hslash}

%\renewcommand{\mathbb}{\mathds}
\usepackage{homework}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Assignment 6}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\begin{solution}[29.5]\hfill
  \begin{enumerate}[(a)]
    \item We have
      \begin{align*}
        \left( \vec{w}\cdot \doublevec{T} \right)_{k} &= \sum_{i,j,k}w_{i}T_{jk}\delta_{ij}\\
                                   &= \sum_{i,k}w_iT_{ik},
      \end{align*}
      which is a first-rank tensor.
    \item Since $\vec{w}\cdot \doublevec{T}$ is a first-rank tensor, and we are taking the dot product of two first rank tensors the expression $\vec{w}\cdot \doublevec{T}\cdot \vec{v}$ is a scalar (or rank zero tensor).
    \item We have
      \begin{align*}
        \doublevec{T}\cdot \doublevec{U} &= \left( \sum_{i,j}T_{ij}e_i\otimes e_j \right)\cdot \left( \sum_{k,\ell}U_{k\ell} e_{k}\otimes e_{\ell} \right)\\
                                         &= \sum_{i,j,k,\ell}T_{ij}U_{k \ell} \left( e_k\cdot e_i \right)\left( e_j\cdot e_{\ell} \right),
      \end{align*}
      which is a scalar.
    \item The expression $\doublevec{T}\vec{v}$ expresses the operation of the linear map
      \begin{align*}
        \doublevec{T} &= \sum_{i,j}T_{ij}e_i\otimes e_j
      \end{align*}
      on
      \begin{align*}
        \vec{v} &= \sum_{i}v_ie_i,
      \end{align*}
      meaning that $\doublevec{T}\vec{v}$ is a vector.
    \item The expression $\doublevec{T}\doublevec{U}$ is a composition of two linear maps on $V\otimes V$, so it is a rank $2$ tensor (or another linear map on $V\otimes V^{\ast}$).
  \end{enumerate}
\end{solution}
\begin{solution}[29.7]
  We have $2^{4}$ or $16$ components in $A_{ijkl}$.
\end{solution}
\begin{solution}[29.10]
  We have
  \begin{align*}
    T_{ij} &= \sum_{k,\ell}R_{ik}R_{j\ell}T_{k\ell},
  \end{align*}
  so that
  \begin{align*}
    \sum_{ij}\epsilon_{ij}T_{ij} &= \sum_{i,j,k,\ell}\epsilon_{ij}R_{ik}R_{j\ell}T_{k\ell}\\
                                 &= \sum_{i,j,k,\ell}R_{ik}R_{j\ell}\epsilon_{k\ell}T_{k\ell}\\
                                 &= \sum_{k,\ell}\epsilon_{k\ell}T_{k\ell}.
  \end{align*}
\end{solution}
\begin{solution}[29.11]\hfill
\end{solution}
\begin{enumerate}[(a)]
  \item We may write $T_{ij}$ as $T = \frac{1}{2}\left( T + T^{T} \right) + \frac{1}{2}\left( T - T^{T} \right)$, which are the symmetric and antisymmetric components.
  \item Taking
    \begin{align*}
      S_{ij} &= \sum_{k,\ell}R_{ik}R_{j\ell}S_{k\ell},
    \end{align*}
    we have
    \begin{align*}
      S_{ji} &= \sum_{k,\ell}R_{jk}R_{i\ell}S_{k\ell}\\
             &= \sum_{k,\ell}R_{j\ell}R_{ik}S_{\ell k}\\
             &= \sum_{k,\ell}R_{ik}R_{j\ell}S_{\ell k}\\
             &= \sum_{k,\ell}R_{ik}R_{j \ell}S_{k \ell}\\
             &= S_{ij}.
    \end{align*}
    Similarly,
    \begin{align*}
      A_{ij} &= \sum_{k,\ell}R_{ik}R_{j\ell}A_{k \ell}\\
      A_{ji} &= \sum_{k,\ell}R_{jk}R_{i\ell}A_{k \ell}\\
             &= -\sum_{k,\ell}R_{ik}R_{j\ell}A_{k\ell}\\
             &= -A_{ij}.
    \end{align*}
    In matrix form, we have
    \begin{align*}
      S_{ji} &= S_{ij}^{T}\\
             &= \left( RS_{k\ell} R^{T} \right)^{T}\\
             &= RS_{k\ell}R^{T}.
    \end{align*}
    and similarly,
    \begin{align*}
      -A_{ji} &= \left( A_{ij} \right)^{T}\\
              &= \left( RA_{k\ell}R^{T} \right)^{T}\\
              &= RA_{k\ell}R^{T}.
    \end{align*}
\end{enumerate}
\begin{solution}[29.12]\hfill
  \begin{enumerate}[(a)]
    \item Let $\mathbf{v} = R\mathbf{v}'$, where $R$ is a rotation matrix. Then, we must have
      \begin{align*}
        \pd{}{x}v_x + \pd{}{y}v_y &= \nabla'\cdot \mathbf{v}\\
                                      &= \nabla\cdot R\mathbf{v}'\\
                                 &= \nabla \cdot \begin{pmatrix}\cos\left( \theta \right) & -\sin\theta \\ \sin\left( \theta \right)\cos\left( \theta \right)\end{pmatrix} \begin{pmatrix}v_x'\\v_y'\end{pmatrix}\\
                                 &= \nabla \cdot \begin{pmatrix}v_x'\cos\left( \theta \right) - v_y'\sin\left( \theta \right) \\ v_x'\sin\left( \theta \right) + v_y'\cos\left( \theta \right)\end{pmatrix}\\
                                 &= \pd{}{x}\left( v_x'\cos\left( \theta \right) - v_y'\sin\left( \theta \right) \right) + \pd{}{y}\left( v_x'\sin\left( \theta \right) + v_y'\cos\left( \theta \right) \right)\\
                                 &= \left( \cos\left( \theta \right) \pd{}{x}+\sin\left( \theta \right)\pd{}{y} \right)v_x' + \left( \cos\left( \theta \right) - \sin\left( \theta \right)\pd{}{y} \right)v_y'\\
                                 &= R^{T}\nabla \cdot \mathbf{v}'
      \end{align*}
    \item Let $\mathbf{v} = R\mathbf{v}'$, where $R$ is a rotation matrix. Then, we must have
      \begin{align*}
        \nabla \cdot \mathbf{v} &= \sum_{i} \pd{}{x_i}v_i\\
                                &= \sum_{i,j}\pd{}{x_i}R_{ij}v_j'\\
                                &= \sum_{i,j}R_{ji}\pd{}{x_j}v_j'\\
                                &= \nabla'\cdot \mathbf{v}',
      \end{align*}
      where we were able to switch the indices on $R$ by the fact that it is not a function of $x_i$.
  \end{enumerate}
\end{solution}
\begin{solution}[29.14]
  We have
  \begin{align*}
    \sigma_{ij}' &= \sum_{k,\ell}R_{ik}R_{j\ell}\sigma_{k\ell}\\
                 &= \sum_{k,\ell} R_{ik}R_{j\ell}\left( Y_{k\ell mn}\ve_{mn} \right)\\
                 &= \sum_{k,\ell}R_{ik}R_{j\ell}\left( \sum_{o,p}R_{mo}R_{np}'\epsilon_{op}' \right)\\
                 &= \sum_{k,\ell,o,p}R_{ik}R_{j\ell}R_{mo}R_{np} Y_{k\ell o p}'\epsilon_{op}',
  \end{align*}
  so that
  \begin{align*}
    Y_{ijmn} &= \sum_{k,\ell,o,p}R_{ik}R_{j\ell}R_{mo}R_{np}Y_{k\ell op}',
  \end{align*}
  meaning the elastic modulus is a rank $4$ tensor.
\end{solution}
\begin{solution}[29.23]
  We have
  \begin{align*}
    T_{ij} &= \sum_{k,\ell}R_{i k}R_{j\ell}\left( \frac{1}{3}\tr\left( T \right)\delta_{k \ell} + \frac{1}{2}\left( T_{k \ell} - T_{\ell k}  \right) + \frac{1}{2}\left( T_{k\ell} + T_{\ell k} - \frac{2}{3}\tr\left( T \right)\delta_{ij} \right) \right)\\
           &= \frac{1}{3}\delta_{ij} + \frac{1}{2}\left( T_{ij} - T_{ji} \right) + \frac{1}{2}\left( T_{ij} + T_{ji} - \frac{2}{3}\delta_{ij} \right).
  \end{align*}
\end{solution}
\begin{solution}[29.24]\hfill
  \begin{enumerate}[(a)]
    \item Letting $\mathbf{A},\mathbf{B},\mathbf{C}$ be vectors, we note that the quantity
    \begin{align*}
      \left( \mathbf{A}\times \mathbf{B} \right)\cdot \mathbf{C} &= \sum_{i,j,k,\ell}\epsilon_{ijk}\delta_{k\ell}A_iB_jC_{\ell}
    \end{align*}
    is a scalar quantity, so it is invariant under rotation. In particular, this means that, since $\mathbf{C}$ transforms as a vector, so too does $\mathbf{A}\times \mathbf{B}$.
    \item Writing
      \begin{align*}
        \left( \mathbf{A}\times \mathbf{B} \right)_{k} &= \sum_{i,j}\epsilon_{ijk}A_iB_j,
      \end{align*}
      we may write
      \begin{align*}
        C_{ik} &= \sum_{j}\epsilon_{ijk}\epsilon_{ijk}B_j,
      \end{align*}
      to yield the expression
      \begin{align*}
        \left( \mathbf{A}\times \mathbf{B} \right)_{k} &= \sum_{i}C_{ik}A_i.
      \end{align*}
      The reason we can't expand this beyond three dimensions is that, in four dimensions, we would need a second-rank tensor to be of the form
      \begin{align*}
        C_{i\ell} &= \sum_{i,j}\epsilon_{ijk\ell}B_iD_{k},
      \end{align*}
      so that this second-rank tensor would act on a vector and yield another vector; however, this means the second-rank tensor requires two vectors as an ``input.''
  \end{enumerate}
\end{solution}
\begin{solution}[29.25]\hfill
  \begin{enumerate}[(a)]
    \item We verify this by plugging in the value of $B_k$ to attempt to recover $i,j$.
      \begin{align*}
        T_{ij} &= \sum_{k}\epsilon_{ijk}B_k\\
               &= \sum_{k}\epsilon_{ijk}\left( \frac{1}{2}\sum_{i,j}\epsilon_{kij}T_{ij} \right)\\
               &= \frac{1}{2}\sum_{i,j,k}\epsilon_{ijk}\epsilon_{ijk}T_{ij}\\
               &= \frac{1}{2}\left( 2T_{ij} \right)\\
               &= T_{ij}.
      \end{align*}
    \item Separating $T_{jk} = \frac{1}{2}\left( T_{jk} + T_{kj} \right) + \frac{1}{2}\left( T_{jk} - T_{kj} \right)$, we take
      \begin{align*}
        B_i &= \frac{1}{2}\sum_{j,k} \epsilon_{ijk} \left( \frac{1}{2}\left( T_{jk} + T_{kj} \right) + \frac{1}{2}\left( T_{jk} - T_{kj} \right) \right)\\
            &= \frac{1}{2}\underbrace{\sum_{j,k}\frac{1}{2}\epsilon_{ijk}\left( T_{jk}  + T_{kj} \right) }_{=0}+ \frac{1}{2}\sum_{j,k}\frac{1}{2}\epsilon_{ijk}\left( T_{jk} - T_{kj} \right).
      \end{align*}
    \item We take
      \begin{align*}
        T_{i'j'} &= \sum_{i,j}R_{ii'}R_{jj'}T_{ij}\\
                 &= \sum_{i,j,k}R_{ii'}R_{jj'}\epsilon_{ijk}B_{k}\\
                 &= \sum_{i',j'}\epsilon_{i' j' k}R_{i'k}B_k,
      \end{align*}
      meaning that rotating the dual tensor gives the dual of a rotated vector.
  \end{enumerate}
\end{solution}

\end{document}
