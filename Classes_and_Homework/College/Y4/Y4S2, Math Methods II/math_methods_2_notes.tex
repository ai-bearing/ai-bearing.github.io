\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright,sfmath,bbold}
%\renewcommand{\mathcal}{\mathtt}

%Euler:
\usepackage{newpxtext,eulerpx,eucal,eufrak}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\renewcommand*{\hbar}{\hslash}
\newcommand{\ds}{\displaystyle}
\DeclareMathOperator{\Ln}{Ln}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\Arctan}{Arctan}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\res}{Res}

%\usepackage{homework}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Mathematical Methods II Notes}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\section{Complex Analysis}%
\subsection{Analyticity and Path-Independence in the Complex Plane}%
\subsubsection{Baby's First Complex Function Theory}%
We are interested in functions of the form $f(z)$, where $z = x+iy$ is some complex number. Note that this is specifically different from a function $g\colon \R^2\rightarrow \Omega$ for some domain $\Omega$; in the latter case, we have independent variables $x$ and $y$, while in the former case, we must express $z = x+iy$.\newline

Now, consider a contour integral
\begin{align*}
  \oint_{C}^{} w(z)\:dz &= \oint_{C}w(z)\:\left(dx + idy\right)\\
                        &= \oint_{C}^{} w(z)\:dx + i\oint_{C}w(z)\:dy.
                        \intertext{Taking $A_x = w(z)$ and $A_y = iw(z)$, we have}
                        &= \oint_{C}^{} \mathbf{A}\cdot\:d\vec{\ell}.
                  \intertext{We want to know if this is equal to, by Green's Theorem,}
                        &= \int_{S}^{} \left(\nabla\times \mathbf{A}\right)\:d\mathbf{a},
\end{align*}
and when this integral is zero. Note that $\left(\nabla\times \mathbf{A}\right)\cdot \hat{n} = 0$, so $\pd{A_y}{x} - \pd{A_x}{y} = 0$.\newline

Note that we can take
\begin{align*}
  w(z) &= u(x,y) + iv(x,y),
\end{align*}
where $z = x + iy$.\newline

After a lot of tedious derivation, we get the Cauchy--Riemann equations.
\begin{theorem}[Cauchy--Riemann Equations]
  \begin{align*}
    \pd{u}{x} &= \pd{v}{y}\\
    \pd{v}{x} &= -\pd{u}{y}.
  \end{align*}
\end{theorem}
Furthermore, the Cauchy--Riemann equations guarantee that $w$ is analytic,\footnote{Equal to its Taylor series, also holomorphic.} which leads to Cauchy's theorem.
\begin{theorem}[Cauchy's Theorem]
  If $C$ is a simple closed curve in a simply connected region, then $w$ is analytic if and only if
  \begin{align*}
    \oint_{C}^{} w(z)\:dz &= 0.\label{thm:cauchy_integral_thm}\tag{\textdagger}
  \end{align*}
\end{theorem}
\begin{fact}
The function $w(z)$ is analytic inside the simply connected region $R$ if any of these hold:
\begin{itemize}
  \item $w$ satisfies the Cauchy--Riemann equations;
  \item $w'(z)$ is unique and exists;
  \item $\pd{w}{\overline{z}} = 0$.
  \item $w$ can be expanded as $w(z) = \sum_{n\geq 0}c_n\left( z-a \right)^n$, convergent on some open neighborhood of $a$ for each $a$ on its domain;\footnote{This is technically the real definition of analytic for the case when we're dealing with a function with domain $\R$.}
  \item $w(z)$ is path-independent everywhere in $R$: $\oint_{C}w(z)\:dz = 0$.
\end{itemize}
\end{fact}
\begin{example}
  Considering $w(z) = z$, we have $u=x$ and $v=y$, so it satisfies the Cauchy--Riemann equations. However, neither $\re\left(z\right)$ nor $\im\left(z\right)$ are analytic, and neither is $\overline{z} = x-iy$.
\end{example}
\begin{remark}
Whenever we say ``analytic at $p$,'' we mean ``analytic in a neighborhood of $p$.''
\end{remark}
Note that since $\C$ is a non-compact locally compact Hausdorff space, we may carry out a one-point compactification of $\C$, by adjoining a point $\set{\infty}$, $\C^{\ast} = \C\cup \set{\infty}$. This compactified $\C^{\ast}$ is often represented as a unit sphere with the north pole, determined by $\left(0,0,1\right)$, is the point at infinity. The correspondence between $\C^{\ast}\setminus\set{\infty}$ and $\C$ is evaluated via stereographic projection.\newline

We define $\frac{z}{\infty} = 0$ and $\frac{z}{0} = \infty$ for any $z\neq 0,\infty$. The correspondence between $z = x + iy$ in the plane to $Z$ on the Riemann sphere with $\R^3$ coordinates $\left(\xi_1,\xi_2,\xi_3\right)$ is
\begin{align*}
  \xi_1 &= \frac{2\re(z)}{\left\vert z \right\vert^2 + 1}\\
  \xi_2 &= \frac{2\im\left(z\right)}{\left\vert z \right\vert^2 + 1}\\
  \xi_3 &= \frac{\left\vert z \right\vert^2 - 1}{\left\vert z \right\vert^2 + 1}.
\end{align*}
Inverting, we may find
\begin{align*}
  x &= \frac{\xi_1}{1-\xi_3}\\
  y &= \frac{\xi_2}{1-\xi_3},
\end{align*}
and with polar coordinates,
\begin{align*}
  z &= \cot\left(\theta/2\right)e^{i\phi}.
\end{align*}

\begin{center}
  \includegraphics[width=7cm]{images/riemann_sphere.pdf}
\end{center}
To determine analyticity at $\infty$, we set $\zeta = \frac{1}{z}$, and analyze the analyticity of $\tilde{w}\left(\zeta\right) = w\left(1/z\right)$ at $0$.
\subsubsection{Cauchy's Integral Formula}%
Consider the function $w(z) = c/z$, integrated around a circle of radius $R$. Then, writing $z = Re^{i\varphi}$, we get
\begin{align*}
  \oint_{\Gamma}w(z)\:dz &= C\int_{0}^{2\pi} \frac{e^{-i\varphi}}{R}\underbrace{iRe^{i\varphi}\:d\varphi}_{dz}\\
                         &= ic\int_{0}^{2\pi} \:d\varphi\\
                         &= 2\pi i c.
\end{align*}
If our contour $C$ runs around our singularity at $z = 0$ a total of $n$ times, then we pick up a factor of $n$.\newline

Now, when we consider
\begin{align*}
  I &= \oint_{C}\frac{dz}{z^n},
\end{align*}
this integral actually yields $0$ for any $n\neq 1$, despite the fact that $0$ is a singularity for $f(z) = \frac{1}{z^n}$. This $0$ is not a reflection of \hyperref[thm:cauchy_integral_thm]{Cauchy's integral theorem}, but of the fact that
\begin{align*}
  z^{-n} &= \frac{d}{dz}\left(\frac{z^{-n+1}}{n+1}\right),
\end{align*}
meaning that $z^{-n}$ is an exact differential, so integrating along a closed curve yields zero change. However, $\frac{1}{z} = \diff{}{z}\left(\ln z\right)$ may be an exact differential, but for complex $z$, $\ln z = \ln\left\vert z \right\vert + i\arg(z) = \ln r + i\varphi$. This yields
\begin{align*}
  \oint_{C} \frac{c}{z}dz &= c\oint_{C} d\left(\ln z\right)\\
                          &= c\left(i\left(\varphi + 2\pi\right) - \varphi\right)\\
                          &= 2\pi i c.
\end{align*}
Ultimately, what this shows is that when we integrate any analytic function $f\left(\zeta\right)$ along a closed contour with a singularity at $z$, only the coefficient on $\frac{1}{\zeta -z}$ will remain. This coefficient is known as the residue at $0$.
\begin{theorem}[Cauchy's Integral Formula]
  If $w$ is analytic in a simply connected region and $C$ is a closed contour winding once around a point $z$ in the region, then
  \begin{align*}
    w(z) &= \frac{1}{2\pi i} \oint_{C}\frac{w\left(\zeta\right)}{\zeta - z}\:d\zeta.\label{thm:cauchy_integral_formula}\tag{\textasteriskcentered\textasteriskcentered}
  \end{align*}
\end{theorem}
Furthermore, this shows that any once-differentiable function is infinitely differentiable, as by differentiating under the integral sign, we get
\begin{align*}
  \diff{^nw}{z^n} &= \frac{n!}{2\pi i} \oint_{C} \frac{w\left(\zeta\right)}{\left(\zeta - z\right)^{n+1}}\: d\zeta.
\end{align*}
\begin{example}[Deriving Liouville's Theorem]
  Consider a circle $C$ centered at radius $r$ centered at at $z$, $\zeta - z = Re^{i\varphi}$. We take $d\zeta = iRe^{i\varphi}\:d\varphi$, and taking derivatives, we have
  \begin{align*}
    w'(z) &= \frac{1}{2\pi R} \int_{0}^{2\pi} w\left(z + Re^{i\varphi}\right)e^{-i\varphi}\:d\varphi.
  \end{align*}
  If $w$ is bounded --- i.e., $\left\vert w(z) \right\vert \leq M$ for all $z$ in a given region --- then
  \begin{align*}
    \left\vert w'(z) \right\vert &= \left\vert \frac{1}{2\pi R} \int_{0}^{2\pi} w\left(z + Re^{i\varphi}\right)e^{-i\varphi}\:d\varphi \right\vert\\
                                 &\leq \frac{1}{2\pi R} \int_{0}^{2\pi} \left\vert w\left(z + Re^{i\varphi}\right) \right\vert\:d\varphi\\
                                 &\leq \frac{M}{R}
  \end{align*}
  for all $R$ within the analytic region.\newline

  In the case where $w$ is entire (i.e., analytic on $\C$), then this inequality holds for all $R\rightarrow\infty$. Thus, $\left\vert w'(z) \right\vert = 0$ for all $z$, meaning that $w$ is constant.\newline

  This is known as Liouville's theorem --- every bounded entire function is constant. This can be used to prove the fundamental theorem of algebra.\newline

  What Liouville's theorem tells us is that any nontrivial behavior will emerge from a function's singularities.
\end{example}
\subsection{Singularities and Branches}%
To understand nontrivial behavior on the complex plane, we need to understand singularities. This will require us to develop understanding of Laurent series.
\subsubsection{Taylor Series}%
We want to integrate $w(z)$ around some point $a$ in an analytic region of $w(z)$. This yields the form
\begin{align*}
  w(z) &= \frac{1}{2\pi i} \oint_{C}^{} \frac{w\left(\zeta\right)}{\zeta - z}\:d\zeta\\
       &= \frac{1}{2\pi i} \oint_{C} \frac{w\left(\zeta\right)}{\left(\zeta - a\right)-\left(z-a\right)}\:d\zeta\\
       &= \frac{1}{2\pi i} \oint_{C} \frac{w\left(\zeta\right)}{\left(\zeta - a\right)\left(1 - \frac{z-a}{\zeta - a}\right)}\:d\zeta.\label{step:geometric_expansion}\tag{\textdaggerdbl}
       \intertext{Since $\zeta$ is on the contour and $z$ is in the contour, $\left\vert \frac{z-a}{\zeta - a} \right\vert < 1$, we may expand as a geometric series. Thus, we get}
       &= \frac{1}{2\pi i} \oint_{C} \frac{w\left(\zeta\right)}{\left(\zeta - a\right)}\left(\sum_{n=0}^{\infty}\left(\frac{z-a}{\zeta - a}\right)^{n}\right)\:d\zeta.
       \intertext{Since the series is uniformly convergent, we are allowed to exchange sum and integral, yielding}
       &= \sum_{n=0}^{\infty}\underbrace{\left(\frac{1}{2\pi i}\oint_{C}\frac{w\left(\zeta\right)}{\left(\zeta - a\right)^{n+1}}\:d\zeta\right)}_{=c_n} \left(z-a\right)^n\\
       &= \sum_{n=0}^{\infty}c_n\left(z-a\right)^n,
       \intertext{where}
  c_n &= \frac{1}{n!} \left.\diff{^nw}{z^n}\right|_{z=a}.
\end{align*}
If our Taylor series reduces to a known series on the real axis, we find this very desirable. We say this is a type of analytic continuation from the real axis to the complex plane. For example,
\begin{align*}
  e^z &= \sum_{k=0}^{\infty}\frac{z^k}{k!}
\end{align*}
is an analytic continuation of $e^x$.\newline

However, more interestingly,
\begin{align*}
  \zeta\left(s\right) &= \sum_{k=1}^{\infty}\frac{1}{k^s}
\end{align*}
converges for all $s > 1$. However, we have also shown that
\begin{align*}
  \zeta\left(s\right) &= \frac{1}{\Gamma(s)}\int_{0}^{\infty} \frac{x^{s-1}}{e^{x}-1}\:dx
\end{align*}
converges for complex $s$ for all real part greater than $1$. Since values of this integral agree with the series representation of $\zeta(s)$ on real axis, we have that this is an analytic continuation of $\zeta(s)$ to the subset of $\C$ defined by $\re(s) > 1$.
\subsubsection{Laurent Series}%
Now, what happens if, at \eqref{step:geometric_expansion}, we have $\left\vert \frac{z-a}{\zeta - a} \right\vert > 1$. The series as constructed would not converge, but what if we have a series that converges everywhere \textit{outside} $C$? This would entail an expansion in reciprocal integer powers of $z-a$. This yields
\begin{align*}
  w(z) &= -\frac{1}{2\pi i} \oint_{C}\frac{w\left(\zeta\right)}{\left(z-a\right)\left(1-\frac{\zeta - a}{z-a}\right)}\:d\zeta\\
       &= -\frac{1}{2\pi i} \oint_{C} \frac{w\left(\zeta\right)}{z-a}\left(\sum_{n=0}^{\infty}\left(\frac{\zeta - a}{z-a}\right)^n\right)\:d\zeta\\
       &= -\sum_{n=0}^{\infty}\left(\frac{1}{2\pi i}\oint_{C}w\left(\zeta - a\right)^n\:d\zeta\right)\frac{1}{\left(z-a\right)^{n+1}}\\
       &= \sum_{n=1}^{\infty}\underbrace{\left(-\frac{1}{2\pi i}\oint_{C}w\left(\zeta - a\right)^{n-1}\:d\zeta\right)}_{=c_{-n}}\frac{1}{\left(z-a\right)^{n}}\\
       &= \sum_{n=1}^{\infty}\frac{c_{-n}}{\left(z-a\right)^n}
\end{align*}
Note that this series has a singularity at $z = a$, but since our series is only defined outside a particular region, that doesn't matter. We call a series in reciprocal powers a Laurent series. More specifically, Laurent series may include expansions in negative powers as well as positive powers.
\begin{example}[Annuli]
  If we have a point $a$, we want to surround $a$ by a special contour to apply Cauchy's integral formula.\newline

  In particular, for any $z$ in the annulus, we get
  \begin{align*}
    w(z) &= \frac{1}{2\pi i} \oint_{c_1 - c_2}^{} \frac{w\left( \zeta \right)}{\zeta - z}\:d\zeta\\
         &= \frac{1}{2\pi i} \oint_{c_1}^{} \frac{w\left( \zeta \right)}{\zeta - z}\:d\zeta - \frac{1}{2\pi i}\oint_{c_2}^{} \frac{w\left( \zeta \right)}{\zeta - z}\:d\zeta\\
         &= \sum_{n=-\infty}^{\infty}c_n\left( z-a \right)^n\\
         &= c_0 + \sum_{n=1}^{\infty}\left( c_{-n}\left( z-a \right)^n + c_n\left( z-a \right)^n \right).
  \end{align*}
\end{example}
\begin{example}
  Consider the function
  \begin{align*}
    w(z) &= \frac{1}{z^2 + z - 2}\\
         &= \frac{1}{\left( z- 1\right)\left( z+2 \right)}\\
         &= \frac{1}{3}\left( \frac{1}{z-1} - \frac{1}{z+2} \right).
  \end{align*}
  Now, we have three regions to expand $w$ in.
  \begin{itemize}
    \item If $\left\vert z \right\vert < 1$, then our series is in both $\displaystyle z^n$ and $\displaystyle z^n$.
    \item If $1 < \left\vert z \right\vert < 2$, then one of our series is going to in $\displaystyle \frac{1}{z^{n}}$ and one is in $\displaystyle z^n$.
    \item If $\left\vert z \right\vert > 2$, then both of our series are in the form of $\displaystyle \frac{1}{z^n}$ and $\displaystyle \frac{1}{z^n}$
  \end{itemize}
  Via tedious, heavily error-prone calculations, we find that
  \begin{align*}
    w_1(z) &= -\frac{1}{3}\sum_{n=0}^{\infty}\left( 1 + \left( -1 \right)^n \left( \frac{1}{2} \right)^{n+1} \right)z^n\\
    w_2(z) &= \frac{1}{3}\sum_{n=0}^{\infty}\left( \frac{1}{z^{n+1}} + \left( -\frac{1}{2} \right)^{n+1}z^n \right)\\
    w_3(z) &= \frac{1}{3} \sum_{n=0}^{\infty} \left( 1-\left( -2 \right)^n \right)\frac{1}{z^{n+1}}.
  \end{align*}
  Sewing all of $w_1,w_2,w_3$ together, then we get a full series representation of $w(z)$.
\end{example}
\begin{definition}
  If $w(z)$ is a function that can be written as $w(z) = \left( z-a \right)^ng(a)$, where $g(a)\neq 0$, then we say $w$ has an $n$-th order zero at $z=a$. If $n = 1$, then we say $w$ has a simple zero at $a$.\newline

  Similarly, if we can write
  \begin{align*}
    w(z) &= \frac{g(a)}{\left( z-a \right)^n}
  \end{align*}
  with $g(a)\neq 0$, then we say $w$ has a pole of order $n$ at $a$. If $n = 1$, then we say $w$ has a simple pole at $a$.
\end{definition}
There are three types of isolated singularities (i.e., isolated points where $w(z)$ is not defined).
\begin{definition}
  Let $w$ be an analytic function with isolated singularity at $a$.
  \begin{itemize}
    \item If $w$ remains bounded in any neighborhood of $a$, then it must be the case that $c_{-n} = 0$ for all $n > 1$, so the Laurent series is a pure Taylor expansion. We say $z = a$ is a removable singularity.\newline

      For instance, the function
      \begin{align*}
        \frac{\sin\left( z-a \right)}{z-a} &= \sum_{n=0}^{\infty}\left( -1 \right)^n \frac{\left( z-a \right)^{2n}}{\left( 2n+1 \right)!}
      \end{align*}
      has a removable singularity at $z=a$.
    \item If not all the $c_{-n}$ are equal to zero, but there is a largest $n > 0$ such that $c_{-n}$ is in the Laurent series expansion, then we say $a$ is an $n$-th order pole. If $n = 1$, we say $a$ is a simple pole.
    \item If there is no largest value of $n$ such that $c_{-n}$ is in the Laurent series --- i.e., that $c_{-n}\neq 0$ for all $n$ --- then we say that $a$ is an essential singularity.\newline

      One of the most important facts about an essential singularity is that the behavior is path dependent. For instance,
      \begin{align*}
        e^{1/z} &= \sum_{n=0}^{\infty}\frac{1}{n!z^n}
      \end{align*}
      has an essential singularity at $z = 0$. We see that $e^{1/z}$ diverges as $z \rightarrow 0$ along the positive real axis, but if $z\to 0$ along the negative real axis, we get $e^{1/z}\to 0$.
  \end{itemize}
  Singularities can also occur at $\infty$, which occurs when $w(1/z)$ has a singularity at $0$.
\end{definition}
\subsubsection{Multivalued Function}%
Consider the function
\begin{align*}
  w(z) &= z^2\\
       &= \underbrace{\left( x^2 - y^2 \right)}_{u(x,y)} + i\underbrace{\left( 2xy \right)}_{v(x,y)}\\
       &= r^2e^{2i\varphi}.
\end{align*}
Note that if we take a path around the origin going around by an angle of $2\pi$, then the resulting path goes around twice. Note that this means the lines $\varphi$ and $\varphi + \pi$ map to the same point in the $w$ plane.\newline

This isn't such a big deal in and of itself, but if we take $w(z) = z^{1/2}$, we get an issue. Instead of $w$ being a two-to-one function, we now have $w$ is a one-to-two function. This is an implicit problem in $\R$ with the function $w(x) =\sqrt{x}$, which we resolve by taking the ``positive'' square root. This is known as choosing a branch.\newline

We have to do something similar in the complex plane. Note that if we go around by an angle of $2\pi$ in the $z$ plane, then we only go around by an angle of $\pi$ in the $w$-plane. As we keep going around the plane, we jump from branch to branch, which brings issues of continuity.\newline

To resolve this, we create a ``branch cut'' that contours are not allowed to cross. 
\begin{example}
  The most common branch cut is to start from the branch point at $z = 0$, in the case of $w(z) = z^{1/2}$ or $w(z) = \ln(z)$, and extend along the real axis, meaning our branch cut is $\left( -\infty,0 \right]$.\newline

  This principal branch restricts \textit{output} values of $\varphi$ to $-\pi < \varphi \leq \pi$.\newline

  For instance, above the cut, we have $\varphi = \pi$, and below the branch cut, we have $\varphi = -\pi$, meaning we have
  \begin{align*}
    \sqrt{z} &= \sqrt{r}e^{i\pi /2}\tag*{$\varphi \to \pi$}\\
             &= i\sqrt{r}\\
    \sqrt{z} &= \sqrt{r}e^{-i\pi/2}\tag*{$\varphi \to -\pi$}\\
             &= -i\sqrt{r}.
  \end{align*}
  This is why the branch cut ``causes'' a discontinuity across the branch, but in $\C\setminus (-\infty,0]$.\newline

  Now, if we have
  \begin{align*}
    \sqrt{z_1}\sqrt{z_2} &= \left( r_1e^{i\varphi_1} \right)^{1/2}\left( r_2e^{i\varphi_2} \right)^{1/2}\\
                         &= \sqrt{r_1r_2}e^{i\left( \varphi_1 + \varphi_2 \right)/2}.
  \end{align*}
  However, if we want to calculate $\sqrt{z_1z_2}$, and if $\left\vert \varphi_1 + \varphi_2 \right\vert > \pi$ < then our product $z_1z_2$ crosses the branch cut, and our discontinuity requires $\varphi_1 + \varphi_2 $ to be converted to $\varphi_1 + \varphi_2 \pm 2\pi$ so as to bring the angle sum back into the principal branch. This means we have
  \begin{align*}
    \sqrt{z_1z_2} &= \left( r_1r_2e^{i\left( \varphi_1 + \varphi_2 \right)/2} \right)\\
                  &= \begin{cases}
                    \sqrt{r_1r_2}e^{i\left( \varphi_1 + \varphi_2 \right)/2} & \left\vert \varphi_1 + \varphi_2 \right\vert \leq \pi\\
                    -\sqrt{r_1r_2}e^{i\left( \varphi_1+\varphi_2 \right)/2} & \left\vert \varphi_1 + \varphi_2 \right\vert > \pi
                  \end{cases}.
  \end{align*}
\end{example}
\begin{example}
  Now, if we have $z_1 = 2e^{i\left( 3\pi/4 \right)}$ and $z_2 = e^{i\left( \pi/2 \right)}$, then we have
  \begin{align*}
    \sqrt{z_1} &= \sqrt{2}e^{i3\left( \pi/8 \right)}\\
    \sqrt{z_2} &= e^{i\left( \pi/4 \right)}.
  \end{align*}
  Note that if we take $\sqrt{z_1z_2}$, then the argument of $z_1z_2$ is $5\pi/4$, so we have to change our argument to $-3\pi/4$ to return to the principal branch before we may calculate the square root. This gives
  \begin{align*}
    \sqrt{z_1z_2} &= \sqrt{2e^{-i\left( 3\pi/4 \right)}}\\
                  &= \sqrt{2}e^{-i\pi + i\left( 5\pi/8 \right)}\\
                  &= -\sqrt{2}e^{i\left( 5\pi/8 \right)}\\
                  &= -\sqrt{z_1}\sqrt{z_2}.
  \end{align*}
\end{example}
Now, it is possible to have a branch point at $\infty$, by determining if $w\left( \frac{1}{z} \right)$ has a branch point at zero. For instance, if $w = z^{1/2}$, this gives
\begin{align*}
  w\left( \frac{1}{z} \right) &= \frac{1}{\zeta^{1/2}}\\
                              &= \frac{1}{\sqrt{r}} e^{-i\varphi/2},
\end{align*}
which has the multivalued behavior around the origin. Thus, $z = \infty$ is a branch point for $z$, and we consider the $(-\infty,0]$ branch cut that connects the branch points at $0$ and $\infty$.
\begin{example}
  Consider
  \begin{align*}
    w(z) &= \sqrt{\left( z-a \right)\left( z-b \right)}.
  \end{align*}
  where $a,b\in\R$ with $a < b$. We expect the only finite branch points to be $a$ and $b$. Introducing polar coordinates, we have
  \begin{align*}
    r_1e^{i\varphi_1} &= z-a\\
    r_2e^{i\varphi_2} &= z-b,
  \end{align*}
  giving
  \begin{align*}
    w(z) &= \sqrt{r_1r_2}e^{i\varphi_1}e^{i\varphi_2}.
  \end{align*}
  Closed contours around \textit{either} $a$ or $b$ are double-valued. However, if our closed contour goes around \textit{both} $a$ and $b$, then both $\varphi_1$ and $\varphi_2$ add up to $2\pi$, meaning we don't have the multivalued behavior.\newline

  Now, to select our branch cut, we need to find out if the point at infinity is a branch point. We take $\zeta = \frac{1}{z}$, and we have
  \begin{align*}
    w(\zeta) &= \frac{1}{\zeta} \sqrt{\left( 1-a\zeta \right)\left( 1-b\zeta \right)},
  \end{align*}
  which blows up at $\infty$, but only takes a singular value.\footnote{Alternatively, we may see that a positively-oriented contour that surrounds both $a$ and $b$ is a negatively-oriented contour around $\infty$. Since such a contour is valid, $\infty$ is not a branch point.}
\end{example}
In general, $z^{1/m}$ for integral $m$ will require $m$ branch cuts.
\begin{example}
  Consider the integral
  \begin{align*}
    I &= \int_{-\infty}^{\infty} \frac{xe^{ikx}}{\sqrt{x^2 + a^2}}\:dx.
  \end{align*}
  This is a hard integral to evaluate. To resolve this, we extend the integrand to the complex plane, and invoke Cauchy's theorem to deform the contour.\newline

  Note that $\sqrt{x^2 + a^2}$ is multivalued, with branch points at $x = \pm ia$. We choose the branch cut such that our integration contour does not cross the branch cut --- i.e., from $-ia$ to $\infty$ to $ia$.\newline

  Now, we may deform the contour so as to closely wrap around the branch cut from $ia$ to $\infty$. Remembering the sign discontinuity over the branch cut, this gives the integral
  \begin{align*}
    \int_{i\infty}^{i\infty} \frac{ze^{ikz}}{\sqrt{x^2 + a^2}}\:dz &= \int_{i\infty}^{ia} \frac{ze^{ikz}}{-i\sqrt{x^2 + a^2}}\:dz + \int_{-a}^{\infty} \frac{ze^{ikz}}{i\sqrt{z^2 + a^2}}\:dz\\
                                                                   &= 2\int_{ia}^{i\infty} \frac{ze^{ikz}}{i\sqrt{z^2 + a^2}}\:dz\\
                                                                   &= 2\int_{a}^{\infty} \frac{ye^{-ky}}{\sqrt{y^2 - a^2}}\:dy\tag*{$z = iy$}\\
                                                                   &= 2aK_1\left( ka \right)\\
                                                                   &\sim e^{-ka}
  \end{align*}
  Here, $K_1$ refers to the modified Bessel function.
\end{example}
\subsubsection{Logarithms}%
In the complex plane, we say
\begin{align*}
  \ln z &= \ln\left(re^{i\varphi} \right)\\
        &= \ln r + i\varphi\\
        &= \ln \left\vert z \right\vert + i\arg(z).
\end{align*}
Unfortunately, this $\ln z$ is a multivalued function --- a very multivalued one indeed. This yields many branch points, including $0$ and $\infty$:
\begin{align*}
  \ln\left( 1/\zeta \right) &= -\ln\left( \zeta \right).
\end{align*}
However, we choose the principal branch, $\pi < \varphi \leq \pi$, giving
\begin{align*}
  \Ln z &= \Ln \left\vert z \right\vert + i\Arg(z).
\end{align*}
\begin{example}
  Consider $\ln\left( z_1z_1 \right)$ and $\Ln\left( z_1z_2 \right)$. If we have
  \begin{align*}
    z_1 &= 1 + i\\
    z_2 &= i,
  \end{align*}
  then
  \begin{align*}
    \arg\left(z_1\right) &= \pi/4\\
    \arg\left( z_2 \right) &= \pi/2,
  \end{align*}
  so
  \begin{align*}
    \arg\left( z_1z_2 \right) &= 3\pi/4\\
                              &= \arg\left(z_1\right) + \arg\left( z_2 \right)\\
                              &= \Arg\left( z_1z_2 \right).
  \end{align*}
  However, if $z_1 = z_2 = -1$, then
  \begin{align*}
    \arg\left( z_1z_2 \right) &= \arg\left( z_1 \right) + \arg\left( z_2 \right)\\
                              &= 2\pi\\
    \Arg\left( z_1z_2 \right) &= \Arg\left( 1 \right)\\
                              &= 0.
  \end{align*}
  Thus, we get that $\Ln\left( z_1z_2 \right)\neq \Ln\left( z_1 \right) + \Ln\left( z_2 \right)$.
\end{example}
\begin{example}[Logarithms vs Inverse Trig]
  Here, we will derive $\arctan(z)$ in terms of the complex logarithm.\newline

  Recall that
  \begin{align*}
    \cos\left( z \right) &= \frac{1}{2}\left( e^{iz} + e^{-iz} \right)\\
    \sin\left( z \right) &= \frac{1}{2i} \left( e^{iz} - e^{-iz} \right),
  \end{align*}
  so we have
  \begin{align*}
    z &= \tan\left( w \right)\\
      &= -i \frac{e^{iw} - e^{-iw}}{e^{iw} + e^{-iw}},
  \end{align*}
  which after much tedious, error-prone symbolic manipulation, gives
  \begin{align*}
    e^{2iw} &= \frac{i-z}{i+z}.
  \end{align*}
  Thus, we have
  \begin{align*}
    w &= \arctan\left( z \right)\\
      &= \frac{1}{2i}\ln\left( \frac{i-z}{i+z} \right).
  \end{align*}
  Note that since $\ln$ has branch points at $0$ and $\infty$, $\ln\left( \frac{i-z}{i+z} \right)$ has branch points when $z = \pm i$.\newline

  Now, we must choose a branch cut. Specifically, we want our branch cut to continue the real $\arctan(x)$. We dub this $\Arctan(x)$. Along the real axis, we have
  \begin{align*}
    \Arctan(x) &= \frac{1}{2i} \Ln\left( \frac{i-x}{i+x} \right)\\
               &= \frac{1}{2i} \left( \Ln\left\vert \frac{i-x}{i+x} \right\vert + i\Arg\left( \frac{i-x}{i+x} \right) \right)\\
               &= \frac{1}{2} \Arg\left( \frac{i-x}{i+x} \right).
  \end{align*}
  The principal values are from $-\pi$ to $\pi$, so the output of $\Arctan(x)$ ranges from $-\pi/2$ to $\pi/2$.
\end{example}
\subsection{Conformal Maps}%
A conformal map is a special type of map $w\colon \C\rightarrow \C$ that ``preserves angles.'' If, in $z$, we map curves whose intersections are at some angle $\varphi$, then the image of those curves also intersect at the angle $\varphi$.
\begin{example}[Our First Conformal Map]
  Consider the map
  \begin{align*}
    w(z) &= z^2\\
         &= \left( x^2 - y^2 \right) + i\left( 2xy \right)\\
         &= u\left( x,y \right) + iv\left( x,y \right).
  \end{align*}
  Examining the line elements in the $z$ and $w$ planes, we have
  \begin{align*}
    ds^2 &= du^2 + dv^2\\
         &= \left( \pd{u}{x}dx + \pd{u}{y}dy \right)^2 + \left( \pd{v}{x}dx + \pd{v}{y}dy \right)^2\\
         &= \left( \pd{u}{x}dx - \pd{v}{x}dy \right)^2 + \left( \pd{v}{x}dx + \pd{u}{x}dy \right)^2\\
         &= \left( \left( \pd{u}{x} \right)^2 + \left( \pd{v}{x} \right)^2 \right) \left( dx^2 + dy^2 \right)\\
         &= \left( \left( \pd{u}{y} \right)^2 + \left( \pd{v}{y} \right)^2 \right) \left( dx^2 + dy^2 \right)\\
         &= 4\left( x^2 + y^2 \right)\left( dx^2 + dy^2 \right)
  \end{align*}
  Note that $dx^2$ and $dy^2$ have identical scale factors. Since angles are determined by the ratio of $dx$ and $dy$, it is the case that \textit{all} angles are preserved. This is what is meant by a conformal map.
\end{example}
\begin{example}[Analyticity and Conformality]
Consider an analytic function $w(z)$, with its Taylor expansion about $z_0$.
\begin{align*}
  w(z) &= w\left( z_0 \right) + w'\left( z_0 \right)\left( z-z_0 \right) + \cdots.
\end{align*}
For a very small $\xi = z-z_0$, we may truncate it into first order, and place into polar form
\begin{align*}
  w(z) - w\left(z_0\right) &= w'\left(z_0\right)\xi\\
                           &= \left\vert w'\left( z_0 \right) \right\vert e^{i\alpha_0}\xi.
\end{align*}
Moving from $z$ to $w$, we get a magnification (or shrinkage) by $\left\vert w'\left( z_0 \right) \right\vert$ and a rotation by $\alpha_0$.\newline

Since, close to $z_0$, $\xi_1 = z_1 - z_0$ and $\xi_2 = z_2 - z_0$ are magnified by (effectively) the same amount, and rotated by (effectively) the same amount, conformality is established.
\end{example}
\begin{definition}
  A conformal map is an analytic function $w(z) $ defined on a domain $\Omega$ such that $w'\left(z_0\right) \neq 0$ for all $z_0\in \Omega$.
\end{definition}
\begin{example}[Möbius Transformations]
  A Möbius transformation is a fractional linear transformation of the form
  \begin{align*}
    w(z) &= \frac{az + b}{cz + d},
  \end{align*}
  where $ad - bc \neq 0$. We can calculate $w'(z)$ to be
  \begin{align*}
    w'(z) &= \frac{ad-bc}{\left( cz + d \right)^2}.
  \end{align*}
  Since $w(z)$ is conformal, it is invertible, so
  \begin{align*}
    w^{-1}(z) &= z(w)\\
              &= \frac{dw-b}{-cw + a}.
  \end{align*}
  The Möbius transformations include $\infty$, as we have $w(\infty) = \frac{a}{c}$, meaning that it is an automorphism of the Riemann sphere. Note that because of the constraint, we only need three numbers to specify a Möbius transformation.\newline

  Consider the Möbius transformation
  \begin{align*}
    w(z) &= \frac{z-i}{z+i}.
  \end{align*}
  We let $z_1 = -1$, $z_2 = 1$, and $z_3 = \infty$. Then, we have
  \begin{align*}
    w\left(z_2\right) &= \frac{-1-i}{-1+i}\\
         &= \frac{2i}{2}\\
         &= i.
  \end{align*}
  Similarly, this gives $w\left( z_3 \right) = 1$. After a bit more playing, we can find that this is a map of the (closed) upper half-plane to the (closed) unit disk, $\mathbb{D}$.\newline

  Now, if we look at the ``ribbon'' between the real axis and the line $\im(z) = i$, we see that it maps to the region
  \begin{align*}
    S &= \mathbb{D}\setminus \set{z | \left\vert z-\frac{1}{2} \right\vert \leq \frac{1}{2}}.
  \end{align*}
\end{example}
\begin{example}
  Consider the map $w(z) = e^{z}$. This gives
  \begin{align*}
    w(z) &= e^{x}e^{iy}\\
         &= \rho e^{i\beta}.
  \end{align*}
  This sends curves of constant $y$ to curves of constant argument, and maps curves of constant $x$ to circles of constant radius.
\end{example}
\subsubsection{Complex Potentials}%
Consider the analytic function
\begin{align*}
  \Omega(z) &= \Phi\left( x,y \right) + i\Psi\left( x,y \right).
\end{align*}
We know that
\begin{align*}
  \pd{\Phi}{x} &= \pd{\Psi}{y}\\
  \pd{\Phi}{y} &= -\pd{\Psi}{x}.
\end{align*}
Thus, we separate to get
\begin{align*}
  \pd{^2\Phi}{x^2} &= \pd{}{x}\pd{\Psi}{y}\\
                   &= \pd{}{y}\pd{\Psi}{x}\\
                   &= -\pd{^2\Phi}{y^2},
\end{align*}
so
\begin{align*}
  \nabla^2\Phi &= 0\\
  \nabla^2\Psi &= 0.
\end{align*}
The converse is also true --- if there is some real harmonic function $\Phi\left( x,y \right)$, there is a conjugate harmonic function $\Psi\left( x,y \right)$ such that $\Omega\left( z \right) = \Phi\left( x,y \right) + i\Psi\left( x,y \right)$ is analytic.\newline

If $\Omega$ is analytic, then $\Phi$ and $\Psi$ must satisfy the Cauchy--Riemann equations, meaning that
\begin{align*}
  \Psi\left( x,y \right) &= \int_{}^{} \pd{\Psi}{y}\:dy + \pd{\Psi}{x}\:dx\\
                         &= \int_{}^{} \pd{\Phi}{x}\:dy - \pd{\Phi}{x}\:dx.
\end{align*}
For $\Psi$ to be a proper single-valued real function, the integral must be path-independent. Using Green's theorem, we may close the path in a simply connected region, and consider it as a surface integral. This gives
\begin{align*}
  \oint_{C} \pd{\Phi}{x}\:dy - \pd{\Phi}{y}\:dx &= \int_{S}^{} \left( \pd{}{x}\left( \pd{\Phi}{x} \right) - \pd{}{y}\left( -\pd{\Phi}{y} \right) \right)\:dxdy\\
                                                &= \int_{x}^{} \left( \pd{^2\Phi}{x^2} + \pd{^2\Phi}{y^2} \right)\:dxdy\\
                                                &= 0.
\end{align*}
We call $\Omega(z) = \Phi\left( x,y \right) + i\Psi\left( x,y \right)$ the complex potential.\newline

This gives
\begin{align*}
  \diff{\Omega}{z} &= \pd{\Phi}{x} + i \pd{\Psi}{x}\\
                   &= \pd{\Phi}{x} - i\pd{\Phi}{y}\\
                   &= \pd{\Psi}{y} + i \pd{\Psi}{x}.\\
                   &= \overline{\mathcal{E}},
\end{align*}
where $\mathcal{E}$ is the complex representation of the electric field, $\mathbf{E}$. We have
\begin{align*}
  \mathcal{E} &= \overline{\pd{\Omega}{z}}\\
              &= \pd{\Phi}{x} + i\pd{\Phi}{y},
\end{align*}
with
\begin{align*}
  E &= \left\vert \diff{\Omega}{z} \right\vert.
\end{align*}
The physics of electric fields is then determined entirely by the complex potential.\newline

What makes harmonic functions useful is that, if there are complicated boundary conditions, we may apply a conformal map and the functions remain harmonic.
\begin{example}[Cylindrical Capacitor]
  Consider a cylindrical capacitor with nonconcentric plates meeting at insulated point $u = 1$ and $v = 0$. The larger cylinder with radius $1$ is grounded, and the smaller cylinder with radius $1/2$ is held at voltage $V_0$. We want to find the electric field.\newline

  We want to find $\widetilde{\Phi}(w)$ such that
  \begin{align*}
    \nabla^2\widetilde{\Phi}\left( u,v \right) &= 0.
  \end{align*}
  This domain is kind of difficult, so we will solve the problem on a simpler domain and use a conformal map. Note that from Figure 20.4 in the book, we may use the Möbius transformation
  \begin{align*}
    w(z) &= \frac{z-i}{z+i}
  \end{align*}
  to transform \textit{to} our cylindrical capacitor \textit{from} a two-plate infinite capacitor with one plate at $\im\left( z \right) = 1$ and one plate at $\im(z) = 0$. From physics, we know that $\Phi\left( x,y \right) = \frac{V_0y}{d}$, where $d = 1$. Thus, the harmonic conjugate, $\Psi = -V_0 x$, gives us a complex potential of $\Phi = -iV_0 z$.\newline
  
  Solving
  \begin{align*}
    \frac{z-i}{z+i} &= u\left( x,y \right) + iv\left( x,y \right),
  \end{align*}
  we find
  \begin{align*}
    x\left( u,v \right) &= -\frac{2v}{\left( 1-u \right)^2 + v^2}\\
    y\left( u,v \right) &= \frac{1-u^2-v^2}{\left( 1-u \right)^2 + v^2}.
  \end{align*}
  Now, this gives
  \begin{align*}
    \widetilde{\Phi}\left( u,v \right) &= \Phi\left( x\left( u,v \right),y\left( u,v \right) \right)\\
                                       &= V_0 \frac{1-u^2-v^2}{\left( 1-u \right)^2 + v^2}.
  \end{align*}
\end{example}
\begin{example}[Fluid Flow]
  Consider fluid flow around a rock with disk of radius $a$; far away from the rock, we have uniform flow speed of $\alpha$.\newline

  Symmetry allows us to focus only on the upper half-plane. Now, there is a conformal map in Table 20.1 of the textbook, which is the map $w(z) = z + \frac{a^2}{z} = u(x,y) + iv\left( x,y \right)$ that maps the upper half-plane to the upper half-plane. Furthermore, this map sends the boundary hugging the rock into the $u$-axis.\newline

  After applying the conformal map, we get the stream lines $\widetilde{\Psi}\left( u,v \right) = \beta v$, as they are streamlines of uniform horizontal flow.\newline

  Building the complex potential, we have
  \begin{align*}
    \widetilde{\Omega}\left( w \right) &= \Phi\left( u,v \right) + i\Psi\left( u,v \right)\\
    \widetilde{\Omega}\left( w \right) &= \beta w,
  \end{align*}
  as we must have $\diff{\Phi}{u} = \diff{\Psi}{v} = \beta$.\newline

  Mapping back into the $z$-plane, we have
  \begin{align*}
    \Omega\left( z \right) &= \beta\left( z + \frac{a^2}{z} \right).
  \end{align*}
  Note that as $z$ becomes very big, the term $\frac{a^2}{z}$ goes to $0$, so we must have $\beta = \alpha$.\newline

  Now, we may find the streamlines and potentials. Note that we have
  \begin{align*}
    \Phi &= \re\left( \Omega \right)\\
    \Psi &= \im\left( \Omega \right).
  \end{align*}
  Now, we have
  \begin{align*}
    \Omega\left( z \right) &= \alpha r\left( e^{i\varphi} + \frac{a^2}{r^2}e^{-i\varphi} \right)\\
                           &= \alpha r \left( \cos\left( \varphi \right) + i\sin\left( \varphi \right) + \frac{a^2}{r^2} \left( \cos\left( \varphi \right) - i\sin\left( \varphi \right) \right) \right).
  \end{align*}
  Taking real and imaginary parts, we have
  \begin{align*}
    \Phi &= \alpha r \left( 1 + \frac{a^2}{r^2} \right)\cos\left( \varphi \right)\\
    \Psi &= \alpha r \left( 1 - \frac{a^2}{r^2} \right)\sin\left( \varphi \right).
  \end{align*}
\end{example}
\begin{example}
  Considering our conformal map
  \begin{align*}
    w(z) &= z + \frac{a^2}{z}
  \end{align*}
  again, we see that if $\left\vert z \right\vert = a$, then $\left\vert u \right\vert \leq 2a$. Meanwhile, if $r > a$, then
  \begin{align*}
    w\left( z \right) &= z + \frac{a^2}{z}\\
                      &= re^{i\varphi} + \frac{a^2}{r}e^{-i\varphi}\\
                      &= \left( r + \frac{a^2}{r} \right)\cos\left( \varphi \right) + i\left( r - \frac{a^2}{r} \right)\sin\left( \varphi \right)\\
                      &= u + iv.
  \end{align*}
  This gives
  \begin{align*}
    \frac{u^2}{\left( r+ \frac{a^2}{r} \right)^2} + \frac{v^2}{\left( r-\frac{a^2}{r} \right)} &= 1.
  \end{align*}
  Note that $w$ fails to be conformal when $\diff{w}{z} = 0$, meaning that it fails to be conformal at $z = \pm a$.\newline

  This is occasionally used in the real world\footnote{I guess people do things over there.} to design airfoils.
\end{example}
\subsection{Residues}%
Consider a function $f(z)$ with an $n$th order pole. Then, $f$ can be written as
\begin{align*}
  f(z) &= \frac{g(z)}{\left( z-a \right)^n},
\end{align*}
where $g(z)$ is analytic and $g(a)\neq 0$. Recalling \hyperref[thm:cauchy_integral_formula]{Cauchy's integral formula}, we see that this expression for $f$ is tantalizingly close to our desired state.\newline

We may expand $g$ in a Taylor series:
\begin{align*}
  g(z) &= \sum_{m=0}^{\infty}\frac{g^{(m)}(a)}{m!}\left( z-a \right)^m.
\end{align*}
Letting $C$ be a positively oriented contour in the analytic domain of $f$ that encircles the singularity, we get
\begin{align*}
  \oint_{C}f(z)\:dz &= \sum_{m=0}^{\infty}\frac{g^{(m)(a)}}{m!} \oint_{C}\left( z-a \right)^{m-n}\:dz.
\end{align*}
Note that if $m-n\neq -1$, then the integral on the right vanishes, so we only obtain a nonzero contribution at $m = n-1$. Thus, we get
\begin{align*}
  \oint_{C}f(z)\:dz &= 2\pi i \frac{g^{(n-1)(a)}}{(n-1)!}.
\end{align*}
\begin{definition}
  Let $f(z)$ be an analytic function with a pole at $z=a$ with order $n$. We define the residue of $f$ at $a$ as
  \begin{align*}
    \Res\left[ f(z),a \right] \coloneq \frac{1}{\left( n-1 \right)!}\lim_{z\rightarrow a}\diff{^{n-1}}{z^{n-1}} \left( \left( z-a \right)^nf(z) \right).
  \end{align*}
\end{definition}
This gives an alternative statement of Cauchy's integral formula, giving
\begin{align*}
  \oint_{C}f(z)\:dz &= 2\pi i \Res\left[ f(z),a \right].
\end{align*}
However, when we have lots of poles for $f$, and $C$ is a contour that surrounds all the poles, we may deform $C$ such that it surrounds each pole. This gives the residue theorem.
\begin{theorem}[Residue Theorem]
  \begin{align*}
    \oint_{C}f(z)\:dz &= 2\pi i \sum_{a\in C}\Res\left[ f(z),a \right] \label{thm:residue_theorem}\tag{\textdagger\textdagger}
  \end{align*}
\end{theorem}
We can find the residue in a variety of ways.
\begin{table}[h!]
  \centering
  \renewcommand{\arraystretch}{1.75}
  \begin{tabular}{c|c}
    Type & Method \\
    \hline\hline
    $n$-th order pole & $\displaystyle\frac{1}{\left( n-1 \right)!}\diff{^{n-1}}{z^{n-1}}\left( \left( z-a \right)^nf(z) \right)$\\
    simple pole & $\displaystyle\lim_{z\rightarrow a}\left( z-a \right)f(z)$\\
    $f = \frac{p}{q}$,$q(a)$ simple zero & $\displaystyle \frac{p(a)}{q'(a)}$\\
    pole at infinity & $\displaystyle\lim_{z\rightarrow 0} \left( -\frac{1}{z^2}f\left( \frac{1}{z} \right) \right)$\\
    pole at infinity, $\lim_{|z|\rightarrow\infty}f(z) =0$& $\displaystyle -\lim_{|z|\rightarrow\infty}\left( zf(z) \right)$
  \end{tabular}
  \caption{Finding $\Res\left[ f(z),a \right]$}
\end{table}
%\begin{itemize}
%  \item If we're given a Laurent series, just take the coefficient $c_{-1}$.
%  \item If we have a simple pole at $z=a$, we get
%    \begin{align*}
%      \Res\left[ f(z),a \right] &= \lim_{z\rightarrow a}\left( \left( z-a \right)f(z) \right).
%    \end{align*}
%  \item If $f(z) =\frac{p(z)}{q(z)}$ with $p,q$ analytic, and $q(a)$ is a simple zero, we take
%    \begin{align*}
%      \Res\left[ f(z),a \right] &= \frac{p(a)}{q'(a)}.
%    \end{align*}
%  \item 
%\end{itemize}

\begin{example}
  We will find the residue for $\cot(z)$ for each of the residues.
  \begin{align*}
    \Res\left[ \cot(z), n\pi\right] &= \lim_{z\rightarrow n\pi} \left( z-n\pi \right)\frac{\cos(z)}{\sin(z)}\\
                                    &= \left( -1 \right)^{n} \lim_{z\rightarrow n\pi} \frac{z-n\pi}{\sin(z)}\\
                                    &= \left( -1 \right)^{n} \lim_{z\rightarrow n\pi}\frac{z-n\pi}{\left( -1 \right)^{n}\sin\left( z-n\pi \right)}\\
                                    &= 1.
  \end{align*}
\end{example}
\begin{example}
  We may find
  \begin{align*}
    \Res\left[ \frac{z}{\sinh(z)},in\pi \right] &= \frac{z}{\diff{}{z}\left( \sinh(z) \right)}\Biggr\vert_{z=in\pi}\\
                                                &= \frac{in\pi}{\cosh(in\pi)}\\
                                                &= \left( -1 \right)^nin\pi
  \end{align*}
\end{example}
\begin{example}
  Let's evaluate 
  \begin{align*}
    \oint_{C} \frac{(z-1)\left( z-2 \right)}{z\left( z+1 \right)\left( 3-z \right)}.
  \end{align*}
  Finding the residue at each pole, we get
  \begin{align*}
    \res\left[ f(z),0 \right] &= \frac{2}{3}\\
    \res\left[ f(z),-1 \right] &= -\frac{3}{2}\\
    \res\left[ f(z),3 \right] &= -\frac{1}{6}.
  \end{align*}
  These are evaluated using the \href{https://en.wikipedia.org/wiki/Heaviside_cover-up_method}{cover-up method}.\newline

  Now, we may find the integral by taking
  \begin{align*}
    \oint_{\left\vert z \right\vert = 2}f(z)\:dz &= -i\frac{5\pi}{3}.
  \end{align*}
\end{example}
\begin{example}
  Let
  \begin{align*}
    f(z) &= \frac{1}{z^2\sinh(z)}\\
         &= \frac{1}{-iz^2\sin\left( iz \right)}.
  \end{align*}
  The simple zeros of $\sinh(z)$ are at $in\pi$, so we have an order $3$ pole at $z = 0$
  \begin{align*}
    \res\left[ f(z),0 \right] &= \frac{1}{\left( n-1 \right)!} \diff{^{2}}{z^2} \left[ z^3f(z) \right]\biggr\vert_{z=0}\\
                              &= \frac{1}{2}\diff{^2}{z^2}\left( \frac{z}{\sinh(z)} \right)\biggr\vert_{z=0}\\
                              &= -\frac{1}{6}.
  \end{align*}
  Thus, integrating about the unit circle, we get
  \begin{align*}
    \oint_{\left\vert z \right\vert = 1} &= -\frac{i\pi}{3}.
  \end{align*}
  If we were to evaluate via the Laurent series, we would have
  \begin{align*}
    \frac{1}{z^2\sinh(z)} &= \frac{1}{z^2}\left( \frac{1}{z + z^2/3 + z^5/5! + \cdots} \right)\\
                          &= \frac{1}{z^3}\left( \frac{1}{1 + z^2/3! + z^4/5! + \cdots} \right)\\
                          &\approx \frac{1}{z^3}\left( 1 - \frac{z^2}{3!} + \cdots \right)\\
                          &= \frac{1}{z^3} - \frac{1}{6z} + \cdots,
  \end{align*}
  giving a residue of $-\frac{1}{6}$.\newline

  Instead of using the contour on the unit circle, if we want to use a circle of radius $4$, we get the residues at $z = \pm i\pi$. To evaluate this, we take
  \begin{align*}
    \res\left[ f(z),i\pi \right] &= \frac{1}{-\pi^2\left( -1 \right)}\\
                                 &= \frac{1}{\pi^2}\\
    \res\left[ f(z),-i\pi \right] &= \frac{1}{\pi^2}.
  \end{align*}
  Evaluating the integral, we would get
  \begin{align*}
    \oint_{C}f(z)\:dz &= 2\pi i \left( -\frac{1}{6} + \frac{2}{\pi^2} \right)\\
                      &= -\frac{i\pi}{3} + \frac{4i}{\pi}.
  \end{align*}
\end{example}
\begin{example}
  We will now use the residue theorem to evaluate a real-valued integral. Consider
  \begin{align*}
    I &= \int_{-\infty}^{\infty} \frac{1}{x^2 + 1}\:dx.
  \end{align*}
  Since this integral goes to zero, we will evaluate
  \begin{align*}
    I' &= \oint_{C} \frac{1}{z^2 + 1}\:dz,
  \end{align*}
  where $C$ is a semicircle with radius $r$ along the real axis from $-r$ to $r$ ``pointing upward,'' so to speak.\newline

  This gives
  \begin{align*}
    \oint_{C} \frac{1}{z^2 + 1}\:dz &= \int_{C_r}f(z)\:dz + \int_{-r}^{r} f(x)\:dx,
  \end{align*}
  which, sending $r$ to infinity, is equal to
  \begin{align*}
    I &= \int_{-\infty}^{\infty} f(x)\:dx.
  \end{align*}
  However, since our expression $\frac{1}{z^2 + 1}$ has poles at $i$ and $-i$, our semicircle gives
  \begin{align*}
    \oint_{C}\frac{1}{z^2 + 1} &= 2\pi i\res\left[ f(z),i \right]\\
                               &= 2\pi i\lim_{z\rightarrow i}\frac{1}{z+i}\\
                               &= 2\pi i\frac{1}{2i}\\
                               &= \pi.
  \end{align*}
\end{example}
  If we have a finite number of isolated singularities, we are always able to draw a contour that encloses all of them, which allows us to use the residue theorem.\newline

  Now, we know that we can have poles at infinity --- and that any positively-oriented contour in the plane is a negatively-oriented contour around $\infty$. Thus, if we have a contour surrounding all our finite singularities, we get
  \begin{align*}
    \sum_{i}\res\left[ f(z),a_i \right] &= -\res\left[ f(z),\infty \right]\\
    \res\left[ f(z),\infty \right] + \sum_{i}\left[ f(z),a_i \right] &= 0,
  \end{align*}
  as we're doing the same integral, but in negative orientation about $\infty$ and positive orientation about our singularities. \newline

    We have
    \begin{align*}
      \res\left[ f(z), \infty\right] &= \res\left[ -\frac{1}{z^2}f\left( \frac{1}{z} \right),0 \right].
    \end{align*}
  \begin{example}
    Now, recalling
    \begin{align*}
      f(z) &= \frac{\left( z-1 \right)\left( z-2 \right)}{z\left( z+1 \right)\left( 3-z \right)}.
    \end{align*}
    The residues are
    \begin{align*}
      \res\left[ f(z),0 \right] &= 2/3\\
      \res\left[ f(z),-1 \right] &= -3/2\\
      \res\left[ f(z),3 \right] &= -1/6.
    \end{align*}
    Now, calculating the residue at infinity, we have
    \begin{align*}
      \res\left[f(z),\infty \right] &= \res\left[ -\frac{1}{z^2}\frac{\left( 1/z-1 \right)\left( 1/z-2 \right)}{1/z\left( 1/z + 1 \right)\left( 3-1/z \right)},0 \right]\\
                                    &= -\res\left[ \frac{\left( z-1 \right)\left( 2z-1 \right)}{z\left( z+1 \right)\left( 3z-1 \right)} \right]\\
                                    &= 1.
    \end{align*}
  \end{example}
  Now, if $\lim_{|z|\rightarrow\infty}f(z) = 0$, then $f$ is pure Laurent series. In that case, if there is a residue, then we find the residue by evaluating
  \begin{align*}
    \res\left[ f(z),\infty \right] &= -\lim_{|z|\rightarrow\infty} zf(z)
  \end{align*}
  \begin{example}
    Consider functions of the form
    \begin{align*}
      f(z) &= \frac{p(z)}{q(z)},
    \end{align*}
    where $q$ is a higher-order polynomial than $p$.\newline

    If $q$ has first-order zeros $a$ and second-order zeros at $b$, then
    \begin{align*}
      f(z) &= \sum_{k=1}^{n}\frac{A_k}{z-a_k} + \frac{B_k}{z-b_k} + + \frac{C_k}{\left( z-b_k \right)^2}.
    \end{align*}
    Note that the coefficients are actually residues. This gives
    \begin{align*}
      A_k &= \res\left[ f(z),a_k \right]\\
      B_k &= \res\left[ f(z),b_k \right]\\
      C_k &= \res\left[ (z-b_k)f(z),b_k \right].
    \end{align*}
    For instance,
    \begin{align*}
      \frac{\left( z-1 \right)\left( z-2 \right)}{z\left( z+1 \right)\left( 3-z \right)} &= \frac{2}{3}\frac{1}{z} - \frac{1}{6}\frac{1}{z-3} - \frac{3}{2}\frac{1}{z+1}.
    \end{align*}
    Now, we may also have
    \begin{align*}
      \frac{\left( z-1 \right)\left( z-2 \right)}{z\left( z+1 \right)^2\left( 3-z \right)} &= \frac{2}{3}\frac{1}{z} - \frac{1}{24}\frac{1}{z-3} - \frac{5}{8}\frac{1}{z+1} - \frac{3}{2}\frac{1}{\left( z+1 \right)^2}.
    \end{align*}
  \end{example}
  \subsubsection{Integrating around a Circle}%
  We want to evaluate angular integrals of the form
  \begin{align*}
    \int_{0}^{2\pi} f\left( \sin\left( n\varphi \right),\cos\left( m\varphi \right) \right)\:d\varphi.
  \end{align*}
  Now, while this is a real integral over a domain, we may reformulate it about the unit circle by using the substitutions
  \begin{align*}
    z &= e^{i\varphi}\\
    d\varphi &= \frac{dz}{iz},
  \end{align*}
  which yields
  \begin{align*}
    \sin\left( n\varphi \right) &= \frac{1}{2i} \left( z^n - \frac{1}{z^n} \right)\\
    \cos\left( m\varphi \right) &= \frac{1}{2}\left( z^m + \frac{1}{z^m} \right).
  \end{align*}
  Thus, our integral becomes
  \begin{align*}
    \int_{0}^{2\pi} f\left( \sin\left( n\varphi \right),\cos\left( m\varphi \right) \right)\:d\varphi &= \oint_{|z|=1} f\left( \frac{1}{2i}\left( z^n - \frac{1}{z^n} \right), \frac{1}{2}\left( z^m + \frac{1}{z^m} \right) \right) \frac{dz}{iz}.
  \end{align*}
  \begin{example}
    Consider
    \begin{align*}
      \int_{0}^{2\pi} \sin^2\left( \varphi \right)\:d\varphi &= -\frac{1}{4}\oint_{|z|=1}\left( z-\frac{1}{z} \right)^2 \frac{dz}{iz}\\
                                                             &= \frac{i}{4}\oint_{|z|=1} \frac{1}{z^3}\left( z^4 -2z^1 + 1 \right)\:dz\\
                                                             &= -\frac{1}{2}\pi\res\left[ \frac{1}{z^3}\left( z^4 - 2z^1 + 1 \right),0 \right].
    \end{align*}
    The residue at $z = 0$ is $-2$ --- this can be found by dividing out by $z^3$.\newline

    Thus, we get the answer of
    \begin{align*}
      \int_{0}^{2\pi} \sin^2\left( \varphi \right)\:d\varphi &= \pi.
    \end{align*}
  \end{example}
  \begin{example}
    Using residues, we can evaluate a lot of integrals that are quite tricky on their face.
    \begin{align*}
      \int_{0}^{2\pi} \frac{\cos\left( 2\varphi \right)}{5-4\sin\left( \varphi \right)}\:d\varphi &= \oint_{|z| = 1} \frac{\frac{1}{2}\left( z^2 + \frac{1}{z^2} \right)}{5 - \frac{4}{2i}\left( z - \frac{1}{z} \right)}\frac{dz}{iz}\\
                                                                                                  &= -\oint_{|z|=1}\frac{z^4 + 1}{2z^2\left( 2z-i \right)\left( z-2i \right)}\:dz.
    \end{align*}
    Now, we have a simple pole at $i/2$, a simple pole at $2i$, and a pole of order $2$ at $0$. We only evaluate the residues at $0$ and $i/2$. We get
    \begin{align*}
      \res\left[ f(z),0 \right] &= -\frac{d}{dz}\left( \frac{z^4 + 1}{2z^2\left( 2z-i \right)\left( z-2i \right)} \right)\biggr\vert_{z=0}\\
                                &= -\frac{5i}{8}\\
      \res\left[ f(z),i/2 \right] &= \frac{17i}{24}.
    \end{align*}
    Thus, we get the result of
    \begin{align*}
      \int_{0}^{2\pi} \frac{\cos\left( 2\varphi \right)}{5-4\sin\left( \varphi \right)}\:d\varphi &= 2\pi i \left( -\frac{5i}{8} + \frac{17i}{24} \right)\\
                                                                                                  &= -\frac{\pi}{6}.
    \end{align*}
  \end{example}
  \subsubsection{Integrating along the Real Axis}%
  If we want to evaluate integrals along the real axis, such as
  \begin{align*}
    I &= \int_{-\infty}^{\infty} f(x)\:dx,
  \end{align*}
  we may be curious as to how we may evaluate this.\newline

  To do this, we recall that we created a contour in the upper half-plane of large enough radius $r$, and evaluated the residues inside the contour. We consider the contour to be equal to $C = C_r + l_r$, where $l_r$ is along the real axis and $C_r$ closes our contour. Thus, we get
  \begin{align*}
    \oint_{C}f(z)\:dz &= \lim_{r\rightarrow\infty} \left( \int_{-r}^{r} f(x)\:dx + \int_{C_r}^{} f(z)\:dz \right).
  \end{align*}
  Note that the polar coordinate Jacobian gives us the requirement that $\lim_{|z|\rightarrow\infty}\left\vert zf(z) \right\vert = 0$. \newline

  When $f(z) = \frac{p(z)}{q(z)}$, this is satisfied when $q$ is of degree at least two more than that of $p$.
  \begin{example}
    Consider
    \begin{align*}
      \int_{-\infty}^{\infty} \frac{2x + 1}{x^4 + 5x^2 + 4}\:dx &= \oint_{C} \frac{2z + 1}{z^4 + 5z^2 + 4}\:dz.
    \end{align*}
    Factoring, we get
    \begin{align*}
      \oint_{C}\frac{2z+1}{z^4 + 5z^2 + 4}\:dz &= \oint_{C} \frac{2z+1}{\left( z-2i \right)\left( z+2i \right)\left( z-i \right)\left( z+i \right)}\:dz.
    \end{align*}
    We only care about the residues in the upper half-plane. We have residues of
    \begin{align*}
      \res\left[ f(z),2i \right] &= -\frac{1}{3} + \frac{i}{12}\\
      \res\left[ f(z),i \right] &= \frac{1}{3} - \frac{i}{6}.
    \end{align*}
    Therefore, we have
    \begin{align*}
      \int_{-\infty}^{\infty} \frac{2x+1}{x^4 + 5x^2 + 4}\:dx &= 2\pi i \left( \frac{1}{3} - \frac{i}{6} - \frac{1}{3} + \frac{i}{12} \right)\\
                                                              &= \frac{\pi}{6}.
    \end{align*}
    Note that if we chose our contour to be in the lower half-plane, then we would have a \textit{negatively} oriented contour, and evaluate at the residues in the lower half-plane.
  \end{example}
  \begin{example}
    Consider
    \begin{align*}
      \int_{-\infty}^{\infty} \frac{1}{x^3 - i}\:dx &= \oint_{C} \frac{1}{z^3 - i}\:dz\\
                                                    &= \oint_{C} \frac{1}{\left( z +i \right)\left( z - e^{i\pi/6} \right)\left( z - e^{5i\pi/6} \right)}.
    \end{align*}
    Closing $C$ in the lower half-plane, we only need the residue at $-i$. This gives
    \begin{align*}
      \oint_{C} \frac{1}{z^3 - i} &= -2\pi i\left( -\frac{1}{3} \right)\\
                                  &= \frac{2\pi i}{3}.
    \end{align*}
  \end{example}
   Consider integrals of the form
   \begin{align*}
     \int_{-\infty}^{\infty} g(x)e^{ikx}\:dx,
   \end{align*}
   where $k$ is real.\newline

   Now, we want to know when exactly we are allowed to ``close up'' the semicircle contour.\newline

   We start by assuming $k$ is positive. Closing in the upper half-plane so as to ensure exponential decay, we have
   \begin{align*}
     \left\vert \int_{C_r}^{} g(z)e^{ikz}\:dz \right\vert &\leq \int_{C_r}^{} \left\vert g(z)e^{ikz} \right\vert\:dz\\
                                                          &= \int_{0}^{\pi} \left\vert g\left( re^{i\varphi} \right) \right\vert re^{-kr\sin\left( \varphi \right)}\:d\varphi.
   \end{align*}
   Since $\sin\left( \varphi \right) \geq 0$ on the range of integration, the integral vanishes as $r\rightarrow\infty$. Therefore, we are allowed to close up the contour whenever $|g(z)|\rightarrow 0$ as $|z|\rightarrow\infty$.
   \begin{example}
     Consider the integral
     \begin{align*}
       I &= \int_{-\infty}^{\infty} \frac{\cos\left( kx \right)}{x^2 + 4}\:dx.
     \end{align*}
     This gives
     \begin{align*}
       I &= \re\left( \int_{-\infty}^{\infty} \frac{e^{ikx}}{x^2 + 4}\:dx \right)\\
         &= \re\left( \oint_{C}\frac{e^{ikz}}{z^2 + 4}\:dz \right)\\
         &= \oint_{C}^{} \frac{e^{ikz}}{\left( z-2i \right)\left( z+2i \right)}\:dz.
     \end{align*}
     We assume $k > 0$. Then, evaluating at $2i$, we have
     \begin{align*}
       I &= \frac{\pi}{2}e^{-2k}.
     \end{align*}
     Now, if $k < 0$, we close our contour in the lower half-plane, we get
     \begin{align*}
       I &= \frac{\pi}{2}e^{2k}.
     \end{align*}
     Thus, our integral is always
     \begin{align*}
       I &= \frac{\pi}{2}e^{-2\left\vert k \right\vert}.
     \end{align*}
   \end{example}
   \subsubsection{Non-Circular Contours}%
   Sometimes, semicircles don't work.
   \begin{example}
     Consider
     \begin{align*}
       \int_{-\infty}^{\infty} \frac{e^{bx}}{e^x + 1}\:dx,
     \end{align*}
     where $0 < b < 1$. Writing our integral, we have
     \begin{align*}
       I &= \int_{}^{} \frac{e^{bz}}{e^{z} + 1}\:dz
     \end{align*}
     This gives poles at $z = \left( 2n + 1 \right)i\pi$, which means we cannot close this contour with a semicircular arc at $\infty$.\newline

     What may work in this case is by drawing a rectangular contour from $-a$ to $a$ such that it encloses exactly one of the poles of our integrand. The vertical segments of this contour go to zero as we send $a\rightarrow\infty$. We call the segment of the contour along the line $a + 2\pi i$ to $a - 2\pi i$ as $I'$.\newline

     This gives
     \begin{align*}
       I + I' &= \oint_{C}\frac{e^{bz}}{e^{z} + 1}\:dz.
     \end{align*}
     Now, we constructed $I'$ such that
     \begin{align*}
       I' &= \int_{\infty}^{-\infty} \frac{e^{b\left( x + 2\pi i \right)}}{e^{x + 2\pi i} + 1} \:dx\\
          &= -e^{2\pi i b}\int_{-\infty}^{\infty} \frac{e^{bx}}{e^{x} + 1}\:dx\\
          &= -e^{2\pi i b} I.
     \end{align*}
     Therefore, we have
     \begin{align*}
       \oint_{C} \frac{e^{bz}}{e^{z} + 1}\:dz &= I\left( 1-e^{2\pi i b} \right)\\
                                              &= 2\pi i \res\left[ \frac{e^{bz}}{e^{z} + 1},i\pi \right],
     \end{align*}
     giving
     \begin{align*}
       I &= \frac{\pi}{\sin\left( \pi b \right)}.
     \end{align*}
   \end{example}
   \begin{example}
     We want to evaluate
     \begin{align*}
       \int_{0}^{\infty} \cos\left( x^2 \right)\:dx\\
       \int_{0}^{\infty} \sin\left( x^2 \right)\:dx.
     \end{align*}
     To evaluate this, we draw a slice-shaped contour going along the real axis and returning to $0$ along $z = re^{i\pi/4}$. Therefore, we evaluate
     \begin{align*}
       \oint_{C}e^{iz^2}\:dz &= \int_{0}^{\infty} e^{ix^2}\:dx + 0 + \int_{\infty}^{0} e^{i\left( re^{i\pi/4} \right)^2}e^{i\pi/4}\:dr\\
                             &= \int_{0}^{\infty} e^{ix^2}\:dx + 0 + \int_{\infty}^{0} e^{-r^2}e^{i\pi/4}\:dr.
     \end{align*}
     Thus, we get
     \begin{align*}
       \int_{0}^{\infty} \cos\left( x^2 \right)\:dx &= \int_{0}^{\infty} \sin\left( x^2 \right)\:dx\\
                                                     &= \frac{1}{2}\sqrt{\frac{\pi}{2}}.
     \end{align*}
   \end{example}
   \subsubsection{Integrating with Branch Cuts}%
   When we're integrating with residues, branch cuts are a feature rather than a bug.
   \begin{example}
     Consider the integral
     \begin{align*}
       I &= \int_{0}^{\infty} \frac{\sqrt{x}}{1 + x^3}\:dx.
     \end{align*}
     We need a branch cut to avoid the multivalued behavior. Our poles are at $e^{i\pi/3},-1,e^{-i\pi/3}$. Since our integral is along the real axis, we take our branch cut along the domain $[0,\infty]$.\newline

     We draw our contour of radius $R$ by hugging the branch without crossing it, with a small circle of radius $\ve$ just outside $0$. This gives the integral
     \begin{align*}
       \oint \frac{\sqrt{z}}{1 + z^3}\:dz &= \int_{0}^{\infty} \frac{\sqrt{z}}{1 + z^3}\:dx + \int_{C_R}^{} \frac{\sqrt{z}}{1 + z^3}\:dz + \int_{\infty}^{0} \frac{\sqrt{z}}{1 + z^3}\:dz + \int_{C_{\ve}}^{} \frac{\sqrt{z}}{1 + z^3}\:dz.
     \end{align*}
     Note that since $\lim_{|z|\rightarrow\infty}\left\vert zf(z) \right\vert = 0$, and $\lim_{|z|\rightarrow 0}\left\vert zf(z) \right\vert = 0$, our integrals along $C_R$ and $C_{\ve}$ go to zero, giving the integral
     \begin{align*}
       I' &= \int_{\infty}^{0} \frac{\sqrt{z}}{1 + z^3}\:dz\\
          &= \int_{\infty}^{0} \frac{\sqrt{e^{2i\pi }}x}{1 +\left( e^{2i\pi}x \right)^3}\:dx\\
          &= \int_{0}^{\infty} \frac{\sqrt{x}}{1 + x^3}\:dx\\
          &= I.
     \end{align*}
     Thus,
     \begin{align*}
       \oint \frac{\sqrt{z}}{1 + z^3}\:dz &= 2I.
     \end{align*}
     Evaluating the residues, we have
     \begin{align*}
       \res\left[ f(z),e^{i\pi/3} \right] &= \lim_{z\rightarrow e^{i\pi/3}}\frac{\sqrt{z}}{3z^2}\\
                                          &= -\frac{i}{3}\\
       \res\left[ f(z),-1 \right] &= \lim_{z\rightarrow-1} \frac{\sqrt{z}}{3z^2}\\
                                  &= \frac{i}{3}\\
       \res\left[ f(z),e^{5\pi i/3} \right] &= -\frac{i}{3},
     \end{align*}
     giving the solution of
     \begin{align*}
       I &= \frac{1}{2}2\pi i\left( -\frac{i}{3} \right)\\
         &= \frac{\pi}{3}.
     \end{align*}
   \end{example}
   \begin{example}
     To evaluate
     \begin{align*}
       I &= \int_{0}^{\infty} \frac{1}{1 + x^3}\:dx,
     \end{align*}
     we start by evaluating
     \begin{align*}
       \int_{0}^{\infty} \frac{\ln(x)}{1 + x^3}\:dx
     \end{align*}
     with the branch cut along the real axis. Using the keyhole contour in the previous example, we have that $C_R$ and $C_{\ve}$ contribute nothing, and $\ln$ picks up a phase of $2\pi i$, so that
     \begin{align*}
       \oint_{C} \frac{\ln(z)}{1 + z^3} &= \int_{0}^{\infty} \frac{\ln(x)}{1 + x^3}\:dx + \int_{\infty}^{0} \frac{\ln(x) + 2\pi i}{1 + x^3}\:dx\\
                                        &= -2\pi i I.
     \end{align*}
     Therefore, 
     \begin{align*}
       I &= -\sum\res \left[ \frac{\ln(x)}{1 + x^3} \right].
     \end{align*}
     Thus, we get the solution of
     \begin{align*}
       \int_{0}^{\infty} \frac{1}{1 + x^3}\:dx &= \frac{2\pi}{3\sqrt{3}}.
     \end{align*}
   \end{example}
   \begin{example}
     Consider the integral
     \begin{align*}
       I &= \int_{0}^{1} \frac{\sqrt{1 - x^2}}{x^2 + a^2}\:dx.
     \end{align*}
     The poles are around $\pm ia$.\newline

     Our problem is that we have multivalued behavior at $\pm 1$. We may take the cut from $-1$ to $1$ along the real axis, and our contour gives a sign flip across the cut.\newline

     We draw a dog-bone style contour hugging the cut in negative orientation to give us $2I$. Thus, we get
     \begin{align*}
       \oint_{C} \frac{\sqrt{1-z^2}}{z^2 + a^2}\:dz &= \int_{-1}^{1} \frac{\sqrt{1-x^2}}{x^2 + a^2}\:dx - \int_{1}^{-1} \frac{\sqrt{1-x^2}}{x^2 + a^2}\:dx\\
                                                    &= 4I,
     \end{align*}
     where the sign flip in the second integral comes from crossing the branch cut.\newline

     Now, to evaluate the sum of the residues, we need to evaluate at three poles --- $ia$, $-ia$, and the pole at $\infty$. Thus, we get
     \begin{align*}
       \res\left[ f(z),\pm ia \right] &= \frac{\sqrt{a^2 + 1}}{2ia}\\
       -\res\left[ f(z),\infty \right] &= \lim_{|z|\rightarrow \infty}zf(z)\\
                                      &= i.
     \end{align*}
     Therefore,
     \begin{align*}
       4I &= 2\pi i \left( \frac{\sqrt{a^2 + 1}}{ia} - i \right)\\
          &= \frac{\pi}{2a}\left( \sqrt{a^2 + 1} - a \right).
     \end{align*}
   \end{example}
   \subsubsection{Poles on the axis}%
   If we want to evaluate integrals with the pole on the contour, we need to use principal values.
   \begin{align*}
     \text{PV}\int_{a}^{b} f(x)\:dx &= \lim_{\ve \rightarrow 0}\left( \int_{a}^{x_0 - \ve} f(x)\:dx + \int_{x_0 + \ve}^{b} f(x)\:dx \right).
   \end{align*}
   Similarly, we want to apply this for the calculus of residues. To do this, we take
   \begin{align*}
     \oint_{C}f(z)\:dz &= \text{PV}\int_{-\infty}^{\infty} f(x)\:dx + \lim_{\ve\rightarrow 0}f(z)\:dz,
   \end{align*}
   where $c_{\pm}$ are small semicircular contour additions of radius $\ve$ to $C$ that hug our pole on the real axis, with $c_{-}$ excluding the pole and $c_{+}$ including the pole. Thus, we have
   \begin{align*}
     \oint_{C}f(z)\:dz &= \text{PV}\int_{-\infty}^{\infty} f(x)\:dx + \lim_{\ve\rightarrow 0} \int_{c_{\pm}}^{} \frac{\left( z-x_0 \right)f(z)}{z-x_0}\:dz.
   \end{align*}
   Introducing $z-x_0 = \ve e^{i\varphi}$, we have $dz = i\ve e^{i\varphi}\:d\varphi$, giving
   \begin{align*}
     \oint_{C} f(z)\:dz &= \text{PV}\int_{-\infty}^{\infty} f(x)\:dx + \res\left[ f(z),x_0 \right]\int_{c_{\pm}}^{} i\:d\varphi.
   \end{align*}
   Thus, we have
   \begin{align*}
     \oint_{C}f(z)\:dz &= \text{PV}\int_{-\infty}^{\infty} f(x)\:dx \pm i\pi \res\left[ f(z),x_0 \right].\\
                       &= 2\pi i \sum_{z_i}\res\left[ f(z)-z_i \right].
   \end{align*}
   Thus, we have
   \begin{align*}
     \text{PV} \int_{-\infty}^{\infty} f(x)\:dx &= \sum_{z_i\text{ in }C} \res\left[ f(z),z_i \right] \mp i\pi \res\left[ f(z),x_0 \right].
   \end{align*}
   Note that we only have \textit{half} the residue when the pole is on the contour. Therefore, we have the result of
   \begin{align*}
     \text{PV}\int_{-\infty}^{\infty} f(x)\:dx &= 2\pi i \left( \sum_{z_i\text{ in }C} \res\left[ f(z),z_i \right] + \frac{1}{2}\sum_{z_i\text{ on }C}\res\left[ f(z),z_i \right] \right).
   \end{align*}
   \begin{example}
     Consider
     \begin{align*}
       \text{PV}\int_{-\infty}^{\infty} \frac{e^{ikx}}{x-a}\:dx.
     \end{align*}
     We have a simple pole at $x = a$.\newline

     We close our contour with a semicircle on the upper half-plane. Since we have no poles inside the contour, we have
     \begin{align*}
       \text{PV}\int_{-\infty}^{\infty} \frac{e^{ikx}}{x-a}\:dx &= \pi i \res\left[ \frac{e^{ikx}}{x-a},a \right]\\
                                                                &= \pi i e^{ika}.
     \end{align*}
     Notice that if $k < 0$, we must close the contour in the lower half-plane, giving
     \begin{align*}
       \text{PV}\int_{-\infty}^{\infty} \frac{e^{ikx}}{x-a}\:dx &= \sgn(k)\pi i e^{ika}.
     \end{align*}
     Taking real and imaginary components, we get
     \begin{align*}
       \text{PV}\int_{-\infty}^{\infty} \frac{\cos\left( kx \right)}{x-a}\:dx &= -\pi\sin\left( ka \right)\\
       \text{PV}\int_{-\infty}^{\infty} \frac{\sin\left( kx \right)}{x}\:dx &= \pi\cos\left( ka \right).
     \end{align*}
   \end{example}
   \begin{example}
     We will evaluate
     \begin{align*}
       I &= \text{PV}\int_{0}^{\infty} \frac{\ln(x)}{x^2 + a^2}\:dx.
     \end{align*}
     We have a troublesome portion at $x = 0$, so we draw our contour to exclude $0$.\newline

     We may close the contour with a large semicircle $C_R$. Since $\lim_{|z|\rightarrow \infty}\left\vert zf(z) \right\vert = 0$ and $\lim_{|z|\rightarrow 0}\left\vert zf(z) \right\vert = 0$, we may take these limits to give
     \begin{align*}
       \oint_{C}\frac{\ln(z)}{z^2 + a^2} &= \int_{-\infty}^{0} \frac{\ln\left( e^{i\pi}x \right)}{x^2 + a^2}\:dx + \int_{0}^{\infty} \frac{\ln\left(e^{i0}x\right)}{x^2 + a^2}\:dx\\
                                         &= \text{PV}\int_{-\infty}^{\infty} \frac{\ln(x)}{x^2 + a^2}\:dx + i\pi \int_{0}^{\infty} \frac{1}{x^2 + a^2}\:dx\\
                                         &= 2\text{PV}\int_{0}^{\infty} \frac{\ln(x)}{x^2 + a^2}\:dx + \frac{i\pi^2}{2a}\\
                                         &= 2\pi i \res\left[ f(z),ia \right].
     \end{align*}
     Thus, we get $I = \frac{\pi}{2a}\ln(a)$.
   \end{example}
   \begin{example}
     Instead of moving our contour up or down by $\ve$ to include (or exclude) a pole, we may move the pole up or down by $\ve$. We consider
     \begin{align*}
       \int_{-\infty}^{\infty} \frac{g(x)}{x-x_0}\:dx &= \lim_{\ve\rightarrow 0} \int_{-\infty}^{\infty} \frac{g(x)}{x -\left( x_0\pm i\ve \right)}\:dx.
     \end{align*}
     Breaking into real or imaginary parts, we have
     \begin{align*}
       \int_{-\infty}^{\infty} \frac{g(x)}{x-\left( x_0 \pm i\ve \right)^2}\:dx &= \int_{-\infty}^{\infty} g(x)\frac{x-x_0}{\left( x-x_0 \right)^2 + \ve^2}\:dx \pm i\ve \int_{-\infty}^{\infty} \frac{g(x)}{\left( x-x_0 \right)^2 + \ve^2}\:dx.
     \end{align*}
     Now, notice that
     \begin{align*}
       \lim_{\ve \rightarrow 0}\frac{\ve}{\left( x-x_0 \right)^2 + \ve^2} &= \begin{cases}
         0 & x\neq x_0\\
         \infty & x = x_0
       \end{cases}.
     \end{align*}
     Now, we may take
     \begin{align*}
       \int_{-\infty}^{\infty} \frac{\ve}{\left( x-x_0 \right)^2 + \ve^2}\:dx &= \ve \oint_{C}\frac{dz}{z + \ve^2},
     \end{align*}
     where $z = \left( x-x_0 \right)^2$. This gives
     \begin{align*}
       \ve\oint_{C}\frac{dz}{z + \ve^2} &= \ve \left( 2\pi i \right)\left( \frac{1}{2i \ve} \right)\\
                                        &= \pi.
     \end{align*}
     Therefore, 
     \begin{align*}
       \lim_{\ve\rightarrow 0} \int_{-\infty}^{\infty} \frac{\ve}{\left( x-x_0 \right)^2 + \ve^2}\:dx &= \pi\delta\left( x-x_0 \right).
     \end{align*}
     Therefore, we recover
     \begin{align*}
       \lim_{\ve\rightarrow 0}\ve\int_{-\infty}^{\infty} \frac{g(x)}{\left( x-x_0 \right)^2+ \ve^2}\:dx &= \pi g\left(x_0\right).
     \end{align*}
     This gives the identity under the integral of
     \begin{align*}
       \frac{1}{z\mp i\ve} &= \text{PV}\frac{1}{x} \pm i\pi\delta(x).
     \end{align*}
   \end{example}
   \begin{example}
     Consider the integral
     \begin{align*}
               \oint_{C_r}^{} \frac{\cos(z)}{z}\:dz &= \lim_{\ve\rightarrow 0} \int_{-\infty}^{\infty} \frac{\cos(x)}{x\pm i\ve}\:dxj\\
                                                    &= \underbrace{\text{PV}\int_{-\infty}^{\infty} \frac{\cos(x)}{x}\:dx}_{=0} \mp i\pi \int_{-\infty}^{\infty} \cos(x)\delta(x)\:dx\\
                                                    &= \mp i\pi
     \end{align*}
   \end{example}
   \subsubsection{Sommerfeld--Watson Transform and Series Summation}%
   Thus far, we've been replacing integrals with sums. Now, we're interested in going the other way around.\newline

   Consider the sum
   \begin{align*}
     S &= \sum_{-\infty}^{\infty}f(n),
   \end{align*}
   given the condition that $f(z)$ is analytic for $z\in\R$ and $\lim_{|z|\rightarrow\infty}\left\vert z^2f(z) \right\vert = 0$.\newline

   We will introduce the auxiliary function
   \begin{align*}
     g(z) &= \pi \cot\left( \pi z \right)\\
          &= \pi \frac{\cos\left( \pi z \right)}{\sin\left( \pi z \right)}.
   \end{align*}
   Note that $g(z)$ has an infinite number of poles at $z = n$ for each $n\in\Z$.\newline

   Now, what we will do here is integrate the product $f(z)g(z)$ around a long enough symmetric contour hugging the real axis. This gives
   \begin{align*}
     \frac{1}{2\pi i}\oint_{C} f(z)g(z)\:dz &= \sum_{n=-\infty}^{\infty}\res\left[ \pi \cot\left( \pi z \right)f(z),n \right]\\
                                            &= \sum_{n=-\infty}^{\infty} f(z)\frac{\pi\cos\left( \pi z \right)}{\diff{}{z}\left( \sin(\pi z) \right)}\biggr\vert_{z=n}.\\
                                            &= \sum_{n=-\infty}^{\infty}f(n).
   \end{align*}
   Now, this doesn't \textit{seem} that helpful, until we remember that our contour $C$ surrounds all the other poles of $f$ in negative orientation.
   \begin{align*}
     \frac{1}{2\pi}\oint_{C}f(z)g(z)\:dz &= -\sum_{i}\res\left[ \pi\cot\left( \pi z \right)f(z),z_i \right].
   \end{align*}
   Thus, we have converted our infinite sum into a finite sum.\newline

   Similarly, if we have an alternating sign series
   \begin{align*}
     S' &= \sum_{n=-\infty}^{\infty}\left( -1 \right)^{n}f(n)\\
        &= -\sum_{i}\res\left[ \pi\csc\left( \pi z \right)f(z),z_i \right]
   \end{align*}
   \begin{example}
     Consider
     \begin{align*}
       S &= \sum_{n=0}^{\infty}\frac{1}{n^2 + a^2}.
     \end{align*}
     Our analogous function is
     \begin{align*}
       f(z) &= \frac{1}{z^2 + a^2}.
     \end{align*}
     Then,
     \begin{align*}
       S' &= -\res\left[ \frac{\pi\csc\left( \pi z \right)}{z^2 + a^2} ,\pm ia\right]\\
         &= -\frac{\pi}{2a}\coth\left( \pi a \right).
     \end{align*}
     Therefore, we have
     \begin{align*}
       \sum_{n=-\infty}^{\infty} \frac{1}{n^2 + a^2} = \frac{\pi}{a}\coth\left( \pi a \right).
     \end{align*}
     Now, we write
     \begin{align*}
       S &= \frac{1}{2a^2} + \frac{1}{2}\sum_{n=-\infty}^{\infty}\frac{1}{n^2 + a^2}.
     \end{align*}
     Thus, we get the sum
     \begin{align*}
       \sum_{n=0}^{\infty}\frac{1}{n^2 + a^2} &= \frac{1}{2a^2}\left( 1 + \pi a \coth\left( \pi a \right) \right).
     \end{align*}
   \end{example}
   \begin{example}
     Now, we may consider
     \begin{align*}
       S' &= \sum_{n=-\infty}^{\infty}\frac{\left( -1 \right)^n}{n^2 + a^2}\\
         &= -\res\left[ \frac{\pi\csc\left( \pi z \right)}{z^2 + a^2},\pm ia \right]\\
         &= -\frac{\pi}{2a}\frac{1}{\sinh\left( \pi a \right)},
     \end{align*}
     giving
     \begin{align*}
       S &= \frac{1}{2a^2}\left( 1 + \frac{\pi a}{\sinh\left( \pi a \right)} \right).
     \end{align*}
   \end{example}
  \subsection{Oscillators and Forcing}%
  Consider a damped harmonic oscillator with position $u(t)$. Then, $u$ obeys Newton's second law,
  \begin{align*}
    \ddot{u} + 2\beta\dot{u} + \omega_0^2u &= 0.
  \end{align*}
  Here, $\beta$ is the damping factor, and $\omega_{0}$ denotes the natural frequency.\newline

  The solutions of this equation are
  \begin{align*}
    u(t) &= e^{-\beta t}\left( ae^{i\Omega t} + be^{-i\Omega t} \right)\\
         &= e^{-\beta t}\left( a\cos\left( \Omega t \right) + b\sin\left( \Omega t \right) \right),
  \end{align*}
  where $\Omega^2 = \omega_0^2 - \beta^2$. This is known as a transient solution.\newline

  There are three types of motion.
  \begin{itemize}
    \item An underdamped system occurs when $\omega_0 > \beta$, so $\Omega$ is real, meaning we get oscillation that is damped out.
    \item An overdamped system occurs when $\beta > \omega_0$, so $\Omega$ is imaginary, and the damping slows down the return of the wave.
    \item When $\omega = \beta$, then $\Omega = 0$, and the solution is of the form $u\left( t \right) = e^{-\beta t}\left( at + b \right)$, and the system returns to equilibrium as quickly as possible. This is known as critical damping.
  \end{itemize}
  A forced system occurs when we have the differential equation
  \begin{align*}
    \ddot{u} + \beta \dot{u} + \omega_0^2u &= f(t).\label{eq:forced_and_damped_oscillator}\tag{\textdagger}
  \end{align*}
  We may consider a forcing function of the form $f(t) = f_0\cos\left( \omega t \right)$. We may also write
  \begin{align*}
    f(t) &= f_0\re\left( e^{i\omega t} \right).
  \end{align*}
  We expect to have a complex steady-state solution of the form
  \begin{align*}
    U_{\omega} &= C(\omega)e^{i\omega t}.
  \end{align*}
  We solve for $U$ by sticking it into the differential equation of \ref{eq:forced_and_damped_oscillator}. This will give the equation
  \begin{align*}
    U_{\omega} &= \frac{f_0e^{i\omega t}}{\left( \omega_0^2 - \omega^2 \right) + 2i\beta\omega}.
  \end{align*}
  Note that the real solution is $u = \re\left( U_{\omega} \right)$, or
  \begin{align*}
    u(t) &= \frac{1}{2}\left( U_{\omega} + U_{-\omega} \right)\\
         &= \frac{1}{2}\left( U_{\omega} + \overline{U_{\omega}} \right).
  \end{align*}
  Now, when we consider a generalized forcing function $f(t)$, where $f$ is a continuum sum of forcing frequencies where the amplitudes are functions of $\omega$, $\hat{f}\left( \omega \right)$, we get an integral:
  \begin{align*}
    u(t) &= \int_{}^{} \frac{F(\omega)e^{i\omega t}}{\left( \omega_0^2-\omega^2 \right) + 2i\beta\omega}\:d\omega.
  \end{align*}
  Plugging this solution into the differential equation, we get
  \begin{align*}
  f(t) &= \int_{-\infty}^{\infty} \hat{f}\left( \omega \right)e^{i\omega t}\:d\omega,
  \end{align*}
  which is a Fourier transform (see \href{https://ai.avinash-iyer.com/Classes_and_Homework/College/Y4/Y4S1,\%20Math\%20Methods/math_methods_notes.pdf}{Math Methods 1}).
  \subsubsection{Impulse Forcing}%
  Consider a hammer blow forcing function, known as an impulse forcing function.\newline

  The impulse forcing is of the form
  \begin{align*}
    f(t) &= f_0\delta\left( \omega\left( t-t_0 \right) \right)\\
         &= \frac{\hat{f}_0}{2\pi} \int_{-\infty}^{\infty} e^{i\omega\left( t-t_0 \right)}\:d\omega,
  \end{align*}
  where
  \begin{align*}
    \hat{f}_0 &= \frac{f_0}{\omega_0}.
  \end{align*}
  We want to find the impulse solution,
  \begin{align*}
    G(t)&\coloneq u_{\delta}(t)\\
        &= \frac{\hat{f}_0}{2\pi} \int_{-\infty}^{\infty} \frac{e^{i\omega\left( t-t_0 \right)}}{\left( \omega_0 - \omega^2 \right) + 2i\beta\omega}\:d\omega.
  \end{align*}
  To do this integral, we will make use of residues. Writing our denominator as
  \begin{align*}
    \left( \omega_0^2 - \omega^2 \right) + 2i\beta\omega &= \left( \omega - \omega_{+} \right)\left( \omega - \omega_{-} \right),
  \end{align*}
  where
  \begin{align*}
    \omega_{\pm}  &= i\beta \pm \sqrt{\omega_0^2 - \beta^2}\\
                  &= i\beta\pm \Omega.
  \end{align*}
  We close our contour in the upper half-plane so that get a decaying exponential. Evaluating the residues, we get
  \begin{align*}
    G(t) &= \hat{f}_0 \frac{e^{-\beta\left( t-t_0 \right)}}{2i\Omega} \left( e^{i\Omega\left( t-t_0 \right)} - e^{-i\Omega\left( t-t_0 \right)}\right)\\
         &= \frac{f_0}{\Omega}\sin\left( \Omega\left( t-t_0 \right) \right)e^{-\beta\left( t-t_0 \right)},
  \end{align*}
  where $t > t_0$.\newline

  Now, if $t < t_0$, then we must close our contour in the lower half-plane, and since there are no poles in the lower half-plane, we get $G(t) = 0$ for $t < t_0$. Thus, we must have
  \begin{align*}
    G(t) &= \frac{f_0}{\Omega} \sin\left( \Omega\left( t-t_0 \right) \right)e^{-\beta\left( t-t_0 \right)}\Theta\left( t-t_0 \right),
  \end{align*}
  where $\Theta$ is the Heaviside step function.
  \begin{itemize}
    \item The imaginary and real parts of $\omega_{\pm}$ give the damping, $\beta$, and parameter, $\Omega$, respectively. Now, we may interpret the different types of damping in this respect.
      \begin{itemize}
        \item If $\Omega$ is real (i.e., underdamped motion), then $\omega_{\pm}$ have constant magnitude of $\omega_0$, meaning that varying the damping only moves the poles around in a circle.
        \item If $\beta = \omega_0$ (i.e., critically damped motion), then the poles converge at $i\beta$ along the imaginary axis.
        \item If $\beta > \omega_0$ (i.e., overdamped mption), then the poles separate along the imaginary axis, giving non-oscillatory motion.
      \end{itemize}
    \item The poles also encode resonance characteristics, where we have $\omega_{\text{res}}^2 = \omega_0^2 - 2\beta^2$.
    \item If $\beta \ll \omega_0$, then the damping is mathematically equivalent to the $i\ve$ prescription moving the resonance pole at $\omega_0$ off the real axis and into the upper half-plane.
  \end{itemize}
  \subsubsection{Waves on a String}%
  Whereas an undamped oscillator is harmonic only in time, a wave is harmonic in both space and time.\newline

  A wave satisfies the equation
  \begin{align*}
    \pd{^2u}{t^2} &= c^2\nabla^2 u,
  \end{align*}
  where $c$ is the wave speed.\newline

  The general solution is of the form
  \begin{align*}
    U\left( x,t \right) &= ce^{i\left( kx = \omega t \right)}.
  \end{align*}
  A forced wave occurs via
  \begin{align*}
    \pd{^2u}{t^2} - \frac{1}{c^2} \pd{^2u}{t^2} &= f(x,t).
  \end{align*}
  Now, we start with the impulse solution,
  \begin{align*}
    f(x,t) &= f_0\delta\left( x-x_0 \right)\delta\left( t-t_0 \right)\\
           &= f_0 \int_{-\infty}^{\infty} \frac{e^{ik\left( x-x_0 \right)}}{2\pi}\:dk \int_{-\infty}^{\infty} \frac{e^{-i\omega\left( t-t_0 \right)}}{2\pi}\:d\omega.
  \end{align*}
  Now, we have
  \begin{align*}
    G(x,t) &= \frac{1}{\left( 2\pi \right)^2}\int_{-\infty}^{\infty} e^{ikx}\:dk\int_{-\infty}^{\infty} \frac{e^{-i\omega t}}{\omega^2/c^2 - k^2}\:d\omega.
  \end{align*}
  To evaluate this integral, we start with the integral in $\omega$, given by
  \begin{align*}
    I &= \frac{c^2}{2\pi}\int_{-\infty}^{\infty} \frac{e^{-i\omega t}}{\omega^2 - c^2k^2}\:d\omega.
  \end{align*}
  Unfortunately here, our simple poles lie on the real axis at $\pm ck$.\newline

  To find the solution, we need boundary and initial conditions to know where we want to make our $i\ve$ adjustment.\newline

  If there is no wave before our impulse hits, we need our integral to vanish whenever $t < 0$ which occurs when we close our contour in the lower half plane, so we subtract $i\ve$ from $\pm ck$. Factoring, we have
  \begin{align*}
    I &= \frac{c^2}{2\pi}\int_{-\infty}^{\infty} \frac{e^{i\omega t}}{\left( \omega-\left( ck-i\ve \right) \right)\left( \omega+\left( ck + i\ve\right) \right)}\:d\omega.
  \end{align*}
  Thus, we have
  \begin{align*}
    I(t > 0) &= -\frac{c}{k}\sin\left( ckt \right)e^{-\ve t},
  \end{align*}
  and in the limit as $\ve\rightarrow 0$, we have
  \begin{align*}
    I(t) &= -\frac{c}{k}\sin\left( ckt \right).
  \end{align*}
  Now, sticking our value of $I$ into the integral in $k$, we have
  \begin{align*}
    G\left( x, t> 0 \right) &= -\frac{c}{2\pi} \int_{-\infty}^{\infty} \frac{e^{ictk} - e^{-ictk}}{2ik}e^{ikx}\:dk\\
                            &= -\frac{c}{2}\Theta\left( ct-\left\vert x \right\vert \right)\Theta\left( t \right).
  \end{align*}
  There are some comments in order.
  \begin{itemize}
    \item The factor of $\Theta\left(t\right)$ effectively states that nothing happens before $t = 0$.
    \item The factor of $\Theta\left( ct - \left\vert x \right\vert \right)$ denotes causality. The term $\left\vert x \right\vert$ denotes the physical symmetry, while we need $ct - \left\vert x \right\vert > 0$ in order to feel an effect.
  \end{itemize}
  \subsubsection{Quantum Mechanics}%
  The Schrödinger equation is
  \begin{align*}
    i \hbar \pd{\psi}{t} + \frac{\hbar^2}{2m} + \pd{^2\psi}{x^2} &= 0.
  \end{align*}
  Here, $\psi$ denotes the wavefunction. The probability of a particle being found within $dx$ of $x$ is $\left\vert \psi \right\vert^2dx$.\newline

  In quantum mechanics, we define operators for energy and momentum as
  \begin{align*}
    E\coloneq +i\hbar \pd{}{t}\\
    P \coloneq -i\hbar \pd{}{x}.
  \end{align*}
  The Schrödinger equation falls from the fact that $E = \frac{P^2}{2m}$. These operators are Hermitian, so their eigenvalues are real.\newline

  We want to figure out the Green's function for the quantum hammer blow at $t = 0$, which we call the propagator.
  \begin{align*}
    i\hbar \pd{G}{t} + \frac{\hbar^2}{2m} \pd{^2G}{x^2} &= i\hbar \delta\left( x \right)\delta\left( t \right).
  \end{align*}
  When we try to evaluate it, we get
  \begin{align*}
    G\left( x,t > 0 \right) &=  \Theta\left( t \right)\sqrt{\frac{m}{2\pi i \hbar t}} e^{imx^2/2\hbar t}.
  \end{align*}
  This solution has a problem, though --- there is no sense of causality. This says that, as long as $t > 0$, there is going to be a measurable reaction at all $x$.\newline

  The main reason is that the Schrödinger equation is not relativistic. Accounting for the relativistic relationship, we have
  \begin{align*}
    E^2 - P^2c^2 &= m^2c^4.
  \end{align*}
  This gives an equation known as the Klein--Gordon equation:
  \begin{align*}
    -\frac{1}{c^2}\pd{^2\phi}{t^2} + \pd{^2\phi}{x^2} &= \frac{m^2c^2}{\hbar^2}\phi.
  \end{align*}
  Solving the Klein--Gordon equation gives infinitely many solutions that have both positive and negative energy, the latter of which is a major issue. However, it can be shown that the Klein--Gordon equation applies to particles with integral spin, with wave function
  \begin{align*}
    \phi(x,t) &= Ce^{i\left( kx-\omega t \right)}
  \end{align*}
  so long as
  \begin{align*}
    \frac{\omega^2}{c^2} - k^2 &= \frac{m^2c^2}{\hbar^2}.
  \end{align*}
  To see how causality fares for the Klein--Gordon wavefunction, we solve it for the unit impulse acting on $x=0$ at $t = 0$.
  \begin{align*}
    \left( -\pd{^2}{t^2} + \pd{^2}{x^2} - m^2 \right) G\left( x,t \right) &= i\delta\left( x \right)\delta\left( t \right),
  \end{align*}
  where we set $\hbar = c = 1$. We get the Green's function
  \begin{align*}
    G\left( x,t \right) &= i \int_{-\infty}^{\infty} \frac{1}{2\pi}e^{ipx}\:dp\int_{-\infty}^{\infty} \frac{e^{-iEt}}{2\pi \left( E^2 - p^2 - m^2 \right)}\:dE.
  \end{align*}
  We have two poles at $\pm\sqrt{p^2 + m^2}$, and when we close the contour we obtain different physical results.\newline

  When $t < 0$, we must close the contour in the upper-half plane, so we exclude the poles from the contour. Since there are no poles in the upper half-plane, we get a factor of $\Theta(t)$.\newline

  Thus, when $t > 0$, we close in the lower half-plane, giving
  \begin{align*}
    I &= \int_{-\infty}^{\infty} \frac{1}{2\pi}\frac{e^{-iEt}}{\left( E + iE \right)^2 - p^2 - m^2}\:dE\\
      &= -\frac{1}{2\pi}2\pi i \left( \frac{e^{-i\sqrt{p^2 + m^2}t}}{2\sqrt{p^2 + m^2}} - \frac{e^{i\sqrt{p^2 + m^2}t}}{2\sqrt{p^2 + m^2}}\right).
  \end{align*}
  Defining $E_p\coloneq \sqrt{p^2 + m^2}$, we have
  \begin{align*}
    I &= -\frac{i}{2E_p}\left( e^{-iE_p t} - e^{iE_p t} \right).
  \end{align*}
  Thus, we have
  \begin{align*}
    G\left( x,t> 0 \right) &= \Theta\left( t \right)\int_{-\infty}^{\infty} \frac{1}{2\pi} \frac{e^{ipx}}{2E_p}\left( e^{-iE_p t} - e^{iE_P t} \right)\:dp\\
                           &= \frac{\Theta\left( t \right)}{4\pi} \int_{-\infty}^{\infty} \frac{1}{E_p}\left( e^{-i\left( E_p t - px \right)} - e^{i\left( E_p t - p_x \right)} \right)\:dp,
  \end{align*}
  where we were allowed to flip the sign of $p$ because the integral is taken over all space.
  \section{Tensors}%
  \subsection{Cartesian Tensors}%
  Recall from intro mechanics that when we have a block on a ramp, we may choose two different orientations of our axes. However, despite our choice of basis, we still get the same result after applying Newton's second law --- in that sense, the vectors represent something physical separate from their components. Using a rotation matrix, we are able to convert the equations
  \begin{align*}
    F_x &= ma_x\\
    F_y &= ma_y,
  \end{align*}
  into
  \begin{align*}
    F_x' &= ma_x'\\
    F_y' &= ma_y'.
  \end{align*}
  This means that an expression like $\mathbf{F} = m\mathbf{a}$ is a \textit{covariant} expression; it is the same equation regardless of reference frame.
  \subsubsection{Stress and Strain}%
  If we have a continuous body, the stress is defined as the internal force per unit area exerted at an imaginary surface within the body. In other words, stress times area equals force.
  \begin{align*}
    dF_i &= \sum_{j}\sigma_{ij}da_j,
  \end{align*}
  where $\sigma_{ij}$ denotes the stress \textit{tensor}. The reason why it is a two-indexed object rather than a one-indexed object is because there are two different types of stress: pressure and shear.\newline

  Pressure is the normal component to the surface, while shear is the tangent component, so $\sigma_{ij}$ denotes the two components; then, the diagonal components denote the pressure and the off-diagonal components the shear.\newline

  Whereas stress describes the internal forces of an object, strain describes deformations due to external forces.\newline

  For instance, in a rod of length $L$ and cross section $A$, if we strain it to a length of $L + \Delta L$, the relationship between induced stress $\sigma = \frac{F}{A}$ normal to surface and strain $\ve = \Delta L + L$ is given by Hooke's law:
  \begin{align*}
    F &= k\Delta L\\
    \sigma &= Y\ve,
  \end{align*}
  where $Y = \frac{kL}{A}$ is the elastic modulus of the material. Now, since $\Delta L$ can be negative, this equation describes both tensile stress and strain and compressive stress and strain.\newline

  Now, if stress is a tensor, then so is strain; moving beyond one dimension, we define a vector field $\mathbf{u}\left( \mathbf{r} \right)$ which gives the magnitude and direction of a body's displacement at position $\mathbf{r}$. To understand internal stress, we need to focus not on the rigid body motion, but only on the relative displacement of points within the body. This means we need to consider
  \begin{align*}
    u_i\left( \mathbf{r} + d\mathbf{r} \right) &= u_i\left( \mathbf{r} \right) + \nabla u_i \cdot d\mathbf{r} + \cdots,
  \end{align*}
  where we allow $\left\vert \nabla u_i \right\vert \ll 1$, meaning we take the first order expansion. This gives
  \begin{align*}
    \mathbf{u}\left( \mathbf{r} + d\mathbf{r} \right) &= \mathbf{u}\left( \mathbf{r} \right) + \sum_{j} \delta_j \mathbf{u}dr_j,
  \end{align*}
  where
  \begin{align*}
    \nabla \mathbf{u} &= \begin{pmatrix} \pd{u_x}{x} & \pd{u_x}{y} & \pd{u_x}{z} \\ \pd{u_y}{x} & \pd{u_y}{y} & \pd{u_y}{z} \\ \pd{u_z}{x} & \pd{u_z}{y} & \pd{u_z}{z}\end{pmatrix}.
  \end{align*}
  This is where strain resides. Specifically, we may decompose $\nabla \mathbf{u}$ into symmetric and antisymmetric components,
  \begin{align*}
    \nabla \mathbf{u} &= \doublevec{\ve} + \doublevec{\phi},
  \end{align*}
  where
  \begin{align*}
    \ve_{ij} &= \frac{1}{2}\left( \partial_ju_i + \partial_iu_j \right)\\
    \phi_{ij} &= \frac{1}{2}\left( \partial_iu_j - \partial_ju_i \right).
  \end{align*}
  The symmetric/antisymmetric decomposition is covariant, so we may express
  \begin{align*}
    \mathbf{u}\left( \mathbf{r} + d\mathbf{r} \right) &= \mathbf{u}\left( \mathbf{r} \right) + \doublevec{\ve}\cdot d\mathbf{r} + \doublevec{\phi}\cdot d\mathbf{r}.
  \end{align*}
  Turning our attention to $\doublevec{\phi}$, we may use the Levi-Civita symbol to write
  \begin{align*}
    \phi_{ij} &= -\sum_{k} \epsilon_{ijk}\varphi_k,
  \end{align*}
  and take
  \begin{align*}
    \sum_{j}\phi_{ij}dr_j &= -\sum_{j,k} \epsilon_{ijk}\varphi_kdr_j\\
                          &= \sum_{j,k}\epsilon_{ikj}\varphi_kdr_j,
  \end{align*}
  meaning
  \begin{align*}
    d\mathbf{u} &= \vec{\varphi}\times d\mathbf{r},
  \end{align*}
  describing the rotation of $d\mathbf{r}$ by $\varphi$ around $\hat{\varphi}$. This is rigid body motion.\newline

  Thus, deformations are described by the strain tensor, $\doublevec{\ve}$. Hooke's law then becomes
  \begin{align*}
    \sigma_{ij} &= \sum_{k,l}Y_{ijkl}\ve_{kl},
  \end{align*}
  where $Y_{ijkl}$ is the elasticity tensor.
  \subsubsection{Equivalence Classes of Rotations}%
  A tensor is an indexed object, that transforms as
  \begin{align*}
    T_{i'j'} &= \sum_{i,j}R_{i',i}R_{j' j}T_{ij},
  \end{align*}
  where the number of rotations depends on the rank of the tensor.\footnote{For those more mathematically inclined, they're basically linear transformations on tensor products of vector spaces and their duals. Each index adds another vector to the tensor product or the dual.}
  \begin{example}[Moment of Inertia Tensor]
    The value of the moment of inertia depends on the choice of axes, but the moment of inertia is a physical quantity unto itself. We define
    \begin{align*}
      I_{k\ell} &= \int_{V}^{} \left( r^2\delta_{k\ell}r_kr_{\ell} \right)\rho\left( \mathbf{r} \right)\:d\tau.
    \end{align*}
    This is a symmetric tensor (symmetric matrix). Recalling that $r^2$ is a scalar, we have
    \begin{align*}
      I_{i'j'} &= \sum_{k,\ell}R_{i' k}R_{j' \ell}I_{k\ell}\\
               &= \sum_{k,\ell}R_{i' k}R_{j' \ell} \left( \int_{V}^{} \left( r^2\delta_{k\ell}-r_kr_{\ell} \right)\rho\left( \mathbf{r} \right)\:d\tau \right)\\
               &= \int_{V}^{} \left( \sum_{k,\ell}R_{i' k}R_{j' \ell}\delta_{k\ell}r^2 - \left( \sum_{k}R_{i' k}r_k \right)\left( \sum_{\ell}R_{j' \ell}r_{\ell} \right) \right)\rho\left( \mathbf{r} \right)\:d\tau\\
               &= \int_{V}^{} \left( \delta_{i' j'}r^2 - r_{i'}r_{j'} \right)\rho\left( \mathbf{r} \right)\:d\tau.
    \end{align*}
    Thus, the moment of inertia is a covariant expression.
  \end{example}
  In a rank $r$ tensor over a vector space $V$, there are $N^{r}$ components.\footnote{This comes from the fact that that there are $N^{r}$ dimensions in a $r$-fold tensor product of vector spaces with dimension $N$.}\newline

  Now we consider derivative operators. A Taylor expansion can be written as
  \begin{align*}
    f\left( \mathbf{r} + \mathbf{a} \right) &= \left( 1 + \sum_{i}a_i \pd{}{x_i} + \frac{1}{2!}\sum_{i,j}a_ia_j\pd{}{x_i}\pd{}{x_j} + \cdots \right) f\left( \mathbf{r} \right).
  \end{align*}
  This gives a tensor decomposition, where $\sum_{i}a_i \pd{}{x_i}$ is a rank $1$ tensor, and $\sum_{i,j}a_ia_j \pd{}{x_i}\pd{}{x_j}$ is a rank $2$ tensor.\newline

  To illustrate covariance, we consider the case of one dimension:
  \begin{align*}
    f\left( x + a \right) &= \left( 1 + a \diff{}{x} + \frac{a^2}{2!}\diff{^2}{x^2} \right)f(x)\\
                          &= \sum_{n=0}^{\infty}\frac{1}{n!}\left( a \diff{}{x} \right)^{n} f(x).
  \end{align*}
  To generalize this to higher dimensions, we can write the operator 
  \begin{align*}
    a \diff{}{x} &= \left( a\hat{i} \right)\cdot \left( \hat{i} \diff{}{x} \right)\\
    &= \mathbf{a}\cdot \nabla.
  \end{align*}
  Thus, using covariance of the derivative, we have
  \begin{align*}
    f\left( \mathbf{r} + \mathbf{a} \right) &= \sum_{n=0}^{\infty}\frac{1}{n!}\left( \mathbf{a}\cdot \nabla \right)^n f\left( \mathbf{r} \right).
  \end{align*}
  \subsubsection{Tensors and Pseudotensors}%
  We might have lied earlier\footnote{Common occurrence in physics} when we said that tensors transform through rotations:
  \begin{align*}
    T_{i'j'} &= \sum_{k,\ell} R_{i' k}R_{j' \ell}T_{k \ell}.
  \end{align*}
  We also need to care about parity. For an illustrative example, if we have a parity flip, we have $\mathbf{r}\xmapsto{p} -\mathbf{r}$ for position, $\mathbf{v}\xmapsto{p}-\mathbf{v}$ for velocity, but we have $\mathbf{\omega}\xmapsto{p} \mathbf{\omega}$ for angular velocity.\newline

  We call vectors that have a sign flip under parity change \textit{polar vectors}, but vectors that do not have a sign flip under parity change are called \textit{pseudovectors} or \textit{axial vectors}.\newline

  As it turns out, pseudovectors are generally brought about through a cross product between two axial vectors (in three dimensions anyway):
  \begin{align*}
    \mathbf{A}\times \mathbf{B} &\xmapsto{p} \left( -\mathbf{A} \right)\times \left( -\mathbf{B} \right)\\
                                &= \mathbf{A}\times \mathbf{B}.
  \end{align*}
  Note that scalars generally don't change under parity transformation, but expressions of the form $\mathbf{C}\cdot \left( \mathbf{A}\times \mathbf{B} \right)$ are \textit{pseudoscalars} that do change under parity transformation.\newline

  All of these are artifacts of the Levi-Civita symbol, $\epsilon_{ijk}$. Note that we have
  \begin{align*}
    \epsilon_{ijk} = \sum_{\ell,m,n} R_{i,\ell}R_{j,m}R_{k,n}\epsilon_{\ell mn},
  \end{align*}
  which yields the determinant of $R$ if $i,j,k$ are in cyclic order and the negative determinant of $R$ if $i,j,k$ are in anticyclic order. To deal with this sign issue, we need a factor of $\det(R)$,\footnote{Remember that $\det(R) = \pm 1$.} giving
  \begin{align*}
    \epsilon_{i' j' k'} &= \det(R) &= \sum_{\ell,m,n} R_{i' \ell}R_{j' m}R_{k' n}\epsilon_{\ell m n}.
  \end{align*}
  \subsubsection{Invariants}%
  The components of a tensor are frame-dependent, but an \textit{invariant} is a scalar combination of components that remains the same under a rotation. For instance, if $\mathbf{v}$ is vector, then $\sum_{i} v_iv_i = \norm{\mathbf{v}}$ is invariant under rotations.\newline

  If we have a rank $2$ tensor, we have
  \begin{align*}
    \sum_{i,j} T_{ij}\delta_{ij} &= \sum_{i}T_{ii}\\
                                 &= \tr\left( T \right).
  \end{align*}
  We may also collapse with the Levi-Civita symbol, yielding another invariant of $\frac{1}{3!}\det\left( T \right)$.\newline

  As it turns out, a rank $2$ tensor in $N$ dimensions has $N$ independent invariants.\newline

  However, you may ask: are we actually measuring the invariance of $T$, or are we measuring the invariance of $\delta_{ij}$ of $\epsilon_{ijk}$. Taking rotations, we have
  \begin{align*}
    \sum_{i,j} R_{k' i}R_{\ell' j}\delta_{ij} &= \sum_{i}R_{k'i}R_{\ell' i}\\
                                              &= \sum_{i}R_{k' i}R_{i \ell'}^{T}\\
                                              &= \delta_{k' \ell'},
  \end{align*}
  meaning that the identity matrix is invariant regardless of coordinate system. This is known as an \textit{invariant tensor}, or an isotropic tensor.
  \begin{example}
    Any $\R^3$ vector can be rewritten as a second-rank antisymmetric tensor by taking
    \begin{align*}
      T_{ij} &= \sum_{k}\epsilon_{ijk}B_k\\
             &= \begin{pmatrix}0 & B_3 & -B_2 \\ -B_3 & 0 & B_1\\B_2 & -B_1 & 0\end{pmatrix}.
    \end{align*}
    Inverting, we get
    \begin{align*}
      B_i &= \frac{1}{2}\sum_{j,k}\epsilon_{ijk}T_{jk}.
    \end{align*}
    Thus, $\doublevec{T}$ is an equivalent way to package the components of $B$ --- i.e., that they are duals of each other.
  \end{example}
  \subsection{Non-Cartesian Tensors}%
  What makes a tensor a tensor is that it maintains its symmetry upon some transformation. However, we may have a shear transformation.\newline

  Consider a coordinate system with
  \begin{align*}
    \hat{e}_u &= \begin{pmatrix}\cos\left( \alpha \right)\\\sin\left( \alpha \right)\end{pmatrix}\\
    \hat{e}_v &= \begin{pmatrix}\sin\left( \beta \right)\\\cos\left( \beta \right)\end{pmatrix},
  \end{align*}
  where $\alpha$ is an angle between the $x$ axis and $\hat{e}_u$, $\beta$ is an angle between the $y$ axis and $\hat{e}_v$, with $\phi$ the angle between $\hat{e}_u$ and $\hat{e}_v$.\newline

  Taking orthogonal projections of $\mathbf{A}$, we have
  \begin{align*}
    A_u &= \mathbf{A}\cdot \hat{e}_u\\
    A_v &= \mathbf{A}\cdot \hat{e}_v.
  \end{align*}
  Unfortunately, $\mathbf{A}\neq A_u\hat{e}_u + A_v\hat{e}_v$.\newline

  However, we can consider $A^{u}$ and $A^{v}$ as components \textit{along} $\hat{e}_u$ by projecting $\mathbf{A}$ parallel to $\hat{e}_v$ to obtain $A^{u}$ and projecting $\mathbf{A}$ parallel to $\hat{e}_u$ to obtain $A^{v}$, giving
  \begin{align*}
    \mathbf{A} &= A^{u}\hat{e}_u + A^{v}\hat{e}_v.
  \end{align*}
  Yet, since $A_u$ and $A_v$ are linearly independent components, we should still be able to construct $\mathbf{A}$ using their components. Specifically, we extend the orthogonal projections of $\mathbf{A}$ on $\hat{e}_u$ and $\hat{e}_v$ until we get a parallelogram. This gives
  \begin{align*}
    \mathbf{A} &= \frac{1}{\sin\left( \phi \right)}\left( A_u\hat{e}^{u} + A_{v}\hat{e}^{v} \right).
               &=  A_u\vec{e}^{u} + A_v\vec{e}^{v}.
  \end{align*}
  The sets $\set{\hat{e}_u,\hat{e}_v}$ and $\set{\vec{e}^u,\vec{e}^v}$ are intimately tied to each other:
  \begin{align*}
    \hat{e}_i\cdot \vec{e}^j &= \delta_{ij}
  \end{align*}
  The vector $\mathbf{A}$ can thus be decomposed along the ``downstairs basis'' and the ``upstairs basis,'' with components defined by
  \begin{align*}
    A_{a} &= \hat{e}_a\cdot \mathbf{A}\\
    A^{a} &=\vec{e}^{a}\cdot \mathbf{A};
  \end{align*}
  with $\vec{e}^{a}$ not necessarily normalized. In this case, we have
  \begin{align*}
    A_u &= \vec{e}_u\cdot \mathbf{A}\\
        &= A^{u} + A^{v}\cos\left( \phi \right)\\
    A_{v} &= \vec{e}_v\cdot \mathbf{A}\\
          &= A^{u}\cos\left( \phi \right) + A^{v}.
  \end{align*}
  \subsubsection{Metric Tensors}%
  We want covariant index notation to apply to all bases in a straightforward manner.\newline

  We start with the Cartesian line element, $d\mathbf{s} = \sum_{i}\hat{e}_idx_i$. We want a general case of $d\mathbf{s} = \sum_{a}\vec{e}_a du^{a}$, where all the scale factors are hidden in the basis $\set{\vec{e}_a}$. We calculate
  \begin{align*}
    ds^2 &= d\mathbf{s}\cdot d\mathbf{s}\\
         &= \left( \sum_{a}\vec{e}_adu^a \right)\cdot \left( \sum_{b}\vec{e}_bdu^b \right)\\
         &= \sum_{a,b}\left( \vec{e}_a\cdot \vec{e}_b \right)du^adu^b\\
         &= \sum_{a,b}g_{ab}du^adu^b,
  \end{align*}
  where $g_{ab} = \vec{e}_a\cdot \vec{e}_b$. We call the matrix $\left( g_{ab} \right)_{ab}$ the \textit{metric tensor}.
  \begin{example}
    In Cartesian coordinates, our metric is
    \begin{align*}
      g_{ab} &= \delta_{ij}\\
             &= \begin{pmatrix}1 & & \\ & 1 & \\ & & 1\end{pmatrix}.
    \end{align*}
    In spherical coordinates, our metric is
    \begin{align*}
      g_{ab} &= \begin{pmatrix}1 & & \\ & r^2 & \\ & & r^2\sin^2\left( \theta \right)\end{pmatrix},
    \end{align*}
    after expanding on the spherical basis (see \href{https://ai.avinash-iyer.com/Classes_and_Homework/College/Y4/Y4S1,%20Math%20Methods/math_methods_notes.pdf}{Math Methods Notes}). This shows a couple things.
      \begin{itemize}
        \item The metric can be a function of position.
        \item The same coordinate transformation that moves us from Cartesian to spherical is the same coordinate transformation that moves from the metric tensor on Cartesian coordinates to the metric tensor on spherical coordinates.
      \end{itemize}
  \end{example}
  \begin{example}
    In relativity, the difference between two points is
    \begin{align*}
      ds^2 &= c^2dt^2 - dr^2.
    \end{align*}
    The Minkowski metric is denoted $\eta_{\mu\nu}$,
    \begin{align*}
      ds^2 &= \sum_{\mu,\nu} \eta_{\mu\nu}dx^{\mu}dx^{\nu},
    \end{align*}
    where $\mu$ and $\nu$ range from $0$ to $3$, and $x^{0} = ct$. In Cartesian, we have
    \begin{align*}
      \eta_{\mu\nu} &= \begin{pmatrix}1 & & & \\ & -1 & & \\ & & -1 & \\ & & & -1\end{pmatrix}.
    \end{align*}
  \end{example}
  \begin{example}
    For the shear system, we have the metric of
    \begin{align*}
      g_{ab} &= \begin{pmatrix}\vec{e}_u\cdot \vec{e}_u & \vec{e}_u\cdot \vec{e}_v\\ \vec{e}_v\cdot \vec{e}_u & \vec{e}_v\cdot \vec{e}_v\end{pmatrix}\\
             &= \begin{pmatrix}1 & \cos\left( \phi \right)\\\cos\left( \phi \right) & 1\end{pmatrix}.
    \end{align*}
    In terms of the line element, we have
    \begin{align*}
      ds^2 &= \sum_{a,b}g_{ab}du^{a}du^{b}\\
           &= du^2 + 2\cos\left( \phi \right)dudv + dv^2.
    \end{align*}
    Note that there is a cross term, meaning our system is not orthogonal. Furthermore, this is actually the law of cosines.\newline

    The norm of a vector $\mathbf{A}$ is
    \begin{align*}
      \norm{\mathbf{A}}^2 &= \left( \sum_{a}A^{a}\vec{e}_a \right)\cdot \left( \sum_{b}A^{b}\vec{e}_b \right)\\
                          &= \sum_{a,b}g_{ab}A^{a}A^{b}\\
                          &= \left( A^{u} \right)^2 + 2\cos\left( \phi \right)A^uA^v + \left( A^v \right)^2.
    \end{align*}
  \end{example}
  \subsubsection{Streamlining Notation}%
  In general, we can write
  \begin{align*}
    \mathbf{A}\cdot \mathbf{B} &= \sum_{a,b}g_{ab}A^{a}B^{b}.
  \end{align*}
  Setting
  \begin{align*}
    A_b &= \sum_{a}g_{ab}A^{a}\\
    B_a &= \sum_{b}g_{ab}B^{b},
  \end{align*}
  we get
  \begin{align*}
    \mathbf{A}\cdot \mathbf{B} &= \sum_{a}A^aB_a\\
                               &= \sum_{b}A_bB^{b}.
  \end{align*}
  We can consider the metric as ``lowering'' the indices of $A^{a}$ or $B^{b}$. The new rule is that any index must be summed over one with an ``upstairs'' index and one with a ``downstairs'' index.\newline

  From now on, we will be using the Einstein summation notation, converting
  \begin{align*}
    A_a &= \sum_{a}M_{ab}B^{b}
  \end{align*}
  to be
  \begin{align*}
    A_a &= M_{ab}B^{b},
  \end{align*}
  where the sum is implied by the double index.\newline

  We should be able to convert $g_{ab}$ to $g^{ab}$. We take
  \begin{align*}
    g^{da}\left( g^{cb}g_{ab} \right) &= g^{da}\left( g_{a}^{c} \right)\\
                                      &= g^{dc}.
  \end{align*}
  Note that if we take
  \begin{align*}
    \mathbf{A}\cdot \mathbf{B} &= A^aB_a\\
                               &= \left( g^{ab}A_{b} \right)\left( g_{ac}B^{c} \right)\\
                               &= \left( g^{ab}g_{ac} \right)A_bB^{c}\\
                               &= A_bB^{b},
  \end{align*}
  so that $g^{ab}g_{ac} = \delta^{b}_{c}$, or that $g^{ab}$ and $g_{ab}$ are inverses of each other.
  \begin{example}
    Recall that the line element in spherical coordinates is
    \begin{align*}
      d\mathbf{s} &= \hat{r}dr + r\hat{\theta} d\theta + r\sin\theta\hat{\phi}d\phi,
    \end{align*}
    giving downstairs basis of
    \begin{align*}
      \vec{e}_r &= \hat{r}\\
      \vec{e}_{\theta} &= r\hat{\theta}\\
      \vec{e}_{\phi} &= r\sin\theta \hat{\phi}.
    \end{align*}
    We usually write this as
    \begin{align*}
      d\mathbf{s} &= \vec{e}_adu^{a},
    \end{align*}
    with the implied sum on $a$. Calculating $g_{ab}$, we get
    \begin{align*}
      g_{ab} &= \begin{pmatrix}1 & & \\ & r^2 & \\ & & r^2\sin^2\left( \theta \right)\end{pmatrix},
    \end{align*}
    and 
    \begin{align*}
      A_r &= g_{ra}A^{a}\\
          &= g_{rr}A^{r} + g_{r\theta}A^{\theta} + g_{r\phi}A^{\phi}\\
          &= A^{r}\\
      A_{\theta} &= g_{\theta a}A^{a}\\
                 &= g_{\theta r}A^{r} + g_{\theta \theta}A^{\theta} + g_{\theta \phi}A^{\phi}\\
                 &= r^2A^{\theta}\\
      A_{\phi} &= g_{\phi a}A^{a}\\
               &= g_{\phi r}A^{r} + g_{\phi \theta}A^{\theta} + g_{\phi \phi}A^{\phi}.\\
               &= r^2\sin^2\left( \theta \right)A^{\phi}.
    \end{align*}
    In particular, this gives
    \begin{align*}
      g^{ab} &= \begin{pmatrix}1 & & \\ & 1/r^2 & \\ & & 1/r^2\sin^2\left( \theta \right)\end{pmatrix}.
    \end{align*}
    Using $\vec{e}^{a} = g^{ab}\vec{e}_{b}$, we have
    \begin{align*}
      \vec{e}^{r} &= g^{ra}\vec{e}_{a}\\
                  &= \vec{e}_{r}\\
                  &= \hat{r}\\
      \vec{e}^{\theta} &= \frac{1}{r}\hat{\theta}\\
      \vec{e}^{\phi} &= \frac{1}{r\sin\theta}\hat{\phi}.
    \end{align*}
    For orthogonal coordinates, the bases $\vec{e}_{a}$ and $\vec{e}^{a}$ are related by the metric to their more familiar orthonormal versions, which we write $\hat{\epsilon_{a}}$, by writing
    \begin{align*}
      \vec{e}_{a} &= \hat{\epsilon}_{a}\sqrt{g_{aa}}\\
      \vec{e}^{a} &= \frac{1}{\sqrt{g_{aa}}}\hat{e}_{a},
    \end{align*}
    without implied sum.
  \end{example}
  To transform from coordinate system to coordinate system (rather than between a coordinate system and its dual), we express transformation of components and bases by taking
  \begin{align*}
    A^{b'} &= T_{a}^{b'}A^{a}\\
    \vec{e}_{b'} &= \vec{e}_{a}S^{a}_{b'}.
  \end{align*}
  \begin{example}
    We find a transformation between coordinate systems by projecting both vector expansions
    \begin{align*}
      \mathbf{A} &= \sum_{a'}\vec{e}_{a'}A^{a'}\\
        &= \sum_{a}\vec{e}_{a}A^{a}.
    \end{align*}
    We project both vector expansions onto the $\vec{e}^{b'}$ basis by taking dot products:
    \begin{align*}
      \vec{e}^{b'}\cdot \mathbf{A} &= \sum_{a'}\left( \vec{e}^{b'}\cdot \vec{e}_{a'} \right)A^{a'}\\
                                   &= \sum_{a}\left( \vec{e}^{b'}\cdot \vec{e}_{a} \right)A^{a}.
    \end{align*}
    Using the mutual orthogonality relation
    \begin{align*}
      \vec{e}^{b'} \cdot \vec{e}_{a} &= g_{a}^{b'}\\
                                     &= \delta_{a'}^{b'},
    \end{align*}
    we get
    \begin{align*}
      A^{b'} &= \sum_{a}\left( \vec{e}^{b'}\cdot e_a \right)A^{a}
    \end{align*}
  \end{example}
  \begin{example}
    Consider the shear system,
    \begin{align*}
      \begin{pmatrix}A^{x}\\A^{y}\end{pmatrix} &= \begin{pmatrix}\vec{e}^{x}\cdot \vec{e}_{u} & \vec{e}^{x}\cdot\vec{e}_{v} \\ \vec{e}^{y}\cdot \vec{e}_{u} & \vec{e}^{y}\cdot \vec{e}_{u}\end{pmatrix} \begin{pmatrix}A^{u}\\A^{v}\end{pmatrix}\\
      T_{a}^{i} &= \begin{pmatrix}\cos\left( \alpha \right) & \sin\left( \beta \right)\\ \sin\left( \alpha \right) & \cos\left( \beta \right)\end{pmatrix}.
    \end{align*}
  \end{example}
  It is important to emphasize that a tensor transformation between systems or frames switches between prime basis and non-prime basis.\newline

  Now, we may define a tensor to be
  \begin{align*}
    A^{a'} &= T_{b}^{a'}A^{b}.
  \end{align*}
  Generally speaking, the rank is the number of indices (both upstairs and downstairs).\newline

  Consider a simple dot product:
  \begin{align*}
    \mathbf{A}\cdot \mathbf{A} &= A^iA_j\\
                               &= \delta_{ij}A^iA^j\\
                               &= g_{ab}A^{a}A^{b}.
  \end{align*}
  This expression allows us to write the dot product in \textit{any} coordinate system, just by knowing the metric.\footnote{This is akin to the fact that a bilinear form is defined by the matrix in the expression $ \varphi\left( x,x \right) = x^{T}Ax $.}
  \subsubsection{General Covariance}%
  The physics\footnote{Who cares about that?} of a system does not depend on the origin. Therefore, we need a way to talk about covariance without specifying an origin.\newline

  Therefore, rather than working in $\mathbf{r}$, or general $\mathbf{u}$, we work in $d\mathbf{r}$ and $d\mathbf{u}$, since the difference between two vectors does not depend on the origin.\newline

  The chain rule then gives us
  \begin{align*}
    du^{a'} &= \pd{u^{a'}}{u^{b}}du^{b},
  \end{align*}
  with an implied sum over $b$. As it turns out,
  \begin{align*}
    \pd{}{u_b} &= \partial_{b},
  \end{align*}
  or that derivative with respect to an upstairs element is a downstairs tensor. The matrix $\left( \pd{u^{a'}}{u^{b}} \right)_{ab}$ is the Jacobian matrix.\newline

  For instance, calculating the Jacobian matrix for polar coordinates, we have
  \begin{align*}
    T_{b}^{a'} &= \pd{u^{a'}}{u^{b}}\\
               &= \pd{\left( r,\phi \right)}{\left( x,y \right)}\\
               &= \begin{pmatrix}\pd{r}{x} & \pd{r}{y}\\\pd{\phi}{x} & \pd{\phi}{y}\end{pmatrix}\\
               &= \begin{pmatrix}x/r & y/r \\ -y/r^2 & x/r^2\end{pmatrix}.
  \end{align*}
  Thus, we get
  \begin{align*}
    dr &= \frac{x}{r}dx + \frac{y}{r}dy\\
    d\phi &= -\frac{x}{r^2}dx + \frac{y}{r^2}dy.
  \end{align*}
  Therefore, we are now going to define \textit{general covariance} of vectors to be of the form
  \begin{align*}
    A^{a} &= \pd{u^{a'}}{u^{b}}A^{b}.
  \end{align*}
  However, note that the gradient transforms as
  \begin{align*}
    \partial_{a'} &= \pd{u^{b}}{u^{a'}} \partial_{b}.
  \end{align*}
  \begin{example}
    We want to understand a symmetry transformation of the metric. We must have
    \begin{align*}
      g_{a'b'} &= \pd{u^{a}}{u^{a'}} \pd{u^{b}}{u^{b'}}g_{ab}.
    \end{align*}
    We will now try to convert from Cartesian to polar. This gives $g_{ab} = \delta_{ij}$, so
    \begin{align*}
      g_{rr} &= \pd{x^{i}}{r}\pd{x^{j}}{r}\delta_{ij}\\
             &= \left( \pd{x}{r} \right)^2 + \left( \pd{y}{r} \right)^2\\
             &= 1\\
      g_{\phi \phi} &= \pd{x^{i}}{\phi} \pd{x^{j}}{\phi}\delta_{ij}\\
                    &= \left( \pd{x}{\phi} \right)^2 + \left( \pd{y}{\phi} \right)^2\\
                    &= r^2\\
      g_{r\phi} &= \pd{x}{r}\pd{x}{\phi} + \pd{y}{r}\pd{y}{\phi}\\
                &= 0.
    \end{align*}
  \end{example}
  \subsubsection{Tensor Derivatives}%
  Now that we have introduced a tensor regime based on derivatives, we are now interested in understanding what happens when we take derivatives \textit{of} tensors.\newline

  If we take $\pd{}{u^{b'}}A^{a'}$, we want this to yield a transformation as a mixed second-rank tensor. The tensor transformation is of the form
  \begin{align*}
    T^{a'}_{b'} &= \pd{u^{a'}}{u^{c}}\pd{u^{d}}{u^{b'}}T^{c}_{d}.
  \end{align*}
  However, taking
  \begin{align*}
    \pd{}{u^{b'}}A^{a'} &= \pd{}{u^{b'}} \left( \pd{u^{a'}}{u^{c}}A^{c} \right)
    \intertext{and using the chain rule to convert back to the non-prime frame}
                        &= \pd{u^{d}}{u^{b'}}\pd{}{u^{d}} \left( \pd{u^{a'}}{u^{c}}A^{c} \right).
                        \intertext{Using the product rule and rearranging, we take}
                        &= \underbrace{\pd{u^{d}}{u^{b'}}\pd{u^{a'}}{u^{c}}\left( \pd{}{u^{d}}A^{c} \right)}_{\text{expected}} + \underbrace{A^{c} \pd{^2u^{a'}}{u^{c}\partial u^{d}}\pd{u^{d}}{u^{b'}}}_{\text{why?}}.
  \end{align*}
  Because we allowed general coordinate transformations, we created the seemingly extraneous term consisting of second derivatives (beyond what we wanted for a second-rank tensor transformation). This is because we forgot to include the change \textit{in the basis itself}, beyond the change in component.\newline

  Then, we have
  \begin{align*}
    \pd{}{u^{b}}\left( A^{a}\vec{e}_{a} \right) &= \pd{A^{a}}{u^{b}}\vec{e}_a + A^{a}\pd{\vec{e}_{a}}{u^{b}}.
  \end{align*}
  The expression $\pd{\vec{e}_a}{u^{b}}$ is our offending character. Since this is a vector-valued integral, we may expand it on the basis
  \begin{align*}
    \pd{\vec{e}_a}{u^{b}} &= \Gamma_{ab}^{c}\vec{e}_c,\label{eq:christoffel_symbol}\tag{$\ast$}
  \end{align*}
  with an implied sum over $c$. The term $\Gamma_{ab}^{c}$ is known as the \textit{Christoffel symbol} for $\pd{\vec{e}_a}{u^{b}}$.
  \begin{example}[Christoffel Symbols in Polar Coordinates]
    Recall that
    \begin{align*}
      \vec{e}_a &= \pd{\mathbf{r}}{u^{a}},
    \end{align*}
    and expanding on the Cartesian basis (yet again with implied sums), we have
    \begin{align*}
      \vec{e}_a &= \pd{x^j}{u^{a}}\pd{\mathbf{r}}{x^j}\\
                &= \pd{x^{j}}{u^{a}}\hat{x}_j.
    \end{align*}
    Now, 
    \begin{align*}
      \pd{\vec{e}_a}{u^{b}} &= \pd{\vec{e}^{a}}{u^{b}}
      \intertext{and recalling that $\vec{e}_{a} = \pd{\mathbf{r}}{u^a}$, we have}
                            &= \pd{^2}{u^{a}\partial u^{b}}\mathbf{r}.
    \end{align*}
    In polar coordinates, we have the $\left( r,\phi \right)$ basis, giving
    \begin{align*}
      \vec{e}_r &= \cos\left( \phi \right)\hat{i} + \sin\left( \phi \right)\hat{j}\\
                &= \hat{r}\\
      \vec{e}_{\phi} &= -r\sin\left( \phi \right)\hat{i} + r\cos\left( \phi \right)\hat{j}\\
                     &= r\hat{\phi}.
    \end{align*}
    Now, using the fact that we pull out a component by taking a dot product, we may fin
    \begin{align*}
      \Gamma_{\phi\phi}^{r} &= \vec{e}_r\cdot \pd{\vec{e}_{\phi}}{\phi}\\
                            &= \hat{r}\cdot \left( -r\hat{r} \right)\\
                            &= -r.
    \end{align*}
    Similarly, we may find the rest of the Christoffel symbols, giving
    \begin{align*}
      \Gamma^{\phi}_{r\phi} &= \frac{1}{r}\\
      \Gamma^{\phi}_{\phi r} &= \frac{1}{r},
    \end{align*}
    and giving $0$ for everything else.\newline

    Therefore, for polar coordinates, we have
    \begin{align*}
      \diff{\mathbf{r}}{t} &= \diff{}{t}\left( r\vec{e}_r \right)\\
                           &= \dot{r}\vec{e}_r + r\diff{u^{j}}{t}\diff{}{u^{j}}\Gamma_{r j}^{k}\vec{e}_k,
    \end{align*}
    with implicit sums over $j$ and $k$. Plugging in our nonzero terms, we have
    \begin{align*}
      \diff{\mathbf{r}}{t} &= \dot{r}\hat{r} + r\dot{\phi}\hat{\phi}.
    \end{align*}
  \end{example}
  Now, we want to obtain a tensor out of this derivative. We will take the expression of \eqref{eq:christoffel_symbol} and relabel the dummy indices
  \begin{align*}
    \pd{}{u^{b}}\left( A^{a}\vec{e}_a \right) &= \pd{A^{a}}{u^{b}}\vec{e}_a + A^{a}\pd{\vec{e}_a}{u^{b}}\\
                                              &= \left( \pd{A^{a}}{u^{b}}+ A^{c}\Gamma^{a}_{bc} \right)\vec{e}_a
  \end{align*}
  The quantity in the parentheses is the component of the derivative on the basis $\vec{e}_a$. We write
  \begin{align*}
    D_{b} A^{a} &= \pd{A^{a}}{u^{b}} + A^{c}\Gamma_{bc}^{a},
  \end{align*}
  with an implied sum over $c$. The expression $D_b A^{a}$ gives the \textit{covariant derivative}.\newline

  Therefore, we may obtain the general rule
  \begin{align*}
    D_cT^{ab} &= \pd{T^{ab}}{u^{c}} + T^{db}\Gamma_{cd}^{a} + T^{ad}\Gamma_{cd}^{b}.
  \end{align*}
  We need one Christoffel symbol summation for each tensor index. Furthermore, if we have a dual tensor, we have
  \begin{align*}
    D_{c}T_{ab} &= \pd{T_{ab}}{u^{c}} - T_{db}\Gamma^{d}_{ac} - T_{ad}\Gamma^{d}_{bc}.
  \end{align*}
  Now, if we take the covariant derivative of a scalar, we get the regular derivative:
  \begin{align*}
    D_a\Phi &= \pd{\Phi}{u^{a}}.
  \end{align*}
  \begin{example}
    Consider the acceleration,
    \begin{align*}
      \mathbf{a} &= \diff{\mathbf{v}}{t},
    \end{align*}
    Then, by the chain rule,
    \begin{align*}
      \diff{\mathbf{v}}{t} &= \left( \frac{d\mathbf{r}}{t}\cdot \nabla \right)\mathbf{v}\\
                           &= \left( \mathbf{v}\cdot \nabla \right)\mathbf{v}.
    \end{align*}
    In components, we have
    \begin{align*}
      a^i &= \diff{v^i}{t}\\
          &= v^{k} \pd{v^{i}}{x^k}.
    \end{align*}
    Note that since we are doing this in Cartesian, we do not need any Christoffel symbols.\newline

    When we are working in general coordinates, we may replace all the $\partial$ symbols with $D_a$, by the principle of covariance. Therefore, in any coordinate system, we have
    \begin{align*}
      a^{b} &= \frac{Dv^{b}}{dt}\\
            &= v^{c}D_cv^{b}\\
            &= v^{c}\left( \pd{v^{b}}{x^{c}} + v^{d}\Gamma^{b}_{cd} \right)\\
            &= \diff{v^{b}}{t} + \Gamma^{b}_{cd}v^cv^d.
    \end{align*}
    Now, if we plug in the values of $\Gamma$ and take the sum over $b,c$ on the right for polar coordinates, we have
    \begin{align*}
      a^{r} &= \ddot{r} - r\dot{\phi}^2\\
      a^{\phi} &= \ddot{\phi} + \frac{2}{r}\dot{r}\dot{\phi}.
    \end{align*}
  \end{example}
  \subsubsection{Free Particle Motion}%
  When we deal with free particle motion (i.e., not subject to any forces), we assume zero acceleration. In other words,
  \begin{align*}
    \frac{Dv^{b}}{t} &= \diff{v^{b}}{t} + \Gamma^{b}_{cd}v^cv^d\\
                     &= 0.
  \end{align*}
  This equation is also written
  \begin{align*}
    \diff{^2u^b}{t} + \Gamma^{b}_{cd} \diff{u^c}{t} \diff{u^d}{t}&= 0,
  \end{align*}
  and is known as the \textit{geodesic equation}.\newline

  Now, if we work through the case of the geodesic with polar coordinates, we have
  \begin{align*}
    \ddot{r}-r\dot{\phi}^2 &= 0\\
    \ddot{\phi} + \frac{2}{r} \dot{r}\dot{\phi} &= 0.
  \end{align*}
  We want to solve for the trajectory. There is only one solution to this set of equations:
  \begin{align*}
    r &= a + bt\\
    \phi &= \phi_0.
  \end{align*}
  This denotes straight line motion. Specifically, the geodesic is the ``straight'' line on any (smooth) manifold, or a straight line on a tangent space.\newline

  Now, in a $2$-manifold that is not $\R^2$, the ``curvature'' appears as the term in the Christoffel symbol.\newline

  Specifically, the metric is what distinguishes one space from another space. We may find the metric by taking
  \begin{align*}
    D_cg_{ab} &= \pd{g_{ab}}{u^{c}} - g_{db}\Gamma_{ac}^{d} - g_{ad}\Gamma_{bc}^{d}\\
              &= 0,
  \end{align*}
  since the metric in Cartesian coordinates is constant. Specifically, this allows us to express $\Gamma$ as
  \begin{align*}
    \Gamma_{ab}^{c} &= \frac{1}{2}g^{cd}\left( \pd{}{u^a}g_{bd} + \pd{}{u^d}g_{da} - \pd{}{u^{d}}g_{ab}\right),
  \end{align*}
  with the implied sum over $d$.\newline

 % \begin{example}[Covariance, the metric, and Christoffel symbols]
 %   The metric in $\R^n$ is $\delta_{ij}$. Note that $\pd{}{u^i}\delta_{ij} = 0$, meaning $D_i\delta_{ij} = 0$. Therefore, by the principle of covariance, we have
 %   \begin{align*}
 %     D_ag_{ab} &= 0.
 %   \end{align*}
 %   This allows us to find
 %   \begin{align*}
 %     \Gamma^{c}_{ab} &+ \frac{1}{2}g^{cd}\left( \pd{}{u^a}g_{bd} + \pd{}{u^b}g_{da} + \pd{}{u^d}g_{ab} \right).
 %   \end{align*}
 % \end{example}
 %Using the case of velocity, we have
 %\begin{align*}
 %  \frac{Dv^{b}}{dt} &= \diff{^2u^{b}}{dt^2} + \Gamma_{cd}^{b}\diff{u^{c}}{t}\diff{u^{d}}{t}.
 %\end{align*}
  Going back to the geodesic equation, we consider the question: if our space is curved, what does it mean to travel in a ``straight line.'' Consider a particle moving along some curve $C$ that covers a distance
  \begin{align*}
    S &= \int_{C}^{} \:ds\\
      &= \int_{C}^{} \sqrt{dx^2 + dy^2}.
      \intertext{However, since $dx^2 + dy^2$ is just the metric in $\R^n$, we may substitute it with other metrics, yielding}
      &= \int_{C}^{} \sqrt{\left\vert g_{ab}du^adu^b \right\vert}\\
      &= \int_{t_1}^{t_2} \sqrt{\left\vert g_{ab}\diff{u^{a}}{t}\diff{u^{b}}{t} \right\vert}\:dt.
  \end{align*}
  Now, the question that arises is what path yields an extremal path. Considering variations of $C$, we take $u(t) \mapsto u(t) + \delta u(t)$; we ask now what $\delta u$ gives $\delta S = 0$.\footnote{This is the Euler--Lagrange equation} Then, using the chain rule, a lot of messy index notation, and the fact that $g_{ab}$ is symmetric, we get
  \begin{align*}
    \delta S &= \frac{1}{2}\int_{t_1}^{t_2} \left( g_{ab}\diff{u^{a}}{t}\diff{u^{b}}{t} \right)^{-1/2}\left( \left( \pd{g_{ab}}{u^{c}}\diff{u^{c}}{t} \right)\diff{u^{a}}{t}\diff{u^{b}}{t} + 2g_{ab}\diff{\left( \delta u^{a} \right)}{t}\diff{u^{b}}{t} \right)\:dt.
  \end{align*}
  We may clean this up a little by taking $\diff{s}{t} = \sqrt{g_{ab}\diff{u^{a}}{t}\diff{u^{b}}{t}}$.
  \begin{align*}
    \delta S &= \frac{1}{2}\int_{s_1}^{s_2}\diff{t}{s}\left( \left( \pd{g_{ab}}{u^{c}}\diff{u^{c}}{t} \right)\diff{u^{a}}{t}\diff{u^{b}}{t} + 2g_{ab}\diff{\left( \delta u^{a} \right)}{t}\diff{u^{b}}{t} \right) \diff{t}{s}\:ds\\
             &= \int_{s_1}^{s_2} \left( \frac{1}{2}\pd{g_{ab}}{u^{c}}\delta u^{c}\diff{u^{a}}{s} \diff{u^{b}}{s} + g_{ab}\diff{\left( \delta u^{a} \right)}{s}\diff{u^{b}}{s} \right)\:ds.
  \end{align*}
  We stipulate that $u + \delta u$ has the same endpoints as $u$. Then, $\delta u\left( s_1 \right) = \delta u\left( s_2 \right) = 0$. To get rid of the term $\diff{\left( \delta u^{a} \right)}{s}$, we integrate the second term by parts, which we may do by paying for with a minus sign (the boundary term vanishes by our stipulation). This gives
  \begin{align*}
    \delta S &= \int_{s_1}^{s_2} \left( \frac{1}{2}\pd{g_{ab}}{u^{c}}\diff{u^a}{s}\diff{u^b}{s} - \pd{g_{bc}}{u^{a}}\diff{u^a}{s}\diff{u^b}{s} - g_{ac}\diff{^2u^a}{s^2} \right)\delta u^{c}\:ds.
  \end{align*}
  Every partial derivative of the metric may be replaced by the Christoffel symbols, giving
  \begin{align*}
    \delta S &= \int_{s_1}^{s_2} \left( \diff{^2u^c}{s^2} + \Gamma_{ab}^{c}\diff{u^a}{s}\diff{u^b}{s} \right)g_{cd}\delta u^{d}\:ds.
  \end{align*}
  Since $\delta u$ is fully arbitrary, the path $u(s)$ that solves $\delta S = 0$ is a solution to
  \begin{align*}
    \diff{^2u^c}{s^2} + \Gamma_{ab}^{c} \diff{u^a}{s}\diff{u^b}{s} &= 0.
  \end{align*}
  Thus, we get the geodesic equation back.
  \section{Orthogonal Functions}%
  Recall the fundamental orthogonality relation
  \begin{align*}
    \braket{\phi_n}{\phi_m} &= k_n\delta_{nm},
  \end{align*}
  where we refer to abstract vectors\footnote{Also known as vectors.} by $\ket{\phi}$ and abstract linear functionals\footnote{Also known as linear functionals.} as $\bra{\phi}$. In the case of $\set{\phi_n}_{n \geq 0}$ as a family of complex-valued functions defined over the interval $[a,b]\subseteq \R$, we have
  \begin{align*}
    \braket{\phi_n}{\phi_m} &= \int_{a}^{b} \overline{\phi_n(x)}\phi_m(x)w(x)\:dx,
  \end{align*}
  where $w$ is a weight function. Recall from \href{https://ai.avinash-iyer.com/Classes_and_Homework/College/Y4/Y4S1,%20Math%20Methods/math_methods_notes.pdf}{Math Methods I} that, if we have a family of polynomials $\set{x^n}_{n\geq 0}$, there are three (primary) ways to make a set of orthogonal (or orthonormal) polynomials upon using Gram--Schmidt.
    \begin{itemize}
      \item The Legendre polynomials have the weight $w(x) =1$ and are defined along $[-1,1]$.
      \item The Laguerre polynomials have the weight $w(x) = e^{-x}$ and are defined along $[0,\infty)$.
      \item The Hermite polynomials have the weight $w(x) = e^{-x^2}$ and are defined along $\R$.
    \end{itemize}
  We can theoretically use the Gram--Schmidt process to generate these families of polynomials, but that sucks.\footnote{Citation needed.} Instead, we are interested in a more concrete, straightforward way to calculate these polynomials.\newline

  Recall from E\&M\footnote{I don't, but others might.} that if $q$ is a charge at point $\mathbf{r}'$ and we have a measurement device at point $\mathbf{r}$, both along the $z$ axis, then the voltage is proportional to $\frac{1}{\norm{\mathbf{r}-\mathbf{r}'}}$. Orienting our axes, we may take $\frac{1}{\left\vert r-r' \right\vert}$.\newline

  If $r' < r$, we take
  \begin{align*}
    \frac{1}{r\left( 1-\frac{r'}{r} \right)} &= \frac{1}{r}\sum_{\ell = 0}^{\infty}\left( \frac{r'}{r} \right)^{\ell},
  \end{align*}
  and if $r' > r$, we tkae
  \begin{align*}
    \frac{1}{r'\left( 1-\frac{r}{r'} \right)} &= \frac{1}{r'}\sum_{\ell = 0}^{\infty}\left( \frac{r}{r'} \right)^{\ell}.
  \end{align*}
  We may write this as
  \begin{align*}
    \frac{1}{\left\vert r-r' \right\vert} &= \frac{1}{r_{>}}\sum_{\ell=0}^{\infty} \left( \frac{r_{<}}{r_{>}} \right)^{\ell}\\
                                          &= \frac{1}{r_{>}} \sum_{\ell = 0}^{\infty}t^{\ell},
  \end{align*}
  where $t = \frac{r_{ < }}{r_{ > }}$. If we reorient our $z$, then the magnitude of the difference $\left\vert r-r' \right\vert$ doesn't change, but we need some function of $\theta$, or that
  \begin{align*}
    \frac{1}{\norm{\mathbf{r}-\mathbf{r}'}} &= \sum_{\ell = 0}^{\infty}\frac{r_{ < }^{\ell}}{r_{>}^{\ell + 1}} f\left( \theta \right).
    \intertext{Note that $f(\theta)\xrightarrow{\theta\rightarrow 0} 1$, and that our $z$ axis is a function purely of $\cos\theta$, so we have}
                                            &= \sum_{\ell = 0}^{\infty}\frac{r_{ < }^{\ell}}{r_{ > }^{\ell + 1}}P_{\ell}\left( \cos\left( \theta \right) \right)\\
                                            &= \frac{1}{r_{ > }}\sum_{\ell = 0}^{\infty} t^{\ell}P_{\ell}\cos\left( \theta \right).
  \end{align*}
  Using the law of cosines, we have
  \begin{align*}
    \frac{1}{\norm{\mathbf{r}-\mathbf{r}'}} &= \frac{1}{\sqrt{r^2 + {r'}^2 - 2rr'\cos\left( \theta \right)}}\\
                                            &= \frac{1}{r_{ > }\sqrt{1 + t^2 - 2xt}}.
  \end{align*}
  Taking $x = \cos\left( \theta \right)$, and defining
  \begin{align*}
    G\left( x,t \right) &= \frac{1}{\sqrt{1 + t^2 - 2xt}}\\
                        &= \sum_{\ell=0}^{\infty}t^{\ell}P_{\ell}\left( x \right)
  \end{align*}
  Then, 
  \begin{align*}
    P_{\ell}\left( x \right) &= \frac{1}{\ell!}\pd{^{\ell}}{t^{\ell}}G\left( x,t \right)\biggr\vert_{t=0}.
  \end{align*}
  This is how we obtain the Legendre polynomials. Specifically, we obtain
  \begin{align*}
    P_0 &= 1\\
    P_1 &= x\\
    P_2 &= \frac{1}{2}\left( 3x^2 - 1 \right).
  \end{align*}
  The function $G(x,t)$ is the \textit{generating function} of the Legendre polynomials.\newline

  Note that there is also a formula for the Legendre polynomials, known as the Rodrigues formula:
  \begin{align*}
    P_{\ell}(x) &= \frac{\left( -1 \right)^{\ell}}{2^{\ell}\ell!} \left( \frac{d}{dx} \right)^{\ell}\left( 1-x^2 \right)^{\ell}.
  \end{align*}
  We will focus on the generating function though. If we take an integral,
  \begin{align*}
    \int_{-1}^{-1} \left( G\left( x,t \right) \right)^2\:dx &= \sum_{n,m\geq 0}^{\infty}t^nt^m \int_{-1}^{1} P_n(x)P_m(x)\:dx\\
    \int_{-1}^{1} \frac{1}{1-2xt + t^2}\:dx &= \frac{1}{t}\left( \ln\left( 1+t \right)-\ln\left( 1-t \right) \right).
  \end{align*}
  Now, taking an expansion in terms of Taylor series on the right side, we have
  \begin{align*}
    \int_{-1}^{1} \frac{1}{1 - 2xt + t^2}\:dx &= \sum_{n=0}^{\infty}\frac{1}{2n+1}t^{2n}.
  \end{align*}
  Since the double sum equals a single sum, we must have a Kronecker delta, yielding
  \begin{align*}
    \int_{-1}^{1} \left( G\left( x,t \right) \right)^2\:dx &= \frac{2}{2n+1}\delta_{mn}.
  \end{align*}
  Furthermore, since
  \begin{align*}
    G\left( x,t \right) &= G\left( -x,-t \right),
  \end{align*}
  we must have
  \begin{align*}
    \sum_{\ell=0}^{\infty}t^{\ell}P_{\ell}\left( x \right) &= \sum_{t=0}^{\infty}\left( -t \right)^{\ell}P_{\ell}\left( -x \right)\\
                                                           &= \sum_{t=0}^{\infty}t^{\ell}\left( -1 \right)^{\ell}P_{\ell}\left( -x \right)
  \end{align*}
  meaning that $P_{\ell}\left( -x \right) = \left( -1 \right)^{\ell}P_{\ell}\left( x \right)$.\newline

  For Hermite polynomials, the generating function is
  \begin{align*}
    G\left( x,t \right) &= e^{2xt - t^2}\\
                        &= \sum_{n=0}^{\infty}\frac{1}{n!}H_{n}\left( x \right)t^{n}.
  \end{align*}
  Consider
  \begin{align*}
    \pd{G}{x} &= 2tG.
  \end{align*}
  Then, in particular, we have 
  \begin{align*}
    \sum_{n=0}^{\infty}\frac{1}{n!}\diff{H_m}{x}t^{n} &= 2t \sum_{m=0}^{\infty}\frac{1}{m!}H_m(x)t^{m}\\
                                                      &= \sum_{m=0}^{\infty}\frac{1}{m!}H_m(x)t^{m+1}.
  \end{align*}
  Letting $m = n-1$, we have
  \begin{align*}
    \sum_{n=0}^{\infty} \frac{1}{n!}\left(\diff{H_n}{x}-2nH_{n-1}(x) \right)t^{n} &= 0
  \end{align*}
  Therefore,
  \begin{align*}
    \diff{H_n}{x} &= 2nH_{n-1},\label{eq:two_term_recurrence_hermite}\tag{\textasteriskcentered}
  \end{align*}
  a two-term recurrence relation. We can also take
  \begin{align*}
    \pd{G}{t} &= 2\left( x-t \right)G,
  \end{align*}
  from which we are able to obtain a three-term recurrence relation
  \begin{align*}
    H_{n+1}(x) - 2xH_{n}(x) + 2nH_{n-1} &= 0.
  \end{align*}
  Taking a derivative of \eqref{eq:two_term_recurrence_hermite} and inserting our three-term recurrence relation, we get the equation
  \begin{align*}
    \diff{^2H_n}{x^2} - 2x \diff{H_n}{x} + 2nH_n &= 0.
  \end{align*}
  There is a similar second-order differential equation for the Legendre polynomials,
  \begin{align*}
    \left( 1-x^2 \right)\diff{^2P_{n}}{x^2} - 2x\diff{P_n}{x} + n\left( n+1 \right)P_n &= 0.
  \end{align*}
  These equations are known as Hermite's equation and Legendre's equation (respectively). However, it is not apparent from this construction that we get orthogonality.\newline

  As it turns out, this comes from eigenvectors and eigenvalues. The equation
  \begin{align*}
    \diff{^2f}{x^2} &= -k^2 f
  \end{align*}
  has the operator $\diff{^2}{x^2}$ and eigenvalue $-k^2$. Since these eigenvalues are real, we must have $\diff{^2}{x^2}$ be a self-adjoint (or Hermitian\footnote{There is a difference that I don't want to try to learn yet.}) operator, admitting an orthonormal eigenbasis.
  \subsection{Beyond the Straight and Narrow}%
  Recall the definition of a Fourier series:
  \begin{align*}
    f(x) &= \frac{a_0}{2} + \sum_{n=1}^{\infty}a_n\cos\left( nx \right) + b_n\sin\left( nx \right)\\
         &= \sum_{n=-\infty}^{\infty}c_ne^{-inx}\\
    a_n &= \frac{1}{\pi}\int_{-\pi}^{\pi} f(x)\cos\left( nx \right)\:dx\\
    b_n &= \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\sin\left( nx \right)\:dx\\
    c_n &= \frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)e^{-inx}\:dx
  \end{align*}
  This is the most simple scenario, where $f$ is defined on the interval $\left[ -\pi,\pi \right]$.\newline

  Consider what happens when we want to do this in more than one dimension. Setting $e^{i\mathbf{k}\cdot \mathbf{r}} = e^{ik_x x}e^{ik_y y}$, where we have a rectangular region $L_1\times L_2$, and setting $k_x = \frac{n\pi}{L_1}$, $k_y = \frac{m\pi}{L_2}$.\newline

  This gives
  \begin{align*}
    f(x,y) &= \sum_{n,m=-\infty}^{\infty} \hat{f}_{n,m} e^{i\pi \left( \frac{n}{L_1}x + \frac{m}{L_2}y \right)}.
  \end{align*}
  Then,
  \begin{align*}
    \hat{f}_{n,m} &= \int_{-L_1}^{L_1}\int_{-L_2}^{L_2} f(x,y)e^{-i\frac{n\pi}{L_1}x}e^{-i\frac{m\pi}{L_2}y}\:dy\:dx.
  \end{align*}
  Obviously, this becomes very difficult in more dimensions, but it can be done.\newline

  Rectangles are not very aesthetically pleasing, though. Instead, we are interested in applying it to a circular plate.\newline

  Our values of $k_x,k_y$ in the case of a rectangle arrived from imposing a periodic boundary condition. However, circles are already periodic, so we may consider a different value. We take
  \begin{align*}
    e^{i\mathbf{k}\cdot \mathbf{r}} &= e^{ikr \cos\left( \gamma \right)}, 
  \end{align*}
  where $\gamma = \phi'-\phi$ is the angle between the $\mathbf{k}$ vector and the $\mathbf{r}$ vector. Since $\cos\left( \gamma \right)$ is periodic, we may expand in terms of $e^{in\gamma}$, giving
  \begin{align*}
    e^{ikr\cos\left( \gamma \right)} &= \sum_{n=-\infty}^{\infty} i^{n} J_n\left( kr \right) e^{-in\phi'}e^{in\phi},
  \end{align*}
  We need the coefficient of $i^{n}$ to ensure that $J_n\left( kr \right)$ is real. We may calculate
  \begin{align*}
    J_n\left( x \right) &= \frac{1}{2\pi i^{n}}\int_{-\pi}^{\pi} e^{ix\cos\left( \gamma \right)}e^{-in\gamma}\:d\gamma
  \end{align*}
  The family $\set{J_n}_{n\in \Z}$ are called (cylindrical) \textit{Bessel functions}. We are also able to write the Bessel functions as
  \begin{align*}
    J_n\left( x \right) &= \frac{1}{2\pi} \int_{-\pi}^{\pi} e^{ix\sin\left( \gamma \right)-in\gamma}\:d\gamma\\
                        &= \frac{1}{\pi} \int_{0}^{\pi} \cos\left( x\sin\left( \gamma \right)-n\gamma \right)\:d\gamma.
  \end{align*}
  Note that in the asymptotic limit, we have
  \begin{align*}
    J_n\left( x \right) &= \sqrt{\frac{2}{\pi x}} \cos\left( x-\frac{n\pi}{2}-\frac{\pi}{4} \right),
  \end{align*}
  and for every small $x$,
  \begin{align*}
    J_n\left( x \right) &= \frac{x^n}{2^n n!}.
  \end{align*}
  Now that we have expanded upon a line, rectangle, and disk, we will now expand upon a sphere. On the unit sphere, we may have a function $f\left( \theta,\phi \right)$, where $0 \leq \theta \leq \phi$ and $0 \leq \phi \leq 2\pi$. We may consider if we're allowed to expand $f$ as follows:
  \begin{align*}
    f\left( \theta,\varphi \right) \stackrel{?}{=} \underbrace{\left( \sum_{\ell = 0}^{\infty}c_{\ell}P_{\ell}\cos\left( \theta \right) \right)}_{\text{Legendre}}\underbrace{\left( \sum_{m=-\infty}^{\infty}d_me^{im\phi} \right)}_{\text{Fourier}}.\label{eq:fake_spherical_harmonics}\tag{\textasteriskcentered}
  \end{align*}
  Unfortunately, this is not the right answer. It might work on the equator, but as we reduce the value of $\theta$ going towards the north pole, we pick up a factor of $\sin\left( \theta \right)$; in the limit as $\theta\rightarrow 0$, $\phi$ is not defined. Therefore, we need some factor of $\sin\left( \theta \right)$.\newline

  We introduce some factors of $\sin^{m}\left( \theta \right) = \left( 1-\cos^2\left( \theta \right) \right)^{m/2}$. This gives an extra order of $m$ in $\cos\left( \theta \right)$, so we need to reduce the order by $m$. We take $\left( \diff{}{x} \right)^{m} P_{\ell}\left( x \right)$.\newline

  We don't have Legendre polynomials anymore; instead, we have 
  \begin{align*}
    P_{\ell,m}\left( x \right) &= \left( 1-x^2 \right)^{m/2} \left( \diff{}{x} \right)^{m}P_{\ell}\left( x \right).
  \end{align*}
  Substituting the Rodrigues formula, we have
  \begin{align*}
    P_{\ell,m}\left( x \right) &= \frac{\left( -1 \right)^{\ell}}{2^{\ell}\ell!}\left( 1-x^2 \right)^{m/2}\left( \diff{}{x} \right)^{\ell + m}\left( 1-x^2 \right)^{\ell}.
  \end{align*}
  These are known as the \textit{associated Legendre functions}, where $m \geq 0$. Note that if $m = 0$, we get the Legendre polynomials, and if $m  > \ell$, this evaluates to $0$. However, we are able to expand to include negative values of $m$ by taking
  \begin{align*}
    P_{\ell,-m}\left( x \right) &= \left( -1 \right)^{m} \frac{\left( \ell - m \right)!}{\left( \ell + m \right)!}P_{\ell,m}\left( x \right).
  \end{align*}
  As it turns out, the family $P_{\ell,m}$ are orthogonal (holding $m$ constant), with orthogonality relation
  \begin{align*}
    \int_{-1}^{1} P_{\ell,m}\left( x \right)P_{\ell',m}\left( x \right)\:dx &= \frac{2}{2\ell + 1}\frac{\left( \ell + m \right)!}{\left( \ell -m \right)!}\delta_{\ell \ell'}.
  \end{align*}
  Thus, the fixed version of \eqref{eq:fake_spherical_harmonics}, is
  \begin{align*}
    Y_{\ell,m}\left( \theta,\phi \right) &= \left( -1 \right)^{m}\sqrt{\frac{2\ell + 1}{4\pi}}\sqrt{\frac{\left( \ell - m \right)!}{\left( \ell + m \right)!}}P_{\ell,m}\left( \cos\left( \theta \right) \right)e^{im\phi}.
  \end{align*}
  The family of $Y_{\ell,m}$ are indeed orthogonal functions. Furthermore, they are orthonormal in both $\ell$ and $m$:
  \begin{align*}
    \int_{}^{} Y_{\ell,m}\left( \theta,\phi \right) \overline{Y_{\ell',m'}\left( \theta,\phi \right)}\:d\Omega &= \delta_{\ell \ell'}\delta_{m m'},
  \end{align*}
  so that
  \begin{align*}
    a_{\ell,m} &= \int_{}^{} f\left( \theta,\phi \right) \overline{Y_{\ell,m}\left( \theta,\phi \right)}\:d\Omega.
  \end{align*}
  We are allowed to take any function and expand it on a basis of spherical harmonics, giving
  \begin{align*}
    f\left( \theta,\phi \right) &= f\left( \hat{n} \right)\\
                                &= \sum_{\ell=0}^{\infty}\sum_{m=-\infty}^{\infty}a_{\ell,m}Y_{\ell,m}\left( \theta,\phi \right).
  \end{align*}
  There are some important symmetry properties. First,
  \begin{align*}
    Y_{\ell,-m} &= \left( -1 \right)^{m} \overline{Y_{\ell,m}},
  \end{align*}
  and under parity transformation, sending $\mathbf{r}\mapsto -\mathbf{r}$, we get
  \begin{align*}
    Y_{\ell,m}\left( \theta,\phi \right) &\mapsto \left( -1 \right)^{\ell}Y_{\ell,m}\left( \theta,\phi \right).
  \end{align*}
  \begin{example}
    In the case of $m = 0$, the $P_{\ell, m}$ are purely functions of $\cos\left( \theta \right)$, while for each increased value of $m$, there are factors of $\sin\left( \theta \right)$, with a factor of $e^{im\phi}$ increased.\newline

    Specifically, 
    \begin{align*}
      Y_{\ell,0} \left( \theta,\phi \right) &= \sqrt{\frac{2\ell + 1}{4\pi}} P_{\ell}\left( \cos\left( \theta \right) \right).
    \end{align*}
    In other words, if $m = 0$, we have azimuthal symmetry. We also say it has ``no $\phi$ dependence."
  \end{example}
  \begin{example}
    Consider the case of $Y_{\ell,m}\left( \hat{z} \right)$. Then, since $\theta = 0$ and $\phi$ is undefined, we must have no $\phi$ dependence, giving
    \begin{align*}
      Y_{\ell,m}\left( \hat{z} \right) &= \sqrt{\frac{2\ell + 1}{4\pi}} P_{\ell}\left( 1 \right)\delta_{m0}\\
                                       &= \sqrt{\frac{2\ell + 1}{4\pi}} \delta_{m 0}.
    \end{align*}
  \end{example}
  Now that we know that $\set{Y_{\ell,m}}_{\ell,m}$ are orthonormal, we need them to be complete too. Specifically, we need
  \begin{align*}
    f\left( \theta,\phi \right) &= \sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell}a_{\ell,m}Y_{\ell,m}\\
    a_{\ell,m} &= \braket{\ell,m}{f}\\
               &= \int_{}^{} f\left( \theta,\phi \right) \overline{Y_{\ell,m}\left( \theta,\phi \right)}\:d\Omega.
  \end{align*}
  \begin{example}
    We want to expand $\delta\left( \hat{n}-\hat{n}' \right)$. This gives
    \begin{align*}
      \delta\left( \cos\left( \theta \right)-\cos\left( \theta' \right) \right)\delta\left( \phi-\phi' \right) &= \delta\left( \hat{n}-\hat{n}' \right)\\
                                                                           &= \sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell}a_{\ell,m}Y_{\ell,m}\left( \theta,\phi \right)\\
                                            &= \sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell} \left( \int_{}^{} \delta\left( \hat{\xi}-\hat{n}' \right) \overline{Y_{\ell,m}\left( \hat{\xi} \right)}\:d\Omega \right) Y_{\ell,m}\left( \hat{n} \right)\\
                                            &= \sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell} \overline{Y_{\ell,m}\left( \hat{n}' \right)} Y_{\ell,m}\left( \hat{n} \right).
    \end{align*}
    Now, if we let $\hat{n}' = \hat{z}$, we have
    \begin{align*}
      \delta\left( \hat{n}-\hat{z} \right) &= \sum_{\ell=0}^{\infty}Y_{\ell,0}\left( \hat{z} \right) Y_{\ell,0}\left( \hat{n} \right)\\
                                           &= \sum_{\ell=0}^{\infty} \left( \sqrt{\frac{2\ell + 1}{4\pi}}P_{\ell}\left( 1 \right) \right) \left( \sqrt{\frac{2\ell + 1}{4\pi}} P_{\ell}\left( \cos\left( \theta \right) \right) \right)\\
                                           &= \sum_{\ell=0}^{\infty}\frac{2\ell + 1}{4\pi}P_{\ell}\left( \cos\left( \theta \right) \right)\\
                                           &= \sum_{\ell=0}^{\infty}\frac{2\ell + 1}{4\pi} P_{\ell}\left( \hat{n}\cdot \hat{z} \right).
    \end{align*}
    Since dot products are coordinate-independent, we must have
    \begin{align*}
      \delta\left( \hat{n}-\hat{n}' \right) &= \sum_{\ell=0}^{\infty}\frac{2\ell + 1}{4\pi} P_{\ell}\left( \cos\left( \gamma \right) \right),
    \end{align*}
    where we defined
    \begin{align*}
      e^{i\mathbf{k}\cdot\mathbf{r}} &= e^{ikr\cos\left( \gamma \right)}.
    \end{align*}
    This allows us to derive the \textit{spherical harmonic addition theorem}. We get
    \begin{align*}
      P_{\ell}\left( \cos\left( \gamma \right) \right) &= \frac{4\pi}{2\ell + 1}\sum_{m=-\ell}^{\ell} \overline{Y_{\ell,m}\left( \theta',\phi' \right)}Y_{\ell,m}\left( \theta,\phi \right).
    \end{align*}
  \end{example}
  \begin{example}
    Recall that
    \begin{align*}
      \frac{1}{\norm{\mathbf{r} - \mathbf{r}'}} &= \sum_{\ell=0}^{\infty} \frac{r^{\ell}_{<}}{r^{\ell + 1}_{>}} P_{\ell}\left( \cos\left( \theta \right) \right)\\
                                                                &= \sum_{\ell=0}^{\infty}\frac{4\pi}{2\ell + 1} \frac{r^{\ell}_{<}}{r^{\ell + 1}_{>}}\sum_{m=-\ell}^{\ell} \overline{Y_{\ell,m}\left( \theta',\phi' \right)} Y_{\ell,m}\left( \theta,\phi \right).
    \end{align*}
    Sticking this sum into an integral, and using the fact that $Y_{0,0} = \frac{1}{\sqrt{4\pi}}$
    \begin{align*}
      \frac{1}{4\pi} \int_{}^{} \frac{1}{\norm{\mathbf{r} - \mathbf{r}'}}\:d\Omega &\sim \int_{}^{} Y_{\ell,m}\left( \theta,\phi \right)\:d\Omega\\
                                                                                   &= \sqrt{4\pi} \int_{}^{} \left( Y_{0,0} \left( \theta,\phi \right) \right) Y_{\theta,\phi}\:d\Omega\\
                                                                                   &= \delta_{\ell 0}\delta_{m 0}.
    \end{align*}
  \end{example}
  Moving from the sphere to the ball., we have
  \begin{align*}
    f\left( \mathbf{r} \right) &= \sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell}a_{\ell,m}\left( r \right)Y_{\ell,m}\left( \theta,\phi \right).
  \end{align*}
  Consider the case of $e^{i\mathbf{k}\cdot \mathbf{r}}$. Then,
  \begin{align*}
    e^{i\mathbf{k}\cdot \mathbf{r}} &= \sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell} a_{\ell,m}\left( r,\mathbf{k} \right) Y_{\ell,m}\left( \hat{r} \right)\\
                                    &= \sum_{\ell,m} \left( \sum_{\ell',m'} \hat{a}_{\ell,m,\ell',m'} Y_{\ell',m'}\left( \hat{k} \right) \right)Y_{\ell,m}\left( \hat{r} \right)
  \end{align*}
  
  \begin{itemize}
    \item Recalling $e^{i\mathbf{k}\cdot \mathbf{r}} = e^{ikr\cos\left( \gamma \right)}$. Therefore, we must have, $a\left( r,k \right) = a\left( rk \right)$. 
    \item Furthermore, using the identity $\cos\left( \gamma \right) = \cos\left( \theta \right)\cos\left( \theta' \right) - \sin\left( \theta \right)\sin\left( \theta' \right)\cos\left( \phi-\phi' \right)$, meaning we must have $m = m'$.
    \item We use the symmetry relation
      \begin{align*}
        Y_{\ell,-m} &= \left( -1 \right)^{m} \overline{Y_{\ell,m}}.
      \end{align*}
    \item Finally, we must have $\ell = \ell'$ to remove ambiguity.
  \end{itemize}
  In total, we get
  \begin{align*}
    e^{i\mathbf{k}\cdot\mathbf{r}} &= \sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell} c_{\ell}\left( kr \right) \overline{Y_{\ell, m}\left( \hat{k} \right)}Y_{\ell,m}\left( \hat{r} \right).
  \end{align*}
  Define
  \begin{align*}
    j_{\ell}\left( kr \right) &= \frac{c_{\ell}\left( kr \right)}{4\pi i^{\ell}}.
  \end{align*}
  This gives
  \begin{align*}
    e^{i\mathbf{k}\cdot \mathbf{r}} &= \sum_{\ell=0}^{\infty}i^{\ell} i^{\ell}\left( 2\ell + 1 \right)j_{\ell}\left( kr \right) P_{\ell}\left( \cos\left( \gamma \right) \right).
  \end{align*}
  The family $j_{\ell}$ are called the \textit{spherical Bessel functions}.\newline

  We find the expression for the spherical Bessel function by projecting onto the Legendre polynomials, giving
  \begin{align*}
    j_{\ell}\left( x \right) &= \frac{\left( -i \right)^{\ell}}{2} \int_{-1}^{1} e^{ixu}P_{\ell}\left( u \right)\:du.
  \end{align*}
  As it turns out the spherical Bessel functions can be written entirely in terms of sines and cosines (with some factors of $x$). For instance,
  \begin{align*}
    j_0 &= \frac{\sin\left( x \right)}{x}\\
    j_1 &= \frac{\sin\left( x \right)}{x^2} - \frac{\cos\left( x \right)}{x}.
  \end{align*}
  \begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{2.5}
    \begin{tabular}{c|c}
      Space & Expression\\
      \hline
      \hline
      Rectangular & $\displaystyle e^{ik_x x}e^{ik_y y}e^{ik_z z}$\\
      Cylndrical & $\displaystyle e^{ik_z z}\sum_{n=-\infty}^{\infty} i^n J_n\left( kr \right)e^{in\left( \phi-\phi' \right)}$\\
      Ball (Spherical Harmonics) & $\displaystyle 4\pi\sum_{\ell=0}^{\infty}\sum_{m=-\ell}^{\ell}i^{\ell}j_{\ell}\left( kr \right) \overline{Y_{\ell,m}\left( \hat{k} \right)}Y_{\ell,m}\left( \hat{r} \right)$\\
      Ball (Legendre Polynomials) & $\displaystyle \sum_{\ell=0}^{\infty}i^{\ell}\left( 2\ell + 1 \right)j_{\ell}\left( kr \right)P_{\ell}\left( \cos\left( \theta \right) \right)$.
    \end{tabular}
    \caption{Expansions of $e^{i\mathbf{k}\cdot \mathbf{r}}$}
  \end{table}
  \section{Differential Equations}%
  To start our investigation of differential equations, we start by discussing linearity.\newline

  Abstractly, we consider some linear operator
  \begin{align*}
    \mathcal{L}_q &= \sum_{j=0}^{n}a_j(q)\diff{^j}{q^j}.
  \end{align*}
  The reason we care about linearity is that if
  \begin{align*}
    \mathcal{L}_q(u) &= 0
  \end{align*}
  has solutions $u_1,\dots,u_n$, then
  \begin{align*}
    \mathcal{L}\left( \sum_{j=1}^{n}a_ju_j \right) &=\sum_{j=1}^{n}a_j\mathcal{L}\left( u_j \right).
  \end{align*}
  Equations of the form
  \begin{align*}
    \mathcal{L}_q\left( u \right) &= 0
  \end{align*}
  for some differential operator $\mathcal{L}$ are known as \textit{homogeneous} equations. Inhomogeneous equations are of the form
  \begin{align*}
    \mathcal{L}_q\left( u \right) &= r\left( q \right).
  \end{align*}
  \subsection{First-Order Equations}%
  Consider the general free-falling mass equation,
  \begin{align*}
    m\diff{v}{t} &= mg,
  \end{align*}
  with $v(0) = v_0$. Then, $v(t) = v_0 + gt$.\newline

  If we add linear drag, $f_d = -bv$, we have
  \begin{align*}
    \diff{v}{t} &= -\frac{b}{m}v + g.\label{eq:linear_drag}\tag{\textdagger}
  \end{align*}
  Separating variables,
  \begin{align*}
    \frac{dv}{bv/m - g} &= -dt,
  \end{align*}
  and integrating, we have
  \begin{align*}
    \ln\left( bv/m - g \right) &= - \frac{bt}{m} + C.
  \end{align*}
  Exponentiating,
  \begin{align*}
    v(t) &= ke^{-bt/m} + mg/b,
  \end{align*}
  where $k = v_0 - mg/b$. We can see that terminal velocity is $mg/b$.\newline

  What makes these examples relatively easy is that these equations are separable. We are able to write it of the form
  \begin{align*}
    \alpha(x) dx + \beta(y) dy &= 0,
  \end{align*}
  meaning the solution is
  \begin{align*}
    \int_{}^{} \alpha(x)\:dx &= - \int_{}^{} \beta(y)\:dy + C.
  \end{align*}
  Now, in the general case, we have
  \begin{align*}
    \diff{y}{x} + q(x)y &= r(x).
  \end{align*}
  To solve this equation, we can introduce an integrating factor to make this equation separable. We want to find $w(x)$ for this purpose. Take
  \begin{align*}
    \diff{}{x}\left( wy \right) &= w\diff{y}{x} + \diff{w}{x} y\label{eq:integrating_factor}\tag{$1^{\ast}$}
  \end{align*}
  We want $\diff{w}{x} = qw$, so that $w(x) = \exp\left( \int_{}^{} q(x)\:dx \right)$. Multiplying on both sides, we have
  \begin{align*}
    \diff{}{x}\left( wy \right) &= wr,
  \end{align*}
  and
  \begin{align*}
    y &= \frac{1}{w(x)} \int_{}^{} w(x)r(x)\:dx.
  \end{align*}
  For instance, considering \eqref{eq:linear_drag} again, we have $w(t) = e^{bt/m}$, and
  \begin{align*}
    v(t) &= e^{-bt/m} \int_{}^{} ge^{bt/m}\:dt\\
         &= e^{-bt/m} \left( \frac{mg}{b}e^{bt/m} + k \right)\\
         &=ke^{-bt} + \frac{mg}{b}.
  \end{align*}
  \begin{example}
    Consider the falling raindrop. Let $m(0) = m_0$.\newline

    Now, as the raindrop falls, it increases in weight. Rewriting
    \begin{align*}
      F &= ma\\
        &= \diff{p}{t},
    \end{align*}
    we instead write
    \begin{align*}
      F &= m \dot{v} + \dot{m} v.\label{eq:falling_raindrop_eq1}\tag{$\ast$}
    \end{align*}
    We neglect drag, so
    \begin{align*}
      m\dot{v} + \dot{m}v &= mg.
    \end{align*}
    We assume that the rate of mass increase is proportional to $4\pi r^2$. We also assume constant density $\rho$ proportional to $m/r^3$. Putting these two together, we may assume
    \begin{align*}
      \diff{m}{t} &= k m^{2/3}.
    \end{align*}
    We are able to write our differential equation in terms of $m$, by using the chain rule to get
    \begin{align*}
      \diff{v}{t} &= \diff{m}{t} \diff{v}{m}\\
                  &= km^{2/3} \diff{v}{m}.
    \end{align*}
    Our differential equation \eqref{eq:falling_raindrop_eq1} now becomes
    \begin{align*}
      \diff{v}{m} + \frac{v}{m} &= \frac{g}{km^{2/3}},
    \end{align*}
    with assumption that $v\left( m_0 \right) = 0$. We use the integrating factor of $w(m) = \exp\left( \int_{}^{} \frac{1}{m}\:dm \right)$, giving
    \begin{align*}
      m \diff{v}{m} + v &= \frac{gm^{1/3}}{k}.\label{eq:falling_raindrop_eq2}\tag{$\ast\ast$}
    \end{align*}
    This gives
    \begin{align*}
      \diff{}{m}\left( mv \right) &= \frac{gm^{1/3}}{k}\\
      mv &= \frac{3}{4}\frac{gm^{4/3}}{k} + C\\
      v &= \frac{3}{4}\frac{gm^{1/3}}{k} + \frac{C}{m}.
    \end{align*}
    Inputting our initial condition, we have
    \begin{align*}
      v(m) &= \frac{3g}{4k}m^{1/3}\left( 1-\left( \frac{m_0}{m} \right)^{4/3} \right).
    \end{align*}
    In particular,
    \begin{align*}
      \dot{v} &= g-  \frac{\dot{m}}{m}v\\
              &= \frac{g}{4}\left( 1 + 3\left( \frac{m_0}{m} \right)^{3/4} \right).
    \end{align*}
    In this case, there is no terminal velocity.
  \end{example}
  \begin{example}
    Consider 
    \begin{align*}
      \alpha\left( x,y \right)dx + \beta\left( x,y \right)dy &= 0.
    \end{align*}
    This appears to be of the form
    \begin{align*}
      d\left( \Phi\left( x,y \right) \right) &= \pd{\Phi}{x}dx + \pd{\Phi}{y}dy\\
                                             &= 0.
    \end{align*}
    In particular, if we can find $\Phi$ that matches the partials, then $\Phi$ is a constant.\newline

    Assuming that $\Phi$ is $C^2$, then the equality of mixed partials gives
    \begin{align*}
      \pd{\alpha}{y} &= \pd{\beta}{x}.\label{eq:exact_inexact}\tag{$\dag$}
    \end{align*}
    If solutions exist, then we say the equation is \textit{exact}.\newline

    Returning to the falling raindrop, and rewriting \eqref{eq:falling_raindrop_eq2}, we have
    \begin{align*}
      mdv + \left( v-\frac{g}{k}m^{1/3} \right)dm &= 0.\label{eq:falling_raindrop_eq3}\tag{$\ddagger$}
    \end{align*}
    Therefore, $\alpha(v,m) = m$ and $\beta(v,m) = v- \frac{g}{k}m^{1/3}$. We want to find if
    \begin{align*}
      \pd{\alpha}{m} &= \pd{\beta}{v}.
    \end{align*}
    The answer is yes, so it can be rendered in exact form. We will write
    \begin{align*}
      \pd{\Phi}{v} &= m\\
      \pd{\Phi}{m} &= v - \frac{g}{k}m^{1/3}.
    \end{align*}
    Integrating the first equation, we have $\Phi(m,v) = mv + c_1(m)$, and integrating the second, $\Phi(m,v) = mv-\frac{3g}{4k}m^{4/3} + c_2(v)$. We can set $c_2(v) = 0$ and $c_1(m) = \frac{3g}{4k}m^{4/3}$.
  \end{example}
  \begin{example}
    If our equation \eqref{eq:exact_inexact} does not hold, then we need to find an integrating factor $w\left( x,y \right)$ such that
    \begin{align*}
      \pd{}{y}\left( w\alpha \right) = \pd{}{x}\left( w\beta \right).
    \end{align*}
    Now, if we take $w = w(x)$, we get the equation
    \begin{align*}
      \diff{w}{x} &= \frac{w}{\beta}\left( \pd{\alpha}{y} - \pd{\beta}{x} \right)\\
                  &= p(x)w(x).
    \end{align*}
    This gives $w(x) = \exp\left( \int_{}^{} p(x)\:dx \right)$. In particular, we need
    \begin{align*}
      p(x) &= \frac{1}{\beta}\left( \pd{\alpha}{y} - \pd{\beta}{x} \right).
    \end{align*}
    If we had $w(y)$, then
    \begin{align*}
      p(y) &= \frac{1}{\alpha}\left( \pd{\beta}{x} - \pd{\alpha}{y} \right).
    \end{align*}
  \end{example}
  \begin{example}
    Consider a refinement of the falling raindrop model.\newline

    Rather than the surface area, we say that the growth rate depends on the volume swept out per unit time. In particular, this volume swept out per unit time is $Av$, so
    \begin{align*}
      \diff{m}{t} &= km^{2/3} v.
    \end{align*}
    This changes \eqref{eq:falling_raindrop_eq3} to
    \begin{align*}
      v dv + \left( \frac{v^2}{m} - \frac{g}{km^{2/3}} \right) dm &= 0.
    \end{align*}
    This is not an exact equation. We find the integrating factor
    \begin{align*}
      p(m) &= \frac{1}{\alpha}\left( \pd{\beta}{v} - \pd{\alpha}{m} \right)\\
           &= \frac{1}{v}\left( \frac{2v}{m} \right)\\
           &= \frac{2}{m}.
    \end{align*}
    Our integrating factor is $w(m) = \exp\left( \int_{}^{} p(m)\:dm \right)$, or $w(m) = m^2$.\newline

    With much tedious symbol-pushing, we find
    \begin{align*}
      \Phi(v,m) &= \frac{1}{2}m^2v^2 - \frac{3g}{7k}m^{7/3}\\
                &= C.
    \end{align*}
    Plugging in initial conditions, with $v\left( m_0 \right)= 0$, we get
    \begin{align*}
      \dot{v} &= \frac{g}{7}\left( 1 + 6\left( \frac{m_0}{m} \right)^{7/3} \right).
    \end{align*}
  \end{example}
  \subsection{Second-Order Equations}%
  When we consider second-order equations, we have the form
  \begin{align*}
    \mathcal{L}\left[ u(x) \right] &= r(x)\\
    \left( \diff{^2}{x^2} + p(x) \diff{}{x} + q(x) \right)\left( u(x) \right) &= r(x).
  \end{align*}
  We expect two linearly independent solutions, $u_1(x)$ and $u_2(x)$.\newline

  Now, we still need boundary conditions. If we have boundary conditions
  \begin{align*}
    u_1(0) &= 1\\
    \diff{u_1}{x}\biggr\vert_{0} &= 0\\
    u_2(0) &= 0\\
    \diff{u_2}{x}\biggr\vert_{0} &= 1.
  \end{align*}
  Then, the solutions are of the form
  \begin{align*}
    u\left( x \right) &= Au_1(x) + Bu_2(x),
  \end{align*}
  where $u(0) = A$ and $u'(0) = B$.
  \begin{example}[A Constant-Coefficient Equation]
    Consider the constant-coefficient second-order equation
    \begin{align*}
      \diff{^2u}{t^2} + 2\beta \diff{u}{t} + \omega_0^2u &= 0.
    \end{align*}
    This is the equation for damped simple harmonic motion.\newline

    We let $D = \diff{}{t}$. We have the equation
    \begin{align*}
      \left( D^2 + 2\beta D + \omega_0^2 \right)u(t) &= \left( D-\alpha_1 \right)\left( D-\alpha_2 \right)u\\
                                                     &= 0,
    \end{align*}
    where
    \begin{align*}
      \alpha_{1,2} &= -\beta \pm \sqrt{\beta^2 - \omega_0^2}.
    \end{align*}
    We have ``factored'' our second-order equation into two first-order equations, yielding
    \begin{align*}
      \left( D-\alpha_2 \right) u(t) &= v(t)\\
      \left( D-\alpha_1 \right)v(t) &= 0.
    \end{align*}
    The second equation yields
    \begin{align*}
      v(t) &= Ae^{\alpha_1 t}.
    \end{align*}
    Now, we must solve
    \begin{align*}
      \left( D-\alpha_2 \right)u(t) &= Ae^{\alpha_1 t}.
    \end{align*}
    With an integrating factor of $w = e^{-\alpha_2 t}$, as in \eqref{eq:integrating_factor}, we get
    \begin{align*}
      u(t) &= e^{\alpha_2 t} \int_{}^{} e^{-\alpha_2 t}v(t)\:dt\\
           &= e^{\alpha_2 t} \int_{}^{} Ae^{\left( \alpha_1 - \alpha_2 \right)t}\:dt\\
           &= \frac{1}{\alpha_1 - \alpha_2} \left( Ae^{\alpha_1 t} + Be^{\alpha_2 t} \right).
    \end{align*}
    We're fine and dandy as long as $\alpha_1\neq \alpha_2$.
  \end{example}
  \begin{example}
    To solve the equation for damped simple harmonic motion, we use the guess
    \begin{align*}
      u &= e^{\alpha t}.
    \end{align*}
    This gives the equation
    \begin{align*}
      e^{\alpha t}\left( \alpha^2 + 2\beta\alpha + \omega_0^2 \right) &= 0.
    \end{align*}
  \end{example}
  \begin{example}
    Consider the equation
    \begin{align*}
      c_2x^2 \diff{^2u}{x^2} + c_2x\diff{u}{x} + c_0 u &= 0,
    \end{align*}
    where $c_i$ are constant.\newline

    To solve this equation, we use the guess $u = x^{\alpha}$.
  \end{example}
  \subsubsection{The Wronskian}%
  We want to establish that our solutions $u_1,u_2$ of a differential equation are indeed linearly independent.\newline

  Now, if $u_1,u_2$ are indeed linearly independent, then our solutions are of the form
  \begin{align*}
    u &= c_1u_1 + c_2u_2\\
    \diff{u}{x} &= c_1 \diff{u_1}{x} + c_2 \diff{u_2}{x}.
  \end{align*}
  In other words, we have
  \begin{align*}
    \begin{pmatrix}u\\\diff{u}{x}\end{pmatrix} &= \begin{pmatrix}u_1 & u_2 \\ \diff{u_1}{x} & \diff{u_2}{x}\end{pmatrix} \begin{pmatrix}c_1\\c_2\end{pmatrix}.
  \end{align*}
  This has a unique solution if and only if the matrix has a nonzero determinant. The \textit{Wronskian} is
  \begin{align*}
    W(x) &= \det \begin{pmatrix}u_1 & u_2\\ \diff{u_1}{x} & \diff{u_2}{x}\end{pmatrix}\\
         &= u_1 \diff{u_2}{x} - u_2 \diff{u_1}{x}.
  \end{align*}
  As it turns out, the Wronskian only needs to be nonzero at one point. This can be seen by taking
  \begin{align*}
    \diff{W(x)}{x} &= -p(x)W(x),
  \end{align*}
  meaning
  \begin{align*}
    W(x) &= W(a)\exp\left[ \int_{a}^{x} p(s)\:ds \right].
  \end{align*}
  This also means that we can find the Wronskian \textit{without} knowing the solutions to the equation.
  \begin{example}
    With simple harmonic motion,
    \begin{align*}
      \diff{^2u}{x^2} + k^2u &= 0,
    \end{align*}
    we have the solutions $\cos\left( kx \right),\sin\left( kx \right)$, meaning
    \begin{align*}
      W(x) &= \det \begin{pmatrix}\cos\left( kx \right) & \sin\left( kx \right)\\ -k\sin\left( kx \right) & k\cos\left( kx \right)\end{pmatrix}\\
           &= k\\
           &\neq 0.
    \end{align*}
    Now, to obviate the issue of $k\rightarrow 0$ yielding a zero Wronskian (instead of linear motion), we take $u_2 = \frac{1}{k}\sin\left( kx \right)$, which gives the Wronskian of $1$, and obviates this issue.\newline

    Now, for the damped oscillation, we get
    \begin{align*}
      W(x) &= 2\sqrt{\beta^2 - \omega_0^2} e^{-2\beta t}.
    \end{align*}
  \end{example}
  Since $W$ determines linear independence, if we're given one solution, we are able to find another solution. Taking
  \begin{align*}
    \diff{}{x}\left( \frac{u_2}{u_1} \right) &= \frac{u_1(x) \diff{u_2}{x} - \diff{u_1}{x}u_2(x)}{u_1(x)^2}\\
                                             &=\frac{W}{u_1^2}.
  \end{align*}
  Therefore,
  \begin{align*}
    u_2(x) &= u_1(x) \int_{}^{} \frac{W(x)}{u_1(x)^2}\:dx\\
           &= u_1(x) \int_{}^{} \frac{\exp\left[ - \int_{}^{x} p(s)\:ds \right]}{u_1(x)^2}\:d
  \end{align*}
  \begin{example}
    Consider Legendre's equation,
    \begin{align*}
      \left( 1-x^2 \right)\diff{^2 \phi}{x^2} - 2x\diff{\phi}{x} + \ell\left( \ell + 1 \right)\phi &= 0.
    \end{align*}
    Here,
    \begin{align*}
      p(x) &= -\frac{2x}{1-x^2},
    \end{align*}
    so that
    \begin{align*}
      W(x) &= W(a)\exp\left[ - \int_{}^{} -\frac{2x}{1-x^2}\:dx \right]\\
           &= \frac{1}{1-x^2}\tag*{$W(a)\coloneq -1$}
    \end{align*}
    We know that a solution of this equation will be $P_{\ell}\left( x \right)$. After we use the Wronskian, we obtain the ``second solution'' to the equation,
    \begin{align*}
      Q_{\ell}\left( x \right) &= P_{\ell}\left( x \right) \int_{}^{} \frac{1}{\left( 1-x^2 \right)P_{\ell}(x)^2}\:dx,
    \end{align*}
    which are known as the \textit{Legendre functions of the second kind}.\newline

    These functions are not particularly well-behaved. For instance,
    \begin{align*}
      Q_0(x) &= \frac{1}{2}\ln\left( \frac{1+x}{1-x} \right)\\
      Q_1(x) &= \frac{1}{2}x\ln\left( \frac{1 + x}{1-x} \right) - 1.
    \end{align*}
    These functions blow up at the boundary.
  \end{example}
  \begin{example}
    When we have an inhomogeneous equation, we will write $u_p(x)$ for the inhomogeneous solution. If we want to know the value of $u_p\left(x_0\right)$, then since $u_1,u_2$ are linearly independent, we should be able to write
    \begin{align*}
      u_p\left( x_0 \right) &= c_1u_1\left( x_0 \right) + c_2u_2\left( x_0 \right).
    \end{align*}
    Now, furthermore, we also have
    \begin{align*}
      \diff{u_p}{x}\biggr\vert_{x_0} &= c_1 \diff{u_1}{x}\biggr\vert_{x_0} + c_2 \diff{u_2}{x}\biggr\vert_{x_0}.
    \end{align*}
    This cannot hold for all values of $x_0$. We can modify this by taking $c_1,c_2$ to be functions of $x$, giving an expression of the form
    \begin{align*}
      u_p(x) &= a_1(x)u_1(x) + a_2(x)u_2(x).
    \end{align*}
    This procedure is known as \textit{variation of parameters}. Now, if this is supposed to work, we must also have
    \begin{align*}
      \diff{u_p}{x} &= a_1(x) \diff{u_1}{x} + a_2(x)\diff{u_2}{x}.
    \end{align*}
    This can only be consistent if
    \begin{align*}
      u_1(x) \diff{a_1}{x} + u_2(x)\diff{a_2}{x} &= 0.
    \end{align*}
    We need one more condition. We're tempted to take another derivative of $u_p$, but this doesn't give us any more information. As it turns out, by playing around with the differential equation, we are able to obtain
    \begin{align*}
      a_1(x) &= - \int_{}^{} \frac{u_2(x)r(x)}{W(x)}\:dx\\
      a_2(x) &= \int_{}^{} \frac{u_1(x)r(x)}{W(x)}\:dx.
    \end{align*}
    
  \end{example}
  \subsubsection{Series Solutions}%
  If our solution $u(x)$ is sufficiently smooth, we should be able to write
  \begin{align*}
    u(x) &= \sum_{m=0}^{\infty}c_mx^m,
  \end{align*}
  where
  \begin{align*}
    c_m &= \frac{1}{m!} \diff{^m u}{x^{m}}\biggr\vert_{x=0}.
  \end{align*}
  We'll do this backwards.\newline

  To solve for the $c_m$, we'll throw this expression for $u$ into the differential equation. We'll tweak our guess just a little bit, where we take
  \begin{align*}
    u(x) &= x^{\alpha} \sum_{m=0}^{\infty}c_mx^{m},\label{eq:series_solution_guess}\tag{$\ddag$}
  \end{align*}
  where $c_0 \neq 0$.
  \begin{example}
    Consider the equation
    \begin{align*}
      4xu'' + 2u' - u &= 0.
    \end{align*}
    Upon sticking in our guess, we have
    \begin{align*}
      \sum_{m=0}^{\infty}c_m2\left( m + \alpha \right)\left( 2m + 2\alpha - 1 \right)x^{m + \alpha - 1} - \sum_{m=0}^{\infty}c_mx^{m + \alpha} &= 0.
    \end{align*}
    We match powers to obtain our guess. The lowest order is $\alpha - 1$, meaning
    \begin{align*}
      c_0\alpha\left( 2\alpha - 1 \right) &= 0.\label{eq:indicial_equation}\tag{$\ast$}
    \end{align*}
    For all higher powers, we have
    \begin{align*}
      c_m &= \frac{1}{2\left( m+\alpha \right)\left( 2m + 2\alpha - 1 \right)}c_{m-1}. \label{eq:two_term_recurrence_series_solution}\tag{$\dag$}
    \end{align*}
    We call \eqref{eq:indicial_equation} the \textit{indicial equation} so that we may find $\alpha$. We get the solutions of $\alpha = 0$ and $\alpha = \frac{1}{2}$.\newline
    
    Taking $\alpha = \frac{1}{2}$, we insert it into \eqref{eq:two_term_recurrence_series_solution} to take
    \begin{align*}
      c_m &= \frac{1}{\left( 2m + 1 \right)\left( 2m \right)} c_{m-1}\\
          &= \frac{1}{\left( 2m+1 \right)\left( 2m \right)} \frac{1}{\left( 2m-1 \right)\left( 2m-2 \right)}c_{m-2}\\
          &\vdots\\
          &= \frac{c_0}{\left( 2m + 1 \right)!}.
    \end{align*}
    We get our first solution,
    \begin{align*}
      u_1(x) &= \sqrt{x}\left( 1 + \frac{1}{3!}x + \frac{1}{5!}x^2 + \cdots \right).
    \end{align*}
    Next, we take $\alpha = 0$, inserting into \eqref{eq:two_term_recurrence_series_solution} to get
    \begin{align*}
      c_m &= \frac{1}{2m\left( 2m-1 \right)}c_{m-1}\\
          &= \frac{1}{2m\left( 2m-1 \right)\left( 2m-2 \right)\left( 2m-3 \right)}c_{m-2}\\
          &\vdots\\
          &= \frac{c_0}{\left( 2m \right)!}.
    \end{align*}
    Thus, we get the second solution of
    \begin{align*}
      u_2(x) &= x^{0}\left( 1 + \frac{x}{2!} + \frac{x^2}{4!} + \cdots \right).
    \end{align*}
    As it turns out, our expressions for $u_1$ and $u_2$ have closed-form solutions,
    \begin{align*}
      u_1(x) &= \sinh\left( \sqrt{x} \right)\\
      u_2\left( x \right) &= \cosh\left( \sqrt{x} \right).
    \end{align*}
  \end{example}
  \begin{example}
    Consider the equation
    \begin{align*}
      x^2\diff{^2u}{x^2} + 2x\diff{u}{x} + x^2 u &= 0.\label{eq:series_solution_2}\tag{$\ast\ast$}
    \end{align*}
    Sticking in \eqref{eq:series_solution_guess}, we have
    \begin{align*}
      \sum_{m=0}^{\infty}c_m\left( m + \alpha \right)\left( m + \alpha + 1 \right)x^{m + \alpha} + \sum_{m=0}^{\infty}c_mx^{m + \alpha + 2} &= 0.
    \end{align*}
    Taking the two lowest orders out,we write explicitly
    \begin{align*}
      c_0\left( \alpha \right)\left( \alpha + 1 \right)x^{\alpha} + c_1\left( \alpha + 1 \right)\left( \alpha + 2 \right) x^{\alpha + 1} + \sum_{m=2}^{\infty}\left( c_m\left( m + \alpha \right)\left( m + \alpha + 1 \right) + c_{m-2} \right)x^{m + \alpha} &= 0.
    \end{align*}
    Since $c_0$ cannot equal zero, the first indicial equation yields $\alpha = 0,-1$.\newline

    If $\alpha = 0$, then $c_1 = 0$, and our recursion relation is
    \begin{align*}
      c_m &= \frac{\left( -1 \right)^{m/2}}{\left( m+1 \right)!} c_0
    \end{align*}
    for $m$ even. Plugging this in, we get
    \begin{align*}
      u_1(x) &= x^{0}\left( 1  - \frac{1}{3!}x^2 + \frac{1}{5!}x^{4} - \frac{1}{7!}x^{6} + \cdots \right)\\
             &= \frac{\sin(x)}{x}.
    \end{align*}
    Now, if $\alpha = -1$, then we may choose $c_1$ to be any value we desire. We will choose $c_1 = 0$. We get the even power recursion relation
    \begin{align*}
      u_2(x) &= \frac{1}{x}\left( 1 - \frac{1}{2!}x^2 + \frac{1}{4!}x^{4} -\frac{1}{6!}x^{6} + \cdots \right)\\
             &= \frac{\cos(x)}{x}.
    \end{align*}
    Now, if we had chosen any other value for $c_1$, the result would have been a linear combination of $u_1$ and $u_2$, which would have been more unwieldy.
  \end{example}
  In \eqref{eq:series_solution_2}, we note that the differential operator
  \begin{align*}
    \mathcal{L}_x &= x^2\diff{^2}{x^2} + 2x\diff{}{x} + x^2
  \end{align*}
  has definite parity. Therefore, we get even and odd solutions.\newline

  Practically, we want to know whether or not we even have a convergent power series. In the general form,
  \begin{align*}
    \diff{^2u}{x^2} + p(x)\diff{u}{x} + q(x)u &= 0,
  \end{align*}
  there is \textit{at least one} series solution centered at $x_0$ if and only if
  \begin{align*}
    \lim_{x\rightarrow x_0}\left( x-x_0 \right)p(x) &= 0\\
    \lim_{x\rightarrow x_0}\left( x-x_0 \right)^2q(x) &= 0.
  \end{align*}
  Now, if we want a second solution, there is a bit more work to be done, but we can show that if we have two roots of the indicial equation, $\alpha_1,\alpha_2$, then if $\alpha_1-\alpha_2\notin \Z$, there exists a series solution. Meanwhile, if $\alpha_1-\alpha_2 \in \Z$, we use the guess of
  \begin{align*}
    u_2 &= Au_1\ln\left\vert x \right\vert + x^{\alpha_2}\sum_{m=0}^{\infty}c_mx^{m}.
  \end{align*}
  \begin{example}[Bessel's Equation]
    Bessel's equation is is a one-parameter family of equations of the form
    \begin{align*}
      x^2\diff{^2u}{x^2} + x\diff{u}{x} + \left( x^2-\lambda^2 \right)u &= 0.
    \end{align*}
    We modify our guess from \eqref{eq:series_solution_guess} by taking
    \begin{align*}
      u(x) &= x^{\alpha}\sum_{m=0}^{\infty}\frac{1}{m!}c_mx^{m}.
    \end{align*}
    Plugging this into the equation, we get
    \begin{align*}
      \sum_{m=0}^{\infty}\frac{1}{m!}c_m\left( \left( m + \alpha \right)^2-\lambda^2 \right)x^{m + \alpha} + \sum_{m=0}^{\infty}\frac{1}{m!}c_mx^{m + \alpha + 2} &= 0.
    \end{align*}
    Shifting indices, we get
    \begin{align*}
      c_0\left( \alpha^2 -\lambda^2 \right)x^{\alpha} + c_1\left( \left( \alpha + 1 \right)^2-\lambda^2 \right)x^{\alpha + 1} &= \sum_{m=2}^{\infty}\left( \frac{c_m}{m!}\left( \left( m + \alpha \right)^2-\lambda^2 \right) - \frac{c_{m-2}}{\left( m-2 \right)!} \right)x^{m + \alpha}\\
                                                                                                                              &= 0.
    \end{align*}
    Since $c_0\neq 0$, we have $\alpha = \pm\lambda$, and $c_1  \left( 1 \pm 2\lambda \right)=  0$. Our recurrence relation is
    \begin{align*}
      c_m &= \frac{-\left( m-1 \right)}{\left( m\pm 2\lambda \right)}c_{m-2}.
    \end{align*}
    Expanding, we get Equation (39.75) in the book. If we divide out by $2^{\lambda}\lambda!$, we get the equation
    \begin{align*}
      J_{\lambda}\left( x \right) &= \sum_{m=0}^{\infty}\frac{\left( -1 \right)^{m}}{m!\Gamma\left( m + \lambda + 1 \right)} \left( \frac{x}{2} \right)^{m + \lambda}.
    \end{align*}
  \end{example}
  \begin{example}
    Legendre's equation is
    \begin{align*}
      \left( 1-x^2 \right)\diff{^2u}{x^2} - 2x\diff{u}{x} + \lambda u &= 0.
    \end{align*}
    This equation has definite parity, and we expect an ``every-other'' recurrence relation. The indicial equations are
    \begin{align*}
      c_0\alpha\left( \alpha - 1 \right) &= 0\\
      c_1\alpha\left( \alpha + 1 \right) &= 0.
    \end{align*}
    We get $\alpha = 1,0$, and the recurrence relation for $\alpha = 1$ of
    \begin{align*}
      c_m &= \frac{m\left( m-1 \right)-\lambda}{\left( m+1 \right)m}c_{m-2}.
    \end{align*}
    Evaluating this expansion, we get
    \begin{align*}
      u_1(x) &= x + \frac{\left( 2-\lambda \right)}{3!}x^3 + \frac{\left( 2-\lambda \right)\left( 12-\lambda \right)}{5!}x^5 + \frac{\left( 2-\lambda \right)\left( 12-\lambda \right)\left( 30-\lambda \right)}{7!}x^7 + \cdots
    \end{align*}
    Meanwhile, if $\alpha = 0$, we select $c_1 = 0$, and get the recurrence relation of
    \begin{align*}
      c_m &= \frac{\left( m-1 \right)\left( m-2 \right)-\lambda}{m\left( m-1 \right)}c_{m-2},
    \end{align*}
    and series of
    \begin{align*}
      u_2 &= 1 - \frac{\lambda}{2!}x^2 - \frac{\lambda\left( 6-\lambda \right)}{4!}x^4 - \frac{\lambda\left( 6-\lambda \right)\left( 20-\lambda \right)}{6!}x^6 + \cdots
    \end{align*}
    Evaluating the radius of convergence,
    \begin{align*}
      x^2 &< \lim_{m\rightarrow\infty} \left\vert \frac{c_{m-2}}{c_m} \right\vert\\
          &= 1.
    \end{align*}
    Note that for large $m$, though, evaluated at $\pm 1$, we have
    \begin{align*}
      \left\vert \frac{c_{m-2}}{c_m} \right\vert &\approx 1- \frac{2}{m}.
    \end{align*}
    This is effectively a harmonic series, which diverges. In order to cause our series to converge at $\pm 1$, we need specific values of $\lambda$ to cause the series to truncate.\newline

    If we set $\lambda = \ell\left( \ell + 1 \right)$, then selecting $\ell = 0,1,2,\dots$, either the even or odd series necessarily truncates.
  \end{example}
  \subsection{Sturm--Liouville Problems}%
  How does a differential equation yield orthogonal solutions?\newline

  We have dealt with orthogonal polynomials via Gram--Schmidt and generating functions, but originally they were defined by differential equations.\newline

  Consider the differential equation
  \begin{align*}
    \diff{^2\phi}{x^2} &= -k^2\phi.\label{eq:simplest_second_order_equation}\tag{$\dag$}
  \end{align*}
  The solutions here are sines and cosines. Now, if we define $D = \diff{}{x}$, then
  \begin{align*}
    D^2\phi &= -k^2\phi
  \end{align*}
  looks suspiciously like an eigenvalue problem.\newline

  Now, this eigenvalue equation does not necessarily guarantee orthogonality. We need one more thing here in order to be able to find orthogonality.\newline

  Recall from linear algebra that if an operator is self-adjoint, then the operator has real eigenvalues and orthogonal eigenvectors. Since $L_2$ is an inner product space, we want to be able to find out the definition of $D^{\ast}$ --- ideally, it is equal to $D$.\newline

  Recall that if $\mathcal{L}$ is a linear operator, the definition of $\mathcal{L}^{\ast}$ is the unique operator such that
  \begin{align*}
    \braket{\psi}{\mathcal{L}\phi} &= \braket{\mathcal{L}^{\ast}\psi}{\phi}.
  \end{align*}
  A self-adjoint (or Hermitian) operator is such that $\mathcal{L}^{\ast} = \mathcal{L}$. We want to be able to find this in the context of our $L_2$ space.\newline

  We note that
  \begin{align*}
    \braket{\psi}{\mathcal{L}\phi} &= \int_{a}^{b} \overline{\psi}\left( x \right) \left[ \mathcal{L}\left( \phi \right)\left( x \right) \right]\:dx\\
                                   &= \int_{a}^{b} \overline{\mathcal{L}^{\ast}\left( \psi \right)\left( x \right)}\phi\left( x \right)\:dx.
  \end{align*}
  Now, if we take $D = \diff{}{x}$, then
  \begin{align*}
    \braket{\psi}{D\phi} &= \int_{a}^{b} \overline{\psi\left( x \right)} \diff{\phi}{x}\:dx\\
                         &= \overline{\psi(x)}\phi(x)\biggr\vert_{a}^{b} - \int_{a}^{b} \overline{ \diff{\psi}{x} }\phi(x)\:dx\\
                         &= \overline{\psi(x)}\phi(x)\biggr\vert-{a}^{b} + \int_{a}^{b} \overline{-\diff{\psi}{x}} \phi(x)\:dx.
  \end{align*}
  If we restrict our view to functions that vanish at the boundary, we have $D^{\ast} = -\diff{}{x}$. The derivative is thus an anti-Hermitian operator.\newline

  Note that if we apply a factor of $i$, then $i\diff{}{x}$ is a Hermitian operator.\newline

  Furthermore, note that $D^2$ is Hermitian, so it has real eigenvalues and orthogonal eigenvectors.
  \subsubsection{Sturm--Liouville Operators}%
  The equation \eqref{eq:simplest_second_order_equation} is very simple. We will generalize by defining
  \begin{align*}
    \mathcal{L}\phi &= \left[ \alpha(x)\diff{^2}{x^2} + \beta(x)\diff{}{x} + \gamma(x) \right]\phi(x)\\
                    &= \lambda\phi(x).
  \end{align*}
  In order to have $\mathcal{L}$ as Hermitian, we set up our inner product and integrate by parts to yield
  \begin{align*}
    \braket{\psi}{\mathcal{L}\phi} &= \int_{a}^{b} \overline{\psi}(x)\left( \alpha\diff{^2}{x^2} + \beta \diff{}{x} + \gamma \right)\phi(x)\:dx\\
                                   &= \int_{}^{} \overline{\left( \alpha \diff{}{x} + \left( 2\diff{\alpha}{x} - \beta \right)\diff{}{x} + \left( \diff{^2\alpha}{x^2} - \diff{\beta}{x} + \gamma \right) \right)\psi(x)}\phi(x)\:dx + \text{boundary terms}.
  \end{align*}
  Note that the operator $\mathcal{L}^{\ast}$ does reduce to $\mathcal{L}$ if $\diff{\alpha}{x} = \beta(x)$. If we introduce the weight factor
  \begin{align*}
    w(x) &= \frac{1}{\alpha(x)} \exp\left( \int_{}^{} \frac{\beta(x)}{\alpha(x)}\:dx \right),
  \end{align*}
  then we will define
  \begin{align*}
    \widetilde{\alpha}(x) &= \alpha(x)w(x)\\
                          &= \exp\left( \int_{}^{} \frac{\beta(x)}{\alpha(x)}\:dx \right),
  \end{align*}
  and similarly, $\widetilde{\beta} = \beta w$, $\widetilde{\gamma} = \gamma w$. Note that
  \begin{align*}
    \diff{\widetilde{\alpha}}{x} &= \frac{\beta(x)}{\alpha(x)} \exp\left( \int_{}^{} \frac{\beta(x)}{\alpha(x)}\:dx \right)\\
                                 &= \beta(x)w(x)\\
                                 &= \widetilde{\beta}(x).
  \end{align*}
  We will replace $\widetilde{\alpha} = -p$ and $\widetilde{\gamma} = q$. This yields an operator in \textit{Sturm--Liouville} form:
  \begin{align*}
    \mathcal{L} &= -\diff{}{x}\left( p(x)\diff{}{x} \right) + q(x)\\
                &= -p(x)\diff{^2}{x^2} - \diff{p}{x}\diff{}{x} + q(x),
  \end{align*}
  where we assume $p,\diff{p}{x},q,w$ are real and well-behaved between $a$ and $b$. The \textit{Sturm--Liouville equation} is of the form
  \begin{align*}
    \mathcal{L}\phi &= \lambda w(x)\phi(x).
  \end{align*}
  Note that our boundary term is
  \begin{align*}
    p(x) \left( \overline{\diff{\psi}{x}}\phi(x) - \overline{\psi(x)}\diff{\phi}{x}\biggr\vert_{a}^{b} \right) &= 0.
  \end{align*}
  The easiest way to deal with this is by making sure $p(x) = 0$ at $a$ and $b$.
  \begin{example}
    Consider Legendre's equation,
    \begin{align*}
      \left( 1-x^2 \right)\diff{^2\phi}{x^2} - 2x\diff{\phi}{x} + \ell\left( \ell + 1 \right)\phi &= 0.
    \end{align*}
    We have $p(x) = 1-x^2 > 0$ between $\pm 1$, and $w(x) = 1$, meaning this equation yields orthogonal solutions between $[-1,1]$.\newline

    Laguerre's equation
    \begin{align*}
      x\diff{^2\phi}{x^2} + \left( 1-x \right)\diff{\phi}{x} + n\phi &= 0
    \end{align*}
    is not in Sturm--Liouville form. Multiplying by
    \begin{align*}
      w(x) &= \frac{1}{x}\exp\left( \int_{}^{} \frac{1-x}{x}\:dx \right)\\
           &= \frac{1}{x}e^{\ln(x) - x}\\
           &= e^{-x}.
    \end{align*}
    We thus get the Sturm--Liouville problem of
    \begin{align*}
      xe^{-x}\diff{^2\phi}{x^2} + \left( 1-x \right)e^{-x}\diff{\phi}{x} + ne^{-x}\phi &= 0\\
      -\diff{}{x}\left( xe^{-x}\diff{\phi}{x} \right) &= ne^{-x}\phi.
    \end{align*}
    Note that $p = xe^{-x}$ vanishes at $0$ and $\infty$.
  \end{example}
  Note that we need to apply conditions on $\phi$ and $\psi$ such that all the solutions $\chi$ form a vector space. The \textit{homogeneous} restrictions
  \begin{align*}
    c_1\chi(a) + d_1\chi'(a) &= 0\\
    c_2\chi(b) + d_2\chi'(b) &= 0,
  \end{align*}
  where at least one of $c_1,d_1$ and $c_2,d_2$ are nonzero. 
  \begin{itemize}
    \item The Dirichlet boundary conditions are of the form $\chi(a) = \chi(b) = 0$.
    \item The Neumann boundary conditions are of the form $\chi'(a) = \chi'(b) = 0$.
    \item Periodic boundary conditions are of the form $\chi(a) = \chi(b)$ and $\chi'(a) = \chi'(b)$.
  \end{itemize}
  \begin{example}
    Consider the equation
    \begin{align*}
      \diff{^2\phi}{x^2} &= -k^2,
    \end{align*}
    which is the Schrödinger equation for a free particle with momentum $\hbar k$. We will implement special periodic boundary conditions
    \begin{align*}
      \phi(0) &= \phi(L)\\
      \phi'(0) &= \phi'(L),
    \end{align*}
    which denotes a circle of circle $L$.\newline

    We will end up finding that
    \begin{align*}
      \phi(x) &= \sum_{n=-\infty}^{\infty}c_ne^{2\pi i x/L}.
    \end{align*}
  \end{example}
  \subsubsection{The Properties of Sturm--Liouville Problems}%
  Any Sturm--Liouville equation has the following features.
  \begin{itemize}
    \item The eigenvectors form an \textit{orthogonal} basis:
      \begin{align*}
        \int_{a}^{b} \overline{\phi_m(x)}\phi_n(x)w(x)\:dx &= k_n\delta_{mn},
      \end{align*}
      complete in $L_2$.\newline

      Any well-behaved function defined in $[a,b]$ can be expanded on the eigenbasis
      \begin{align*}
        \psi(x) &= \sum_{n}c_n\phi_n(x),
      \end{align*}
      where
      \begin{align*}
        c_m &= \braket{\phi_m}{f}\\
            &= \frac{1}{k_n} \int_{a}^{b} \overline{\phi_m(x)}f(x)w(x)\:dx.
      \end{align*}
    \item The eigenvalues $\lambda$ are real.
    \item If $[b-a]$ is finite, then the spectrum forms a discrete, countably infinite ordered set, such that
      \begin{align*}
        \lambda_1 \leq \lambda_2 \leq \lambda_3 \leq \cdots,
      \end{align*}
      which is used to label $\phi_n$. Since $\lim_{n\rightarrow\infty}\lambda_n = \infty$, there is no largest eigenvalue. The lower bound is
      \begin{align*}
        \lambda_n &\geq \frac{1}{k_n} \left[ -p(x) \overline{\phi_n(x)} \diff{\phi_n}{x}\biggr\vert_{a}^{b} + \int_{a}^{b} q(x) \left\vert \phi_n(x) \right\vert^2\:dx \right].
      \end{align*}
      If both $q$ and $-p \overline{\phi_n}\diff{\phi_n}{x}\biggr\vert_{a}^{b}$ are positive, then all eigenvalues are positive.
    \item The real eigenvectors are oscillatory: between $a$ and $b$, there are $n-1$ nodes $\alpha_i$ such that $\phi_n\left( \alpha_0 \right) = 0$. Note that oscillatory does not imply periodic, nor do the $\phi_n$ have fixed amplitude.
    \item For separated boundary conditions, the spectrum is non-degenerate. With periodic conditions, at most a double degeneracy can occur.
  \end{itemize}
  Sturm--Liouville equations occur all the time, especially in quantum mechanics. For instance, the time-independent Schrödinger equation,
  \begin{align*}
    \left( -\frac{\hbar^2}{2m}\diff{^2}{x^2} + V(x) \right)\phi(x) &= E\psi(x)
  \end{align*}
    is a Sturm--Liouville equation.
    \begin{example}
      We will contain a particle in a box of radius $a$, with potential
      \begin{align*}
        V(r) &= \begin{cases}
          0 & 0 \leq r < a\\
          \infty & r\geq a
        \end{cases}.
      \end{align*}
      We will have to replace the $\diff{^2}{x^2}$ in the Schrödinger equation with the Laplacian, $\nabla^2$, giving
      \begin{align*}
        \frac{1}{r} \pd{}{r}\left( r\pd{u}{r} \right) + \frac{1}{r^2}\pd{^2u}{\phi^2} &= -k^2 u,
      \end{align*}
      where $k^2 = \frac{2mE}{\hbar^2}$. We will let $u\left( \mathbf{r} \right)$ reduce to a purely radial function $R(r)$, which allows us to take
      \begin{align*}
        \diff{}{r}\left( r\diff{R}{r} \right) &= -k^2rR(r).
      \end{align*}
      This is Bessel's equation. There are two solutions: the Bessel function, $J_0(r)$, and the Neumann function, $N_0(r)$.\footnote{Also denoted $Y_0$.} Note that Neumann functions diverge at $0$, so our solutions are the Bessel functions.\newline

      Even though these solutions are not periodic, they are oscillatory. These solutions must be continuous at $r = a$, and confining a particle at $R(a) = 0$, Thus, we have $k_n = \frac{\alpha_n}{a}$, where $\alpha_n$ are zeros of the Bessel function.\newline

      The circular well has energies of
      \begin{align*}
        E_n &= \frac{\hbar^2\alpha_n^2}{2ma^2}.
      \end{align*}
      We get the orthogonality condition
      \begin{align*}
        \int_{0}^{a} J_0\left( \frac{\alpha_n r}{a} \right) J_0\left( \frac{\alpha_m r}{a} \right)r\:dr &= \frac{a^2}{2}J_0'\left( \alpha_n \right)^2\delta_{mn}.
      \end{align*}
      We have the eigenfunctions
      \begin{align*}
        \phi\left( r \right) &= \sum_{n}c_nR_n(r)\\
                             &= \sum_{n}c_nJ_0\left( \frac{\alpha_n r}{a} \right),
      \end{align*}
      where
      \begin{align*}
        c_n &= \frac{2}{a^2J_1\left( \alpha_n \right)^2} \int_{0}^{a} J_0\left( \frac{\alpha_n r}{a} \right)\psi\left( r \right)\:dr.
      \end{align*}
      This is known as the \textit{Fourier--Bessel} series.
    \end{example}
    
\end{document}
