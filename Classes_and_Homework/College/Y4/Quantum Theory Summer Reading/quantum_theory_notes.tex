\documentclass[10pt]{extarticle}

\title{}
\author{}
\date{}
\usepackage[shortlabels]{enumitem}

%paper setup
\usepackage{geometry}
\geometry{letterpaper, portrait, margin=1in}
\usepackage{fancyhdr}
% sans serif font:
\usepackage{cmbright}
\usepackage{sfmath}

%symbols
\usepackage{amsmath}
\usepackage{bigints}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage[hidelinks]{hyperref} %hyperlinks
\usepackage{gensymb} %more symbols
\usepackage{multirow,array} %better tables
\usepackage{multicol} %multiple columns per page
\usepackage{bbold} %better blackboard bold
\newtheorem*{remark}{Remark}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{cd}
\tikzset{middleweight/.style={pos = 0.5}}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\setlength{\parindent}{0pt} %I don't like indentation
\usepackage{cancel} %better X-throughs
\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Quantum Theory for Mathematicians: Notes}

%canonical sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\bra}[1]{\left\langle#1\right\vert}
\newcommand{\ket}[1]{\left\vert#1\right\rangle}
\newcommand{\braket}[2]{\left\langle#1\mid#2\right\rangle}
\newcommand{\set}[1]{\left\{#1\right\}}

%common other symbols
\newcommand{\mcc}{\mathcal{C}} %cantor set
\newcommand{\mco}{\mathcal{O}} %holomorphic functions
\newcommand{\mfp}{\mathfrak{p}} %prime ideal

%inner products and norms
\newcommand{\iprod}[2]{\left\langle #1,#2\right\rangle}
\newcommand{\norm}[1]{\left\Vert #1\right\Vert}

\setcounter{secnumdepth}{0}

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}%[section]
\newtheorem*{axiom}{Axiom}%[section]
\newtheorem*{lemma}{Lemma}%[theorem]
\newtheorem*{proposition}{Proposition}%[section]

\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem*{example}{Example}

\theoremstyle{remark}
%\newtheorem*{remark}{Remark}
\newtheorem*{authnote}{Author's Remark}
\newtheorem*{profnote}{Professor's Remark}

\usepackage[document]{ragged2e}
\renewcommand{\newline}{\hfill\break}
\renewcommand{\thefootnote}{\roman{footnote}}
\begin{document}

  \section{Classical Mechanics}%
  \subsection{Motion in $\R^{1}$}%
  Let $x(t)$ denote position. Then, $v(t) = \frac{dx}{dt} = \dot{x}(t)$ is velocity (where the $\cdot$ denotes derivative with respect to time), $a(t) = \dot{v}(t) = \ddot{x}(t)$, etc.\newline

  Considering Newton's second law, $F(x(t)) = m\ddot{x}(t)$, every exact solution requires initial conditions of $x(t_0)$ and $v(t_0)$. Solutions to Newton's second law are known as trajectories.\newline

  Considering a spring of constant $k$, $F(x) = -kx$ yields the differential equation $m\ddot{x} + kx = 0$. The general solution is
  \begin{align*}
    x(t) = a\cos(\omega t) + b\cos(\omega t),
  \end{align*}
  with $\omega = \sqrt{k/m}$ denoting the frequency. The spring is an example of a simple harmonic oscillator.
  \subsection{Conservation of Energy}%
  For a general force function $F(x)$, the kinetic energy is $\frac{1}{2}mv^2$, and the potential energy is
  \begin{align*}
    V(x) = -\int F(x)dx,
  \end{align*}
  meaning $F(x) = -\frac{dV}{dx}$. The total energy is thus found as
  \begin{align*}
    E(x,v) = \frac{1}{2}mv^2 + V(x).
  \end{align*}
  \begin{proposition}[Conservation of Energy]
  If a particle with trajectory $x(t)$ satisfies $m\ddot{x} = F(x)$, then the energy $E$ is conserved.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \frac{d}{dt}E(x(t),\dot{x}(t)) &= \frac{d}{dt}\left(\frac{1}{2}m(\dot{x}(t))^2 + V(x(t))\right)\\
                                     &= m\dot{x}(t)\ddot{x}(t) + \frac{dV}{dx}\dot{x}(t)\\
                                     &= \dot{x}(t)\left(m\ddot{x}(t) - F(x(t))\right).
    \end{align*}
  \end{proof}
  By using the conservation of energy, we can reduce the second order differential equation $F(x) = m\ddot{x}$ to a system of first order differential equations in $x(t)$ and $v(t)$ respectively:
  \begin{align*}
    \frac{dx}{dt} &= v(t)\\
    \frac{dv}{dt} &= \frac{1}{m}F(x(t)).
  \end{align*}
  If $(x(t),v(t))$ satisfies this set of equations, then $x(t)$ satisfies Newton's second law. We say the set of all possible $(x,v)$ forms the phase space for the particle in $\R^1$.\newline

  In phase space, conservation of energy implies that the set of all $(x,v)$ must lie on the level curve of the energy function: $\set{(x,v)\mid E(x,v) = E(x_0,v_0)}$.\newline

  Using the conservation of energy, we find that, though Newton's second law is a second order differential equation in time, it is actually a first order differential equation:
  \begin{align*}
    \frac{m}{2}\left(\dot{x}(t)\right)^2 + V(x(t) &= E(x(t_0),v(t_0))\\
    \dot{x}(t) &= \sqrt{\frac{2(E_0 - V(x(t)))}{m}}
  \end{align*}
  \subsection{Damping}%
  Suppose we also introduce a force that depends on velocity --- in the case of a damped simple harmonic oscillator, the equation for force changes from $F = -kx$ to $F = -kx - \gamma\dot{x}$, with $\gamma > 0$. The damping force acts in the opposite direction of velocity, meaning the particle slows down.\newline

  The equation of motion is then
  \begin{align*}
    m\ddot{x} + \gamma\dot{x} + kx = 0.
  \end{align*}
  For $\gamma$ small, the solutions are a sum sines and cosines multiplied by some exponential decay factor, but for $\gamma$ large, the solutions are only the exponential decay.\newline
  \begin{proposition}[Energy Conservation in Damped Systems]
    Suppose a particle moves along $x(t)$ that satisfies $F(x,\dot{x}) = F_1(x) - \gamma\dot{x}$, with $\frac{dV}{dx} = -F_1(x)$ and $\gamma > 0$. Then,
    \begin{align*}
      \frac{d}{dt}E(x(t),\dot{x}(t)) &= -\gamma\dot{x}(t)^2.
    \end{align*}
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{d}{dt}E(x(t),\dot{x}(t)) &= \dot{x}(t)\left(m\ddot{x}(t) - F_1(x(t))\right)\\
                                       &= \dot{x}(t)\left(m\ddot{x}(t) - (m\ddot{x}(t) + \gamma\dot{x}(t))\right)\\
                                       &= -\gamma\dot{x}(t)^2
      \end{align*}
  \end{proof}
  \subsection{Motion in $\R^n$}%
  The position of a particle $\mathbf{x} = (x_1,\dots,x_n)$ lends itself to velocity $\mathbf{v} = (v_1,\dots,v_n) = (\dot{x}_1,\dots,\dot{x}_n)$, and $\mathbf{a} = (\ddot{x}_1,\dots,\ddot{x}_n)$. Similar to in $\R^1$, Newton's second law is denoted
  \begin{align*}
    m\mathbf{\ddot{x}} = \mathbf{F}(\mathbf{x}(t),\mathbf{\dot{x}}(t)).
  \end{align*}
  \begin{proposition}[Conservation of Energy in $n$ Dimensions]
    The energy function
    \begin{align*}
      E(\mathbf{x},\mathbf{\dot{x}}) = \frac{1}{2}m\norm{\mathbf{\dot{x}}}^2 + V(\mathbf{x})
    \end{align*}
    is only satisfied where $\mathbf{F} = -\nabla V$.
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{d}{dt}\left(\frac{1}{2}m\norm{\mathbf{\dot{x}}}^2 + V(\mathbf{x})\right) &= m\sum_{j=1}^{n}\dot{x}_j\ddot{x}_j + \sum_{j=1}^{n}\frac{\partial V}{\partial x_j}\dot{x}_j(t)\\
                                                                                 &= \mathbf{\dot{x}}(t)\left(m\mathbf{\ddot{x}}(t) + \nabla V\right)\\
                                                                                 &= \dot{x}(t)\left(\mathbf{F}(x) + \nabla V(\mathbf{x})\right),
      \end{align*}
      which is equal to zero only if $-\nabla V = \mathbf{F}$.
  \end{proof}
  If $\mathbf{F}$ is a smooth $\R^n$ valued function on $U\subset \R^n$, then $\mathbf{F}$ is conservative if there exists a smooth real-valued function $V$ such that $\mathbf{F} = -\nabla V$.\newline

  In other words, $\mathbf{F}$ is conservative if $\mathbf{F}$ is a gradient field, implying that $\nabla \times \mathbf{F} = 0$.\\

  If $\mathbf{F}(\mathbf{x},\mathbf{y}) = -\nabla V(\mathbf{x}) + \mathbf{F}_{2}(\mathbf{x},\mathbf{y})$, with $\mathbf{v}\cdot \mathbf{F}_{2} = 0$ for all $\mathbf{x}$ and $\mathbf{v}$, then energy is conserved along a given trajectory.
  \subsection{Systems of Particles}%
  Let $\mathbf{x}^j = \left(x_{1}^j,x_2^j,\dots,x_n^j\right)$ denote the $j$th particle of a system of $N$ particles. Newton's second law is thus reformulated as
  \begin{align*}
    m_j\mathbf{\ddot{x}}^j = \mathbf{F}^j\left(\mathbf{x}^1,\dots,\mathbf{x}^N,\mathbf{\dot{x}}^1,\dots,\mathbf{\dot{x}}^N\right).
  \end{align*}
  The total energy is determined by
  \begin{align*}
    E(\mathbf{x}^1,\dots\mathbf{x}^N,\mathbf{v}^1,\dots,\mathbf{v}^N) &= \left(\sum_{j=1}^{N}\frac{1}{2}m_j\norm{\mathbf{v}^j}^2\right) + V(\mathbf{x}^1,\dots,\mathbf{x}^N).
  \end{align*}
  \begin{proposition}[Conservation of Energy in a System of Particles]
    
  The energy function is constant along each trajectory if $\nabla^{j}V = -\mathbf{F}^j$, where $\nabla^j$ denotes the gradient with respect to $\mathbf{x}^j$.\newline

  The force function along a simply connected domain $U$ in $\R^{nN}$ satisfies $\nabla^j V = -\mathbf{F}^j$ if and only if
  \begin{align*}
    \frac{\partial F_{k}^j}{\partial x_m^l} = \frac{\partial F_m^l}{\partial x_k^j}
  \end{align*}
  for all $j,k,l,m$.
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{dE}{dt} &= \sum_{j=1}^{N}\left(m_j\mathbf{\dot{x}}^j\cdot\mathbf{\ddot{x}}^j + \nabla^{j}V\cdot \mathbf{x}^j\right)\\
                      &= \sum_{j=1}^{N}\mathbf{\dot{x}}^j\left(m_j\mathbf{\ddot{x}}^j + \nabla^jV\right)\\
                      &= \sum_{j=1}^{N}\mathbf{\dot{x}}\left(\mathbf{F}^j + \nabla^j V\right),
      \end{align*}
      which is equal to zero if $\nabla^j V = -\mathbf{F}^j$.\newline

      Applying a higher dimension version of $\nabla \times \mathbf{F}$ to each coordinate pair $(a,b)$, we find the identity that shows $\mathbf{F}$ is a gradient field.
  \end{proof}
  \subsection{Momentum of a System of Particles}%
  The momentum of a particle $\mathbf{p}^j$ is defined by
  \begin{align*}
    \mathbf{p}^j = m_j\mathbf{\dot{x}}^j.
  \end{align*}
  Observe that $\frac{d\mathbf{p}^j}{dt} = m_j\mathbf{\ddot{x}}^j = \mathbf{F}^j$. The total momentum is then
  \begin{align*}
    \mathbf{p} &= \sum_{j=1}^{N}\mathbf{p}^j.
  \end{align*}
  Newton's third law, which states ``for every action there is an equal and opposite reaction'' applies if 
  \begin{itemize}
    \item $\displaystyle \mathbf{F}^j = \sum_{k\neq j}\mathbf{F}^{j,k}(\mathbf{x}^j,\mathbf{y}^j)$;
    \item $\displaystyle \mathbf{F}^{j,k}(\mathbf{x}_j,\mathbf{x}_k) = -\mathbf{F}^{k,j}(\mathbf{x}^k,\mathbf{x}^j)$.
  \end{itemize}
  If each $\mathbf{F}^j$ is also a conservative force, then satisfying these conditions yields potential energy in the form of
  \begin{align*}
    V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N) &= \sum_{j < k}V^{j,k}(\mathbf{x}^{j} - \mathbf{x}^{k}).
  \end{align*}
  \begin{proposition}[Newton's Third Law and Conservation of Momentum]
  If the system of particles satisfies the conditions of
  \begin{itemize}
    \item $\displaystyle \mathbf{F}^j = \sum_{k\neq j}\mathbf{F}^{j,k}(\mathbf{x}^j,\mathbf{y}^j)$
    \item and $\displaystyle \mathbf{F}^{j,k}(\mathbf{x}_j,\mathbf{x}_k) = -\mathbf{F}^{k,j}(\mathbf{x}^k,\mathbf{x}^j)$,
  \end{itemize}
  then total momentum is conserved.
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{d\mathbf{p}}{dt} &= \sum_{j=1}^{N}\mathbf{F^{j}}\\
                            &= \sum_{j=1}^{N}\sum_{k\neq j}\mathbf{F}^{j,k}(\mathbf{x}^j,\mathbf{x}^{k}),
      \end{align*}
      and since $F^{j,k}(\mathbf{x}^j,\mathbf{x}^k) + \mathbf{F}^{k,j}(\mathbf{x}^k,\mathbf{x}^j) = 0$, we find $\frac{d\mathbf{p}}{dt} = 0$.
  \end{proof}
  \begin{proposition}[Translation Invariance of Potential]
  Let $V$ denote the potential for a conservative force. Then, momentum is conserved if and only if $V$ is translation invariant, meaning that for all $\mathbf{a}\in \R^n$,
  \begin{align*}
    V(\mathbf{x}^1 + \mathbf{a},\mathbf{x}^2 + \mathbf{a},\dots,\mathbf{x}^N+\mathbf{a}) &= V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N).
  \end{align*}
  \end{proposition}
  \begin{proof}
  Let $\mathbf{a} = t\mathbf{e}_k$. Then, differentiating at $t=0$ with respect to $t$, we find
      \begin{align*}
        0 &= \sum_{j=1}^{N}\frac{\partial V}{\partial x_{k}^j}\\
          &= -\sum_{j=1}^{N}F^{j}_k\\
          &= -\sum_{j=1}^{N}\frac{dp_{k}^j}{dt}\\
          &= -\frac{dp_k}{dt},
      \end{align*}
      with $p_k$ denoting the $k$th component of $\mathbf{p}$. Therefore, $\mathbf{p}$ is constant in time.\newline

      If $\mathbf{p}$ is conserved, then the sum of all forces is $0$ at each point for all $t$, meaning that for all $t$,
      \begin{align*}
        \frac{d}{dt}V(\mathbf{x}^1 + t\mathbf{a} , \mathbf{x}^2 + t\mathbf{a},\dots,\mathbf{x}^N + t\mathbf{a}) &= \sum_{j=1}^{N}\nabla^jV(\mathbf{x}^1 + t\mathbf{a},\mathbf{x}^2 + t\mathbf{a},\dots,\mathbf{x}^n + t\mathbf{a})\cdot \mathbf{a}\\
                                                                                              &= -\left(\sum_{j=1}^{N}\mathbf{F}^j(\mathbf{x}^1 + t\mathbf{a}, \mathbf{x}^2 + t\mathbf{a},\dots,\mathbf{x}^N + t\mathbf{a})\right)\cdot \mathbf{a}\\
                                                                                              &= 0
      \end{align*}
      meaning $V$ is equal at $t=0$ and $t=1$.
  \end{proof}
  \subsection{Center of Mass}%
  For a system of $N$ particles, the center of mass is denoted
  \begin{align*}
    \mathbf{c} &= \sum_{j=1}^{N}\frac{m_j}{\sum_{j=1}^{N}m_j}\mathbf{x}_j.
  \end{align*}
  We denote $\sum_{j=1}^{N}m_j = M$. Differentiating $\mathbf{c}$, we get
  \begin{align*}
    \frac{d\mathbf{c}}{dt} &= \frac{1}{M}\sum_{j=1}^{N}m_j\mathbf{\dot{x}}^j\\
                        &= \frac{\mathbf{p}}{M}.
  \end{align*}
  Notice that if $ \mathbf{p} $ is conserved, then $ \mathbf{c}(t) = \mathbf{c}(t_0) + (t-t_0)\frac{ \mathbf{p} }{M}$.\newline

  For a system of two particles, if $V( \mathbf{x}^1, \mathbf{x}^2 )$ is invariant under translation, then $V( \mathbf{x}^1, \mathbf{x}^2 ) = \tilde{V}( \mathbf{x}^1- \mathbf{x}^2 )$, and $\tilde{V}(\mathbf{a}) = V\left(\mathbf{a},0\right)$.\newline

  The positions $\mathbf{x}^1$ and $\mathbf{x}^2$ can be recovered from knowledge about $\mathbf{c}$ and the relative position $\mathbf{y} := \mathbf{x}^1 - \mathbf{x}^2$:
  \begin{align*}
    \mathbf{x}^1 &= \frac{\mathbf{c} + m_2\mathbf{y}}{m_1 + m_2}\\
    \mathbf{x}^2 &= \frac{\mathbf{c} - m_1\mathbf{y}}{m_1 + m_2}.
  \end{align*}
  Thus, we can calculate
  \begin{align*}
    \mathbf{\ddot{y}} &= \mathbf{\ddot{x}}^1 - \mathbf{\ddot{x}}^2\\
                      &= -\frac{1}{m_1}\nabla\tilde{V}\left(\mathbf{x}^1 - \mathbf{x}^2\right) - \frac{1}{m_2}\nabla\tilde{V}\left(\mathbf{x}^1 - \mathbf{x}^2\right).
  \end{align*}
  \subsubsection{Motion of Relative Position under Translation Invariant Potential}%
  For a two particle system with translation invariant potential, the relative position $\mathbf{y} = \mathbf{x}^1 - \mathbf{x}^2$ is a solution to the differential equation
  \begin{align*}
    \mu\mathbf{\ddot{y}} &= -\nabla\tilde{V}(\mathbf{y}),
  \end{align*}
  where
  \begin{align*}
    \mu &= \frac{m_1m_2}{m_1 + m_2}.
  \end{align*}
  This implies that when momentum is conserved, the relative position of the two particle system evolves as a one-particle system with effective mass $\mu$.
  \subsection{Angular Momentum}%
  A particle moving in $\R^2$ with position $\mathbf{x}$, velocity $\mathbf{v}$, and momentum $\mathbf{p} = m\mathbf{v}$ has angular momentum $J$ denoted as
  \begin{align*}
    J = x_1p_2 - x_2p_1,
  \end{align*}
  or $J = \norm{\mathbf{x}\times \mathbf{p}} = \norm{\mathbf{x}}\norm{\mathbf{p}}\sin\phi$, with $\phi$ measured counterclockwise. In polar coordinates, we find
  \begin{align*}
    J &= mr^2\frac{d\theta}{dt}\\
      &= 2M\frac{dA}{dt},
  \end{align*}
  where $A= 1/2 \int r^2 d\theta$ denotes the area swept out by $\mathbf{x}(t)$.
  \begin{proposition}[Conservation of Angular Momentum]
    Suppose a particle of mass $m$ is moving in $\R^2$ under the influence of a conservative force with potential $V(\mathbf{x})$. $V$ is invariant under rotation if and only if $J$ is conserved.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \frac{dJ}{dt} &= \frac{dx_1}{dt}p_2 + x_1\frac{dp_2}{dt} - \frac{dx_2}{dt}p_1 - x_2\frac{dp_1}{dt}\\
                    &= \frac{1}{m}p_1p_2 - x_1\frac{\partial V}{\partial x_2} - \frac{1}{m}p_2p_1 + x_2\frac{\partial V}{\partial x_1}\\
                    &= x_2\frac{\partial V}{\partial x_1} - x_1\frac{\partial V}{\partial x_2}.
    \end{align*}
    Alternatively, consider $R_{\theta} = \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}$. Differentiating $V$ along $R_{\theta}$, we get
    \begin{align*}
      \frac{d}{d\theta}V(R_{\theta}\mathbf{x})\biggr\vert_{\theta = 0} &= \frac{\partial V}{\partial x}\frac{dx}{d\theta} + \frac{\partial V}{\partial y}\frac{dy}{d\theta}\\
                                                                       &= -x_2\frac{\partial V}{\partial x_1} + x_1\frac{\partial V}{\partial x_2}\\
                                                                       &= -\frac{dJ}{dt}\left(\mathbf{x}\right)
    \end{align*}
    Thus, $\frac{dJ}{dt} = 0$ if and only if the angular derivative of $V$ is zero.
  \end{proof}
  As a result of the conservation of angular momentum, we thus get Kepler's Second Law: if $\mathbf{x}(t)$ is the trajectory of a particle under the influence of a force with rotationally invariant potential, then the area swept out by $\mathbf{x}(t)$ between $t=a$ and $t=b$ is $\frac{b-a}{2m}J$.\newline

  In $\R^3$, $\mathbf{J}$ is a vector given by $\mathbf{x}\times \mathbf{p}$. Meanwhile, in $\R^n$, the angular momentum is a skew-symmetric matrix defined by
  \begin{align*}
    J_{jk} &= x_{j}p_k - x_kp_j.
  \end{align*}
  The total angular momentum of a system of $N$ particles in $\R^n$ is given by $\mathbf{J}$ with entries
  \begin{align*}
    J_{jk} &= \sum_{l=1}^{N}\left(x_{j}^lp_{k}^l - x_{k}^l - p_{j}^l\right).
  \end{align*}
  Similar to the case of linear momentum, angular momentum is constant in the presence of a conservative force if and only if the potential function $V$ is rotationally invariant. That is,
  \begin{align*}
    V(R\mathbf{x}^1,R\mathbf{x}^2,\dots,R\mathbf{x}^N) &= V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N)
  \end{align*}
  for all rotation matrices $R$.
  \subsection{Hamiltonian Mechanics}%
  The Hamiltonian is the total energy function, but formulated in terms of position and momentum rather than position and velocity. If a particle in $\R^n$ has the usual energy function, we write
  \begin{align*}
    H(\mathbf{x},\mathbf{p}) &= \frac{1}{2m}\sum_{j=1}^{n}p_j^2 + V(\mathbf{x}),
  \end{align*}
  where $p_j = m_j\dot{x}_j$. Observe that the equations of motion can be written as
  \begin{align*}
    \frac{dx_j}{dt} &= \frac{\partial H}{\partial p_j}\\
    \frac{dp_j}{dt} &= -\frac{\partial H}{\partial x_j}.
  \end{align*}
  In the basic formulation, we can see that the first equation is just $\dot{x}_j = p_j/m$, and $\dot{p}_j = F_j$.The equations of motion written with Hamiltonians are known as Hamilton's equations.
  \subsubsection{Poisson Bracket}%
  Let $f$ and $g$ be two smooth functions on $\R^{2n}$, with each element of $\R^{2n}$ being denoted by $(\mathbf{x},\mathbf{p})$. The Poisson bracket of $f$ and $g$ is equal to
  \begin{align*}
    \set{f,g} (\mathbf{x},\mathbf{p}) &= \sum_{j=1}^{n}\left(\frac{\partial f}{\partial x_j}\frac{\partial g}{\partial p_j} - \frac{\partial f}{\partial p_j}\frac{\partial g}{\partial x_j}\right).
  \end{align*}
  The Poisson bracket satisfies the following properties:
  \begin{itemize}
    \item Linearity: $\set{f,g+ch} = \set{f,g} + c\set{f,h}$
    \item Antisymmetry: $\set{g,f} = -\set{f,g}$
    \item Product Rule: $\set{f,gh} = \set{f,g}h + g\set{f,h}$
    \item Jacobi Identity: $\set{f,\set{g,h}} + \set{h,\set{f,g}} + \set{g,\set{h,f}} = 0$.
  \end{itemize}
  It can be easily verified that the following Poisson bracket relations hold:
  \begin{align*}
    \set{x_j,x_k} &= 0\\
    \set{p_j,p_k} &= 0\\
    \set{x_j,p_k} &= \delta_{jk},
  \end{align*}
  where $\delta_{jk}$ denotes the Kronecker delta function.
  \begin{proposition}[Solutions of Hamilton's Equations]
    If $(\mathbf{x}(t),\mathbf{p}(t))$ is a solution to Hamilton's Equations, then for any smooth $f$ on $\R^{2n}$, we have
    \begin{align*}
      \frac{df}{dt} &= \set{f,h}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \frac{df}{dt} &= \sum_{j=1}^{n}\left(\frac{\partial f}{\partial x_j}\frac{dx_j}{dt} + \frac{\partial f}{\partial p_j}\frac{dp_j}{dt}\right) \\
                    &= \sum_{j=1}^{n}\left(\frac{\partial f}{\partial x_j}\frac{\partial H}{\partial p_j}+ \frac{\partial f}{\partial p_j}\left(-\frac{\partial H}{\partial x_j}\right)\right)\\
                    &= \set{f,H}.
    \end{align*}
  \end{proof}
  \subsubsection{Conserved Quantities}%
  Let $f\in C^{1}(\R^{2n})$ be called conserved if $f(\mathbf{x}(t),\mathbf{p}(t))$ is independent of $t$ for each solution to Hamilton's equation. Then, $f$ is a conserved quantity if and only if
  \begin{align*}
    \set{f,H} &= 0.
  \end{align*}
  Note that $H$ is also a conserved quantity.
  \subsubsection{Flow and Liouville's Theorem}%
  Solving Hamilton's equations on $\R^{2n}$ yields a flow $\Phi_t$\footnote{the $\Phi_t$ are diffeomorphisms, or differentiable isomorphisms with differentiable inverses} with $\Phi_t(\mathbf{x},\mathbf{p})$ equal to the solution at time $t$ with initial condition $(\mathbf{x},\mathbf{p})$.\newline

  The $\Phi_t$ aren't necessarily defined on all of $\R^{2n}$, but if $\Phi_t$ is defined on $\R^{2n}$ for all $t$, then we say $\Phi_t$ is complete.
  \begin{proposition}[Liouville\footnote{not from complex analysis}]
    The Hamiltonian flow preserves the $2n$-dimensional measure.
    \begin{align*}
      dx_1 dx_2 \cdots dx_n dp_1 dp_2 \cdots dp_n.
    \end{align*}
    More specifically, if $E$ is a measurable subset of the domain of $\Phi_t$, then $\mu\left(\Phi_t(E)\right) = \mu(E)$.
  \end{proposition}
  \begin{proof}
    Hamilton's equations can be written as
        \begin{align*}
          \frac{d}{dt} \begin{bmatrix}x_1\\\vdots\\x_n\\p_1\\\vdots\\p_n\end{bmatrix} &= \begin{bmatrix}\frac{\partial H}{\partial p_1}\\\vdots \frac{\partial H}{\partial p_n}\\-\frac{\partial H}{\partial x_1}\\\vdots\\-\frac{\partial H}{\partial x_n}\end{bmatrix}.
        \end{align*}
        Hamilton's equations describe the flow along the vector field appearing on the right side --- by a result in vector calculus,\footnote{Author's Note: I do not know this result yet, but hopefully I will soon!} the flow preserves the $2n$-dimensional area measure if and only if the divergence of the vector field is zero.
        \begin{align*}
          \nabla \cdot \begin{bmatrix}\frac{\partial H}{\partial p_1}\\\vdots \frac{\partial H}{\partial p_n}\\-\frac{\partial H}{\partial x_1}\\\vdots\\-\frac{\partial H}{\partial x_n}\end{bmatrix} &= \sum_{k=1}^{n}\frac{\partial}{\partial x_k}\frac{\partial H}{\partial p_{k}} - \frac{\partial}{\partial p_k}\frac{\partial H}{\partial x_{k}}\\
         &= \sum_{k=1}^{n}\frac{\partial^{2} H}{\partial x_k \partial p_k} - \frac{\partial^2 H}{\partial p_k\partial x_k}\\
          &= 0
        \end{align*}
  \end{proof}
  The condition of zero divergence is equivalent to $\Phi_t$ preserving a particular symplectic form $\omega$ defined by
  \begin{align*}
    \omega\left((\mathbf{x},\mathbf{p}),(\mathbf{x}',\mathbf{p}')\right) &= \mathbf{x}\cdot p' - \mathbf{p}\cdot x',
  \end{align*}
  meaning that for any $t$ and any $(\mathbf{x},\mathbf{p})\in \R^{2n}$, the partial derivatives of $\Phi_t$ preserves $\omega$.\newline

  Alternatively, this is equivalent to $\Phi_t$ preserving Poisson brackets:
  \begin{align*}
    \set{f\circ \Phi_t,g\circ\Phi_t} &= \set{f,g}\circ \Phi_t.
  \end{align*}
  Thus, $\Phi_t$ is an example of a symplectomorphism.
  \subsubsection{Hamiltonian Flow and Hamiltonian Generators}%
  We say $f\in C^{1}(\R^{2n})$ is the Hamiltonian generator of the flow that results from solving Hamilton's equations with $f$ substituted for $H$:
  \begin{align*}
    \frac{dx_j}{dt} &= \frac{\partial f}{\partial p_j}\\
    \frac{dp_j}{dt} &= -\frac{\partial f}{\partial x_j}.
  \end{align*}
  It is possible to see that
  \begin{align*}
    f_{\mathbf{a}}(\mathbf{x},\mathbf{p}) &= \mathbf{a}\cdot \mathbf{p}
  \end{align*}
  yields the flow
  \begin{align*}
    \mathbf{x}(t) &= \mathbf{x}_0 + t\mathbf{a}\\
    \mathbf{p}(t) &= \mathbf{p}_0,
  \end{align*}
  and
  \begin{align*}
    g_{\mathbf{b}}(\mathbf{x},\mathbf{p}) &= \mathbf{b}\cdot \mathbf{x}
  \end{align*}
  yields the flow
  \begin{align*}
    \mathbf{x}(t) &= \mathbf{x}_0\\
    \mathbf{p}(t) &= \mathbf{p}_0 - t\mathbf{b}.
  \end{align*}
  Thus, the Hamiltonian flow generated by momentum yields translation in position, and the Hamiltonian flow generated by position yields translation in momentum.\newline

  In this light, we can think of \textit{the} Hamiltonian as the Hamiltonian generator that yields time evolution.Other Hamiltonian generators represent some other family of symmetries of the system.
  \begin{proposition}[Hamiltonian Flow generated by Angular Momentum]
    For a particle moving in $\R^2$, the Hamiltonian  flow generated by 
    \begin{align*}
      J(\mathbf{x},\mathbf{p}) &= x_1p_2 - x_2p_1
    \end{align*}
    consists of simultaneous rotations of $\mathbf{x}$ and $\mathbf{p}$.
    \begin{align*}
      \begin{bmatrix}x_1(t)\\x_2(t)\end{bmatrix} &= \begin{bmatrix}\cos t & -\sin t\\\sin t& \cos t\end{bmatrix} \begin{bmatrix}x_1(0)\\x_2(0)\end{bmatrix}\\
      \begin{bmatrix}p_1(t)\\p_2(t)\end{bmatrix} &= \begin{bmatrix}\cos t & -\sin t \\ \sin t & \cos t\end{bmatrix} \begin{bmatrix}p_1(0) \\ p_2(0)\end{bmatrix}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Plugging $J$ Hamilton's equations, we get
        \begin{align*}
          \frac{dx_1}{dt} &= \frac{\partial J}{\partial p_1} = -x_2\\
          \frac{dp_1}{dt} &= -\frac{\partial J}{\partial x_1} = -p_2\\
          \frac{dx_2}{dt} &= \frac{\partial J}{\partial p_2} = x_1\\
          \frac{dp_2}{dt} &= -\frac{\partial J}{\partial x_2} = p_1.
        \end{align*}
  \end{proof}
  It's important to note that the parameter $t$ in the Hamiltonian flow for $J$ is the rotation, not time. That is, $J$ is the Hamiltonian generator of rotations.\newline

  If $f$ is any smooth function, it is the case that the time derivative of any other function $g$ along the Hamiltonian flow generated by $f$ is $\frac{dg}{dt} = \set{g,f}$. In particular, the derivative of $H$ along the flow generated by $f$ is $\set{H,f}$, meaning that $f$ is constant along the flow generated by $H$ if and only if $\set{f,H} = 0$, which is true if and only if $H$ is constant along the flow generated by $H$.\newline

  Thus, we find that $f$ is conserved for solutions of Hamilton's equations if and only if $H$ is invariant under the Hamiltonian flow generated by $f$. Of particular note, we find that $J$ is conserved if and only if $H$ is invariant under rotations of $\mathbf{x}$ and $\mathbf{p}$.\footnote{There is another section on Kepler's Laws in the chapter on Classical Mechanics that I didn't really read in depth. I might include it in the future.}
  %\subsection{Kepler's Problem}%
  %Consider an orbit, where the sun with mass $M$ exerts a force $\mathbf{F}$ on a planet with mass $m$. Then, by Newton's universal law of gravitation, the force is found by
  %\begin{align*}
  %  \mathbf{F} &= -GmM \frac{\mathbf{x}}{\norm{\mathbf{x}}^3},
  %\end{align*}
  %with $G$ equal to the gravitational constant. We denote $k = GmM$, and find that in Newton's second law,
  %\begin{align*}
  %  m\mathbf{\ddot{x}} &= -GmM\frac{\mathbf{x}}{\norm{\mathbf{x}}^3}\\
  %  \mathbf{\ddot{x}} &= -GM\frac{\mathbf{x}}{\norm{\mathbf{x}}^3}.
  %\end{align*}
  %The potential associated with $\mathbf{F}$ is
  %\begin{align*}
  %  V(\mathbf{x}) &= -\frac{k}{\norm{\mathbf{x}}}.
  %\end{align*}
  %Since $V$ is invariant under rotations, $\mathbf{J} = \mathbf{x}\times \mathbf{p}$ will always be constant and perpendicular to $\mathbf{x}(t)$. We call the plane perpendicular to $\mathbf{J}$ the plane of motion.
  %\subsection{Runge--Lenz Vector}%
  %We define the Runge--Lenz vector on $\R^{3}\setminus\{0\} \times \R^{3}$ by
  %\begin{align*}
  %  \mathbf{A}(\mathbf{x},\mathbf{p}) &= \frac{1}{mk}\mathbf{p}\times \mathbf{J} - \frac{\mathbf{x}}{\norm{\mathbf{x}}},
  %\end{align*}
  %where $\mathbf{x}$ represents position and $\mathbf{p}$ represents momentum. Recall that $k = GmM$.
  %\begin{proposition}[Runge--Lenz Vector under Orbit]
  %  The Runge--Lenz vector is a conserved quantity for the orbit.
  %\end{proposition}
  %\begin{proof}
  %  \begin{align*}
  %    \mathbf{\dot{A}}(t) &= \frac{1}{mk}\mathbf{F}\times \mathbf{J} - \frac{1}{\norm{\mathbf{x}}}\frac{\mathbf{p}}{m} + \frac{\mathbf{x}}{\norm{\mathbf{x}}^2}\sum_{j=1}^{3}\frac{\partial\norm{\mathbf{x}}}{\partial x_j}\frac{dx_j}{dt}\\
  %                        &= -\frac{1}{m}\norm{\mathbf{x}}^3\mathbf{x}\times \left(\mathbf{x}\times \mathbf{p}\right) - \frac{1}{\norm{\mathbf{x}}}\frac{\mathbf{p}}{m} + \frac{\mathbf{x}}{\norm{\mathbf{x}}^2}\sum_{j=1}^{3}\frac{x_j}{\norm{\mathbf{x}}}\frac{p_j}{m}\\
  %                        &= \frac{1}{m}\left(-\frac{1}{\norm{\mathbf{x}}^3}\mathbf{x}(\mathbf{x}\cdot \mathbf{p}) + \frac{1}{\norm{\mathbf{x}}^3}\mathbf{p}(\mathbf{x}\cdot \mathbf{x}) - \frac{\mathbf{p}}{\norm{\mathbf{x}}} + \frac{(\mathbf{x}\cdot \mathbf{p})}{\norm{\mathbf{x}}^3}\right)\\
  %                        &= 0
  %  \end{align*}
  %\end{proof}
  %\subsubsection{Trajectories for the Kepler Problem}%
  %The magnitude of the Runge--Lenz vector $\mathbf{A}$ is found by
  %\begin{align*}
  %  \norm{\mathbf{A}}^2 &= 1 + \frac{2\norm{\mathbf{J}}^2}{mk^2}E,
  %\end{align*}
  %where $\displaystyle E = \frac{\norm{\mathbf{p}}^2}{2m} - \frac{k}{\norm{\mathbf{x}}}$.\\

  %Additionally, if $\mathbf{\hat{x}} = \frac{\mathbf{x}}{\norm{\mathbf{x}}}$, then
  %\begin{align*}
  %  \mathbf{A}\cdot \mathbf{\hat{x}} &= \frac{\norm{\mathbf{J}}^2}{mk\norm{\mathbf{x}}}-1
  %\end{align*}
  %for all nonzero $\mathbf{x}$. Thus,
  %\begin{align*}
  %  \norm{\mathbf{x}} &= \frac{\norm{\mathbf{J}}^2}{mk(1+\mathbf{A}\cdot \mathbf{\hat{x}})}.
  %\end{align*}
  \section{Introduction to Quantum Mechanics}%
  Observable quantities such as position and momentum in quantum mechanics are represented by operators on a complex-valued Hilbert space (an inner product space that is complete with respect to the induced metric) --- specifically, these quantities are \textit{unbounded} linear operators.\newline

  In physics, the inner product is linear in the second factor and conjugate linear in the first factor:\footnote{Notice that this is different with math, where the inner product is linear in the first factor and conjugate linear in the second factor.} 
  \begin{align*}
    \iprod{\phi}{\lambda \psi} &= \lambda\iprod{\phi}{\psi}\\
    \iprod{\lambda\phi}{\psi} &= \overline{\lambda}\iprod{\phi}{\psi}.
  \end{align*}
  \subsection{A Taste of Operator Theory}%
  A linear operator $A: \mathbf{H} \rightarrow \mathbf{H}$ is bounded if it has finite operator norm:\footnote{I'm using more operator-theoretic language than the book uses because I'm \xcancel{pretentious} a mathematician, not a physicist.}
  \begin{align*}
    \sup_{\norm{\psi} \leq 1}\norm{A\psi} < \infty.
  \end{align*}
  For each bounded operator $A$, there exists a unique bounded operator $A^{\ast}$ such that $\iprod{\phi}{A\psi} = \iprod{A^{\ast}\phi}{\psi}$. The existence of $A^{\ast}$ follows from the Riesz representation theorem.\newline

  A bounded operator is said to be self-adjoint if $A^{\ast} = A$. Self-adjoint operators are nice for a variety of reasons, and as a result we desire for our operators in quantum mechanics to be self-adjoint. However, this brings a significant problem --- unbounded self-adjoint operators are not necessarily defined on $\mathbf{H}$.\newline

  We define unbounded operators as linear operators defined on a dense subspace of $\mathbf{H}$:
  \begin{align*}
    A: \text{Dom}(A) \subseteq \mathbf{H} &\rightarrow \mathbf{H}\newline
    \intertext{subject to}
    \overline{\text{Dom}(A)} &= \mathbf{H}.
  \end{align*}
  In addition to the domain of $A$ not necessarily being equal to $\mathbf{H}$, the linear functional $\iprod{\phi}{A\cdot}$ is not necessarily bounded (meaning we cannot use the Riesz representation theorem to find $A^{\ast}\phi$). The adjoint of $A$, as a result, will be defined on a subspace of $\mathbf{H}$.\newline

  A vector $\phi \in \mathbf{H}$ is said to belong to the domain $\text{Dom}(A^{\ast})$ if the linear functional $\iprod{\phi}{A\cdot}$ on $\text{Dom}(A)$ is bounded. Then, we define $A^{\ast}$ to be the unique vector $\chi$ such that $\iprod{\chi}{\psi} = \iprod{\phi}{A\psi}$ for all $\psi \in \text{Dom}(A)$.\newline

  Having defined adjoints of an unbounded operator, we can now commit to defining self-adjoint operators. The operator $A$ is symmetric if $\iprod{\phi}{A\psi} = \iprod{A\phi}{\psi}$ --- a symmetric operator is self-adjoint if $\text{Dom}(A) = \text{Dom}(A^{\ast})$ and $A^{\ast}\phi = A\phi$ for all $\phi \in \text{Dom}(A)$. Finally, $A$ is essentially self-adjoint if the closure of the graph of $A$ in $\mathbf{H}\times \mathbf{H}$ is self-adjoint.\newline

  In sum, $A$ is self-adjoint if $A$ and $A^{\ast}$ are the same operator with the same domain, more or less.
  \begin{definition}[Properties of Symmetric Operators]
    Let $A$ be a symmetric operator on $\mathbf{H}$. Then, the following hold:
    \begin{enumerate}[(1)]
      \item For all $\psi\in \text{Dom}(A)$, the quantity $\iprod{\psi}{A\psi}$ is real. More generally, if $\psi, A\psi, \dots, A^{m-1}\psi$ belong to $\text{Dom}(A)$, then $\iprod{\psi}{A^{m}\psi}$ is real.
      \item Suppose $\lambda$ is an eigenvector for $A$. Then, $\lambda \in \R$.
    \end{enumerate}
  \end{definition}
  \begin{proof}
    \begin{enumerate}[(1)]
      \item Since $A$ is symmetric,
        \begin{align*}
          \iprod{\psi}{A\psi} &= \iprod{A\psi}{\psi}\\
                              &= \overline{\iprod{\psi}{A\psi}},
        \end{align*}
        for all $\psi \in \text{Dom}(A)$. Similarly, if $\psi,A\psi,\dots,A^{m-1}\psi\in \text{Dom}(A)$, then we use the symmetry of $A$ to show that
        \begin{align*}
          \iprod{\psi}{A^{m}\psi} &= \iprod{A^{m}\psi}{\psi}\\
                                  &= \overline{\iprod{\psi}{A^{m}\psi}}.
        \end{align*}
      \item If $\psi$ is an eigenvector for $A$ with eigenvalue $\lambda$, then
        \begin{align*}
          \lambda \iprod{\psi}{\psi} &= \iprod{\psi}{A\psi}\\
                                     &= \iprod{A\psi}{\psi}\\
                                     &= \overline{\lambda}\iprod{\psi}{\psi}.
        \end{align*}
        Since $\psi$ is nonzero by definition, it must be the case that $\lambda = \overline{\lambda}$.
    \end{enumerate}
  \end{proof}
  In physical terms, $\iprod{\psi}{A\psi}$ represents the expected value for measurements of $A$ in the state $\psi$, with $\lambda$ representing a possible value of this measurement. This is why we want both numbers to be real.\newline

  A self-adjoint $A$ allows us to use the spectral theorem to assign each $\psi \in \mathbf{H}$ a probability measure on the real numbers.
  \subsection{Position and Momentum Operators}%
  Consider a particle moving along the real line with wave function $\psi: \R\rightarrow \C$. Although $\psi$ will evolve over time, let the particle be fixed in time for now.\newline

  We want to define $\psi$ to be a unit vector in $L^{2}(\R)$, meaning
  \begin{align*}
    \int_{\R}\left\vert \psi(x) \right\vert^2dx &= 1.
  \end{align*}
  The probability that the position of the particle belongs to some $E\subseteq \R$ is
  \begin{align*}
    \int_{E}\left\vert \psi(x) \right\vert^2dx,
  \end{align*}
  where $E$ is necessarily a Lebesgue-measurable set.\newline

  The expectation value of the position is thus
  \begin{align*}
    E(x) &= \int_{\R}x\left\vert \psi(x) \right\vert^2dx,
  \end{align*}
  assuming the convergence of the integral, and the $m$th moment of the position is calculated as
  \begin{align*}
    E(x^m) &= \int_{\R}x^m\left\vert \psi(x) \right\vert^2dx,
  \end{align*}
  again assuming convergence of the integral.
  \begin{definition}[Position Operator]
    The position operator is defined as $X = M_{x}$, meaning $(X\psi)(x) = x\psi(x)$. With this in mind, we can then see that
    \begin{align*}
      E(x) &= \iprod{\psi}{X\psi}
    \end{align*}
    under the standard inner product on $L^{2}(\R)$. The expectation value of $X$ for the state $\psi$ is denoted $\langle X \rangle_{\psi} := \iprod{\psi}{X\psi}$.\footnote{I don't like this notation either.}\newline

    The higher moments of position are similarly defined:
    \begin{align*}
      E(x^m) &= \iprod{\psi}{X^{m}\psi},
    \end{align*}
    where $X^{m}$ denotes $m$-degree composition of $X$.\newline

    Since $X$ is an unbounded linear operator, it is not necessarily the case that $X\psi \in L^{2}(\R)$ if $\psi\in L^{2}(\R)$.\footnote{Other famous examples of unbounded linear operators on Hilbert spaces include the derivative operator on $A^{2}(\mathbb{D})$, the space of holomorphic functions on the complex unit disc. I'm doing research on properties of variations of this space.}
  \end{definition}
  Momentum is encoded in the oscillations of the wave function --- the de Broglie hypothesis provides a special relationship between the frequency of oscillation (as a function of position at a fixed time) and the momentum.
  \begin{proposition}[De Broglie Hypothesis]
    If the wave function of a particle has spatial frequency $k$, then the momentum $p$ of the particle is
    \begin{align*}
      p &= \hbar k,
    \end{align*}
    where $\hbar$ denotes Planck's constant.
  \end{proposition}
  To be more precise, the de Broglie hypothesis applies to wave functions of the form $\psi(x) = e^{ikx}$, which represents particles that have momentum $p = \hbar k$. Let's develop this a bit further.\newline

  Since $e^{ikx}$ is not square integrable over $\R$, we instead move to the circle, where $\psi$ has period $2\pi$ over $\R$, and
  \begin{align*}
    \int_{0}^{2\pi}\left\vert \psi(x) \right\vert^2 &= 1.
  \end{align*}
  For any integer $k$, the normalized wave function $\frac{e^{ikx}}{\sqrt{2\pi}}$ will represent the particle with momentum $p = \hbar k$. This momentum value is definite; that is, $p = \hbar k$ with probability $1$ for a particle with wave function $\frac{e^{ikx}}{\sqrt{2\pi}}$.\newline

  Of note, the functions $\set{\frac{e^{ikx}}{\sqrt{2\pi}}}$ for $k\in \Z$ form an orthonormal basis for the Hilbert space of square integrable functions with period $2\pi$.\newline

  The wave functions for particles on a circle are thus all of the form
  \begin{align*}
    \psi(x) &= \sum_{k=-\infty}^{\infty} a_k\frac{e^{ikx}}{\sqrt{2\pi}},
  \end{align*}
  where the sum is convergent in $L^{2}([0,2\pi])$.\footnote{You may recognize these as Fourier series.} If $\psi$ is a unit vector, then\footnote{We use Parseval's identity to relate the $L^{2}$ norm of $\psi$ to the $\ell^{2}$ norm of $\set{a_k}_{k\in \Z}$. Try proving it yourself! Hint: use the Pythagorean theorem.}
  \begin{align*}
    \norm{\psi}_{L^{2}([0,2\pi])}^2 &= \sum_{k=-\infty}^{\infty}|a_k|^2\\
                                    &= 1.
  \end{align*}
  For a particle with wave function $\psi$ expressed as a Fourier series, the momentum isn't definite. We will have to consider that measurement will yield one of the values of $\hbar k$ with probability $|a_k|^2$.
  \begin{align*}
    E(p) &= \sum_{k=-\infty}^{\infty}\hbar k |a_k|^2,
  \end{align*}
  with higher moments defined by
  \begin{align*}
    E(p^m) &= \sum_{k=-\infty}^{\infty}(\hbar k)^{m}|a_k|^2,
  \end{align*}
  assuming absolute convergence.
  \begin{definition}[Momentum Operator]
    To encode $P$, our momentum operator, such that for the wave function $\psi\in L^{2}([0,2\pi])$, $E(p^m) = \iprod{\psi}{P^{m}\psi}$, we must have $P$ satisfying
    \begin{align*}
      Pe^{ikx} &= \hbar k e^{ikx}.
    \end{align*}
    Thus, we would assume that
    \begin{align*}
      P &= -i\hbar \frac{d}{dx}.
    \end{align*}
    Even on the real line, we would still expect $P = -i\hbar \frac{d}{dx}$; though $e^{ikx}$ is not square integrable, we can represent any wave function $\psi \in L^{2}(\R)$ as an integral with the Fourier transform:
    \begin{align*}
      \psi(x) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{ikx}\hat{\psi}(k)dk,\\
      \hat(\psi)(x) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-ikx}\psi(x)dx.
    \end{align*}
    Plancherel's theorem\footnote{also known as Parseval's theorem when applied to Fourier series rather than the Fourier transform} states that the Fourier transform is unitary --- that is,
    \begin{align*}
      \int_{-\infty}^{\infty}|\psi(x)|^2dx = \int_{-\infty}^{\infty}\left\vert \hat{\psi}(k) \right\vert^2dk &= 1.
    \end{align*}
    We can imagine $\hat{\psi}(k)$ as the probability density for the momentum of the particle\footnote{well, for $p/\hbar$, but that's basically the same}.\newline

    Thus, we have defined the momentum operator as
    \begin{align*}
      P &= -i\hbar \frac{d}{dx},
    \end{align*}
    with, for sufficiently nice $\psi\in L^{2}(\R)$
    \begin{align*}
      E(p^{m}) &= \iprod{\psi}{P^{m}\psi}\\
               &= \int_{-\infty}^{\infty}(\hbar k)^m \left\vert \hat{\psi}(k) \right\vert^2 dk
    \end{align*}
    for all positive integers $m$.
  \end{definition}
  \begin{proposition}[Commutator of Position and Momentum]
    \begin{align*}
      [X,P] &= i\hbar I,
    \end{align*}
    a relation known as the canonical commutation relation.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      PX\psi &= -i\hbar \frac{d}{dx}\left(x\psi(x)\right)\\
             &= -i\hbar\psi(x) - i\hbar x \frac{d\psi}{dx}\\
             &= -i\hbar I\psi + XP\psi
    \end{align*}
  \end{proof}
  \begin{remark}
    Note the parallel between $\set{x,p} = 1$ in the Poisson bracket and $[X,P] = i\hbar I$ in the commutator.
  \end{remark}
  \begin{proposition}[Symmetry of Position and Momentum Operators]
    For all sufficiently nice $\phi$ and $\psi$ in $L^{2}(\R)$,
    \begin{align*}
      \iprod{\phi}{X\psi} &= \iprod{X\phi}{\psi}\\
      \iprod{\phi}{P\psi} &= \iprod{P\phi}{\psi}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Let $\phi,\psi\in L^{2}(\R)$ with $x\psi(x),x\phi(x)\in L^{2}(\R)$. Then, since $x\in \R$,
    \begin{align*}
      \int_{-\infty}^{\infty}\overline{\phi(x)}x\psi(x)dx &= \int_{-\infty}^{\infty}\overline{x\phi(x)}\psi(x)dx,
    \end{align*}
    with both integrals convergent.\newline

    Meanwhile, assume $\phi$ and $\psi$ are continuously differentiable, vanish at $\pm \infty$, and $\phi,\psi,\frac{d\phi}{dx},\frac{d\psi}{dx} \in L^{2}(\R)$. Note that $\frac{d\overline{\phi}}{dx} = \overline{\frac{d\phi}{dx}}$. Then, integrating by parts,
    \begin{align*}
      -i\hbar \int_{-n}^{n}\overline{\phi(x)}\frac{d\psi}{dx}dx &= -i\hbar\overline{\phi(x)}\psi(x)\biggr\vert_{-n}^{n} + i\hbar \int_{-n}^{n}\overline{\frac{d\phi}{dx}}\psi(x)dx,
      \intertext{meaning}
      \int_{-\infty}^{\infty}\overline{\phi(x)}\left(-i\hbar \frac{d\psi}{dx}\right) dx &= i\hbar\int_{-\infty}^{\infty}\overline{\frac{d\phi}{dx}}\psi(x)dx\\
                                                                                        &= \int_{-\infty}^{\infty} \overline{\left(-i\hbar\frac{d\phi}{dx}\right)}\psi(x)dx.
    \end{align*}
  \end{proof}
  Thus, we have shown that $X$ and $P$ are symmetric operators on certain dense subspaces of $L^{2}(\R)$. We will have to wait until later to prove that $X$ and $P$ are essentially self-adjoint.
  \subsection{Kinematic Axioms of Quantum Mechanics}%
  These aren't really axioms\footnote{Physicists once again caught stealing the valor of mathematicians.} (as in, first principles from which quantum mechanics is derived), but they're essential principles of quantum mechanics. For ease of use, I'm labeling them with my personal interpretation of their content (rather than the numbering system used in the book).
  \begin{axiom}[Principle of Representation]
    The state of a quantum mechanical system is represented by $\psi \in \mathbf{H}$ for some Hilbert space $\mathbf{H}$. If $\psi_1$ and $\psi_2$ are two unit vectors in $\mathbf{H}$ with $\psi_1 = c\psi_2$ for some $c\in \C$, $\psi_1$ and $\psi_2$ represent the same state.
  \end{axiom}
  Each $\psi$ represents a pure state --- there are mixed states, but those will be discussed later.
  \begin{axiom}[Principle of Correspondence]
    To each real-valued $f$ on the classical phase space, there is an unbounded, self-adjoint operator $\hat{f}$ on the Hilbert space.
  \end{axiom}
  For a particle in $\R^{1}$, the phase space in $\R^{2}$ is represented by $(x,p)$ for $x$ position and $p$ momentum; the analogue to the classical phase space in quantum mechanics is $L^{2}(\R)$ with the position function $f(x,p) = x$ being $M_x$ (the position operator), and the momentum function $g(x,p) = p$ being $P = -i\hbar\frac{d}{dx}$ (the momentum operator).
  \begin{axiom}[Principle of Measurement]
    If a system is in a state represented by $\psi \in \mathbf{H}$, then the measurement for the $m$th moment of $f$ satisfies
    \begin{align*}
      E(f^{m}) &= \iprod{\psi}{\left(\hat{f}\right)^m\psi}.
    \end{align*}
    In particular, the measurement for $f$ is
    \begin{align*}
      E(f) &= \iprod{\psi}{\hat{f}\psi}.
    \end{align*}
  \end{axiom}
  \begin{remark}
    Note that in the quantum system, we are measuring the \textit{classical} $f$. However, rather than a definitive value, we need to find the expectation of the operator $\hat{f}$ using the probabilities derived from $\psi$.\newline

    Since $\hat{f}$ is self-adjoint, $E(f^{m})$ is real, we can construct a probability measure $\mu_{A,\psi}$ that is the probability distribution for measurements of $A$ in the state $\psi$.
  \end{remark}
  \begin{proposition}[Eigenvectors]
    If a quantum system is in a state described by $\psi\in \mathbf{H}$, and $\hat{f}\psi = \lambda\psi$ for some $\lambda\in \R$\footnote{Recall that the eigenvalues of self-adjoint operators are real.}, then
    \begin{align*}
      E(f^{m}) &= \lambda^{m}.
    \end{align*}
    The aforementioned probability measure consistent with this condition is $\delta_{\lambda}$ (where $f$ has value $\lambda$ with probability $1$).
  \end{proposition}
  Essentially, if $\psi$ is an eigenvector for $\hat{f}$, then measurements for $f$ are determined. Thus, we would need to find the probability measure such that
  \begin{align*}
    \int_{\R}x^{m}d\mu &= \lambda^{m},
  \end{align*}
  which only works if $\mu = \delta_{\lambda}$.\\

  Note that if $\psi$ is a linear combination of eigenvectors for $\hat{f}$, then the measurements of $f$ are not deterministic.\footnote{The spectral decomposition is very cool.}
  \begin{example}
    Suppose $\hat{f}$ has an orthonormal basis $\set{e_j}$ with distinct real eigenvalues $\lambda_j$. Let $\psi$ be a unit vector in $\mathbf{H}$ with the expansion
    \begin{align*}
      \psi &= \sum_{j=1}^{\infty}a_je_j.
    \end{align*}
    Then, the measurement in the state $\psi$ of $f$ will necessarily be some value of $\lambda_j$ with probability $|a_j|^2$.
  \end{example}
  \begin{axiom}[Principle of Wave Function Collapse]
    Suppose a quantum system begins in a state $\psi$, and a measurement of the observable $f$ is performed. If the measurement is $\lambda \in \R$, then immediately after the measurement, the system will be in a state $\psi'$ such that
    \begin{align*}
      \hat{f}\psi' &= \lambda \psi',
    \end{align*}
    where $\hat{f}$ is the self-adjoint operator representation of $f$.
  \end{axiom}
  \begin{remark}
    Since $\psi'$ is an eigenvector of $\hat{f}$, this means that a second measurement of $f$ occurring immediately after the first measurement will yield the value $\lambda$ with probability $1$ (see the above proposition about eigenvectors).
  \end{remark}
  The wave function collapse principle only applies to measurements very shortly after the initial measurement; the system will still evolve over time.
  \subsubsection{Hydrogen Atom Measurements and Uncertainty}%
  The Hamiltonian operator $\hat{H}$ for the hydrogen atom has eigenvalues of the form
  \begin{align*}
    -\frac{R}{n^2},
  \end{align*}
  where $R$ is the Rydberg constant and $n\in \N$ denotes a state. The negative value of these eigenvalues is important; they denote that the electron is bound to the nucleus.\newline

  If an electron is placed into a state with energy $-\frac{R}{n_1^2}$ (where $n_1 > 1$), it will eventually decay into a state with lower energy, $-\frac{R}{n_2^2}$, with $n_2 < n_1$. The process of decay releases a photon with energy
  \begin{align*}
    E_{\text{photon}} &= \frac{R}{n_2^2} - \frac{R}{n_1^2}.
  \end{align*}
  The frequency of a photon is proportional to its energy; by observing the frequency of the photon, we can determine the change in energy (and thus the values of $n_1$ and $n_2$).\newline

  A bound state for the hydrogen atom will be a linear combination of the eigenvectors of $\hat{H}$ with eigenvalues of the aforementioned form.\newline

  Just as the standard deviation of a random variable $Y$ is found by $\sigma^2 = E(Y^2) - \left(E(Y)\right)^2$, we can find the \textit{uncertainty} of a self-adjoint operator $A$, defined as
  \begin{align*}
    \left(\Delta_{\psi}A\right)^2 &= \left\langle A^2\right\rangle_{\psi} - \left(\left\langle A\right\rangle_{\psi}\right)^2,
  \end{align*}
  recalling that $\langle A \rangle_{\psi} = \langle \psi,A\psi\rangle$.\newline

  For a single $A$, we can find $\psi$ such that $\Delta_{\psi}A < \varepsilon$ for any $\varepsilon > 0$; however, if $A$ and $B$ do not commute, then $\Delta_{\psi}A$ and $\Delta_{\psi}B$ cannot both be made arbitrarily small.\newline

  In particular, we know that $X$ and $P$ do not commute; thus yields the famous Heisenberg Uncertainty Principle:
  \begin{align*}
    \left(\Delta_{\psi}X\right)\left(\Delta_{\psi}P\right) \geq \frac{\hbar}{2}
  \end{align*}
  for all $\psi$ such that $\Delta_{\psi}X$ and $\Delta_{\psi}P$ are defined.
  \subsection{The Schrödinger Equation}%
  In the previous section, we considered the wave function $\psi$ at a fixed time; however, time is not stationary\footnote{Citation needed} and neither ought our system be, meaning we need a way to evolve the quantum system over time.\newline

  In a classical system, the Hamiltonian is the generator of time evolution\footnote{The Hamiltonian is also equal to the total energy of the system.}--- thus, by the Principle of Representation, there is an operator, $\hat{H}$, which is the Hamiltonian operator for the system.\newline

  We motivated the definition of the momentum operator via the de Broglie hypothesis, which stated that $p = \hbar k$, where $k$ is the spatial frequency of the wave function. Similarly, we can look at the relation between energy and the temporal frequency of the wave function,
  \begin{align*}
    E = \hbar \omega,
  \end{align*}
  to motivate time evolution.\newline

  Suppose that $\psi_0$ has energy $E$ (so $\psi_0$ is an eigenvector for $\hat{H}$). Then, the wave function's time dependence should be solely based on the frequency $\omega = E/\hbar$, meaning that if the $t=0$ state of the system is $\psi_0$, the state of the system at any other time $t$ should be
  \begin{align*}
    \psi(t) &= \psi_0 e^{-i\omega t}\\
            &= \psi_0 e^{-it\frac{E}{\hbar}}.
  \end{align*}
  This $\psi(t)$ is a function to the differential equation
  \begin{align*}
    \frac{d\psi}{dt} &= -\frac{iE}{\hbar}\psi\\
                     &= \frac{E}{i\hbar}\psi
  \end{align*}
  with initial value $\psi_0$. Finally, rewriting $\hat{H} = E$, we get the Schrödinger equation.
  \begin{axiom}[Schrödinger Equation]
    The time evolution of the wave function $\psi$ in a quantum system is generated by
    \begin{align*}
      \frac{d\psi}{dt} &= \frac{1}{i\hbar}\hat{H}\psi,
    \end{align*}
    where $\hat{H}$ denotes the Hamiltonian operator.
  \end{axiom}
  \begin{proposition}[Schrödinger Equation for Operators]
    Let $A$ be a self-adjoint operator on $\mathbf{H}$. Assuming particular domain conditions hold, then
    \begin{align*}
      \frac{d}{dt}\langle A\rangle_{\psi(t)} &= \left\langle \frac{1}{i\hbar}[A,\hat{H}]\right\rangle_{\psi(t)},
    \end{align*}
    where $\langle A \rangle_{\psi} = \iprod{\psi}{A\psi}$ and $[A,B] = AB-BA$.
  \end{proposition}
  \begin{remark}
    Remember that for a function $f$ acting on the classical phase space, $\frac{df}{dt} = \set{f,H}$ acting along a solution to Hamilton's equations.
  \end{remark}
  \begin{proof}
    Let $\psi(t)$ be a solution to the Schrödinger equation, with $\psi(t) \in \text{Dom}(A) \cap \text{Dom}(\hat{H})$, $A\psi(t) \in \text{Dom}(\hat{H})$, and $H\psi(t) \in \text{Dom}(A)$. Then,
    \begin{align*}
      \frac{d}{dt}\iprod{\psi(t)}{A\psi(t)} &= \iprod{\frac{d\psi}{dt}}{A\psi} + \iprod{\psi}{A\frac{d\psi}{dt}}\\
                                            &= \frac{i}{\hbar}\iprod{\hat{H}\psi}{A\psi} - \frac{i}{\hbar}\iprod{\psi}{A\hat{H}\psi}\\
                                            &= \frac{1}{i\hbar}\iprod{\psi}{[A,\hat{H}]\psi}.
    \end{align*}
  \end{proof}
  \begin{remark}
    If $[A,\hat{H}] = 0$, then nothing interesting happens; thus, for operators to yield time evolution, we need them to \textit{not} commute with $\hat{H}$.\newline

    For a particle moving in $\R$, we see that noncommutativity holds for $X$ and $P$.\newline

    If $[A,\hat{H}] = 0$, then we see that $E(A^{m})$ is independent of time, effectively meaning that $A$ is conserved.
  \end{remark}
  \begin{proposition}[Time Independence of Inner Products of Solutions to the Schrödinger Equation]
    If $\phi(t)$ and $\psi(t)$ are solutions to the Schrödinger equation, then $\iprod{\phi(t)}{\psi(t)}$ and $\norm{\psi(t)}$ are independent of time.
  \end{proposition}
  \begin{proof}
    Using the product rule for inner products, we find
    \begin{align*}
      \frac{d}{dt}\iprod{\phi(t)}{\psi(t)} &= \iprod{\frac{1}{i\hbar}\hat{H}\phi(t)}{\psi(t)} + \iprod{\phi(t)}{\frac{1}{i\hbar}\hat{H}\psi(t)}\\
                                           &= -\frac{1}{i\hbar}\iprod{\hat{H}\phi(t)}{\psi(t)} + \frac{1}{i\hbar}\iprod{\phi(t)}{\hat{H}\psi(t)}\\
                                           &= 0.
    \end{align*}
  \end{proof}
  \subsection{Solving the Schrödinger Equation}%
  The Schrödinger equation is an equation of the form
  \begin{align*}
    \frac{dv}{dt} &= Av
  \end{align*}
  for some linear operator $A$ over a Hilbert space. Considering the finite-dimensional Hilbert space $\C^{n}$, $A$ is an $n\times n$ matrix. The aforementioned equation thus has the solution
  \begin{align*}
    v(t) &= v_0e^{At},
  \end{align*}
  where $v_0$ is the initial condition. If $A$ is a diagonalizable matrix, we can calculate $e^{At}$ using the eigenvectors.\footnote{In particular, if $A$ is normal, the eigenvectors are the columns of the unitary diagonalization $UDU^{\ast}$, and form an orthonormal basis.}\newline

  The Schrödinger equation simply replaces $\C^{n}$ with $\mathbf{H}$ and $A$ with $\frac{1}{i\hbar}\hat{H}$.
  \begin{proposition}[Solution to the Schrödinger Equation via Exponentiation]
    If $\hat{H}$ is a self-adjoint operator on $\mathbf{H}$, then
    \begin{align*}
      \psi(t) &= \psi_0 e^{-it\hat{H}/\hbar}
    \end{align*}
    should be a solution to the Schrödinger equation given that $e^{-it\hat{H}/\hbar}$ is reasonably defined.
  \end{proposition}
  \begin{remark}
    Clearly,\footnote{My professor loves this phrase.} $\psi(t)$ is a solution to the Schrödinger equation.
  \end{remark}
  If $\hat{H}$ is a bounded operator, then $e^{-it\hat{H}/\hbar}$ can be defined by a convergent power series. However, this is rarely the case.\\

  If $\hat{H}$ is unbounded, then we will have to use the spectral theorem --- the full spectral theorem will have to wait for a while, but we can examine the case of a point spectrum.\newline

  If $\set{e_j}$ is an orthonormal basis for $\mathbf{H}$ consisting of eigenvectors of $\hat{H}$ with $\hat{H}e_j = \lambda_je_j$, then we define the exponential by requiring
  \begin{align*}
    e_je^{-it\hat{H}/\hbar} &= e_je^{-it\lambda_j/\hbar}.
  \end{align*}
  This construction makes $e^{-it\hat{H}/\hbar}$ a unitary operator,\footnote{Notice that $\left|e^{-itE_j/\hbar}\right| = 1$.} and thus bounded.\newline

  It is not necessarily true that every self-adjoint operator $A$ has an orthonormal basis (even self-adjoint ones); nevertheless, the spectral theorem tells us that there is a decomposition of $\mathbf{H}$ into generalized eigenspaces for $A$.\newline

  Unfortunately, this doesn't mean we have \textit{solved} the Schrödinger equation. All it tells us is that $e^{-it\hat{H}/\hbar}$ is bounded, meaning it's defined for all $\psi_0 \in \mathbf{H}$; however, $\psi_0$ must belong to the domain of $\hat{H}$ (which is not $\mathbf{H}$, but only a dense subset of it) in order to be a solution.\newline

  If $\psi$ is an eigenvector of the Hamiltonian, then the equation
  \begin{align*}
    \hat{H}\psi &= E\psi
  \end{align*}
  for some $E\in \R$ is known as the time-independent Schrödinger equation. When we solve the time-independent Schrödinger equation, we are trying to find nonzero values of $E$ and their corresponding $\psi$ that satisfy the equation.\newline

  If $\psi_0$ is a solution to the time-independent Schrödinger equation, then $\displaystyle \psi(t) = \psi_0 e^{-itE/\hbar}$ is the solution to the time-dependent Schrödinger equation with initial condition $\psi_0$. Since $\psi(t)$ is a constant multiple of $\psi_0$, we say that the solution to the time-independent Schrödinger is a stationary state.
  \subsection{The Schrödinger Equation in $\R$}%
  Consider a particle moving on the real line. The Hamiltonian for such a particle is written in the form of
  \begin{align*}
    H(x,p) &= \frac{p^2}{2m} + V(x),
  \end{align*}
  where $V$ denotes the potential. In the quantum case, we may then consider
  \begin{align*}
    \hat{H} &= \frac{P^2}{2m} + V(X)
  \end{align*}
  to be the Hamiltonian operator. Here, $V(X)$ denotes multiplication by $V(x)$, where $x$ is an input to $\psi$. Thus, we find
  \begin{align*}
    \hat{H}\psi(x) &= -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} + V(x)\psi(x).
  \end{align*}
  An operator of the form above is known as the Schrödinger operator; the Hamiltonian operator refers to the operator that generates time-evolution, regardless of its form. The time-dependent Schrödinger equation thus becomes
  \begin{align*}
    \frac{\partial \psi(x,t)}{\partial t} &= \frac{i\hbar}{2m}\frac{\partial^2\psi(x,t)}{\partial x^2} - \frac{i}{\hbar}V(x)\psi(x,t).
  \end{align*}
  The time-dependent Schrödinger equation is thus a linear partial differential equation.\footnote{Unfortunately I haven't taken PDEs yet, so I can't really explain how to solve this.} For a particle on $\R$, the time-independent Schrödinger equation is a linear ordinary differential equation with nonconstant coefficients.\footnote{Neither have I taken ODEs, but I will be taking it in Fall 2024!}
  \subsection{Expected Position and Momentum for Solutions to the Schrödinger Equation}%
  \begin{proposition}[Time Evolution of Position and Momentum]
    Let $\psi(t)$ be a solution to the time-dependent Schrödinger equation with sufficiently nice $V$ and sufficiently nice $\psi_0 = \psi(0)$. Then,
    \begin{align*}
      \frac{d}{dt}\left\langle X\right\rangle_{\psi(t)} &= \frac{1}{m}\left\langle P\right\rangle_{\psi(t)}\\
      \frac{d}{dt}\left\langle P\right\rangle_{\psi(t)} &= -\left\langle V'(X)\right\rangle_{\psi(t)}.
    \end{align*}
  \end{proposition}
  \begin{remark}
    We require these assumptions to ensure that $\hat{H}$ is a self-adjoint operator and that the domain conditions hold.\newline

    Note that $-\langle V'(X)\rangle_{\psi(t)}\neq -V'\left(\left\langle X\right\rangle_{\psi(t)}\right)$. This solution to the Schrödinger equation is not the quantum analogue to the solutions of Hamilton's equations.
  \end{remark}
  Although expected position and expected momentum do not exactly follow the classical trajectories, if $\psi(x)$ is very closely concentrated about $x = x_0$, then the particle will approximately follow the classical trajectory.
  \subsection{The Heisenberg Approach}%
  The previous approach (known as the Schrödinger approach) towards analyzing the quantum system comes from understanding how the wave functions evolve over time, with the operators remaining stationary. We will now examine an approach where the operators change over time with the states remaining stationary.\newline

  In the Heisenberg approach, every self-adjoint $A$ evolves in time according to the operator differential equation\footnote{I will call this the ``modified Schrödinger equation.''}
  \begin{align*}
    \frac{dA(t)}{dt} &= \frac{1}{i\hbar}[A(t),\hat{H}].
  \end{align*}
  Note that since $\hat{H}$ commutes with itself, it remains constant in time --- this is akin to the classical Hamiltonian remaining constant along a solution to Hamilton's equations.\newline

  The spectral theorem provides us a way to construct a family of unitary operators $e^{-it\hat{H}/\hbar}$ that computes time-evolution of states in the Schrödinger approach; similarly, we can find that the solutions in the operator differential equation are of the form
  \begin{align*}
    A(t) &= Ae^{-it\hat{H}/\hbar}e^{it\hat{H}/\hbar}.
  \end{align*}
  If $\psi$ is the state of the system, then the expectation of $A(t)$ in the state is defined to be $\iprod{\psi}{A(t)\psi}$; then
  \begin{align*}
    \iprod{\psi}{A(t)\psi} &= \iprod{\psi}{e^{it\hat{H}/\hbar}Ae^{-it\hat{H}/\hbar}\psi}\\
                           &= \iprod{e^{-it\hat{H}/\hbar}\psi}{Ae^{-it\hat{H}/\hbar}\psi}\\
                           &= \iprod{\psi(t)}{A\psi(t)},
  \end{align*}
  which is the time-evolved state of the system. Since $\iprod{\psi(t)}{A\psi(t)}$ is the expectation of the value of $A$ in the state $\psi(t)$, it must be the case that the Heisenberg approach and the Schrödinger approach are equivalent in their physics.
  \begin{proposition}[Hamiltonian in the Heisenberg Approach]
    Let $\hat{H} = \frac{P^2}{2m} + V(X)$, where $V$ is a bounded below polynomial. Then, for any $t\in \R$,
    \begin{align*}
      \hat{H} &= \frac{1}{2m}\left(P(t)\right)^2 + V(X(t)).
    \end{align*}
  \end{proposition}
  \begin{remark}
    Notice that the Hamiltonian is independent of time (since $[\hat{H},\hat{H}] = 0$), even though $P$ and $X$ depend on time.
  \end{remark}
  \begin{lemma}[Moments of Solutions]
    Suppose $A$ is a self-adjoint operator on $\mathbf{H}$ and $A(t)$ is a solution to the modified Schrödinger equation with $A(0) = A$. Then, $\left(A(t)\right)^m$ is also a solution to the modified Schrödinger equation. In other words, the time evolution of the $m$th power of $A$ is the same as the $m$th power of the time evolution of $A$.
  \end{lemma}
  \begin{proof}
    \begin{align*}
      e^{it\hat{H}/\hbar}A^{m}e^{-it\hat{H}/\hbar} &= \prod_{k=1}^{m}\left(e^{it\hat{H}/\hbar}Ae^{-it\hat{H}/\hbar}\right)\\
                                                   &= \left(e^{it\hat{H}/\hbar}Ae^{-it\hat{H}/\hbar}\right).
    \end{align*}
  \end{proof}
  \begin{proposition}[Analogue to Hamilton's Equations]
    Suppose $\hat{H} = \frac{\left(P(t)\right)^2}{2m} + V\left(X(t)\right)$. Then,
    \begin{align*}
      \frac{dX}{dt} &= \frac{1}{m}P(t)\\
      \frac{dP}{dt} &= -V'(X(t)).
    \end{align*}
  \end{proposition}
  \begin{remark}
    Notice that it is the functions $X(t)$ and $P(t)$ that satisfy the analogue to Hamilton's equations; the expectation values satisfy
    \begin{align*}
      \frac{d}{dt}\left\langle X(t)\right\rangle_{\psi} &= \frac{1}{m}\left\langle P(t)\right\rangle_{\psi}\\
      \frac{d}{dt}\left\langle P(t)\right\rangle_{\psi} &= -\left\langle V'(X(t))\right\rangle_{\psi}.
    \end{align*}
  \end{remark}
  \subsection{Particle in a Box}%
  We want to solve the time-independent Schrödinger equation for the case of a particle that is confined to move in the interval $x\in [0,L]$. That is, we want to find all the eigenvectors and eigenvalues of $\hat{H}\psi = E\psi$. \newline

  The particle has potential $0$ for $x\in [0,L]$ and very large $C$ outside $[0,L]$. In the classical case, the particle has to have very high energy to escape the box; in the quantum case, if $E$ is an eigenvalue satisfying $\hat{H}\psi = E\psi$, with $E \ll C$, then $\psi$ rapidly decays outside the box. In general, we expect that the solutions to the time-independent Schrödinger equation vanish as we approach $x = 0$ or $x=L$.\newline

  Essentially, we are looking for $\psi \in C^{2}([0,L])$ such that
  \begin{align*}
    -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} &= E\psi(x)
  \end{align*}
  subject to the boundary conditions $\psi(0) = \psi(L) = 0$.\newline

  For $E > 0$, the solution to this differential equation is a linear combination of two complex exponentials (or, since $\hat{H}$ is symmetric, a linear combination of sines and cosines):
  \begin{align*}
    \psi(x) &= a\sin\left(\frac{\sqrt{2mE}}{\hbar}x\right) + b\cos\left(\frac{\sqrt{2mE}}{\hbar}x\right).
  \end{align*}
  Implementing the boundary condition $\psi(0) = 0$ yields $b = 0$; imposing $\psi(L) = 0$ \textit{could} give us $a = 0$, but that would be boring. Instead, we want to find solutions such that
  \begin{align*}
    \sin \left(\frac{\sqrt{2mE}}{\hbar}L\right) = 0
  \end{align*}
  that are not identically zero. This forces $E$ to be of the form
  \begin{align*}
    E_j &= \frac{j^2\pi^2\hbar^2}{2mL^2},
  \end{align*}
  with $j \in \N$. It is easy to verify that for $E \leq 0$, the only solutions are where $\psi = \mathbb{0}$.
  \begin{proposition}[Eigenvectors of the Solution]
    The following eigenvectors are a solution to the Schrödinger of the particle in a box satisfying $\psi(0) = \psi(L) = 0$:
    \begin{align*}
      \psi_j &= \sqrt{\frac{2}{L}}\sin\left(\frac{j\pi x}{L}\right).
    \end{align*}
    The $\psi_j$ correspond to each value of $E_j$, and form an orthonormal basis for $L^{2}([0,L])$.
  \end{proposition}
  \begin{proof}
    It has already been verified that the $\psi_j$ are eigenvectors with eigenvalue $E_j$. To verify that the $\psi_j$ are orthonormal, we see that
    \begin{align*}
      \frac{2}{L}\int_{0}^{L}\sin^2\left(\frac{j\pi x}{L}\right)dx &= \frac{2}{L}\left(\frac{L}{2}\right)\\
                                                                   &= 1,
    \end{align*}
    and
    \begin{align*}
      \frac{2}{L}\int_{0}^{L}\sin\left(\frac{j_1\pi x}{L}\right) \sin\left(\frac{j_2\pi x}{L}\right) dx &= 0.
    \end{align*}
    To verify that it is a basis, we use the fact that the $\psi_j$ are a Fourier sine series for $L^{2}([0,L])$.
  \end{proof}
  The Hamiltonian operator for the particle in a box with $V = 0$ is
  \begin{align*}
    \hat{H}\psi &= -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}.
  \end{align*}
  This operator isn't defined over $L^{2}([0,L])$, but only a dense subspace. The domain of $\hat{H}$ should be chosen such that $\hat{H}$ is essentially self-adjoint, meaning
  \begin{align*}
    \iprod{\phi}{\hat{H}\psi} &= \iprod{\hat{H}\phi}{\psi}
  \end{align*}
  for $\phi,\psi \in \text{Dom}(\hat{H})$. For this to hold, $\phi$ and $\psi$ must satisfy the boundary conditions necessary for the boundary terms in integration by parts to be $0$.
  \subsection{$n$-Dimensional Single-Particle Quantum Mechanics}%
  It is relatively straightforward to generalize from a quantum particle moving in $\R$ to one moving in $\R^n$. The Hilbert space is $L^{2}(\R^n)$, and instead of one position operator, there are $n$:
  \begin{align*}
    X_j\psi(\mathbf{x}) &= x_j\psi(\mathbf{x}).
  \end{align*}
  Similarly, there are $n$ momentum operators given by
  \begin{align*}
    P_j\psi(\mathbf{x}) &= -i\hbar \frac{\partial \psi}{\partial x_j}.
  \end{align*}
  As with $\R$, $[X_j,P_j] = i\hbar I$, but it is also the case that $[X_j,X_k] = 0$ and $[P_j,P_k] = 0$.
  \begin{proposition}[Canonical Commutation Relations in $n$ Dimensions]
    The position and momentum operators satisfy
    \begin{align*}
      \frac{1}{i\hbar}[X_j,X_k] &= 0\\
      \frac{1}{i\hbar}[P_j,P_k] &= 0\\
      \frac{1}{i\hbar}[X_j,P_k] &+ \delta_{jk}I
    \end{align*}
    for all $1 \leq j,k \leq n$.
  \end{proposition}
  These are the counterparts to the classical Poisson bracket, where $\frac{1}{i\hbar}[\cdot,\cdot]$ plays the same role as the Poisson bracket.\newline

  The Hamiltonian operator in $n$ dimensions is defined analogously to the classical Hamiltonian:
  \begin{align*}
    \hat{H} &= \sum_{j=1}^{n}\frac{P_j^2}{2m} + V(\mathbf{X}),
  \end{align*}
  where $V(\mathbf{X})$ results from applying $V$ to the family $\mathbf{X} = (X_1,\dots,X_n)$; we may also identify $V(\mathbf{X})$ with the operator of multiplication by $V(\mathbf{x})$, in which case we can write the Hamiltonian operator as
  \begin{align*}
    \hat{H}\psi(\mathbf{x}) &= -\frac{\hbar}{2m}\Delta \psi(\mathbf{x}) + V(\mathbf{x})\psi(\mathbf{x}),
  \end{align*}
  where $\delta = \sum_{j=1}^{n}\frac{\partial^2}{\partial x_j^2}$.\newline

  Now that we are in multiple dimensions, we can now introduce the angular momentum operator, $\hat{J}_{jk}$:
  \begin{align*}
    \hat{J}_{jk} &= X_jP_k - X_kP_j.
  \end{align*}
  As in the classical case, $\hat{J}_{jk} = 0$ when $j=k$; when $j\neq k$, $X_j$ and $P_k$ commute, meaning the order of the factors in $\hat{J}_{jk}$ is not important. In particular,
  \begin{align*}
    \hat{J}_{jk} &+ -i\hbar \left(x_j\frac{\partial}{\partial x_k} - x_k\frac{\partial}{\partial x_j}\right).
  \end{align*}
  The operator $\displaystyle \left(x_j\frac{\partial}{\partial k} - x_k\frac{\partial}{\partial j}\right)$ as the angular derivative, $\frac{\partial}{\partial \theta}$ in the $(x_j,x_k)$ plane.\newline

  When $n=3$, we use the quantum version of the angular momentum vector;
  \begin{align*}
    \hat{J}_1 &= X_2P_3 - X_3P_2\\
    \hat{J}_2 &= X_3P_1 - X_1P_3\\
    \hat{J}_3 &= X_1P_2 - X_2P_1.
  \end{align*}
  When $n=3$, every $\hat{J}_{jk}$ is either $\hat{J}_1,\hat{J}_2,\hat{J}_3$ or their negative.
  \subsection{Systems of Multiple Particles}%
  Suppose we have a system of $N$ particles moving in $\R^n$. If the particles are of different types, then the Hilbert space is $L^2(R^{nN})$. The wave function $\psi$ is a function of $\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N$, with $\mathbf{x}^j$ a vector in $\R^n$.\newline

  If $\psi$ is a unit vector in $L^{2}(\R^{nN})$, then $|\psi(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N)|^2$ is the joint probability distribution of the positions of the $N$ particles.\newline

  We introduce position operators $X_{k}^{j}$ to denote the $k$th component of the position of the $j$th particle, and similarly so with momentum operators $P_{k}^{j}$. The Hamiltonian operator is then
  \begin{align*}
    \hat{H}\psi\left(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N\right) &= \sum_{j=1}^{N}\frac{\hbar}{2m_j}\Delta_{j}\psi(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N) + V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N)\psi(\mathbf{x}),
  \end{align*}
  where $m_j$ denotes the mass of the $j$th particle, and $\Delta_{j}$ denotes the Laplacian with respect to $\mathbf{x}^j$, fixing all other variables.\newline

  The Hilbert space for a composite system is taken to be the tensor product of the individual Hilbert spaces; there is a natural isomorphism\footnote{Category theory term} between $L^{2}(\R^{nN})$ and the $N$ copies of $\R^n$.\newline

  If the particles are identical, then things get more complicated --- to start, we are supposed to believe that identical particles are indistinguishable, meaning if we exchange $\mathbf{x}^1$ and $\mathbf{x}^2$ with each other, then they represent the same state. Thus, we get that
  \begin{align*}
    \psi\left(\mathbf{x}^2,\mathbf{x}^1,\mathbf{x}^3,\dots,\mathbf{x}^N\right) &= u\psi \left(\mathbf{x}^1,\mathbf{x}^2,\mathbf{x}^3,\dots,\mathbf{x}^N\right)
  \end{align*}
  for some complex number $u$ with $|u| = 1$.\footnote{We used the fact that $\psi_1 = c\psi_2$ means $\psi_1$ and $\psi_2$ are equivalent states.} Exchanging again yields $u^2 = 1$, meaning $u = 1$ or $u = -1$.\newline

  Particles with $u = 1$ are known as bosons, and particles with $u = -1$ are known as fermions. The classification of a particle is determined by its spin --- we also say that particles without spin are bosons.\newline

  For a collection of $N$ identical spinless particles in $\R^{3}$, we say that the Hilbert space is the symmetric subspace of $L^{2}\left(\R^{3N}\right)$, or the space of all functions in $L^{2}\left(R^{3N}\right)$ that are invariant under variable permutation.
  \subsection{Dirac Notation}%
  Dirac notation is physicists' preferred way to deal with ideas such as inner products and operators acting on vectors in Hilbert space.
  \begin{definition}[Bras and Kets]
    A vector $\psi\in \mathbf{H}$ is referred to as a ket, denoted $\ket{\psi}$.\newline

    A continuous linear functional in $\mathbf{H}^{\ast}$ is a bra; we use $\bra{\phi}$ to denote the unique linear functional such that $\bra{\phi}(\psi) = \iprod{\phi}{\psi}$. The existence of $\bra{\phi}$ follows from the Riesz representation theorem.\newline

    We denote inner products as $\braket{\phi}{\psi}$.
  \end{definition}
  If $A$ is an operator on $\mathbf{H}$, and $\phi$ is a vector in $\mathbf{H}$, we can form the linear functional $\bra{\phi}A$ to denote the linear map $\psi \mapsto \braket{\phi}{A\psi}$. Physicists like to write this as $\bra{\phi} A \ket{\psi}$, to denote either the linear functional $\bra{\phi}A$ applied to the vector $\ket{\psi}$ or the linear functional $\phi$ applied to the vector $A\ket{\psi}$.
  \begin{definition}[Outer Products]
    For any $\phi$ and $\psi$ in $\mathbf{H}$, the outer product, $\ket{\phi}\bra{\psi}$ denotes the operator given by
    \begin{align*}
      \left(\ket{\phi}\bra{\psi}\right)(\chi) &= \braket{\psi}{\chi} \ket{\phi}.
    \end{align*}
    In math terms,
    \begin{align*}
      \chi \xmapsto{\ket{\phi}\bra{\psi}}\iprod{\psi}{\chi}\phi.
    \end{align*}
  \end{definition}
  Notationally, if a family of vectors is labeled by indices, we write the ket vectors with purely the indices. For instance, if $\set{\phi_{n}}\subseteq \mathbf{H}$, we write $\ket{n}$ rather than $\ket{\phi_n}$.\newline

  If an operator $\hat{H}$ has an orthonormal basis of eigenvectors $\psi_n$, the decomposition would be in the form
  \begin{align*}
    I &= \sum_{n}\ket{n}\bra{n},
  \end{align*}
  where $\ket{n}$ is a unit vector and $\ket{n}\bra{n}$ denotes the orthogonal projection onto the one dimensional subspace spanned by $\ket{n}$.\newline

  Physicists also like to denote the complex conjugate of $z$ as $z^{\ast}$ and the adjoint of an operator as $A^{\dagger}$\footnote{I don't like this either.}; self-adjoint operators are known as Hermitian operators.\newline

  To express the adjoint of an operator using Dirac notation, we say that if $A$ is a bounded operator on $\mathbf{H}$, then $A^{\dagger}$ is the unique bounded operator such that
  \begin{align*}
    \bra{\psi}A &= \bra{A^{\dagger}\psi}.
  \end{align*}
  Physicists tend to define the irreducible canonical commutation relations between certain operators on a Hilbert space in order to define said Hilbert space --- this is because the Stone--von Neumann theorem\footnote{We will return to this in the future.} says that if certain conditions hold, the canonical commutation relations uniquely define the Hilbert space up to unitary equivalence.\newline

  Given this irreducible representation, and a vector $\psi \in \mathbf{H}$, the position wave function is defined by
  \begin{align*}
    \psi(x) &= \braket{x}{\psi}.
  \end{align*}
  We can analogously define the momentum wave function.
  \section{Interlude: Fourier Transform and Schwartz Space}%
  In order to go any further, it's probably pretty useful to understand the basics of the Fourier transform. To do this, the following are notes of some content from the \textit{Princeton Lectures on Analysis}. 
  \subsection{Functions of Moderate Decrease}%
  A function $f$ defined on $\R$ that is integrable over a closed and bounded interval may not be integrable over $\R$. For instance, the function $f = \frac{1}{1+|x|}$ is such a function.\newline

  In order to study functions that \textit{are} integrable over $\R$, we will find it useful to restrict our view to certain classes of functions --- namely, ones that decrease ``fast enough.''
  \begin{definition}[Function of Moderate Decrease]
    A function $f\in C(\R)$ is said to be of moderate decrease if there exists $A > 0$ such that
    \begin{align*}
      |f(x)| \leq \frac{A}{1 + x^{1 + \varepsilon}}
    \end{align*}
    for all $x\in \R$ and some $\varepsilon > 0$. We denote the set of all functions of moderate decrease as $\mathcal{M}(\R)$; $\mathcal{M}(\R)$ is a vector space over $\C$.
  \end{definition}
  If $f$ is of moderate decrease, we can define
  \begin{align*}
    \int_{-\infty}^{\infty}f(x)dx &= \lim_{N\rightarrow\infty}\int_{-N}^{N}f(x)dx.
  \end{align*}
  The integral over $\R$ of $f,g\in \mathcal{M}(\R)$ (with $a,b\in \C$) has the following properties:
  \begin{itemize}
    \item Linearity:
      \begin{align*}
        \int_{-\infty}^{\infty}\left(af(x) + bg(x)\right) dx &= a\int_{-\infty}^{\infty}f(x)dx + b\int_{-\infty}^{\infty}g(x)dx.
      \end{align*}
    \item Translation Invariance:
      \begin{align*}
        \int_{-\infty}^{\infty}f(x-h)dx &= \int_{-\infty}^{\infty}f(x)dx.
      \end{align*}
    \item Scaling under dilation:
      \begin{align*}
        \delta\int_{-\infty}^{\infty}f(\delta x)dx &= \int_{-\infty}^{\infty}f(x)dx
      \end{align*}
    \item Continuity:
      \begin{align*}
        \lim_{h\rightarrow 0}\int_{-\infty}^{\infty}|f(x-h) - f(x)|dx &= 0.
      \end{align*}
  \end{itemize}
  \subsection{Schwartz Space}%
  The Schwartz space is a subspace of $f\in C^{\infty}(\R)$ where, for all $k,\ell \geq 0$,
  \begin{align*}
    \sup_{x\in \R}\left\vert x^{k}f^{(\ell)}(x) \right\vert < \infty.
  \end{align*}
  The space $\mathcal{S}(\R)\subseteq C^{\infty}(\R)$ is also a vector space over $\C$. Notice that if $f\in \mathcal{S}(\R)$, then so too are $f'(x)$ and $xf(x)$.\newline

  Important classes of Schwartz functions include the Gaussians and the bump functions (compactly supported $C^{\infty}(\R)$ functions).\footnote{Bump functions are fascinating because even though they're $C^{\infty}$, they are not analytic (or equivalent to their Taylor series, denoted $C^{\omega}$). Compare this to functions defined over the complex numbers, where $C^{1}(\C) = C^{\infty}(\C) = C^{\omega}(\C)$.}
  \subsection{The Fourier Transform on $\mathcal{S}(\R)$}%
  \begin{definition}[Fourier Transform]
    The Fourier transform of a function $f\in \mathcal{S}(\R)$ is defined by
    \begin{align*}
      \hat{f}(k) &= \int_{-\infty}^{\infty}f(t)e^{-2\pi i k t}dt.
    \end{align*}
    I will use $\mathcal{F}$ to denote the Fourier transform; $\mathcal{F}(f(t)) = \hat{f}(k)$.
  \end{definition}
  \begin{proposition}[Properties of the Fourier Transform]
    Let $f\in \mathcal{S}(\R)$. Then:
    \begin{itemize}
      \item $\displaystyle\mathcal{F}(f(x+h)) = \hat{f}(k)e^{2\pi i h k}$ for $h\in \R$;
      \item $\displaystyle \mathcal{F}\left(f(x)e^{-2\pi i x h}\right) = \hat{f}(k + h)$ for $h\in \R$;
      \item $\displaystyle \mathcal{F}\left(f(\delta x)\right) = \delta^{-1}\hat{f}(\delta^{-1}k)$ for $\delta > 0$;
      \item $\mathcal{F}(f'(x)) = 2\pi i k \hat{f}(k)$;
      \item $\mathcal{F}(-2\pi i x f(x)) = \frac{d\hat{f}}{dk}$.
    \end{itemize}
  \end{proposition}
  One of the important facts of the Fourier transform is that it preserves the Schwartz space.
  \begin{theorem}[Preservation of Schwartz Space under Fourier Transform]
    If $f\in \mathcal{S}(\R)$, then so too is $\hat{f}$.
  \end{theorem}
  \begin{proof}
    Let $f\in \mathcal{S}(\R)$. Then, $\hat{f}$ is bounded,\footnote{I'm not sure exactly why.} so for every nonnegative $k,\ell$, it is the case that
    \begin{align*}
      \xi^{k}\left(\frac{d}{d\xi}\right)^{\ell}\hat{f}(\xi)
    \end{align*}
    is bounded, as it is the Fourier transform of the (bounded) expression
    \begin{align*}
      \frac{1}{(2\pi i)^k}\left(\frac{d}{dx}\right)^k \left((-2\pi i x)^{\ell}f(x)\right).
    \end{align*}
    Thus, $\hat{f}\in \mathcal{S}(\R)$.
  \end{proof}
  \subsection{Good Kernels for the Fourier Transform}%
  Consider the fact that
  \begin{align*}
    \int_{-\infty}^{\infty}e^{-\pi x^2}dx &= 1.
  \end{align*}
  We can see that $e^{-\pi x^2}\in \mathcal{S}(\R)$, meaning we can examine its Fourier transform.
  \begin{theorem}[Fourier Transform of $e^{-\pi x^2}$]
    Let $f(x) = e^{-\pi x^2}$. Then, $\hat{f}(\xi) = f(\xi)$.
  \end{theorem}
  \begin{proof}
    Define $F(\xi) = \hat{f}(\xi) = \int_{-\infty}^{\infty}e^{-\pi x^2}e^{-2\pi i \xi}dx$. Observe that $F(0) = 1$.\newline

    Using the fact that $\frac{df}{dx} = -2\pi x f(x)$ and that $\mathcal{F}\left(-2\pi i x f(x)\right) = \frac{d}{d\xi}\hat{f}(\xi)$, we find by differentiating under the integral sign that
    \begin{align*}
      F'(\xi) &= \int_{-\infty}^{\infty}f(x)(-2\pi i x)e^{-2\pi i x \xi}dx\\
              &= i\int_{-\infty}^{\infty}f'(x)e^{-2\pi i x \xi}dx\\
              \intertext{meaning}
      F'(\xi) &= i\left(\frac{d}{d\xi}\hat{f}\xi\right)\\
              &= -2\pi \xi F(\xi).
    \end{align*}
    Thus, $F(\xi) = e^{-\pi \xi^2}$.\footnote{I skipped a step here that was kind of annoying even though I maybe shouldn't have.}
  \end{proof}
  From this, we can see that if $K_{\delta}(x) = \delta^{-1/2}e^{-\pi x^2/\delta}$, then $\widehat{K_{\delta}}(\xi) = e^{-\pi \delta \xi^2}$. We can see that as $\delta \rightarrow 0$, $K_{\delta}$ peaks at the origin, while $\widehat{K_{\delta}}$ becomes flatter.\newline

  Importantly, $K_{\delta}(x) = \delta^{-1/2}e^{-\pi x^2/\delta}$ also satisfies the following properties:
  \begin{itemize}
    \item $\displaystyle \int_{-\infty}^{\infty}K_{\delta}(x)dx = 1$;
    \item $\displaystyle \int_{-\infty}^{\infty}\left\vert K_{\delta}(x)\right\vert dx \leq M$;
    \item for every $\eta > 0$, we have $\displaystyle\int_{|x| > \eta}\left\vert K_{\delta}(x)\right\vert dx \rightarrow 0$ as $\delta \rightarrow 0$.
  \end{itemize}
  This makes the collection $\set{K_{\delta}}$ a family of good kernels for the Fourier transform.\newline

  In particular, this means that $(f\ast K_{\delta})(x) \rightarrow f(x)$ uniformly as $\delta \rightarrow 0$ for any $f\in \mathcal{S}(\R)$, where $\ast$ denotes the convolution.
  \subsection{Fourier Inversion}%
  \begin{proposition}[Multiplication Identity]
    If $f,g\in \mathcal{S}(\R)$, then
    \begin{align*}
      \int_{-\infty}^{\infty}f(x)\hat{g}(x)dx &= \int_{-\infty}^{\infty}\hat{f}(y)g(y)dy.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Consider a continuous multivariate function of moderate decrease in $x$ and $y$, $|F(x,y)| \leq \frac{A}{(1+x^2)(1+y^2)}$. Set $F_1(x) = \int_{-\infty}^{\infty}F(x,y)dy$ and $F_2(y) = \int_{-\infty}^{\infty}F(x,y)dx$. Then, by Fubini's Theorem, we have
    \begin{align*}
      \int_{-\infty}^{\infty}F_1(x)dx &= \int_{-\infty}^{\infty}F(y)dy.
    \end{align*}
    If $F(x,y) = f(x)g(y)e^{-2\pi i x y}$, then $F_1(x) = f(x)\hat{g}(x)$ and $F_2(y) = \hat{f}(y)g(y)$.
  \end{proof}
  \begin{theorem}[Fourier Inversion]
    If $f\in \mathcal{S}(\R)$, then\footnote{Note the sign change.}
    \begin{align*}
      f(x) &= \int_{-\infty}^{\infty}\hat{f}(\xi)e^{2\pi i x \xi}d\xi.
    \end{align*}
  \end{theorem}
  \begin{proof}
    We start by claiming that
    \begin{align*}
      f(0) &= \int_{-\infty}^{\infty}\hat{f}(\xi)d\xi.
    \end{align*}
    Let $G_{\delta}(x) = e^{-\pi \delta x^2}$ such that $\widehat{G_{\delta}}(\xi) = K_{\delta}(\xi)$. By the multiplication formula, we get
    \begin{align*}
      \int_{-\infty}^{\infty}f(x)K_{\delta}(x) dx &= \int_{-\infty}^{\infty}\hat{f}(\xi)G_{\delta}(\xi)d\xi.
    \end{align*}
    Since $K_{\delta}$ is a good kernel, the first integral goes to $f(0)$ as $\delta \rightarrow 0$, and since the second integral converges to $\int_{-\infty}^{\infty}\hat{f}(\xi)d\xi$ as $\delta \rightarrow 0$, we find that the claim is true.\newline

    Let $F(y) = f(y+x)$, so
    \begin{align*}
      f(x) &= F(0)\\
           &= \int_{-\infty}^{\infty}\hat{F}(\xi)d\xi\\
           &= \int_{-\infty}^{\infty}\hat{f}(\xi)e^{2\pi i x \xi}d\xi.
    \end{align*}
  \end{proof}
  This implies the existence of two maps, $\mathcal{F}: \mathcal{S}(\R)\rightarrow \mathcal{S}(\R)$ and $\mathcal{F}^{\ast}:\mathcal{S}(\R)\rightarrow \mathcal{S}(\R)$, where
  \begin{align*}
    \mathcal{F}(f)(\xi) &= \int_{-\infty}^{\infty}f(x)e^{-2\pi i x \xi}dx\\
    \mathcal{F}^{\ast}(g)(x) &= \int_{-\infty}^{\infty} g(\xi)e^{2\pi i x \xi}d\xi.
  \end{align*}
  We say $\mathcal{F}$ is the Fourier transform, and $\mathcal{F}^{\ast}$ is the inverse Fourier transform; $\mathcal{F}\circ \mathcal{F}^{\ast} = \mathcal{F}^{\ast}\circ \mathcal{F} = I$.
  \subsection{Plancherel's Theorem}%
  In order to prove that $\norm{\mathcal{F}}_{\text{op}} = 1$, we need to understand the Fourier transform's effects on convolutions.
  \begin{proposition}[Convolutions under the Fourier Transform]
    Let $f,g\in \mathcal{S}(\R)$. Then,
    \begin{itemize}
      \item $f\ast g \in \mathcal{S}(\R)$;
      \item $f\ast g = g\ast f$;
      \item $\widehat{\left(f\ast g\right)}(\xi) = \hat{f}(\xi)\hat{g}(\xi)$.
    \end{itemize}
  \end{proposition}
  \begin{proof}
  To prove that $f\ast g$ is rapidly decreasing, we observe that for $\ell \geq 0$,
  \begin{align*}
    \sup_{x}|x|^{\ell}|g(x-y)| &\leq A_{\ell}(1+|y|)^{\ell},
  \end{align*}
  since $g$ is rapidly decreasing. Thus, we see that
  \begin{align*}
    \sup_{x}\left\vert x^{\ell}(f\ast g)(x) \right\vert &\leq A_{\ell}\int_{-\infty}^{\infty}|f(y)|\left(1 + |y|\right)^{\ell}dy,
  \end{align*}
  meaning $x^{\ell}(f\ast g)(x)$ is bounded for every $\ell \geq 0$. Similarly, since
  \begin{align*}
    \left(\frac{d}{dx}\right)^k \left(f\ast g\right)(x) &= \left(f\ast \left(\frac{d}{dx}\right)^{k}g\right)(x),
  \end{align*}
  it is the case that $f\ast g \in \mathcal{S}(\R)$. \newline

  To show that $f\ast g = g\ast f$, simply substitute $x - y = u$ in the formula of $f\ast g$:
  \begin{align*}
    \left(f\ast g\right)(x) &= \int_{-\infty}^{\infty}f(y)g(x-y)dy\\
               &= \int_{-\infty}^{\infty}f(x-u)g(u)du\\
               &= \left(g\ast f\right)(x)
  \end{align*}
  Finally, consider $F(x,y) = f(y)g(x-y)e^{-2\pi i x \xi}$. We set $F_1(x) = (f\ast g)(x)e^{-2\pi i x \xi}$ (by integrating with respect to $y$), and $F_2(x) = f(y)e^{-2\pi i y \xi}\hat{g}(\xi)$. Thus,
  \begin{align*}
    \int_{-\infty}^{\infty}F_1(x)dx &= \int_{-\infty}^{\infty}F_2(y)dy.
  \end{align*}
  \end{proof}
  The Schwartz space is a Hilbert space with inner product
  \begin{align*}
    \iprod{f}{g} &= \int_{-\infty}^{\infty}f(x)\overline{g(x)} dx.
  \end{align*}
  \begin{theorem}[Plancherel]
    If $f\in \mathcal{S}(\R)$, then $\norm{\hat{f}} = \norm{f}$.
  \end{theorem}
  \begin{proof}
    Let $f\in \mathcal{S}(\R)$. Define $\tilde{f} = \overline{f(-x)}$. Then, $\widehat{\tilde{f}}(\xi) = \overline{\hat{f}(\xi)}$. Let $h = f\ast \tilde{f}$. Clearly,
    \begin{align*}
      \hat{h}(\xi) &= \left\vert \hat{f}(\xi) \right\vert^2\\
      h(0) &= \int_{-\infty}^{\infty}|f(x)|^2dx.
    \end{align*}
    Plancherel's theorem follows from the inversion formula applied at $x=0$, meaning
    \begin{align*}
      \int_{-\infty}^{\infty}\hat{h}(\xi)d\xi &= h(0).
    \end{align*}
  \end{proof}
  \section{The Schrödinger Equation for a Free Particle}%
  We want to solve the Schrödinger equation using a variety of methods for a ``free'' particle in $\R$ --- that is, a particle with identically zero potential. The free Schrödinger equation is thus
  \begin{align*}
    \frac{d\psi}{dt} &= \frac{i\hbar}{2m}\frac{\partial^2\psi}{\partial x^2}
  \end{align*}
  subject to the initial condition $\psi(x,0) = \psi_0(x)$. Before fully solving the equation, we begin by observing the time evolution of the expectation of the position and momentum operators:
  \begin{align*}
    \frac{d}{dt}\langle X \rangle_{\psi(t)} &= \frac{1}{m}\langle P \rangle_{\psi(t)}\\
    \frac{d}{dt}\langle P \rangle_{\psi(t)} &= 0.
  \end{align*}
  Thus, the expectation of $X$ is linear in time:
  \begin{align*}
    \langle X \rangle_{\psi(t)} &= \langle X \rangle_{\psi_0} + \frac{t}{m}\langle P \rangle_{\psi_0}\\
    \langle P \rangle_{\psi(t)} &= \langle P \rangle_{\psi_0}.
  \end{align*}
  The free Schrödinger equation is a special case where the expectation of $X$ and $P$ exactly follows the classical solution.
  \subsection{Using the Fourier Transform to solve the Free Schrödinger Equation}%
  We want to find solutions to the free Schrödinger equation of the form
  \begin{align*}
    \psi(x,t) &= e^{i(kx - \omega(k)t)},
  \end{align*}
  where $k$ is the spatial frequency and $\omega(k)$ is the frequency in time for a given spatial frequency.\footnote{I think my PDEs knowledge is lacking in order to explain why the solution is of this form.} Plugging this solution form into the free Schrödinger equation yields
  \begin{align*}
    \omega(k) &= \frac{\hbar k^2}{2m}.
  \end{align*}
  Formulae that express the frequency in time as a function of spatial frequency in a solution of some partial differential equation are called dispersion relations.\newline

  We can also see that the solution form can be written as
  \begin{align*}
    \psi(x,t) &= e^{ik\left(x - \frac{\omega(k)}{k}t\right)},
  \end{align*}
  meaning that time evolution shifts the initial function to the write by $\frac{\omega(k)}{k}t$, so $\psi(x,t)$ is moving to the right over time with speed $\frac{\omega(k)}{k}$.
  \begin{definition}[Phase Velocity]
    The phase velocity of a particle with momentum $p = \hbar k$ is
    \begin{align*}
      \frac{\omega(k)}{k} &= \frac{\hbar k}{2m}\\
                          &= \frac{p}{2m},
    \end{align*}
    which is half the velocity of a classical particle with momentum $p$.
  \end{definition}
  The ``real'' velocity, $\frac{p}{m}$ is referred to as the group velocity, whereas the phase velocity originates in the pure exponential solution to the free Schrödinger equation.
  \begin{proposition}[Solution to the Schrodinger Equation]
    Suppose $\psi_0$ is a Schwartz function. Let $\hat{\psi_0}$ denote the Fourier transform of $\psi_0$, and define $\psi(x,t)$ by
    \begin{align*}
      \psi(x,t) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\hat{\psi_0}(k)e^{i\left(kx - \omega(k)t\right)}dk.
    \end{align*}
    Then, $\psi(x,t)$ solves the free Schrödinger equation.
  \end{proposition}
  \begin{proof}
    Notice that, by differentiating under the integral sign, and remembering that $\omega(k) = \frac{\hbar k^2}{2m}$, we have\footnote{I did not show the convergence of the integral here, but it follows a similar argument to the convergence of the derivative with respect to $x$.}
    \begin{align*}
      \frac{d\psi}{dt} &= \frac{i\hbar}{2m}\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}-k^2\hat{\psi}_0(k)e^{i(kx - \omega(k)t)}dk\right).
    \end{align*}
    To show that the integral on the right is equal to $\frac{d^2\psi}{dx^2}$, we will need to show that the integral indeed converges. Taking the derivative of the $e^{ikx}$, we find that
    \begin{align*}
      \left\vert \frac{e^{ik(x+h)-e^{ikx}}}{h} \right\vert\leq |k|
    \end{align*}
    for all $h > 0$. Thus, with $|k\hat{\psi}_0(k)|$ as our dominating function, the dominated convergence theorem states that
    \begin{align*}
      \lim_{h\rightarrow 0}\int_{-\infty}^{\infty}\hat{\psi}_0(k)\frac{e^{i\left(k(x+h) - \omega(k)t\right)} - e^{i(kx - \omega(k)t)}}{h}dk &= \int_{-\infty}^{\infty}-ik\hat(\psi)_0(k)e^{i\left(kx - \omega(k)t\right)}dk.
    \end{align*}
    Following a similar pattern, we find that
    \begin{align*}
      \frac{d\psi}{dt} &= \frac{i\hbar}{2m}\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}-k^2\hat{\psi}_0(k)e^{i(kx - \omega(k)t)}dk\right).\\
                       &= \frac{i\hbar}{2m}\frac{d^2\psi}{dx^2}.
    \end{align*}
    Finally, we use the Fourier inversion formula to show that $\psi(x,0) = \psi_0(x)$.\footnote{The book's proof is quite a bit more vague than this proof, and I'll probably have to go back and clarify a lot of this in the future.}
  \end{proof}
\end{document}
