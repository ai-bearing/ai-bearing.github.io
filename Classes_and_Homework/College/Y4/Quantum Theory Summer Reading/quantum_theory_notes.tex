\documentclass[10pt]{extarticle}

\title{}
\author{}
\date{}
\usepackage[shortlabels]{enumitem}

%paper setup
\usepackage{geometry}
\geometry{letterpaper, portrait, margin=1in}
\usepackage{fancyhdr}
% sans serif font:
\usepackage{cmbright}
\usepackage{sfmath}

%symbols
\usepackage{amsmath}
\usepackage{bigints}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{gensymb} %more symbols
\usepackage{multirow,array} %better tables
\usepackage{multicol} %multiple columns per page
\usepackage{bbold} %better blackboard bold
\newtheorem*{remark}{Remark}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{cd}
\tikzset{middleweight/.style={pos = 0.5}}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\setlength{\parindent}{0pt} %I don't like indentation
\usepackage{cancel} %better X-throughs
\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Quantum Theory for Mathematicians: Notes}

%canonical sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\bra}[1]{\left\langle#1\right\vert}
\newcommand{\ket}[1]{\left\vert#1\right\rangle}
\newcommand{\braket}[2]{\left\langle#1\mid#2\right\rangle}
\newcommand{\set}[1]{\left\{#1\right\}}

%common other symbols
\newcommand{\mcc}{\mathcal{C}} %cantor set
\newcommand{\mco}{\mathcal{O}} %holomorphic functions
\newcommand{\mfp}{\mathfrak{p}} %prime ideal

%inner products and norms
\newcommand{\iprod}[2]{\left\langle #1,#2\right\rangle}
\newcommand{\norm}[1]{\left\Vert #1\right\Vert}

\setcounter{secnumdepth}{0}

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}%[section]
\newtheorem*{axiom}{Axiom}%[section]
\newtheorem*{lemma}{Lemma}%[theorem]
\newtheorem*{proposition}{Proposition}%[section]

\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem*{example}{Example}

\theoremstyle{remark}
%\newtheorem*{remark}{Remark}
\newtheorem*{authnote}{Author's Remark}
\newtheorem*{profnote}{Professor's Remark}


\usepackage[document]{ragged2e}
\renewcommand{\newline}{\hfill\break}
\renewcommand{\thefootnote}{\roman{footnote}}
\usepackage{hyperref} %
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=black,
    urlcolor=black,
    citecolor=black
}
\begin{document}
\tableofcontents
  \section{Preface}%
  Hello, you have stumbled across my notes on quantum mechanics from a mathematical perspective. Welcome! These notes are going to be largely based on the book \textit{Quantum Theory for Mathematicians} by Brian C. Hall, a book I got on recommendation of a friend at my REU. This is mostly going to be an experiment in seeing the level of analysis I'll be able to put in my brain over the course of a summer without access to much in the way of resources.\newline

  As far as sectioning goes, I'll mostly be following the chapters of the book; if there are certain details that are especially important that I can't see myself learning on the fly, I will probably source another book in order to find a more detailed explanation (the appendices of the main book are not particularly strong as far as explanatory power).\newline

  There will be footnotes, either with moderately snarky comments or me explaining to myself some of the results that the book assumes the reader instantly understands.\newline

  If I'm being honest, this project will probably be a failure as far as actually understanding quantum mechanics goes, but hopefully by the end of it I'll have a decent understanding of spectral theory, measure theory, and maybe solve the SchrÃ¶dinger equation or quantum harmonic oscillator.
  \section{Prelude: Fourier Transform and Schwartz Space}%
  It's probably pretty useful to understand the basics of the Fourier transform before we jump in.\footnote{I actually did this in the middle of learning but decided to move it up to the front, alongside some other preliminaries I'm going to have to draw on.} To do this, the following are notes of some content from the \textit{Princeton Lectures on Analysis}. 
  \subsection{Functions of Moderate Decrease}%
  A function $f$ defined on $\R$ that is integrable over a closed and bounded interval may not be integrable over $\R$. For instance, the function $f = \frac{1}{1+|x|}$ is such a function.\newline

  In order to study functions that \textit{are} integrable over $\R$, we will find it useful to restrict our view to certain classes of functions --- namely, ones that decrease ``fast enough.''
  \begin{definition}[Function of Moderate Decrease]
    A function $f\in C(\R)$ is said to be of moderate decrease if there exists $A > 0$ such that
    \begin{align*}
      |f(x)| \leq \frac{A}{1 + x^{1 + \varepsilon}}
    \end{align*}
    for all $x\in \R$ and some $\varepsilon > 0$. We denote the set of all functions of moderate decrease as $\mathcal{M}(\R)$; $\mathcal{M}(\R)$ is a vector space over $\C$.
  \end{definition}
  If $f$ is of moderate decrease, we can define
  \begin{align*}
    \int_{-\infty}^{\infty}f(x)dx &= \lim_{N\rightarrow\infty}\int_{-N}^{N}f(x)dx.
  \end{align*}
  The integral over $\R$ of $f,g\in \mathcal{M}(\R)$ (with $a,b\in \C$) has the following properties:
  \begin{itemize}
    \item Linearity:
      \begin{align*}
        \int_{-\infty}^{\infty}\left(af(x) + bg(x)\right) dx &= a\int_{-\infty}^{\infty}f(x)dx + b\int_{-\infty}^{\infty}g(x)dx.
      \end{align*}
    \item Translation Invariance:
      \begin{align*}
        \int_{-\infty}^{\infty}f(x-h)dx &= \int_{-\infty}^{\infty}f(x)dx.
      \end{align*}
    \item Scaling under dilation:
      \begin{align*}
        \delta\int_{-\infty}^{\infty}f(\delta x)dx &= \int_{-\infty}^{\infty}f(x)dx
      \end{align*}
    \item Continuity:
      \begin{align*}
        \lim_{h\rightarrow 0}\int_{-\infty}^{\infty}|f(x-h) - f(x)|dx &= 0.
      \end{align*}
  \end{itemize}
  \subsection{Schwartz Space}%
  The Schwartz space is a subspace of $f\in C^{\infty}(\R)$ where, for all $k,\ell \geq 0$,
  \begin{align*}
    \sup_{x\in \R}\left\vert x^{k}f^{(\ell)}(x) \right\vert < \infty.
  \end{align*}
  The space $\mathcal{S}(\R)\subseteq C^{\infty}(\R)$ is also a vector space over $\C$. Notice that if $f\in \mathcal{S}(\R)$, then so too are $f'(x)$ and $xf(x)$.\newline

  Important classes of Schwartz functions include the Gaussians and the bump functions (compactly supported $C^{\infty}(\R)$ functions).\footnote{Bump functions are fascinating because even though they're $C^{\infty}$, they are not analytic (or equivalent to their Taylor series, denoted $C^{\omega}$). Compare this to functions defined over the complex numbers, where $C^{1}(\C) = C^{\infty}(\C) = C^{\omega}(\C)$.}
  \subsection{The Fourier Transform on $\mathcal{S}(\R)$}%
  \begin{definition}[Fourier Transform]
    The Fourier transform of a function $f\in \mathcal{S}(\R)$ is defined by
    \begin{align*}
      \hat{f}(k) &= \int_{-\infty}^{\infty}f(t)e^{-2\pi i k t}dt.
    \end{align*}
    I will use $\mathcal{F}$ to denote the Fourier transform; $\mathcal{F}(f(t)) = \hat{f}(k)$. 
  \end{definition}
  \begin{proposition}[Properties of the Fourier Transform]
    Let $f\in \mathcal{S}(\R)$. Then:
    \begin{itemize}
      \item $\displaystyle\mathcal{F}(f(x+h)) = \hat{f}(k)e^{2\pi i h k}$ for $h\in \R$;
      \item $\displaystyle \mathcal{F}\left(f(x)e^{-2\pi i x h}\right) = \hat{f}(k + h)$ for $h\in \R$;
      \item $\displaystyle \mathcal{F}\left(f(\delta x)\right) = \delta^{-1}\hat{f}(\delta^{-1}k)$ for $\delta > 0$;
      \item $\mathcal{F}(f'(x)) = 2\pi i k \hat{f}(k)$;
      \item $\mathcal{F}(-2\pi i x f(x)) = \frac{d\hat{f}}{dk}$.
    \end{itemize}
  \end{proposition}
  One of the important facts of the Fourier transform is that it preserves the Schwartz space.
  \begin{theorem}[Preservation of Schwartz Space under Fourier Transform]
    If $f\in \mathcal{S}(\R)$, then so too is $\hat{f}$.
  \end{theorem}
  \begin{proof}
    Let $f\in \mathcal{S}(\R)$. Then, $\hat{f}$ is bounded,\footnote{I'm not sure exactly why.} so for every nonnegative $k,\ell$, it is the case that
    \begin{align*}
      \xi^{k}\left(\frac{d}{d\xi}\right)^{\ell}\hat{f}(\xi)
    \end{align*}
    is bounded, as it is the Fourier transform of the (bounded) expression
    \begin{align*}
      \frac{1}{(2\pi i)^k}\left(\frac{d}{dx}\right)^k \left((-2\pi i x)^{\ell}f(x)\right).
    \end{align*}
    Thus, $\hat{f}\in \mathcal{S}(\R)$.
  \end{proof}
  \subsection{Good Kernels for the Fourier Transform}%
  Consider the fact that
  \begin{align*}
    \int_{-\infty}^{\infty}e^{-\pi x^2}dx &= 1.
  \end{align*}
  We can see that $e^{-\pi x^2}\in \mathcal{S}(\R)$, meaning we can examine its Fourier transform.
  \begin{theorem}[Fourier Transform of $e^{-\pi x^2}$]
    Let $f(x) = e^{-\pi x^2}$. Then, $\hat{f}(\xi) = f(\xi)$.
  \end{theorem}
  \begin{proof}
    Define $F(\xi) = \hat{f}(\xi) = \int_{-\infty}^{\infty}e^{-\pi x^2}e^{-2\pi i \xi}dx$. Observe that $F(0) = 1$.\newline

    Using the fact that $\frac{df}{dx} = -2\pi x f(x)$ and that $\mathcal{F}\left(-2\pi i x f(x)\right) = \frac{d}{d\xi}\hat{f}(\xi)$, we find by differentiating under the integral sign that
    \begin{align*}
      F'(\xi) &= \int_{-\infty}^{\infty}f(x)(-2\pi i x)e^{-2\pi i x \xi}dx\\
              &= i\int_{-\infty}^{\infty}f'(x)e^{-2\pi i x \xi}dx\\
              \intertext{meaning}
      F'(\xi) &= i\left(\frac{d}{d\xi}\hat{f}\xi\right)\\
              &= -2\pi \xi F(\xi).
    \end{align*}
    Thus, $F(\xi) = e^{-\pi \xi^2}$.\footnote{I skipped a step here that was kind of annoying even though I maybe shouldn't have.}
  \end{proof}
  From this, we can see that if $K_{\delta}(x) = \delta^{-1/2}e^{-\pi x^2/\delta}$, then $\widehat{K_{\delta}}(\xi) = e^{-\pi \delta \xi^2}$. We can see that as $\delta \rightarrow 0$, $K_{\delta}$ peaks at the origin, while $\widehat{K_{\delta}}$ becomes flatter.\newline

  Importantly, $K_{\delta}(x) = \delta^{-1/2}e^{-\pi x^2/\delta}$ also satisfies the following properties:
  \begin{itemize}
    \item $\displaystyle \int_{-\infty}^{\infty}K_{\delta}(x)dx = 1$;
    \item $\displaystyle \int_{-\infty}^{\infty}\left\vert K_{\delta}(x)\right\vert dx \leq M$;
    \item for every $\eta > 0$, we have $\displaystyle\int_{|x| > \eta}\left\vert K_{\delta}(x)\right\vert dx \rightarrow 0$ as $\delta \rightarrow 0$.
  \end{itemize}
  This makes the collection $\set{K_{\delta}}$ a family of good kernels for the Fourier transform.\newline

  In particular, this means that $(f\ast K_{\delta})(x) \rightarrow f(x)$ uniformly as $\delta \rightarrow 0$ for any $f\in \mathcal{S}(\R)$, where $\ast$ denotes the convolution.
  \subsection{Fourier Inversion}%
  \begin{proposition}[Multiplication Identity]
    If $f,g\in \mathcal{S}(\R)$, then
    \begin{align*}
      \int_{-\infty}^{\infty}f(x)\hat{g}(x)dx &= \int_{-\infty}^{\infty}\hat{f}(y)g(y)dy.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Consider a continuous multivariate function of moderate decrease in $x$ and $y$, $|F(x,y)| \leq \frac{A}{(1+x^2)(1+y^2)}$. Set $F_1(x) = \int_{-\infty}^{\infty}F(x,y)dy$ and $F_2(y) = \int_{-\infty}^{\infty}F(x,y)dx$. Then, by Fubini's Theorem, we have
    \begin{align*}
      \int_{-\infty}^{\infty}F_1(x)dx &= \int_{-\infty}^{\infty}F(y)dy.
    \end{align*}
    If $F(x,y) = f(x)g(y)e^{-2\pi i x y}$, then $F_1(x) = f(x)\hat{g}(x)$ and $F_2(y) = \hat{f}(y)g(y)$.
  \end{proof}
  \begin{theorem}[Fourier Inversion]
    If $f\in \mathcal{S}(\R)$, then\footnote{Note the sign change.}
    \begin{align*}
      f(x) &= \int_{-\infty}^{\infty}\hat{f}(\xi)e^{2\pi i x \xi}d\xi.
    \end{align*}
  \end{theorem}
  \begin{proof}
    We start by claiming that
    \begin{align*}
      f(0) &= \int_{-\infty}^{\infty}\hat{f}(\xi)d\xi.
    \end{align*}
    Let $G_{\delta}(x) = e^{-\pi \delta x^2}$ such that $\widehat{G_{\delta}}(\xi) = K_{\delta}(\xi)$. By the multiplication formula, we get
    \begin{align*}
      \int_{-\infty}^{\infty}f(x)K_{\delta}(x) dx &= \int_{-\infty}^{\infty}\hat{f}(\xi)G_{\delta}(\xi)d\xi.
    \end{align*}
    Since $K_{\delta}$ is a good kernel, the first integral goes to $f(0)$ as $\delta \rightarrow 0$, and since the second integral converges to $\int_{-\infty}^{\infty}\hat{f}(\xi)d\xi$ as $\delta \rightarrow 0$, we find that the claim is true.\newline

    Let $F(y) = f(y+x)$, so
    \begin{align*}
      f(x) &= F(0)\\
           &= \int_{-\infty}^{\infty}\hat{F}(\xi)d\xi\\
           &= \int_{-\infty}^{\infty}\hat{f}(\xi)e^{2\pi i x \xi}d\xi.
    \end{align*}
  \end{proof}
  This implies the existence of two maps, $\mathcal{F}: \mathcal{S}(\R)\rightarrow \mathcal{S}(\R)$ and $\mathcal{F}^{\ast}:\mathcal{S}(\R)\rightarrow \mathcal{S}(\R)$, where
  \begin{align*}
    \mathcal{F}(f)(\xi) &= \int_{-\infty}^{\infty}f(x)e^{-2\pi i x \xi}dx\\
    \mathcal{F}^{\ast}(g)(x) &= \int_{-\infty}^{\infty} g(\xi)e^{2\pi i x \xi}d\xi.
  \end{align*}
  We say $\mathcal{F}$ is the Fourier transform, and $\mathcal{F}^{\ast}$ is the inverse Fourier transform; $\mathcal{F}\circ \mathcal{F}^{\ast} = \mathcal{F}^{\ast}\circ \mathcal{F} = I$.
  \subsection{Plancherel's Theorem}%
  In order to prove that $\norm{\mathcal{F}}_{\text{op}} = 1$, we need to understand the Fourier transform's effects on convolutions.
  \begin{proposition}[Convolutions under the Fourier Transform]
    Let $f,g\in \mathcal{S}(\R)$. Then,
    \begin{itemize}
      \item $f\ast g \in \mathcal{S}(\R)$;
      \item $f\ast g = g\ast f$;
      \item $\widehat{\left(f\ast g\right)}(\xi) = \hat{f}(\xi)\hat{g}(\xi)$.
    \end{itemize}
  \end{proposition}
  \begin{proof}
  To prove that $f\ast g$ is rapidly decreasing, we observe that for $\ell \geq 0$,
  \begin{align*}
    \sup_{x}|x|^{\ell}|g(x-y)| &\leq A_{\ell}(1+|y|)^{\ell},
  \end{align*}
  since $g$ is rapidly decreasing. Thus, we see that
  \begin{align*}
    \sup_{x}\left\vert x^{\ell}(f\ast g)(x) \right\vert &\leq A_{\ell}\int_{-\infty}^{\infty}|f(y)|\left(1 + |y|\right)^{\ell}dy,
  \end{align*}
  meaning $x^{\ell}(f\ast g)(x)$ is bounded for every $\ell \geq 0$. Similarly, since
  \begin{align*}
    \left(\frac{d}{dx}\right)^k \left(f\ast g\right)(x) &= \left(f\ast \left(\frac{d}{dx}\right)^{k}g\right)(x),
  \end{align*}
  it is the case that $f\ast g \in \mathcal{S}(\R)$. \newline

  To show that $f\ast g = g\ast f$, simply substitute $x - y = u$ in the formula of $f\ast g$:
  \begin{align*}
    \left(f\ast g\right)(x) &= \int_{-\infty}^{\infty}f(y)g(x-y)dy\\
               &= \int_{-\infty}^{\infty}f(x-u)g(u)du\\
               &= \left(g\ast f\right)(x)
  \end{align*}
  Finally, consider $F(x,y) = f(y)g(x-y)e^{-2\pi i x \xi}$. We set $F_1(x) = (f\ast g)(x)e^{-2\pi i x \xi}$ (by integrating with respect to $y$), and $F_2(x) = f(y)e^{-2\pi i y \xi}\hat{g}(\xi)$. Thus,
  \begin{align*}
    \int_{-\infty}^{\infty}F_1(x)dx &= \int_{-\infty}^{\infty}F_2(y)dy.
  \end{align*}
  \end{proof}
  The Schwartz space is a Hilbert space with inner product
  \begin{align*}
    \iprod{f}{g} &= \int_{-\infty}^{\infty}f(x)\overline{g(x)} dx.
  \end{align*}
  \begin{theorem}[Plancherel]
    If $f\in \mathcal{S}(\R)$, then $\norm{\hat{f}} = \norm{f}$.
  \end{theorem}
  \begin{proof}
    Let $f\in \mathcal{S}(\R)$. Define $\tilde{f} = \overline{f(-x)}$. Then, $\widehat{\tilde{f}}(\xi) = \overline{\hat{f}(\xi)}$. Let $h = f\ast \tilde{f}$. Clearly,
    \begin{align*}
      \hat{h}(\xi) &= \left\vert \hat{f}(\xi) \right\vert^2\\
      h(0) &= \int_{-\infty}^{\infty}|f(x)|^2dx.
    \end{align*}
    Plancherel's theorem follows from the inversion formula applied at $x=0$, meaning
    \begin{align*}
      \int_{-\infty}^{\infty}\hat{h}(\xi)d\xi &= h(0).
    \end{align*}
  \end{proof}
  \section{Prelude: Sesquilinear and Quadratic Forms on Hilbert Spaces}%
  It will be useful to understand the general theory of sesquilinear forms as we go deeper into theories of operators on Hilbert spaces. To do this, I will draw information from the book \textit{Introduction to Hilbert Spaces with Applications} by Debnath and Mikusinski, as well as a result or two from \textit{Quantum Theory for Mathematicians}.

  \begin{definition}[Sesquilinear Form]
    Let $E$ be a $\C$-vector space. A sesquilinear form\footnote{Debnath and Mikusinski call this a bilinear functional.} is a map $\varphi: E\times E \rightarrow \C$ with the following properties:
    \begin{enumerate}[(a)]
      \item $\displaystyle \varphi(\alpha x_1 + \beta x_1,y) = \alpha \varphi(x_1,y) + \beta\varphi(x_2,y)$;
      \item $\displaystyle \varphi(x,\alpha y_1 + \beta y_2) = \bar{\alpha}\varphi(x,y_1) + \bar{\beta}\varphi(x,y_2)$;
    \end{enumerate}
    for any $\alpha,\beta \in \C$ and $x,x_1,x_2,y,y_1,y_2 \in E$.
  \end{definition}
  A familiar sesquilinear form is the inner product, but we want to develop properties and ideas that are generalized over all sesquilinear forms rather than simply sticking to the inner product.
  \begin{definition}[Classes of Sesquilinear Forms]
    Let $\varphi$ be a sesquilinear form on $E$.
    \begin{enumerate}[(a)]
      \item We say $\varphi$ is symmetric if $\varphi(x,y) = \overline{\varphi(y,x)}$ for all $x,y\in E$.\footnote{I'd personally prefer to call this ``conjugate symmetry.''}
      \item We say $\varphi$ is positive if $\varphi(x,x) \geq 0$ for every $x\in E$.
      \item We say $\varphi$ is \textit{strictly} positive if $\varphi(x,x) = 0 $ if and only if $x = 0$.
      \item If $E$ has a normed space, then $\varphi$ is bounded if $|\varphi(x,y)| \leq K\norm{x}\norm{y}$ for some $K > 0$ and all $x,y\in E$.\footnote{The Cauchy-Schwarz inequality over Hilbert spaces with the standard inner product is a particular case of this.} The operator norm is
        \begin{align*}
          \norm{\varphi} &= \sup_{\norm{x},\norm{y} = 1}\left\vert \varphi(x,y) \right\vert.
        \end{align*}
    \end{enumerate}
  \end{definition}
  Note that, by the definition of the operator norm, $\left\vert \varphi(x,y) \right\vert\leq \norm{\varphi}\norm{x}\norm{y}$ for all $x,y\in E$.
  \begin{definition}[Quadratic Form]
    For a particular sesquilinear form $\varphi$ on $E$, the function $\Phi: E\rightarrow \C$ defined by $\Phi(x) = \varphi(x,x)$ is known as the quadratic form for $\varphi$.\newline

    If $E$ has a norm, then $\Phi(x)$ is bounded if $\left\vert \Phi(x) \right\vert\leq K\norm{x}^2$ for all $x\in E$ and some $K > 0$. The norm of $\Phi$ is defined by
    \begin{align*}
      \norm{\Phi} &= \sup_{\norm{x} = 1}\left\vert \Phi(x) \right\vert.
    \end{align*}
  \end{definition}
  For a bounded quadratic form $\Phi$ on a normed space, $\left\vert \Phi(x) \right\vert\leq \norm{\phi}\norm{x}^2$.\newline

  The standard inner product in an inner product space is a special case where $\norm{x}^2 = \iprod{x}{x}$.
  \begin{theorem}[Polarization Identity]
    For any $\varphi$ a sesquilinear form on $E$ (and associated quadratic form $\Phi$),
    \begin{align*}
      4\varphi(x,y) &= \Phi(x+y) - \Phi(x-y) + i\Phi(x + iy) - i\Phi(x - iy)
    \end{align*}
    for all $x,y\in E$.
  \end{theorem}
  \begin{proof}[Proof of Polarization Identity]
    For any $\alpha,\beta in \C$, we have
    \begin{align*}
      \Phi(\alpha x + \beta y) &= \left\vert \alpha \right\vert^2\Phi(x) + \alpha\bar{\beta}\varphi(x,y) + \bar{\alpha}\beta \varphi(y,x) + \left\vert \beta \right\vert^2\Phi(y).
    \end{align*}
    Keeping $\alpha = 1$, the cases of $\beta = 1,-1,i,-i$ yield
    \begin{align*}
      \Phi(x+y) &= \Phi(x) + \varphi(x,y) + \varphi(y,x) + \Phi(y);
      -\Phi(x-y) &= -\Phi(x) + \varphi(x,y) + \varphi(y,x) - \Phi(y);
      i\Phi(x+iy) &= i\Phi(x) + \varphi(x,y) - \varphi(y,x) + i\Phi(y);
      -i\Phi(x-iy) &= -i\Phi(x) + \varphi(x,y) - \varphi(y,x) - i\Phi(y).
    \end{align*}
    Adding all these together, we get the polarization identity.
  \end{proof}
  \begin{proposition}[Passing Quadratic Forms into Operators]
    Let $Q$ be a quadratic form over a Hilbert space $\mathbf{H}$. Then, there is a unique $A\in \mathcal{B}\left(\mathbf{H}\right)$\footnote{The set of all bounded linear operators on $\mathbf{H}$ into itself.} such that $Q(\psi) = \iprod{\psi}{A\psi}$ for all $\psi \in \mathbf{H}$.\newline

    If $Q(\psi) \in \R$ for all $\psi \in \mathbf{H}$, then $A$ is self-adjoint.
  \end{proposition}
  \begin{proof}
    Since $Q$ is bounded, so too is the sesquilinear form from which it originates (which we will call $L$). Therefore, there exists a constant $C$ such that $\left\vert L(\phi,\psi) \right\vert\leq C\norm{\phi}\norm{\psi}$, and for any $\phi \in \mathbf{H}$, the linear functional $\psi \mapsto L(\phi,\psi)$ is bounded.\newline

    By the Riesz representation theorem, there exists a unique $\chi \in \mathbf{H}$ with $\norm{\chi} \leq C\norm{\phi}$, and $L(\phi,\psi) = \iprod{\chi}{\psi}$.\newline

    Define $B: \mathbf{H}\rightarrow \mathbf{H}$ by $B\phi = \chi$ --- thus meaning $L(\phi,\psi) = \iprod{B\phi}{\psi}$. We can see that $B$ is bounded linear. Setting $A = B^{\ast}$, we find the desired operator; we can see that $A$ is unique since, if $\iprod{\phi}{A\psi} = 0$ for all $\phi,\psi \in \mathbf{H}$, then $A$ is the zero operator.\newline

    If $Q(\psi)$ is real for all $\psi \in \mathbf{H}$, then
    \begin{align*}
      \iprod{\phi}{A\psi} &= L(\phi,\psi)\\
                          &= \overline{L(\psi,\phi)}\\
                          &= \overline{\iprod{\psi}{A\phi}}\\
                          &= \iprod{A\phi}{\psi},
    \end{align*}
    meaning $A$ is self-adjoint.
  \end{proof}
  \section{Classical Mechanics}%
  \subsection{Motion in $\R^{1}$}%
  Let $x(t)$ denote position. Then, $v(t) = \frac{dx}{dt} = \dot{x}(t)$ is velocity (where the $\cdot$ denotes derivative with respect to time), $a(t) = \dot{v}(t) = \ddot{x}(t)$, etc.\newline

  Considering Newton's second law, $F(x(t)) = m\ddot{x}(t)$, every exact solution requires initial conditions of $x(t_0)$ and $v(t_0)$. Solutions to Newton's second law are known as trajectories.\newline

  Considering a spring of constant $k$, $F(x) = -kx$ yields the differential equation $m\ddot{x} + kx = 0$. The general solution is
  \begin{align*}
    x(t) = a\cos(\omega t) + b\cos(\omega t),
  \end{align*}
  with $\omega = \sqrt{k/m}$ denoting the frequency. The spring is an example of a simple harmonic oscillator.
  \subsection{Conservation of Energy}%
  For a general force function $F(x)$, the kinetic energy is $\frac{1}{2}mv^2$, and the potential energy is
  \begin{align*}
    V(x) = -\int F(x)dx,
  \end{align*}
  meaning $F(x) = -\frac{dV}{dx}$. The total energy is thus found as
  \begin{align*}
    E(x,v) = \frac{1}{2}mv^2 + V(x).
  \end{align*}
  \begin{proposition}[Conservation of Energy]
  If a particle with trajectory $x(t)$ satisfies $m\ddot{x} = F(x)$, then the energy $E$ is conserved.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \frac{d}{dt}E(x(t),\dot{x}(t)) &= \frac{d}{dt}\left(\frac{1}{2}m(\dot{x}(t))^2 + V(x(t))\right)\\
                                     &= m\dot{x}(t)\ddot{x}(t) + \frac{dV}{dx}\dot{x}(t)\\
                                     &= \dot{x}(t)\left(m\ddot{x}(t) - F(x(t))\right).
    \end{align*}
  \end{proof}
  By using the conservation of energy, we can reduce the second order differential equation $F(x) = m\ddot{x}$ to a system of first order differential equations in $x(t)$ and $v(t)$ respectively:
  \begin{align*}
    \frac{dx}{dt} &= v(t)\\
    \frac{dv}{dt} &= \frac{1}{m}F(x(t)).
  \end{align*}
  If $(x(t),v(t))$ satisfies this set of equations, then $x(t)$ satisfies Newton's second law. We say the set of all possible $(x,v)$ forms the phase space for the particle in $\R^1$.\newline

  In phase space, conservation of energy implies that the set of all $(x,v)$ must lie on the level curve of the energy function: $\set{(x,v)\mid E(x,v) = E(x_0,v_0)}$.\newline

  Using the conservation of energy, we find that, though Newton's second law is a second order differential equation in time, it is actually a first order differential equation:
  \begin{align*}
    \frac{m}{2}\left(\dot{x}(t)\right)^2 + V(x(t) &= E(x(t_0),v(t_0))\\
    \dot{x}(t) &= \sqrt{\frac{2(E_0 - V(x(t)))}{m}}
  \end{align*}
  \subsection{Damping}%
  Suppose we also introduce a force that depends on velocity --- in the case of a damped simple harmonic oscillator, the equation for force changes from $F = -kx$ to $F = -kx - \gamma\dot{x}$, with $\gamma > 0$. The damping force acts in the opposite direction of velocity, meaning the particle slows down.\newline

  The equation of motion is then
  \begin{align*}
    m\ddot{x} + \gamma\dot{x} + kx = 0.
  \end{align*}
  For $\gamma$ small, the solutions are a sum sines and cosines multiplied by some exponential decay factor, but for $\gamma$ large, the solutions are only the exponential decay.\newline
  \begin{proposition}[Energy Conservation in Damped Systems]
    Suppose a particle moves along $x(t)$ that satisfies $F(x,\dot{x}) = F_1(x) - \gamma\dot{x}$, with $\frac{dV}{dx} = -F_1(x)$ and $\gamma > 0$. Then,
    \begin{align*}
      \frac{d}{dt}E(x(t),\dot{x}(t)) &= -\gamma\dot{x}(t)^2.
    \end{align*}
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{d}{dt}E(x(t),\dot{x}(t)) &= \dot{x}(t)\left(m\ddot{x}(t) - F_1(x(t))\right)\\
                                       &= \dot{x}(t)\left(m\ddot{x}(t) - (m\ddot{x}(t) + \gamma\dot{x}(t))\right)\\
                                       &= -\gamma\dot{x}(t)^2
      \end{align*}
  \end{proof}
  \subsection{Motion in $\R^n$}%
  The position of a particle $\mathbf{x} = (x_1,\dots,x_n)$ lends itself to velocity $\mathbf{v} = (v_1,\dots,v_n) = (\dot{x}_1,\dots,\dot{x}_n)$, and $\mathbf{a} = (\ddot{x}_1,\dots,\ddot{x}_n)$. Similar to in $\R^1$, Newton's second law is denoted
  \begin{align*}
    m\mathbf{\ddot{x}} = \mathbf{F}(\mathbf{x}(t),\mathbf{\dot{x}}(t)).
  \end{align*}
  \begin{proposition}[Conservation of Energy in $n$ Dimensions]
    The energy function
    \begin{align*}
      E(\mathbf{x},\mathbf{\dot{x}}) = \frac{1}{2}m\norm{\mathbf{\dot{x}}}^2 + V(\mathbf{x})
    \end{align*}
    is only satisfied where $\mathbf{F} = -\nabla V$.
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{d}{dt}\left(\frac{1}{2}m\norm{\mathbf{\dot{x}}}^2 + V(\mathbf{x})\right) &= m\sum_{j=1}^{n}\dot{x}_j\ddot{x}_j + \sum_{j=1}^{n}\frac{\partial V}{\partial x_j}\dot{x}_j(t)\\
                                                                                 &= \mathbf{\dot{x}}(t)\left(m\mathbf{\ddot{x}}(t) + \nabla V\right)\\
                                                                                 &= \dot{x}(t)\left(\mathbf{F}(x) + \nabla V(\mathbf{x})\right),
      \end{align*}
      which is equal to zero only if $-\nabla V = \mathbf{F}$.
  \end{proof}
  If $\mathbf{F}$ is a smooth $\R^n$ valued function on $U\subset \R^n$, then $\mathbf{F}$ is conservative if there exists a smooth real-valued function $V$ such that $\mathbf{F} = -\nabla V$.\newline

  In other words, $\mathbf{F}$ is conservative if $\mathbf{F}$ is a gradient field, implying that $\nabla \times \mathbf{F} = 0$.\\

  If $\mathbf{F}(\mathbf{x},\mathbf{y}) = -\nabla V(\mathbf{x}) + \mathbf{F}_{2}(\mathbf{x},\mathbf{y})$, with $\mathbf{v}\cdot \mathbf{F}_{2} = 0$ for all $\mathbf{x}$ and $\mathbf{v}$, then energy is conserved along a given trajectory.
  \subsection{Systems of Particles}%
  Let $\mathbf{x}^j = \left(x_{1}^j,x_2^j,\dots,x_n^j\right)$ denote the $j$th particle of a system of $N$ particles. Newton's second law is thus reformulated as
  \begin{align*}
    m_j\mathbf{\ddot{x}}^j = \mathbf{F}^j\left(\mathbf{x}^1,\dots,\mathbf{x}^N,\mathbf{\dot{x}}^1,\dots,\mathbf{\dot{x}}^N\right).
  \end{align*}
  The total energy is determined by
  \begin{align*}
    E(\mathbf{x}^1,\dots\mathbf{x}^N,\mathbf{v}^1,\dots,\mathbf{v}^N) &= \left(\sum_{j=1}^{N}\frac{1}{2}m_j\norm{\mathbf{v}^j}^2\right) + V(\mathbf{x}^1,\dots,\mathbf{x}^N).
  \end{align*}
  \begin{proposition}[Conservation of Energy in a System of Particles]
    
  The energy function is constant along each trajectory if $\nabla^{j}V = -\mathbf{F}^j$, where $\nabla^j$ denotes the gradient with respect to $\mathbf{x}^j$.\newline

  The force function along a simply connected domain $U$ in $\R^{nN}$ satisfies $\nabla^j V = -\mathbf{F}^j$ if and only if
  \begin{align*}
    \frac{\partial F_{k}^j}{\partial x_m^l} = \frac{\partial F_m^l}{\partial x_k^j}
  \end{align*}
  for all $j,k,l,m$.
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{dE}{dt} &= \sum_{j=1}^{N}\left(m_j\mathbf{\dot{x}}^j\cdot\mathbf{\ddot{x}}^j + \nabla^{j}V\cdot \mathbf{x}^j\right)\\
                      &= \sum_{j=1}^{N}\mathbf{\dot{x}}^j\left(m_j\mathbf{\ddot{x}}^j + \nabla^jV\right)\\
                      &= \sum_{j=1}^{N}\mathbf{\dot{x}}\left(\mathbf{F}^j + \nabla^j V\right),
      \end{align*}
      which is equal to zero if $\nabla^j V = -\mathbf{F}^j$.\newline

      Applying a higher dimension version of $\nabla \times \mathbf{F}$ to each coordinate pair $(a,b)$, we find the identity that shows $\mathbf{F}$ is a gradient field.
  \end{proof}
  \subsection{Momentum of a System of Particles}%
  The momentum of a particle $\mathbf{p}^j$ is defined by
  \begin{align*}
    \mathbf{p}^j = m_j\mathbf{\dot{x}}^j.
  \end{align*}
  Observe that $\frac{d\mathbf{p}^j}{dt} = m_j\mathbf{\ddot{x}}^j = \mathbf{F}^j$. The total momentum is then
  \begin{align*}
    \mathbf{p} &= \sum_{j=1}^{N}\mathbf{p}^j.
  \end{align*}
  Newton's third law, which states ``for every action there is an equal and opposite reaction'' applies if 
  \begin{itemize}
    \item $\displaystyle \mathbf{F}^j = \sum_{k\neq j}\mathbf{F}^{j,k}(\mathbf{x}^j,\mathbf{y}^j)$;
    \item $\displaystyle \mathbf{F}^{j,k}(\mathbf{x}_j,\mathbf{x}_k) = -\mathbf{F}^{k,j}(\mathbf{x}^k,\mathbf{x}^j)$.
  \end{itemize}
  If each $\mathbf{F}^j$ is also a conservative force, then satisfying these conditions yields potential energy in the form of
  \begin{align*}
    V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N) &= \sum_{j < k}V^{j,k}(\mathbf{x}^{j} - \mathbf{x}^{k}).
  \end{align*}
  \begin{proposition}[Newton's Third Law and Conservation of Momentum]
  If the system of particles satisfies the conditions of
  \begin{itemize}
    \item $\displaystyle \mathbf{F}^j = \sum_{k\neq j}\mathbf{F}^{j,k}(\mathbf{x}^j,\mathbf{y}^j)$
    \item and $\displaystyle \mathbf{F}^{j,k}(\mathbf{x}_j,\mathbf{x}_k) = -\mathbf{F}^{k,j}(\mathbf{x}^k,\mathbf{x}^j)$,
  \end{itemize}
  then total momentum is conserved.
  \end{proposition}
  \begin{proof}
      \begin{align*}
        \frac{d\mathbf{p}}{dt} &= \sum_{j=1}^{N}\mathbf{F^{j}}\\
                            &= \sum_{j=1}^{N}\sum_{k\neq j}\mathbf{F}^{j,k}(\mathbf{x}^j,\mathbf{x}^{k}),
      \end{align*}
      and since $F^{j,k}(\mathbf{x}^j,\mathbf{x}^k) + \mathbf{F}^{k,j}(\mathbf{x}^k,\mathbf{x}^j) = 0$, we find $\frac{d\mathbf{p}}{dt} = 0$.
  \end{proof}
  \begin{proposition}[Translation Invariance of Potential]
  Let $V$ denote the potential for a conservative force. Then, momentum is conserved if and only if $V$ is translation invariant, meaning that for all $\mathbf{a}\in \R^n$,
  \begin{align*}
    V(\mathbf{x}^1 + \mathbf{a},\mathbf{x}^2 + \mathbf{a},\dots,\mathbf{x}^N+\mathbf{a}) &= V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N).
  \end{align*}
  \end{proposition}
  \begin{proof}
  Let $\mathbf{a} = t\mathbf{e}_k$. Then, differentiating at $t=0$ with respect to $t$, we find
      \begin{align*}
        0 &= \sum_{j=1}^{N}\frac{\partial V}{\partial x_{k}^j}\\
          &= -\sum_{j=1}^{N}F^{j}_k\\
          &= -\sum_{j=1}^{N}\frac{dp_{k}^j}{dt}\\
          &= -\frac{dp_k}{dt},
      \end{align*}
      with $p_k$ denoting the $k$th component of $\mathbf{p}$. Therefore, $\mathbf{p}$ is constant in time.\newline

      If $\mathbf{p}$ is conserved, then the sum of all forces is $0$ at each point for all $t$, meaning that for all $t$,
      \begin{align*}
        \frac{d}{dt}V(\mathbf{x}^1 + t\mathbf{a} , \mathbf{x}^2 + t\mathbf{a},\dots,\mathbf{x}^N + t\mathbf{a}) &= \sum_{j=1}^{N}\nabla^jV(\mathbf{x}^1 + t\mathbf{a},\mathbf{x}^2 + t\mathbf{a},\dots,\mathbf{x}^n + t\mathbf{a})\cdot \mathbf{a}\\
                                                                                              &= -\left(\sum_{j=1}^{N}\mathbf{F}^j(\mathbf{x}^1 + t\mathbf{a}, \mathbf{x}^2 + t\mathbf{a},\dots,\mathbf{x}^N + t\mathbf{a})\right)\cdot \mathbf{a}\\
                                                                                              &= 0
      \end{align*}
      meaning $V$ is equal at $t=0$ and $t=1$.
  \end{proof}
  \subsection{Center of Mass}%
  For a system of $N$ particles, the center of mass is denoted
  \begin{align*}
    \mathbf{c} &= \sum_{j=1}^{N}\frac{m_j}{\sum_{j=1}^{N}m_j}\mathbf{x}_j.
  \end{align*}
  We denote $\sum_{j=1}^{N}m_j = M$. Differentiating $\mathbf{c}$, we get
  \begin{align*}
    \frac{d\mathbf{c}}{dt} &= \frac{1}{M}\sum_{j=1}^{N}m_j\mathbf{\dot{x}}^j\\
                        &= \frac{\mathbf{p}}{M}.
  \end{align*}
  Notice that if $ \mathbf{p} $ is conserved, then $ \mathbf{c}(t) = \mathbf{c}(t_0) + (t-t_0)\frac{ \mathbf{p} }{M}$.\newline

  For a system of two particles, if $V( \mathbf{x}^1, \mathbf{x}^2 )$ is invariant under translation, then $V( \mathbf{x}^1, \mathbf{x}^2 ) = \tilde{V}( \mathbf{x}^1- \mathbf{x}^2 )$, and $\tilde{V}(\mathbf{a}) = V\left(\mathbf{a},0\right)$.\newline

  The positions $\mathbf{x}^1$ and $\mathbf{x}^2$ can be recovered from knowledge about $\mathbf{c}$ and the relative position $\mathbf{y} := \mathbf{x}^1 - \mathbf{x}^2$:
  \begin{align*}
    \mathbf{x}^1 &= \frac{\mathbf{c} + m_2\mathbf{y}}{m_1 + m_2}\\
    \mathbf{x}^2 &= \frac{\mathbf{c} - m_1\mathbf{y}}{m_1 + m_2}.
  \end{align*}
  Thus, we can calculate
  \begin{align*}
    \mathbf{\ddot{y}} &= \mathbf{\ddot{x}}^1 - \mathbf{\ddot{x}}^2\\
                      &= -\frac{1}{m_1}\nabla\tilde{V}\left(\mathbf{x}^1 - \mathbf{x}^2\right) - \frac{1}{m_2}\nabla\tilde{V}\left(\mathbf{x}^1 - \mathbf{x}^2\right).
  \end{align*}
  \subsubsection{Motion of Relative Position under Translation Invariant Potential}%
  For a two particle system with translation invariant potential, the relative position $\mathbf{y} = \mathbf{x}^1 - \mathbf{x}^2$ is a solution to the differential equation
  \begin{align*}
    \mu\mathbf{\ddot{y}} &= -\nabla\tilde{V}(\mathbf{y}),
  \end{align*}
  where
  \begin{align*}
    \mu &= \frac{m_1m_2}{m_1 + m_2}.
  \end{align*}
  This implies that when momentum is conserved, the relative position of the two particle system evolves as a one-particle system with effective mass $\mu$.
  \subsection{Angular Momentum}%
  A particle moving in $\R^2$ with position $\mathbf{x}$, velocity $\mathbf{v}$, and momentum $\mathbf{p} = m\mathbf{v}$ has angular momentum $J$ denoted as
  \begin{align*}
    J = x_1p_2 - x_2p_1,
  \end{align*}
  or $J = \norm{\mathbf{x}\times \mathbf{p}} = \norm{\mathbf{x}}\norm{\mathbf{p}}\sin\phi$, with $\phi$ measured counterclockwise. In polar coordinates, we find
  \begin{align*}
    J &= mr^2\frac{d\theta}{dt}\\
      &= 2M\frac{dA}{dt},
  \end{align*}
  where $A= 1/2 \int r^2 d\theta$ denotes the area swept out by $\mathbf{x}(t)$.
  \begin{proposition}[Conservation of Angular Momentum]
    Suppose a particle of mass $m$ is moving in $\R^2$ under the influence of a conservative force with potential $V(\mathbf{x})$. $V$ is invariant under rotation if and only if $J$ is conserved.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \frac{dJ}{dt} &= \frac{dx_1}{dt}p_2 + x_1\frac{dp_2}{dt} - \frac{dx_2}{dt}p_1 - x_2\frac{dp_1}{dt}\\
                    &= \frac{1}{m}p_1p_2 - x_1\frac{\partial V}{\partial x_2} - \frac{1}{m}p_2p_1 + x_2\frac{\partial V}{\partial x_1}\\
                    &= x_2\frac{\partial V}{\partial x_1} - x_1\frac{\partial V}{\partial x_2}.
    \end{align*}
    Alternatively, consider $R_{\theta} = \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}$. Differentiating $V$ along $R_{\theta}$, we get
    \begin{align*}
      \frac{d}{d\theta}V(R_{\theta}\mathbf{x})\biggr\vert_{\theta = 0} &= \frac{\partial V}{\partial x}\frac{dx}{d\theta} + \frac{\partial V}{\partial y}\frac{dy}{d\theta}\\
                                                                       &= -x_2\frac{\partial V}{\partial x_1} + x_1\frac{\partial V}{\partial x_2}\\
                                                                       &= -\frac{dJ}{dt}\left(\mathbf{x}\right)
    \end{align*}
    Thus, $\frac{dJ}{dt} = 0$ if and only if the angular derivative of $V$ is zero.
  \end{proof}
  As a result of the conservation of angular momentum, we thus get Kepler's Second Law: if $\mathbf{x}(t)$ is the trajectory of a particle under the influence of a force with rotationally invariant potential, then the area swept out by $\mathbf{x}(t)$ between $t=a$ and $t=b$ is $\frac{b-a}{2m}J$.\newline

  In $\R^3$, $\mathbf{J}$ is a vector given by $\mathbf{x}\times \mathbf{p}$. Meanwhile, in $\R^n$, the angular momentum is a skew-symmetric matrix defined by
  \begin{align*}
    J_{jk} &= x_{j}p_k - x_kp_j.
  \end{align*}
  The total angular momentum of a system of $N$ particles in $\R^n$ is given by $\mathbf{J}$ with entries
  \begin{align*}
    J_{jk} &= \sum_{l=1}^{N}\left(x_{j}^lp_{k}^l - x_{k}^l - p_{j}^l\right).
  \end{align*}
  Similar to the case of linear momentum, angular momentum is constant in the presence of a conservative force if and only if the potential function $V$ is rotationally invariant. That is,
  \begin{align*}
    V(R\mathbf{x}^1,R\mathbf{x}^2,\dots,R\mathbf{x}^N) &= V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N)
  \end{align*}
  for all rotation matrices $R$.
  \subsection{Hamiltonian Mechanics}%
  The Hamiltonian is the total energy function, but formulated in terms of position and momentum rather than position and velocity. If a particle in $\R^n$ has the usual energy function, we write
  \begin{align*}
    H(\mathbf{x},\mathbf{p}) &= \frac{1}{2m}\sum_{j=1}^{n}p_j^2 + V(\mathbf{x}),
  \end{align*}
  where $p_j = m_j\dot{x}_j$. Observe that the equations of motion can be written as
  \begin{align*}
    \frac{dx_j}{dt} &= \frac{\partial H}{\partial p_j}\\
    \frac{dp_j}{dt} &= -\frac{\partial H}{\partial x_j}.
  \end{align*}
  In the basic formulation, we can see that the first equation is just $\dot{x}_j = p_j/m$, and $\dot{p}_j = F_j$.The equations of motion written with Hamiltonians are known as Hamilton's equations.
  \subsubsection{Poisson Bracket}%
  Let $f$ and $g$ be two smooth functions on $\R^{2n}$, with each element of $\R^{2n}$ being denoted by $(\mathbf{x},\mathbf{p})$. The Poisson bracket of $f$ and $g$ is equal to
  \begin{align*}
    \set{f,g} (\mathbf{x},\mathbf{p}) &= \sum_{j=1}^{n}\left(\frac{\partial f}{\partial x_j}\frac{\partial g}{\partial p_j} - \frac{\partial f}{\partial p_j}\frac{\partial g}{\partial x_j}\right).
  \end{align*}
  The Poisson bracket satisfies the following properties:
  \begin{itemize}
    \item Linearity: $\set{f,g+ch} = \set{f,g} + c\set{f,h}$
    \item Antisymmetry: $\set{g,f} = -\set{f,g}$
    \item Product Rule: $\set{f,gh} = \set{f,g}h + g\set{f,h}$
    \item Jacobi Identity: $\set{f,\set{g,h}} + \set{h,\set{f,g}} + \set{g,\set{h,f}} = 0$.
  \end{itemize}
  It can be easily verified that the following Poisson bracket relations hold:
  \begin{align*}
    \set{x_j,x_k} &= 0\\
    \set{p_j,p_k} &= 0\\
    \set{x_j,p_k} &= \delta_{jk},
  \end{align*}
  where $\delta_{jk}$ denotes the Kronecker delta function.
  \begin{proposition}[Solutions of Hamilton's Equations]
    If $(\mathbf{x}(t),\mathbf{p}(t))$ is a solution to Hamilton's Equations, then for any smooth $f$ on $\R^{2n}$, we have
    \begin{align*}
      \frac{df}{dt} &= \set{f,h}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    \begin{align*}
      \frac{df}{dt} &= \sum_{j=1}^{n}\left(\frac{\partial f}{\partial x_j}\frac{dx_j}{dt} + \frac{\partial f}{\partial p_j}\frac{dp_j}{dt}\right) \\
                    &= \sum_{j=1}^{n}\left(\frac{\partial f}{\partial x_j}\frac{\partial H}{\partial p_j}+ \frac{\partial f}{\partial p_j}\left(-\frac{\partial H}{\partial x_j}\right)\right)\\
                    &= \set{f,H}.
    \end{align*}
  \end{proof}
  \subsubsection{Conserved Quantities}%
  Let $f\in C^{1}(\R^{2n})$ be called conserved if $f(\mathbf{x}(t),\mathbf{p}(t))$ is independent of $t$ for each solution to Hamilton's equation. Then, $f$ is a conserved quantity if and only if
  \begin{align*}
    \set{f,H} &= 0.
  \end{align*}
  Note that $H$ is also a conserved quantity.
  \subsubsection{Flow and Liouville's Theorem}%
  Solving Hamilton's equations on $\R^{2n}$ yields a flow $\Phi_t$\footnote{the $\Phi_t$ are diffeomorphisms, or differentiable isomorphisms with differentiable inverses} with $\Phi_t(\mathbf{x},\mathbf{p})$ equal to the solution at time $t$ with initial condition $(\mathbf{x},\mathbf{p})$.\newline

  The $\Phi_t$ aren't necessarily defined on all of $\R^{2n}$, but if $\Phi_t$ is defined on $\R^{2n}$ for all $t$, then we say $\Phi_t$ is complete.
  \begin{proposition}[Liouville\footnote{not from complex analysis}]
    The Hamiltonian flow preserves the $2n$-dimensional measure.
    \begin{align*}
      dx_1 dx_2 \cdots dx_n dp_1 dp_2 \cdots dp_n.
    \end{align*}
    More specifically, if $E$ is a measurable subset of the domain of $\Phi_t$, then $\mu\left(\Phi_t(E)\right) = \mu(E)$.
  \end{proposition}
  \begin{proof}
    Hamilton's equations can be written as
        \begin{align*}
          \frac{d}{dt} \begin{bmatrix}x_1\\\vdots\\x_n\\p_1\\\vdots\\p_n\end{bmatrix} &= \begin{bmatrix}\frac{\partial H}{\partial p_1}\\\vdots \frac{\partial H}{\partial p_n}\\-\frac{\partial H}{\partial x_1}\\\vdots\\-\frac{\partial H}{\partial x_n}\end{bmatrix}.
        \end{align*}
        Hamilton's equations describe the flow along the vector field appearing on the right side --- by a result in vector calculus,\footnote{Author's Note: I do not know this result yet, but hopefully I will soon!} the flow preserves the $2n$-dimensional area measure if and only if the divergence of the vector field is zero.
        \begin{align*}
          \nabla \cdot \begin{bmatrix}\frac{\partial H}{\partial p_1}\\\vdots \frac{\partial H}{\partial p_n}\\-\frac{\partial H}{\partial x_1}\\\vdots\\-\frac{\partial H}{\partial x_n}\end{bmatrix} &= \sum_{k=1}^{n}\frac{\partial}{\partial x_k}\frac{\partial H}{\partial p_{k}} - \frac{\partial}{\partial p_k}\frac{\partial H}{\partial x_{k}}\\
         &= \sum_{k=1}^{n}\frac{\partial^{2} H}{\partial x_k \partial p_k} - \frac{\partial^2 H}{\partial p_k\partial x_k}\\
          &= 0
        \end{align*}
  \end{proof}
  The condition of zero divergence is equivalent to $\Phi_t$ preserving a particular symplectic form $\omega$ defined by
  \begin{align*}
    \omega\left((\mathbf{x},\mathbf{p}),(\mathbf{x}',\mathbf{p}')\right) &= \mathbf{x}\cdot p' - \mathbf{p}\cdot x',
  \end{align*}
  meaning that for any $t$ and any $(\mathbf{x},\mathbf{p})\in \R^{2n}$, the partial derivatives of $\Phi_t$ preserves $\omega$.\newline

  Alternatively, this is equivalent to $\Phi_t$ preserving Poisson brackets:
  \begin{align*}
    \set{f\circ \Phi_t,g\circ\Phi_t} &= \set{f,g}\circ \Phi_t.
  \end{align*}
  Thus, $\Phi_t$ is an example of a symplectomorphism.
  \subsubsection{Hamiltonian Flow and Hamiltonian Generators}%
  We say $f\in C^{1}(\R^{2n})$ is the Hamiltonian generator of the flow that results from solving Hamilton's equations with $f$ substituted for $H$:
  \begin{align*}
    \frac{dx_j}{dt} &= \frac{\partial f}{\partial p_j}\\
    \frac{dp_j}{dt} &= -\frac{\partial f}{\partial x_j}.
  \end{align*}
  It is possible to see that
  \begin{align*}
    f_{\mathbf{a}}(\mathbf{x},\mathbf{p}) &= \mathbf{a}\cdot \mathbf{p}
  \end{align*}
  yields the flow
  \begin{align*}
    \mathbf{x}(t) &= \mathbf{x}_0 + t\mathbf{a}\\
    \mathbf{p}(t) &= \mathbf{p}_0,
  \end{align*}
  and
  \begin{align*}
    g_{\mathbf{b}}(\mathbf{x},\mathbf{p}) &= \mathbf{b}\cdot \mathbf{x}
  \end{align*}
  yields the flow
  \begin{align*}
    \mathbf{x}(t) &= \mathbf{x}_0\\
    \mathbf{p}(t) &= \mathbf{p}_0 - t\mathbf{b}.
  \end{align*}
  Thus, the Hamiltonian flow generated by momentum yields translation in position, and the Hamiltonian flow generated by position yields translation in momentum.\newline

  In this light, we can think of \textit{the} Hamiltonian as the Hamiltonian generator that yields time evolution.Other Hamiltonian generators represent some other family of symmetries of the system.
  \begin{proposition}[Hamiltonian Flow generated by Angular Momentum]
    For a particle moving in $\R^2$, the Hamiltonian  flow generated by 
    \begin{align*}
      J(\mathbf{x},\mathbf{p}) &= x_1p_2 - x_2p_1
    \end{align*}
    consists of simultaneous rotations of $\mathbf{x}$ and $\mathbf{p}$.
    \begin{align*}
      \begin{bmatrix}x_1(t)\\x_2(t)\end{bmatrix} &= \begin{bmatrix}\cos t & -\sin t\\\sin t& \cos t\end{bmatrix} \begin{bmatrix}x_1(0)\\x_2(0)\end{bmatrix}\\
      \begin{bmatrix}p_1(t)\\p_2(t)\end{bmatrix} &= \begin{bmatrix}\cos t & -\sin t \\ \sin t & \cos t\end{bmatrix} \begin{bmatrix}p_1(0) \\ p_2(0)\end{bmatrix}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Plugging $J$ Hamilton's equations, we get
        \begin{align*}
          \frac{dx_1}{dt} &= \frac{\partial J}{\partial p_1} = -x_2\\
          \frac{dp_1}{dt} &= -\frac{\partial J}{\partial x_1} = -p_2\\
          \frac{dx_2}{dt} &= \frac{\partial J}{\partial p_2} = x_1\\
          \frac{dp_2}{dt} &= -\frac{\partial J}{\partial x_2} = p_1.
        \end{align*}
  \end{proof}
  It's important to note that the parameter $t$ in the Hamiltonian flow for $J$ is the rotation, not time. That is, $J$ is the Hamiltonian generator of rotations.\newline

  If $f$ is any smooth function, it is the case that the time derivative of any other function $g$ along the Hamiltonian flow generated by $f$ is $\frac{dg}{dt} = \set{g,f}$. In particular, the derivative of $H$ along the flow generated by $f$ is $\set{H,f}$, meaning that $f$ is constant along the flow generated by $H$ if and only if $\set{f,H} = 0$, which is true if and only if $H$ is constant along the flow generated by $H$.\newline

  Thus, we find that $f$ is conserved for solutions of Hamilton's equations if and only if $H$ is invariant under the Hamiltonian flow generated by $f$. Of particular note, we find that $J$ is conserved if and only if $H$ is invariant under rotations of $\mathbf{x}$ and $\mathbf{p}$.\footnote{There is another section on Kepler's Laws in the chapter on Classical Mechanics that I didn't really read in depth. I might include it in the future.}
  %\subsection{Kepler's Problem}%
  %Consider an orbit, where the sun with mass $M$ exerts a force $\mathbf{F}$ on a planet with mass $m$. Then, by Newton's universal law of gravitation, the force is found by
  %\begin{align*}
  %  \mathbf{F} &= -GmM \frac{\mathbf{x}}{\norm{\mathbf{x}}^3},
  %\end{align*}
  %with $G$ equal to the gravitational constant. We denote $k = GmM$, and find that in Newton's second law,
  %\begin{align*}
  %  m\mathbf{\ddot{x}} &= -GmM\frac{\mathbf{x}}{\norm{\mathbf{x}}^3}\\
  %  \mathbf{\ddot{x}} &= -GM\frac{\mathbf{x}}{\norm{\mathbf{x}}^3}.
  %\end{align*}
  %The potential associated with $\mathbf{F}$ is
  %\begin{align*}
  %  V(\mathbf{x}) &= -\frac{k}{\norm{\mathbf{x}}}.
  %\end{align*}
  %Since $V$ is invariant under rotations, $\mathbf{J} = \mathbf{x}\times \mathbf{p}$ will always be constant and perpendicular to $\mathbf{x}(t)$. We call the plane perpendicular to $\mathbf{J}$ the plane of motion.
  %\subsection{Runge--Lenz Vector}%
  %We define the Runge--Lenz vector on $\R^{3}\setminus\{0\} \times \R^{3}$ by
  %\begin{align*}
  %  \mathbf{A}(\mathbf{x},\mathbf{p}) &= \frac{1}{mk}\mathbf{p}\times \mathbf{J} - \frac{\mathbf{x}}{\norm{\mathbf{x}}},
  %\end{align*}
  %where $\mathbf{x}$ represents position and $\mathbf{p}$ represents momentum. Recall that $k = GmM$.
  %\begin{proposition}[Runge--Lenz Vector under Orbit]
  %  The Runge--Lenz vector is a conserved quantity for the orbit.
  %\end{proposition}
  %\begin{proof}
  %  \begin{align*}
  %    \mathbf{\dot{A}}(t) &= \frac{1}{mk}\mathbf{F}\times \mathbf{J} - \frac{1}{\norm{\mathbf{x}}}\frac{\mathbf{p}}{m} + \frac{\mathbf{x}}{\norm{\mathbf{x}}^2}\sum_{j=1}^{3}\frac{\partial\norm{\mathbf{x}}}{\partial x_j}\frac{dx_j}{dt}\\
  %                        &= -\frac{1}{m}\norm{\mathbf{x}}^3\mathbf{x}\times \left(\mathbf{x}\times \mathbf{p}\right) - \frac{1}{\norm{\mathbf{x}}}\frac{\mathbf{p}}{m} + \frac{\mathbf{x}}{\norm{\mathbf{x}}^2}\sum_{j=1}^{3}\frac{x_j}{\norm{\mathbf{x}}}\frac{p_j}{m}\\
  %                        &= \frac{1}{m}\left(-\frac{1}{\norm{\mathbf{x}}^3}\mathbf{x}(\mathbf{x}\cdot \mathbf{p}) + \frac{1}{\norm{\mathbf{x}}^3}\mathbf{p}(\mathbf{x}\cdot \mathbf{x}) - \frac{\mathbf{p}}{\norm{\mathbf{x}}} + \frac{(\mathbf{x}\cdot \mathbf{p})}{\norm{\mathbf{x}}^3}\right)\\
  %                        &= 0
  %  \end{align*}
  %\end{proof}
  %\subsubsection{Trajectories for the Kepler Problem}%
  %The magnitude of the Runge--Lenz vector $\mathbf{A}$ is found by
  %\begin{align*}
  %  \norm{\mathbf{A}}^2 &= 1 + \frac{2\norm{\mathbf{J}}^2}{mk^2}E,
  %\end{align*}
  %where $\displaystyle E = \frac{\norm{\mathbf{p}}^2}{2m} - \frac{k}{\norm{\mathbf{x}}}$.\\

  %Additionally, if $\mathbf{\hat{x}} = \frac{\mathbf{x}}{\norm{\mathbf{x}}}$, then
  %\begin{align*}
  %  \mathbf{A}\cdot \mathbf{\hat{x}} &= \frac{\norm{\mathbf{J}}^2}{mk\norm{\mathbf{x}}}-1
  %\end{align*}
  %for all nonzero $\mathbf{x}$. Thus,
  %\begin{align*}
  %  \norm{\mathbf{x}} &= \frac{\norm{\mathbf{J}}^2}{mk(1+\mathbf{A}\cdot \mathbf{\hat{x}})}.
  %\end{align*}
  \section{Introduction to Quantum Mechanics}%
  Observable quantities such as position and momentum in quantum mechanics are represented by operators on a complex-valued Hilbert space (an inner product space that is complete with respect to the induced metric) --- specifically, these quantities are \textit{unbounded} linear operators.\newline

  In physics, the inner product is linear in the second factor and conjugate linear in the first factor:\footnote{Notice that this is different with math, where the inner product is linear in the first factor and conjugate linear in the second factor.} 
  \begin{align*}
    \iprod{\phi}{\lambda \psi} &= \lambda\iprod{\phi}{\psi}\\
    \iprod{\lambda\phi}{\psi} &= \overline{\lambda}\iprod{\phi}{\psi}.
  \end{align*}
  \subsection{A Taste of Operator Theory}%
  A linear operator $A: \mathbf{H} \rightarrow \mathbf{H}$ is bounded if it has finite operator norm:\footnote{I'm using more operator-theoretic language than the book uses because I'm \xcancel{pretentious} a mathematician, not a physicist.}
  \begin{align*}
    \sup_{\norm{\psi} \leq 1}\norm{A\psi} < \infty.
  \end{align*}
  For each bounded operator $A$, there exists a unique bounded operator $A^{\ast}$ such that $\iprod{\phi}{A\psi} = \iprod{A^{\ast}\phi}{\psi}$. The existence of $A^{\ast}$ follows from the Riesz representation theorem.\newline

  A bounded operator is said to be self-adjoint if $A^{\ast} = A$. Self-adjoint operators are nice for a variety of reasons, and as a result we desire for our operators in quantum mechanics to be self-adjoint. However, this brings a significant problem --- unbounded self-adjoint operators are not necessarily defined on $\mathbf{H}$.\newline

  We define unbounded operators as linear operators defined on a dense subspace of $\mathbf{H}$:
  \begin{align*}
    A: \text{Dom}(A) \subseteq \mathbf{H} &\rightarrow \mathbf{H}\newline
    \intertext{subject to}
    \overline{\text{Dom}(A)} &= \mathbf{H}.
  \end{align*}
  In addition to the domain of $A$ not necessarily being equal to $\mathbf{H}$, the linear functional $\iprod{\phi}{A\cdot}$ is not necessarily bounded (meaning we cannot use the Riesz representation theorem to find $A^{\ast}\phi$). The adjoint of $A$, as a result, will be defined on a subspace of $\mathbf{H}$.\newline

  A vector $\phi \in \mathbf{H}$ is said to belong to the domain $\text{Dom}(A^{\ast})$ if the linear functional $\iprod{\phi}{A\cdot}$ on $\text{Dom}(A)$ is bounded. Then, we define $A^{\ast}$ to be the unique vector $\chi$ such that $\iprod{\chi}{\psi} = \iprod{\phi}{A\psi}$ for all $\psi \in \text{Dom}(A)$.\newline

  Having defined adjoints of an unbounded operator, we can now commit to defining self-adjoint operators. The operator $A$ is symmetric if $\iprod{\phi}{A\psi} = \iprod{A\phi}{\psi}$ --- a symmetric operator is self-adjoint if $\text{Dom}(A) = \text{Dom}(A^{\ast})$ and $A^{\ast}\phi = A\phi$ for all $\phi \in \text{Dom}(A)$. Finally, $A$ is essentially self-adjoint if the closure of the graph of $A$ in $\mathbf{H}\times \mathbf{H}$ is self-adjoint.\newline

  In sum, $A$ is self-adjoint if $A$ and $A^{\ast}$ are the same operator with the same domain, more or less.
  \begin{definition}[Properties of Symmetric Operators]
    Let $A$ be a symmetric operator on $\mathbf{H}$. Then, the following hold:
    \begin{enumerate}[(1)]
      \item For all $\psi\in \text{Dom}(A)$, the quantity $\iprod{\psi}{A\psi}$ is real. More generally, if $\psi, A\psi, \dots, A^{m-1}\psi$ belong to $\text{Dom}(A)$, then $\iprod{\psi}{A^{m}\psi}$ is real.
      \item Suppose $\lambda$ is an eigenvector for $A$. Then, $\lambda \in \R$.
    \end{enumerate}
  \end{definition}
  \begin{proof}
    \begin{enumerate}[(1)]
      \item Since $A$ is symmetric,
        \begin{align*}
          \iprod{\psi}{A\psi} &= \iprod{A\psi}{\psi}\\
                              &= \overline{\iprod{\psi}{A\psi}},
        \end{align*}
        for all $\psi \in \text{Dom}(A)$. Similarly, if $\psi,A\psi,\dots,A^{m-1}\psi\in \text{Dom}(A)$, then we use the symmetry of $A$ to show that
        \begin{align*}
          \iprod{\psi}{A^{m}\psi} &= \iprod{A^{m}\psi}{\psi}\\
                                  &= \overline{\iprod{\psi}{A^{m}\psi}}.
        \end{align*}
      \item If $\psi$ is an eigenvector for $A$ with eigenvalue $\lambda$, then
        \begin{align*}
          \lambda \iprod{\psi}{\psi} &= \iprod{\psi}{A\psi}\\
                                     &= \iprod{A\psi}{\psi}\\
                                     &= \overline{\lambda}\iprod{\psi}{\psi}.
        \end{align*}
        Since $\psi$ is nonzero by definition, it must be the case that $\lambda = \overline{\lambda}$.
    \end{enumerate}
  \end{proof}
  In physical terms, $\iprod{\psi}{A\psi}$ represents the expected value for measurements of $A$ in the state $\psi$, with $\lambda$ representing a possible value of this measurement. This is why we want both numbers to be real.\newline

  A self-adjoint $A$ allows us to use the spectral theorem to assign each $\psi \in \mathbf{H}$ a probability measure on the real numbers.
  \subsection{Position and Momentum Operators}%
  Consider a particle moving along the real line with wave function $\psi: \R\rightarrow \C$. Although $\psi$ will evolve over time, let the particle be fixed in time for now.\newline

  We want to define $\psi$ to be a unit vector in $L^{2}(\R)$, meaning
  \begin{align*}
    \int_{\R}\left\vert \psi(x) \right\vert^2dx &= 1.
  \end{align*}
  The probability that the position of the particle belongs to some $E\subseteq \R$ is
  \begin{align*}
    \int_{E}\left\vert \psi(x) \right\vert^2dx,
  \end{align*}
  where $E$ is necessarily a Lebesgue-measurable set.\newline

  The expectation value of the position is thus
  \begin{align*}
    E(x) &= \int_{\R}x\left\vert \psi(x) \right\vert^2dx,
  \end{align*}
  assuming the convergence of the integral, and the $m$th moment of the position is calculated as
  \begin{align*}
    E(x^m) &= \int_{\R}x^m\left\vert \psi(x) \right\vert^2dx,
  \end{align*}
  again assuming convergence of the integral.
  \begin{definition}[Position Operator]
    The position operator is defined as $X = M_{x}$, meaning $(X\psi)(x) = x\psi(x)$. With this in mind, we can then see that
    \begin{align*}
      E(x) &= \iprod{\psi}{X\psi}
    \end{align*}
    under the standard inner product on $L^{2}(\R)$. The expectation value of $X$ for the state $\psi$ is denoted $\langle X \rangle_{\psi} := \iprod{\psi}{X\psi}$.\footnote{I don't like this notation either.}\newline

    The higher moments of position are similarly defined:
    \begin{align*}
      E(x^m) &= \iprod{\psi}{X^{m}\psi},
    \end{align*}
    where $X^{m}$ denotes $m$-degree composition of $X$.\newline

    Since $X$ is an unbounded linear operator, it is not necessarily the case that $X\psi \in L^{2}(\R)$ if $\psi\in L^{2}(\R)$.\footnote{Other famous examples of unbounded linear operators on Hilbert spaces include the derivative operator on $A^{2}(\mathbb{D})$, the space of holomorphic functions on the complex unit disc. I'm doing research on properties of variations of this space.}
  \end{definition}
  Momentum is encoded in the oscillations of the wave function --- the de Broglie hypothesis provides a special relationship between the frequency of oscillation (as a function of position at a fixed time) and the momentum.
  \begin{proposition}[De Broglie Hypothesis]
    If the wave function of a particle has spatial frequency $k$, then the momentum $p$ of the particle is
    \begin{align*}
      p &= \hbar k,
    \end{align*}
    where $\hbar$ denotes Planck's constant.
  \end{proposition}
  To be more precise, the de Broglie hypothesis applies to wave functions of the form $\psi(x) = e^{ikx}$, which represents particles that have momentum $p = \hbar k$. Let's develop this a bit further.\newline

  Since $e^{ikx}$ is not square integrable over $\R$, we instead move to the circle, where $\psi$ has period $2\pi$ over $\R$, and
  \begin{align*}
    \int_{0}^{2\pi}\left\vert \psi(x) \right\vert^2 &= 1.
  \end{align*}
  For any integer $k$, the normalized wave function $\frac{e^{ikx}}{\sqrt{2\pi}}$ will represent the particle with momentum $p = \hbar k$. This momentum value is definite; that is, $p = \hbar k$ with probability $1$ for a particle with wave function $\frac{e^{ikx}}{\sqrt{2\pi}}$.\newline

  Of note, the functions $\set{\frac{e^{ikx}}{\sqrt{2\pi}}}$ for $k\in \Z$ form an orthonormal basis for the Hilbert space of square integrable functions with period $2\pi$.\newline

  The wave functions for particles on a circle are thus all of the form
  \begin{align*}
    \psi(x) &= \sum_{k=-\infty}^{\infty} a_k\frac{e^{ikx}}{\sqrt{2\pi}},
  \end{align*}
  where the sum is convergent in $L^{2}([0,2\pi])$.\footnote{You may recognize these as Fourier series.} If $\psi$ is a unit vector, then\footnote{We use Parseval's identity to relate the $L^{2}$ norm of $\psi$ to the $\ell^{2}$ norm of $\set{a_k}_{k\in \Z}$. Try proving it yourself! Hint: use the Pythagorean theorem.}
  \begin{align*}
    \norm{\psi}_{L^{2}([0,2\pi])}^2 &= \sum_{k=-\infty}^{\infty}|a_k|^2\\
                                    &= 1.
  \end{align*}
  For a particle with wave function $\psi$ expressed as a Fourier series, the momentum isn't definite. We will have to consider that measurement will yield one of the values of $\hbar k$ with probability $|a_k|^2$.
  \begin{align*}
    E(p) &= \sum_{k=-\infty}^{\infty}\hbar k |a_k|^2,
  \end{align*}
  with higher moments defined by
  \begin{align*}
    E(p^m) &= \sum_{k=-\infty}^{\infty}(\hbar k)^{m}|a_k|^2,
  \end{align*}
  assuming absolute convergence.
  \begin{definition}[Momentum Operator]
    To encode $P$, our momentum operator, such that for the wave function $\psi\in L^{2}([0,2\pi])$, $E(p^m) = \iprod{\psi}{P^{m}\psi}$, we must have $P$ satisfying
    \begin{align*}
      Pe^{ikx} &= \hbar k e^{ikx}.
    \end{align*}
    Thus, we would assume that
    \begin{align*}
      P &= -i\hbar \frac{d}{dx}.
    \end{align*}
    Even on the real line, we would still expect $P = -i\hbar \frac{d}{dx}$; though $e^{ikx}$ is not square integrable, we can represent any wave function $\psi \in L^{2}(\R)$ as an integral with the Fourier transform:
    \begin{align*}
      \psi(x) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{ikx}\hat{\psi}(k)dk,\\
      \hat(\psi)(x) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-ikx}\psi(x)dx.
    \end{align*}
    Plancherel's theorem\footnote{also known as Parseval's theorem when applied to Fourier series rather than the Fourier transform} states that the Fourier transform is unitary --- that is,
    \begin{align*}
      \int_{-\infty}^{\infty}|\psi(x)|^2dx = \int_{-\infty}^{\infty}\left\vert \hat{\psi}(k) \right\vert^2dk &= 1.
    \end{align*}
    We can imagine $\hat{\psi}(k)$ as the probability density for the momentum of the particle\footnote{well, for $p/\hbar$, but that's basically the same}.\newline

    Thus, we have defined the momentum operator as
    \begin{align*}
      P &= -i\hbar \frac{d}{dx},
    \end{align*}
    with, for sufficiently nice $\psi\in L^{2}(\R)$
    \begin{align*}
      E(p^{m}) &= \iprod{\psi}{P^{m}\psi}\\
               &= \int_{-\infty}^{\infty}(\hbar k)^m \left\vert \hat{\psi}(k) \right\vert^2 dk
    \end{align*}
    for all positive integers $m$.
  \end{definition}
  \begin{proposition}[Commutator of Position and Momentum]
    \begin{align*}
      [X,P] &= i\hbar I,
    \end{align*}
    a relation known as the canonical commutation relation.
  \end{proposition}
  \begin{proof}
    \begin{align*}
      PX\psi &= -i\hbar \frac{d}{dx}\left(x\psi(x)\right)\\
             &= -i\hbar\psi(x) - i\hbar x \frac{d\psi}{dx}\\
             &= -i\hbar I\psi + XP\psi
    \end{align*}
  \end{proof}
  \begin{remark}
    Note the parallel between $\set{x,p} = 1$ in the Poisson bracket and $[X,P] = i\hbar I$ in the commutator.
  \end{remark}
  \begin{proposition}[Symmetry of Position and Momentum Operators]
    For all sufficiently nice $\phi$ and $\psi$ in $L^{2}(\R)$,
    \begin{align*}
      \iprod{\phi}{X\psi} &= \iprod{X\phi}{\psi}\\
      \iprod{\phi}{P\psi} &= \iprod{P\phi}{\psi}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Let $\phi,\psi\in L^{2}(\R)$ with $x\psi(x),x\phi(x)\in L^{2}(\R)$. Then, since $x\in \R$,
    \begin{align*}
      \int_{-\infty}^{\infty}\overline{\phi(x)}x\psi(x)dx &= \int_{-\infty}^{\infty}\overline{x\phi(x)}\psi(x)dx,
    \end{align*}
    with both integrals convergent.\newline

    Meanwhile, assume $\phi$ and $\psi$ are continuously differentiable, vanish at $\pm \infty$, and $\phi,\psi,\frac{d\phi}{dx},\frac{d\psi}{dx} \in L^{2}(\R)$. Note that $\frac{d\overline{\phi}}{dx} = \overline{\frac{d\phi}{dx}}$. Then, integrating by parts,
    \begin{align*}
      -i\hbar \int_{-n}^{n}\overline{\phi(x)}\frac{d\psi}{dx}dx &= -i\hbar\overline{\phi(x)}\psi(x)\biggr\vert_{-n}^{n} + i\hbar \int_{-n}^{n}\overline{\frac{d\phi}{dx}}\psi(x)dx,
      \intertext{meaning}
      \int_{-\infty}^{\infty}\overline{\phi(x)}\left(-i\hbar \frac{d\psi}{dx}\right) dx &= i\hbar\int_{-\infty}^{\infty}\overline{\frac{d\phi}{dx}}\psi(x)dx\\
                                                                                        &= \int_{-\infty}^{\infty} \overline{\left(-i\hbar\frac{d\phi}{dx}\right)}\psi(x)dx.
    \end{align*}
  \end{proof}
  Thus, we have shown that $X$ and $P$ are symmetric operators on certain dense subspaces of $L^{2}(\R)$. We will have to wait until later to prove that $X$ and $P$ are essentially self-adjoint.
  \subsection{Kinematic Axioms of Quantum Mechanics}%
  These aren't really axioms\footnote{Physicists once again caught stealing the valor of mathematicians.} (as in, first principles from which quantum mechanics is derived), but they're essential principles of quantum mechanics. For ease of use, I'm labeling them with my personal interpretation of their content (rather than the numbering system used in the book).
  \begin{axiom}[Principle of Representation]
    The state of a quantum mechanical system is represented by $\psi \in \mathbf{H}$ for some Hilbert space $\mathbf{H}$. If $\psi_1$ and $\psi_2$ are two unit vectors in $\mathbf{H}$ with $\psi_1 = c\psi_2$ for some $c\in \C$, $\psi_1$ and $\psi_2$ represent the same state.
  \end{axiom}
  Each $\psi$ represents a pure state --- there are mixed states, but those will be discussed later.
  \begin{axiom}[Principle of Correspondence]
    To each real-valued $f$ on the classical phase space, there is an unbounded, self-adjoint operator $\hat{f}$ on the Hilbert space.
  \end{axiom}
  For a particle in $\R^{1}$, the phase space in $\R^{2}$ is represented by $(x,p)$ for $x$ position and $p$ momentum; the analogue to the classical phase space in quantum mechanics is $L^{2}(\R)$ with the position function $f(x,p) = x$ being $M_x$ (the position operator), and the momentum function $g(x,p) = p$ being $P = -i\hbar\frac{d}{dx}$ (the momentum operator).
  \begin{axiom}[Principle of Measurement]
    If a system is in a state represented by $\psi \in \mathbf{H}$, then the measurement for the $m$th moment of $f$ satisfies
    \begin{align*}
      E(f^{m}) &= \iprod{\psi}{\left(\hat{f}\right)^m\psi}.
    \end{align*}
    In particular, the measurement for $f$ is
    \begin{align*}
      E(f) &= \iprod{\psi}{\hat{f}\psi}.
    \end{align*}
  \end{axiom}
  \begin{remark}
    Note that in the quantum system, we are measuring the \textit{classical} $f$. However, rather than a definitive value, we need to find the expectation of the operator $\hat{f}$ using the probabilities derived from $\psi$.\newline

    Since $\hat{f}$ is self-adjoint, $E(f^{m})$ is real, we can construct a probability measure $\mu_{A,\psi}$ that is the probability distribution for measurements of $A$ in the state $\psi$.
  \end{remark}
  \begin{proposition}[Eigenvectors]
    If a quantum system is in a state described by $\psi\in \mathbf{H}$, and $\hat{f}\psi = \lambda\psi$ for some $\lambda\in \R$\footnote{Recall that the eigenvalues of self-adjoint operators are real.}, then
    \begin{align*}
      E(f^{m}) &= \lambda^{m}.
    \end{align*}
    The aforementioned probability measure consistent with this condition is $\delta_{\lambda}$ (where $f$ has value $\lambda$ with probability $1$).
  \end{proposition}
  Essentially, if $\psi$ is an eigenvector for $\hat{f}$, then measurements for $f$ are determined. Thus, we would need to find the probability measure such that
  \begin{align*}
    \int_{\R}x^{m}d\mu &= \lambda^{m},
  \end{align*}
  which only works if $\mu = \delta_{\lambda}$.\\

  Note that if $\psi$ is a linear combination of eigenvectors for $\hat{f}$, then the measurements of $f$ are not deterministic.\footnote{The spectral decomposition is very cool.}
  \begin{example}
    Suppose $\hat{f}$ has an orthonormal basis $\set{e_j}$ with distinct real eigenvalues $\lambda_j$. Let $\psi$ be a unit vector in $\mathbf{H}$ with the expansion
    \begin{align*}
      \psi &= \sum_{j=1}^{\infty}a_je_j.
    \end{align*}
    Then, the measurement in the state $\psi$ of $f$ will necessarily be some value of $\lambda_j$ with probability $|a_j|^2$.
  \end{example}
  \begin{axiom}[Principle of Wave Function Collapse]
    Suppose a quantum system begins in a state $\psi$, and a measurement of the observable $f$ is performed. If the measurement is $\lambda \in \R$, then immediately after the measurement, the system will be in a state $\psi'$ such that
    \begin{align*}
      \hat{f}\psi' &= \lambda \psi',
    \end{align*}
    where $\hat{f}$ is the self-adjoint operator representation of $f$.
  \end{axiom}
  \begin{remark}
    Since $\psi'$ is an eigenvector of $\hat{f}$, this means that a second measurement of $f$ occurring immediately after the first measurement will yield the value $\lambda$ with probability $1$ (see the above proposition about eigenvectors).
  \end{remark}
  The wave function collapse principle only applies to measurements very shortly after the initial measurement; the system will still evolve over time.
  \subsubsection{Hydrogen Atom Measurements and Uncertainty}%
  The Hamiltonian operator $\hat{H}$ for the hydrogen atom has eigenvalues of the form
  \begin{align*}
    -\frac{R}{n^2},
  \end{align*}
  where $R$ is the Rydberg constant and $n\in \N$ denotes a state. The negative value of these eigenvalues is important; they denote that the electron is bound to the nucleus.\newline

  If an electron is placed into a state with energy $-\frac{R}{n_1^2}$ (where $n_1 > 1$), it will eventually decay into a state with lower energy, $-\frac{R}{n_2^2}$, with $n_2 < n_1$. The process of decay releases a photon with energy
  \begin{align*}
    E_{\text{photon}} &= \frac{R}{n_2^2} - \frac{R}{n_1^2}.
  \end{align*}
  The frequency of a photon is proportional to its energy; by observing the frequency of the photon, we can determine the change in energy (and thus the values of $n_1$ and $n_2$).\newline

  A bound state for the hydrogen atom will be a linear combination of the eigenvectors of $\hat{H}$ with eigenvalues of the aforementioned form.\newline

  Just as the standard deviation of a random variable $Y$ is found by $\sigma^2 = E(Y^2) - \left(E(Y)\right)^2$, we can find the \textit{uncertainty} of a self-adjoint operator $A$, defined as
  \begin{align*}
    \left(\Delta_{\psi}A\right)^2 &= \left\langle A^2\right\rangle_{\psi} - \left(\left\langle A\right\rangle_{\psi}\right)^2,
  \end{align*}
  recalling that $\langle A \rangle_{\psi} = \langle \psi,A\psi\rangle$.\newline

  For a single $A$, we can find $\psi$ such that $\Delta_{\psi}A < \varepsilon$ for any $\varepsilon > 0$; however, if $A$ and $B$ do not commute, then $\Delta_{\psi}A$ and $\Delta_{\psi}B$ cannot both be made arbitrarily small.\newline

  In particular, we know that $X$ and $P$ do not commute; thus yields the famous Heisenberg Uncertainty Principle:
  \begin{align*}
    \left(\Delta_{\psi}X\right)\left(\Delta_{\psi}P\right) \geq \frac{\hbar}{2}
  \end{align*}
  for all $\psi$ such that $\Delta_{\psi}X$ and $\Delta_{\psi}P$ are defined.
  \subsection{The SchrÃ¶dinger Equation}%
  In the previous section, we considered the wave function $\psi$ at a fixed time; however, time is not stationary\footnote{Citation needed} and neither ought our system be, meaning we need a way to evolve the quantum system over time.\newline

  In a classical system, the Hamiltonian is the generator of time evolution\footnote{The Hamiltonian is also equal to the total energy of the system.}--- thus, by the Principle of Representation, there is an operator, $\hat{H}$, which is the Hamiltonian operator for the system.\newline

  We motivated the definition of the momentum operator via the de Broglie hypothesis, which stated that $p = \hbar k$, where $k$ is the spatial frequency of the wave function. Similarly, we can look at the relation between energy and the temporal frequency of the wave function,
  \begin{align*}
    E = \hbar \omega,
  \end{align*}
  to motivate time evolution.\newline

  Suppose that $\psi_0$ has energy $E$ (so $\psi_0$ is an eigenvector for $\hat{H}$). Then, the wave function's time dependence should be solely based on the frequency $\omega = E/\hbar$, meaning that if the $t=0$ state of the system is $\psi_0$, the state of the system at any other time $t$ should be
  \begin{align*}
    \psi(t) &= \psi_0 e^{-i\omega t}\\
            &= \psi_0 e^{-it\frac{E}{\hbar}}.
  \end{align*}
  This $\psi(t)$ is a function to the differential equation
  \begin{align*}
    \frac{d\psi}{dt} &= -\frac{iE}{\hbar}\psi\\
                     &= \frac{E}{i\hbar}\psi
  \end{align*}
  with initial value $\psi_0$. Finally, rewriting $\hat{H} = E$, we get the SchrÃ¶dinger equation.
  \begin{axiom}[SchrÃ¶dinger Equation]
    The time evolution of the wave function $\psi$ in a quantum system is generated by
    \begin{align*}
      \frac{d\psi}{dt} &= \frac{1}{i\hbar}\hat{H}\psi,
    \end{align*}
    where $\hat{H}$ denotes the Hamiltonian operator.
  \end{axiom}
  \begin{proposition}[SchrÃ¶dinger Equation for Operators]
    Let $A$ be a self-adjoint operator on $\mathbf{H}$. Assuming particular domain conditions hold, then
    \begin{align*}
      \frac{d}{dt}\langle A\rangle_{\psi(t)} &= \left\langle \frac{1}{i\hbar}[A,\hat{H}]\right\rangle_{\psi(t)},
    \end{align*}
    where $\langle A \rangle_{\psi} = \iprod{\psi}{A\psi}$ and $[A,B] = AB-BA$.
  \end{proposition}
  \begin{remark}
    Remember that for a function $f$ acting on the classical phase space, $\frac{df}{dt} = \set{f,H}$ acting along a solution to Hamilton's equations.
  \end{remark}
  \begin{proof}
    Let $\psi(t)$ be a solution to the SchrÃ¶dinger equation, with $\psi(t) \in \text{Dom}(A) \cap \text{Dom}(\hat{H})$, $A\psi(t) \in \text{Dom}(\hat{H})$, and $H\psi(t) \in \text{Dom}(A)$. Then,
    \begin{align*}
      \frac{d}{dt}\iprod{\psi(t)}{A\psi(t)} &= \iprod{\frac{d\psi}{dt}}{A\psi} + \iprod{\psi}{A\frac{d\psi}{dt}}\\
                                            &= \frac{i}{\hbar}\iprod{\hat{H}\psi}{A\psi} - \frac{i}{\hbar}\iprod{\psi}{A\hat{H}\psi}\\
                                            &= \frac{1}{i\hbar}\iprod{\psi}{[A,\hat{H}]\psi}.
    \end{align*}
  \end{proof}
  \begin{remark}
    If $[A,\hat{H}] = 0$, then nothing interesting happens; thus, for operators to yield time evolution, we need them to \textit{not} commute with $\hat{H}$.\newline

    For a particle moving in $\R$, we see that noncommutativity holds for $X$ and $P$.\newline

    If $[A,\hat{H}] = 0$, then we see that $E(A^{m})$ is independent of time, effectively meaning that $A$ is conserved.
  \end{remark}
  \begin{proposition}[Time Independence of Inner Products of Solutions to the SchrÃ¶dinger Equation]
    If $\phi(t)$ and $\psi(t)$ are solutions to the SchrÃ¶dinger equation, then $\iprod{\phi(t)}{\psi(t)}$ and $\norm{\psi(t)}$ are independent of time.
  \end{proposition}
  \begin{proof}
    Using the product rule for inner products, we find
    \begin{align*}
      \frac{d}{dt}\iprod{\phi(t)}{\psi(t)} &= \iprod{\frac{1}{i\hbar}\hat{H}\phi(t)}{\psi(t)} + \iprod{\phi(t)}{\frac{1}{i\hbar}\hat{H}\psi(t)}\\
                                           &= -\frac{1}{i\hbar}\iprod{\hat{H}\phi(t)}{\psi(t)} + \frac{1}{i\hbar}\iprod{\phi(t)}{\hat{H}\psi(t)}\\
                                           &= 0.
    \end{align*}
  \end{proof}
  \subsection{Solving the SchrÃ¶dinger Equation}%
  The SchrÃ¶dinger equation is an equation of the form
  \begin{align*}
    \frac{dv}{dt} &= Av
  \end{align*}
  for some linear operator $A$ over a Hilbert space. Considering the finite-dimensional Hilbert space $\C^{n}$, $A$ is an $n\times n$ matrix. The aforementioned equation thus has the solution
  \begin{align*}
    v(t) &= v_0e^{At},
  \end{align*}
  where $v_0$ is the initial condition. If $A$ is a diagonalizable matrix, we can calculate $e^{At}$ using the eigenvectors.\footnote{In particular, if $A$ is normal, the eigenvectors are the columns of the unitary diagonalization $UDU^{\ast}$, and form an orthonormal basis.}\newline

  The SchrÃ¶dinger equation simply replaces $\C^{n}$ with $\mathbf{H}$ and $A$ with $\frac{1}{i\hbar}\hat{H}$.
  \begin{proposition}[Solution to the SchrÃ¶dinger Equation via Exponentiation]
    If $\hat{H}$ is a self-adjoint operator on $\mathbf{H}$, then
    \begin{align*}
      \psi(t) &= \psi_0 e^{-it\hat{H}/\hbar}
    \end{align*}
    should be a solution to the SchrÃ¶dinger equation given that $e^{-it\hat{H}/\hbar}$ is reasonably defined.
  \end{proposition}
  \begin{remark}
    Clearly,\footnote{My professor loves this phrase.} $\psi(t)$ is a solution to the SchrÃ¶dinger equation.
  \end{remark}
  If $\hat{H}$ is a bounded operator, then $e^{-it\hat{H}/\hbar}$ can be defined by a convergent power series. However, this is rarely the case.\\

  If $\hat{H}$ is unbounded, then we will have to use the spectral theorem --- the full spectral theorem will have to wait for a while, but we can examine the case of a point spectrum.\newline

  If $\set{e_j}$ is an orthonormal basis for $\mathbf{H}$ consisting of eigenvectors of $\hat{H}$ with $\hat{H}e_j = \lambda_je_j$, then we define the exponential by requiring
  \begin{align*}
    e_je^{-it\hat{H}/\hbar} &= e_je^{-it\lambda_j/\hbar}.
  \end{align*}
  This construction makes $e^{-it\hat{H}/\hbar}$ a unitary operator,\footnote{Notice that $\left|e^{-itE_j/\hbar}\right| = 1$.} and thus bounded.\newline

  It is not necessarily true that every self-adjoint operator $A$ has an orthonormal basis (even self-adjoint ones); nevertheless, the spectral theorem tells us that there is a decomposition of $\mathbf{H}$ into generalized eigenspaces for $A$.\newline

  Unfortunately, this doesn't mean we have \textit{solved} the SchrÃ¶dinger equation. All it tells us is that $e^{-it\hat{H}/\hbar}$ is bounded, meaning it's defined for all $\psi_0 \in \mathbf{H}$; however, $\psi_0$ must belong to the domain of $\hat{H}$ (which is not $\mathbf{H}$, but only a dense subset of it) in order to be a solution.\newline

  If $\psi$ is an eigenvector of the Hamiltonian, then the equation
  \begin{align*}
    \hat{H}\psi &= E\psi
  \end{align*}
  for some $E\in \R$ is known as the time-independent SchrÃ¶dinger equation. When we solve the time-independent SchrÃ¶dinger equation, we are trying to find nonzero values of $E$ and their corresponding $\psi$ that satisfy the equation.\newline

  If $\psi_0$ is a solution to the time-independent SchrÃ¶dinger equation, then $\displaystyle \psi(t) = \psi_0 e^{-itE/\hbar}$ is the solution to the time-dependent SchrÃ¶dinger equation with initial condition $\psi_0$. Since $\psi(t)$ is a constant multiple of $\psi_0$, we say that the solution to the time-independent SchrÃ¶dinger is a stationary state.
  \subsection{The SchrÃ¶dinger Equation in $\R$}%
  Consider a particle moving on the real line. The Hamiltonian for such a particle is written in the form of
  \begin{align*}
    H(x,p) &= \frac{p^2}{2m} + V(x),
  \end{align*}
  where $V$ denotes the potential. In the quantum case, we may then consider
  \begin{align*}
    \hat{H} &= \frac{P^2}{2m} + V(X)
  \end{align*}
  to be the Hamiltonian operator. Here, $V(X)$ denotes multiplication by $V(x)$, where $x$ is an input to $\psi$. Thus, we find
  \begin{align*}
    \hat{H}\psi(x) &= -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} + V(x)\psi(x).
  \end{align*}
  An operator of the form above is known as the SchrÃ¶dinger operator; the Hamiltonian operator refers to the operator that generates time-evolution, regardless of its form. The time-dependent SchrÃ¶dinger equation thus becomes
  \begin{align*}
    \frac{\partial \psi(x,t)}{\partial t} &= \frac{i\hbar}{2m}\frac{\partial^2\psi(x,t)}{\partial x^2} - \frac{i}{\hbar}V(x)\psi(x,t).
  \end{align*}
  The time-dependent SchrÃ¶dinger equation is thus a linear partial differential equation.\footnote{Unfortunately I haven't taken PDEs yet, so I can't really explain how to solve this.} For a particle on $\R$, the time-independent SchrÃ¶dinger equation is a linear ordinary differential equation with nonconstant coefficients.\footnote{Neither have I taken ODEs, but I will be taking it in Fall 2024!}
  \subsection{Expected Position and Momentum for Solutions to the SchrÃ¶dinger Equation}%
  \begin{proposition}[Time Evolution of Position and Momentum]
    Let $\psi(t)$ be a solution to the time-dependent SchrÃ¶dinger equation with sufficiently nice $V$ and sufficiently nice $\psi_0 = \psi(0)$. Then,
    \begin{align*}
      \frac{d}{dt}\left\langle X\right\rangle_{\psi(t)} &= \frac{1}{m}\left\langle P\right\rangle_{\psi(t)}\\
      \frac{d}{dt}\left\langle P\right\rangle_{\psi(t)} &= -\left\langle V'(X)\right\rangle_{\psi(t)}.
    \end{align*}
  \end{proposition}
  \begin{remark}
    We require these assumptions to ensure that $\hat{H}$ is a self-adjoint operator and that the domain conditions hold.\newline

    Note that $-\langle V'(X)\rangle_{\psi(t)}\neq -V'\left(\left\langle X\right\rangle_{\psi(t)}\right)$. This solution to the SchrÃ¶dinger equation is not the quantum analogue to the solutions of Hamilton's equations.
  \end{remark}
  Although expected position and expected momentum do not exactly follow the classical trajectories, if $\psi(x)$ is very closely concentrated about $x = x_0$, then the particle will approximately follow the classical trajectory.
  \subsection{The Heisenberg Approach}%
  The previous approach (known as the SchrÃ¶dinger approach) towards analyzing the quantum system comes from understanding how the wave functions evolve over time, with the operators remaining stationary. We will now examine an approach where the operators change over time with the states remaining stationary.\newline

  In the Heisenberg approach, every self-adjoint $A$ evolves in time according to the operator differential equation\footnote{I will call this the ``modified SchrÃ¶dinger equation.''}
  \begin{align*}
    \frac{dA(t)}{dt} &= \frac{1}{i\hbar}[A(t),\hat{H}].
  \end{align*}
  Note that since $\hat{H}$ commutes with itself, it remains constant in time --- this is akin to the classical Hamiltonian remaining constant along a solution to Hamilton's equations.\newline

  The spectral theorem provides us a way to construct a family of unitary operators $e^{-it\hat{H}/\hbar}$ that computes time-evolution of states in the SchrÃ¶dinger approach; similarly, we can find that the solutions in the operator differential equation are of the form
  \begin{align*}
    A(t) &= Ae^{-it\hat{H}/\hbar}e^{it\hat{H}/\hbar}.
  \end{align*}
  If $\psi$ is the state of the system, then the expectation of $A(t)$ in the state is defined to be $\iprod{\psi}{A(t)\psi}$; then
  \begin{align*}
    \iprod{\psi}{A(t)\psi} &= \iprod{\psi}{e^{it\hat{H}/\hbar}Ae^{-it\hat{H}/\hbar}\psi}\\
                           &= \iprod{e^{-it\hat{H}/\hbar}\psi}{Ae^{-it\hat{H}/\hbar}\psi}\\
                           &= \iprod{\psi(t)}{A\psi(t)},
  \end{align*}
  which is the time-evolved state of the system. Since $\iprod{\psi(t)}{A\psi(t)}$ is the expectation of the value of $A$ in the state $\psi(t)$, it must be the case that the Heisenberg approach and the SchrÃ¶dinger approach are equivalent in their physics.
  \begin{proposition}[Hamiltonian in the Heisenberg Approach]
    Let $\hat{H} = \frac{P^2}{2m} + V(X)$, where $V$ is a bounded below polynomial. Then, for any $t\in \R$,
    \begin{align*}
      \hat{H} &= \frac{1}{2m}\left(P(t)\right)^2 + V(X(t)).
    \end{align*}
  \end{proposition}
  \begin{remark}
    Notice that the Hamiltonian is independent of time (since $[\hat{H},\hat{H}] = 0$), even though $P$ and $X$ depend on time.
  \end{remark}
  \begin{lemma}[Moments of Solutions]
    Suppose $A$ is a self-adjoint operator on $\mathbf{H}$ and $A(t)$ is a solution to the modified SchrÃ¶dinger equation with $A(0) = A$. Then, $\left(A(t)\right)^m$ is also a solution to the modified SchrÃ¶dinger equation. In other words, the time evolution of the $m$th power of $A$ is the same as the $m$th power of the time evolution of $A$.
  \end{lemma}
  \begin{proof}
    \begin{align*}
      e^{it\hat{H}/\hbar}A^{m}e^{-it\hat{H}/\hbar} &= \prod_{k=1}^{m}\left(e^{it\hat{H}/\hbar}Ae^{-it\hat{H}/\hbar}\right)\\
                                                   &= \left(e^{it\hat{H}/\hbar}Ae^{-it\hat{H}/\hbar}\right).
    \end{align*}
  \end{proof}
  \begin{proposition}[Analogue to Hamilton's Equations]
    Suppose $\hat{H} = \frac{\left(P(t)\right)^2}{2m} + V\left(X(t)\right)$. Then,
    \begin{align*}
      \frac{dX}{dt} &= \frac{1}{m}P(t)\\
      \frac{dP}{dt} &= -V'(X(t)).
    \end{align*}
  \end{proposition}
  \begin{remark}
    Notice that it is the functions $X(t)$ and $P(t)$ that satisfy the analogue to Hamilton's equations; the expectation values satisfy
    \begin{align*}
      \frac{d}{dt}\left\langle X(t)\right\rangle_{\psi} &= \frac{1}{m}\left\langle P(t)\right\rangle_{\psi}\\
      \frac{d}{dt}\left\langle P(t)\right\rangle_{\psi} &= -\left\langle V'(X(t))\right\rangle_{\psi}.
    \end{align*}
  \end{remark}
  \subsection{Particle in a Box}%
  We want to solve the time-independent SchrÃ¶dinger equation for the case of a particle that is confined to move in the interval $x\in [0,L]$. That is, we want to find all the eigenvectors and eigenvalues of $\hat{H}\psi = E\psi$. \newline

  The particle has potential $0$ for $x\in [0,L]$ and very large $C$ outside $[0,L]$. In the classical case, the particle has to have very high energy to escape the box; in the quantum case, if $E$ is an eigenvalue satisfying $\hat{H}\psi = E\psi$, with $E \ll C$, then $\psi$ rapidly decays outside the box. In general, we expect that the solutions to the time-independent SchrÃ¶dinger equation vanish as we approach $x = 0$ or $x=L$.\newline

  Essentially, we are looking for $\psi \in C^{2}([0,L])$ such that
  \begin{align*}
    -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} &= E\psi(x)
  \end{align*}
  subject to the boundary conditions $\psi(0) = \psi(L) = 0$.\newline

  For $E > 0$, the solution to this differential equation is a linear combination of two complex exponentials (or, since $\hat{H}$ is symmetric, a linear combination of sines and cosines):
  \begin{align*}
    \psi(x) &= a\sin\left(\frac{\sqrt{2mE}}{\hbar}x\right) + b\cos\left(\frac{\sqrt{2mE}}{\hbar}x\right).
  \end{align*}
  Implementing the boundary condition $\psi(0) = 0$ yields $b = 0$; imposing $\psi(L) = 0$ \textit{could} give us $a = 0$, but that would be boring. Instead, we want to find solutions such that
  \begin{align*}
    \sin \left(\frac{\sqrt{2mE}}{\hbar}L\right) = 0
  \end{align*}
  that are not identically zero. This forces $E$ to be of the form
  \begin{align*}
    E_j &= \frac{j^2\pi^2\hbar^2}{2mL^2},
  \end{align*}
  with $j \in \N$. It is easy to verify that for $E \leq 0$, the only solutions are where $\psi = \mathbb{0}$.
  \begin{proposition}[Eigenvectors of the Solution]
    The following eigenvectors are a solution to the SchrÃ¶dinger of the particle in a box satisfying $\psi(0) = \psi(L) = 0$:
    \begin{align*}
      \psi_j &= \sqrt{\frac{2}{L}}\sin\left(\frac{j\pi x}{L}\right).
    \end{align*}
    The $\psi_j$ correspond to each value of $E_j$, and form an orthonormal basis for $L^{2}([0,L])$.
  \end{proposition}
  \begin{proof}
    It has already been verified that the $\psi_j$ are eigenvectors with eigenvalue $E_j$. To verify that the $\psi_j$ are orthonormal, we see that
    \begin{align*}
      \frac{2}{L}\int_{0}^{L}\sin^2\left(\frac{j\pi x}{L}\right)dx &= \frac{2}{L}\left(\frac{L}{2}\right)\\
                                                                   &= 1,
    \end{align*}
    and
    \begin{align*}
      \frac{2}{L}\int_{0}^{L}\sin\left(\frac{j_1\pi x}{L}\right) \sin\left(\frac{j_2\pi x}{L}\right) dx &= 0.
    \end{align*}
    To verify that it is a basis, we use the fact that the $\psi_j$ are a Fourier sine series for $L^{2}([0,L])$.
  \end{proof}
  The Hamiltonian operator for the particle in a box with $V = 0$ is
  \begin{align*}
    \hat{H}\psi &= -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}.
  \end{align*}
  This operator isn't defined over $L^{2}([0,L])$, but only a dense subspace. The domain of $\hat{H}$ should be chosen such that $\hat{H}$ is essentially self-adjoint, meaning
  \begin{align*}
    \iprod{\phi}{\hat{H}\psi} &= \iprod{\hat{H}\phi}{\psi}
  \end{align*}
  for $\phi,\psi \in \text{Dom}(\hat{H})$. For this to hold, $\phi$ and $\psi$ must satisfy the boundary conditions necessary for the boundary terms in integration by parts to be $0$.
  \subsection{$n$-Dimensional Single-Particle Quantum Mechanics}%
  It is relatively straightforward to generalize from a quantum particle moving in $\R$ to one moving in $\R^n$. The Hilbert space is $L^{2}(\R^n)$, and instead of one position operator, there are $n$:
  \begin{align*}
    X_j\psi(\mathbf{x}) &= x_j\psi(\mathbf{x}).
  \end{align*}
  Similarly, there are $n$ momentum operators given by
  \begin{align*}
    P_j\psi(\mathbf{x}) &= -i\hbar \frac{\partial \psi}{\partial x_j}.
  \end{align*}
  As with $\R$, $[X_j,P_j] = i\hbar I$, but it is also the case that $[X_j,X_k] = 0$ and $[P_j,P_k] = 0$.
  \begin{proposition}[Canonical Commutation Relations in $n$ Dimensions]
    The position and momentum operators satisfy
    \begin{align*}
      \frac{1}{i\hbar}[X_j,X_k] &= 0\\
      \frac{1}{i\hbar}[P_j,P_k] &= 0\\
      \frac{1}{i\hbar}[X_j,P_k] &+ \delta_{jk}I
    \end{align*}
    for all $1 \leq j,k \leq n$.
  \end{proposition}
  These are the counterparts to the classical Poisson bracket, where $\frac{1}{i\hbar}[\cdot,\cdot]$ plays the same role as the Poisson bracket.\newline

  The Hamiltonian operator in $n$ dimensions is defined analogously to the classical Hamiltonian:
  \begin{align*}
    \hat{H} &= \sum_{j=1}^{n}\frac{P_j^2}{2m} + V(\mathbf{X}),
  \end{align*}
  where $V(\mathbf{X})$ results from applying $V$ to the family $\mathbf{X} = (X_1,\dots,X_n)$; we may also identify $V(\mathbf{X})$ with the operator of multiplication by $V(\mathbf{x})$, in which case we can write the Hamiltonian operator as
  \begin{align*}
    \hat{H}\psi(\mathbf{x}) &= -\frac{\hbar}{2m}\Delta \psi(\mathbf{x}) + V(\mathbf{x})\psi(\mathbf{x}),
  \end{align*}
  where $\delta = \sum_{j=1}^{n}\frac{\partial^2}{\partial x_j^2}$.\newline

  Now that we are in multiple dimensions, we can now introduce the angular momentum operator, $\hat{J}_{jk}$:
  \begin{align*}
    \hat{J}_{jk} &= X_jP_k - X_kP_j.
  \end{align*}
  As in the classical case, $\hat{J}_{jk} = 0$ when $j=k$; when $j\neq k$, $X_j$ and $P_k$ commute, meaning the order of the factors in $\hat{J}_{jk}$ is not important. In particular,
  \begin{align*}
    \hat{J}_{jk} &+ -i\hbar \left(x_j\frac{\partial}{\partial x_k} - x_k\frac{\partial}{\partial x_j}\right).
  \end{align*}
  The operator $\displaystyle \left(x_j\frac{\partial}{\partial k} - x_k\frac{\partial}{\partial j}\right)$ as the angular derivative, $\frac{\partial}{\partial \theta}$ in the $(x_j,x_k)$ plane.\newline

  When $n=3$, we use the quantum version of the angular momentum vector;
  \begin{align*}
    \hat{J}_1 &= X_2P_3 - X_3P_2\\
    \hat{J}_2 &= X_3P_1 - X_1P_3\\
    \hat{J}_3 &= X_1P_2 - X_2P_1.
  \end{align*}
  When $n=3$, every $\hat{J}_{jk}$ is either $\hat{J}_1,\hat{J}_2,\hat{J}_3$ or their negative.
  \subsection{Systems of Multiple Particles}%
  Suppose we have a system of $N$ particles moving in $\R^n$. If the particles are of different types, then the Hilbert space is $L^2(R^{nN})$. The wave function $\psi$ is a function of $\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N$, with $\mathbf{x}^j$ a vector in $\R^n$.\newline

  If $\psi$ is a unit vector in $L^{2}(\R^{nN})$, then $|\psi(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N)|^2$ is the joint probability distribution of the positions of the $N$ particles.\newline

  We introduce position operators $X_{k}^{j}$ to denote the $k$th component of the position of the $j$th particle, and similarly so with momentum operators $P_{k}^{j}$. The Hamiltonian operator is then
  \begin{align*}
    \hat{H}\psi\left(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N\right) &= \sum_{j=1}^{N}\frac{\hbar}{2m_j}\Delta_{j}\psi(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N) + V(\mathbf{x}^1,\mathbf{x}^2,\dots,\mathbf{x}^N)\psi(\mathbf{x}),
  \end{align*}
  where $m_j$ denotes the mass of the $j$th particle, and $\Delta_{j}$ denotes the Laplacian with respect to $\mathbf{x}^j$, fixing all other variables.\newline

  The Hilbert space for a composite system is taken to be the tensor product of the individual Hilbert spaces; there is a natural isomorphism\footnote{Category theory term} between $L^{2}(\R^{nN})$ and the $N$ copies of $\R^n$.\newline

  If the particles are identical, then things get more complicated --- to start, we are supposed to believe that identical particles are indistinguishable, meaning if we exchange $\mathbf{x}^1$ and $\mathbf{x}^2$ with each other, then they represent the same state. Thus, we get that
  \begin{align*}
    \psi\left(\mathbf{x}^2,\mathbf{x}^1,\mathbf{x}^3,\dots,\mathbf{x}^N\right) &= u\psi \left(\mathbf{x}^1,\mathbf{x}^2,\mathbf{x}^3,\dots,\mathbf{x}^N\right)
  \end{align*}
  for some complex number $u$ with $|u| = 1$.\footnote{We used the fact that $\psi_1 = c\psi_2$ means $\psi_1$ and $\psi_2$ are equivalent states.} Exchanging again yields $u^2 = 1$, meaning $u = 1$ or $u = -1$.\newline

  Particles with $u = 1$ are known as bosons, and particles with $u = -1$ are known as fermions. The classification of a particle is determined by its spin --- we also say that particles without spin are bosons.\newline

  For a collection of $N$ identical spinless particles in $\R^{3}$, we say that the Hilbert space is the symmetric subspace of $L^{2}\left(\R^{3N}\right)$, or the space of all functions in $L^{2}\left(R^{3N}\right)$ that are invariant under variable permutation.
  \subsection{Dirac Notation}%
  Dirac notation is physicists' preferred way to deal with ideas such as inner products and operators acting on vectors in Hilbert space.
  \begin{definition}[Bras and Kets]
    A vector $\psi\in \mathbf{H}$ is referred to as a ket, denoted $\ket{\psi}$.\newline

    A continuous linear functional in $\mathbf{H}^{\ast}$ is a bra; we use $\bra{\phi}$ to denote the unique linear functional such that $\bra{\phi}(\psi) = \iprod{\phi}{\psi}$. The existence of $\bra{\phi}$ follows from the Riesz representation theorem.\newline

    We denote inner products as $\braket{\phi}{\psi}$.
  \end{definition}
  If $A$ is an operator on $\mathbf{H}$, and $\phi$ is a vector in $\mathbf{H}$, we can form the linear functional $\bra{\phi}A$ to denote the linear map $\psi \mapsto \braket{\phi}{A\psi}$. Physicists like to write this as $\bra{\phi} A \ket{\psi}$, to denote either the linear functional $\bra{\phi}A$ applied to the vector $\ket{\psi}$ or the linear functional $\phi$ applied to the vector $A\ket{\psi}$.
  \begin{definition}[Outer Products]
    For any $\phi$ and $\psi$ in $\mathbf{H}$, the outer product, $\ket{\phi}\bra{\psi}$ denotes the operator given by
    \begin{align*}
      \left(\ket{\phi}\bra{\psi}\right)(\chi) &= \braket{\psi}{\chi} \ket{\phi}.
    \end{align*}
    In math terms,
    \begin{align*}
      \chi \xmapsto{\ket{\phi}\bra{\psi}}\iprod{\psi}{\chi}\phi.
    \end{align*}
  \end{definition}
  Notationally, if a family of vectors is labeled by indices, we write the ket vectors with purely the indices. For instance, if $\set{\phi_{n}}\subseteq \mathbf{H}$, we write $\ket{n}$ rather than $\ket{\phi_n}$.\newline

  If an operator $\hat{H}$ has an orthonormal basis of eigenvectors $\psi_n$, the decomposition would be in the form
  \begin{align*}
    I &= \sum_{n}\ket{n}\bra{n},
  \end{align*}
  where $\ket{n}$ is a unit vector and $\ket{n}\bra{n}$ denotes the orthogonal projection onto the one dimensional subspace spanned by $\ket{n}$.\newline

  Physicists also like to denote the complex conjugate of $z$ as $z^{\ast}$ and the adjoint of an operator as $A^{\dagger}$\footnote{I don't like this either.}; self-adjoint operators are known as Hermitian operators.\newline

  To express the adjoint of an operator using Dirac notation, we say that if $A$ is a bounded operator on $\mathbf{H}$, then $A^{\dagger}$ is the unique bounded operator such that
  \begin{align*}
    \bra{\psi}A &= \bra{A^{\dagger}\psi}.
  \end{align*}
  Physicists tend to define the irreducible canonical commutation relations between certain operators on a Hilbert space in order to define said Hilbert space --- this is because the Stone--von Neumann theorem\footnote{We will return to this in the future.} says that if certain conditions hold, the canonical commutation relations uniquely define the Hilbert space up to unitary equivalence.\newline

  Given this irreducible representation, and a vector $\psi \in \mathbf{H}$, the position wave function is defined by
  \begin{align*}
    \psi(x) &= \braket{x}{\psi}.
  \end{align*}
  We can analogously define the momentum wave function.
  \section{The SchrÃ¶dinger Equation for a Free Particle}%
  We want to solve the SchrÃ¶dinger equation using a variety of methods for a ``free'' particle in $\R$ --- that is, a particle with identically zero potential. The free SchrÃ¶dinger equation is thus
  \begin{align*}
    \frac{d\psi}{dt} &= \frac{i\hbar}{2m}\frac{\partial^2\psi}{\partial x^2}
  \end{align*}
  subject to the initial condition $\psi(x,0) = \psi_0(x)$. Before fully solving the equation, we begin by observing the time evolution of the expectation of the position and momentum operators:
  \begin{align*}
    \frac{d}{dt}\langle X \rangle_{\psi(t)} &= \frac{1}{m}\langle P \rangle_{\psi(t)}\\
    \frac{d}{dt}\langle P \rangle_{\psi(t)} &= 0.
  \end{align*}
  Thus, the expectation of $X$ is linear in time:
  \begin{align*}
    \langle X \rangle_{\psi(t)} &= \langle X \rangle_{\psi_0} + \frac{t}{m}\langle P \rangle_{\psi_0}\\
    \langle P \rangle_{\psi(t)} &= \langle P \rangle_{\psi_0}.
  \end{align*}
  The free SchrÃ¶dinger equation is a special case where the expectation of $X$ and $P$ exactly follows the classical solution.
  \subsection{Using the Fourier Transform to solve the Free SchrÃ¶dinger Equation}%
  We want to find solutions to the free SchrÃ¶dinger equation of the form
  \begin{align*}
    \psi(x,t) &= e^{i(kx - \omega(k)t)},
  \end{align*}
  where $k$ is the spatial frequency and $\omega(k)$ is the frequency in time for a given spatial frequency.\footnote{I think my PDEs knowledge is lacking in order to explain why the solution is of this form.} Plugging this solution form into the free SchrÃ¶dinger equation yields
  \begin{align*}
    \omega(k) &= \frac{\hbar k^2}{2m}.
  \end{align*}
  Formulae that express the frequency in time as a function of spatial frequency in a solution of some partial differential equation are called dispersion relations.\newline

  We can also see that the solution form can be written as
  \begin{align*}
    \psi(x,t) &= e^{ik\left(x - \frac{\omega(k)}{k}t\right)},
  \end{align*}
  meaning that time evolution shifts the initial function to the write by $\frac{\omega(k)}{k}t$, so $\psi(x,t)$ is moving to the right over time with speed $\frac{\omega(k)}{k}$.
  \begin{definition}[Phase Velocity]
    The phase velocity of a particle with momentum $p = \hbar k$ is
    \begin{align*}
      \frac{\omega(k)}{k} &= \frac{\hbar k}{2m}\\
                          &= \frac{p}{2m},
    \end{align*}
    which is half the velocity of a classical particle with momentum $p$.
  \end{definition}
  The ``real'' velocity, $\frac{p}{m}$ is referred to as the group velocity, whereas the phase velocity originates in the pure exponential solution to the free SchrÃ¶dinger equation.
  \begin{proposition}[Solution to the Schrodinger Equation]
    Suppose $\psi_0$ is a Schwartz function. Let $\hat{\psi_0}$ denote the Fourier transform of $\psi_0$, and define $\psi(x,t)$ by
    \begin{align*}
      \psi(x,t) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\hat{\psi_0}(k)e^{i\left(kx - \omega(k)t\right)}dk.
    \end{align*}
    Then, $\psi(x,t)$ solves the free SchrÃ¶dinger equation.\footnote{The reason why physicists insert a factor of $\sqrt{2\pi}$ is because, rather than dealing with cycles per second, $k$ is angular frequency (or radians per second). To normalize will require a factor of $2\pi$ at the conversion from $x$ to $k$, but we also want the Fourier transform to remain unitary.}
  \end{proposition}
  \begin{proof}
    Notice that, by differentiating under the integral sign, and remembering that $\omega(k) = \frac{\hbar k^2}{2m}$, we have\footnote{I did not show the convergence of the integral here, but it follows a similar argument to the convergence of the derivative with respect to $x$.}
    \begin{align*}
      \frac{d\psi}{dt} &= \frac{i\hbar}{2m}\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}-k^2\hat{\psi}_0(k)e^{i(kx - \omega(k)t)}dk\right).
    \end{align*}
    To show that the integral on the right is equal to $\frac{d^2\psi}{dx^2}$, we will need to show that the integral indeed converges. Taking the derivative of the $e^{ikx}$, we find that
    \begin{align*}
      \left\vert \frac{e^{ik(x+h)-e^{ikx}}}{h} \right\vert\leq |k|
    \end{align*}
    for all $h > 0$. Thus, with $|k\hat{\psi}_0(k)|$ as our dominating function, the dominated convergence theorem states that
    \begin{align*}
      \lim_{h\rightarrow 0}\int_{-\infty}^{\infty}\hat{\psi}_0(k)\frac{e^{i\left(k(x+h) - \omega(k)t\right)} - e^{i(kx - \omega(k)t)}}{h}dk &= \int_{-\infty}^{\infty}-ik\hat(\psi)_0(k)e^{i\left(kx - \omega(k)t\right)}dk.
    \end{align*}
    Following a similar pattern, we find that
    \begin{align*}
      \frac{d\psi}{dt} &= \frac{i\hbar}{2m}\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}-k^2\hat{\psi}_0(k)e^{i(kx - \omega(k)t)}dk\right).\\
                       &= \frac{i\hbar}{2m}\frac{d^2\psi}{dx^2}.
    \end{align*}
    Finally, we use the Fourier inversion formula to show that $\psi(x,0) = \psi_0(x)$.\footnote{The book's proof is quite a bit more vague than this proof, and I'll probably have to go back and clarify a lot of this in the future.}
  \end{proof}
  \begin{proposition}[Fourier Transform of $\psi(x,t)$]
    If $\psi(x,t)$ is defined as
    \begin{align*}
      \psi(x,t) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\hat{\psi}_0(k)e^{i(kx - \omega(k)t)}dk,
    \end{align*}
    then the Fourier transform of $\psi(x,t)$ with respect to $x$, keeping $t$ fixed, is
    \begin{align*}
      \hat{\psi}(k,t) &= \hat{\psi}_0(k)e^{-i\frac{\hbar k^2 t}{2m}}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Rewriting $\psi(x,t)$ as
    \begin{align*}
      \psi(x,t) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{ikx}\left(\hat{\psi}_0e^{-i\omega(k)t}\right)dk,
    \end{align*}
    we can see that the inner expression must be the Fourier transform of $\psi(x,t)$ with respect to $x$ (seeing as the function $e^{ikx}$ does not have any values of $t$ inside it, and the Fourier transform is injective since it is unitary). Plugging in $\omega(k) = \frac{\hbar k^2}{2m}$, we get the final expression.
  \end{proof}
  From these two results, we can see that for any $\psi_0 \in L^{2}(\R)$, we define $\psi(x,t)$ to be the unique element of $L^{2}(\R)$ whose Fourier transform with respect to $x$ is $\hat{\psi}_0(k)e^{-i\frac{\hbar k^2 t}{2m}}$. Notice here that $\psi_0$ and $\psi$ need not be differentiable, but they will still satisfy a version of the SchrÃ¶dinger equation.
  \subsection{Solving via Convolutions}%
  Recall that
  \begin{align*}
    \mathcal{F}(\phi \ast \psi) &= \hat{\phi}\hat{\psi},
  \end{align*}
  where $\ast$ denotes convolution. In other words, the \textit{inverse} Fourier transform of the function $\hat{\phi}\hat{\psi}$ is $\phi \ast \psi$\footnote{Barring a factor of $\sqrt{2\pi}$, of course.}. Thus, by the property of the Fourier transform kernel, we ought to have
  \begin{align*}
    \psi(x,t) &+ \psi_0(x)\ast K_t(x),
  \end{align*}
  where
  \begin{align*}
    K_t &= \frac{1}{\sqrt{2\pi}}\mathcal{F}^{-1}\left(e^{-i\frac{\hbar k^2 t}{2m}}\right).
  \end{align*}
  However, unfortunately, $e^{-i\frac{\hbar k^2 t}{2m}}$ is not the Fourier transform of some function in $L^{1}(\R)\cap L^{2}(\R)$, since it does not tend to zero at $\infty$. Fortunately, we \textit{can} calculate $K_t(x)$ using regular techniques of integration.
  \begin{align*}
    K_t(x) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{ikx}e^{-i\frac{\hbar k^2t}{2m}}dk\\
           &= \sqrt{\frac{m}{i2\pi \hbar t}}e^{i\frac{mx^2}{2t\hbar}},
  \end{align*}
  where the square root has positive real part. This is known as the fundamental solution of the free SchrÃ¶dinger equation.
  \begin{theorem}[Fundamental Solution to the SchrÃ¶dinger Equation]
    Let $\psi_0\in L^{2}(\R)\cap L^{1}(\R)$. Then, $\psi(x,t)$ defined by
    \begin{align*}
      \psi(x,t) &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \hat{\psi}_0(k)e^{i(kx - \omega(k)t)} dk
    \end{align*}
    is computed for all $t\neq 0$ as
    \begin{align*}
      \psi(x,t) &= \sqrt{\frac{m}{2\pi i t\hbar}}\int_{-\infty}^{\infty} e^{i\frac{m}{2t\hbar}(x-y)^2}\psi_0(y) dy.
    \end{align*}
    This expression is equal to $\frac{1}{\sqrt{2\pi}}\left(K_t\ast \psi_0\right)$, where $K_t$ is the kernel as described above.
  \end{theorem}
  \begin{proof}
    Let $E\subseteq \R$, and let $\mathbb{1}_E$ denote the indicator function of $E$. Then, $K_t \mathbb{1}_{[-n,n]}$ belongs to $L^{1}(\R) \cap L^{2}(\R)$ for any $n \in \Z_{\geq 0}$. The Fourier transform thus tells us that
    \begin{align*}
      \mathcal{F}\left(\left(K_t \mathbb{1}_{[-n,n]}\right)\ast \psi_0\right) &= \sqrt{2\pi}\mathcal{F}\left(K_t \mathbb{1}_{[-n,n]}\right) \mathcal{F}\left(\psi_0\right).
    \end{align*}
    Since $\psi_0 \in L^{1}(\R)$, we can see that $K_t \mathbb{1}_{[-n,n]}\ast \psi_0$ converges pointwise to $K_t\ast \psi_0$; similarly, we can see that $\mathcal{K_t \mathbb{1}_{[-n,n]}}$ is bounded by a constant function independent of $n$ and
    \begin{align*}
      \mathcal{F}\left(K_t \mathbb{1}_{[-n,n]}\right) \xrightarrow{\text{p.w.}} \frac{1}{\sqrt{2\pi}}e^{-i\frac{\hbar k^2t}{2m}}.
    \end{align*}
    By dominated convergence, it is thus the case that
    \begin{align*}
      \mathcal{F}\left(\left(K_t \mathbb{1}_{-n,n}\right)\ast \psi_0\right) \rightarrow \hat{\psi}e^{-i\frac{\hbar k^2t}{2m}}.
    \end{align*}
    Thus, $K_t\mathbb{1}_{[-n,n]}\ast \psi_0$ must converge in $L^2(\R)$ (since the Fourier transform is unitary), and the $L^2$ limit must converge with the pointwise limit.
  \end{proof}
  A function tends to be the \textit{fundamental} solution to an equation if it converges to the $\delta$ function as $t\rightarrow 0$; we can see that $\mathcal{F}\left(K_t\right)$ converges to $\frac{1}{\sqrt{2\pi}}$ as $t\rightarrow 0$, meaning $K_t \rightarrow \delta$.\footnote{I'm anticipating that I will depend a lot on various tables evaluating Fourier transforms if I take quantum.}
  \subsection{Solving by Propagation of the Wave Function: First Approach}%
  To come soon!\footnote{I don't really like PDEs and these sections are confusing me, so I'll probably do these much later.}
  \section{Introduction to Spectral Theory}%
  A matrix $A$ is self-adjoint if $A^{\ast} = A$, where $A^{\ast}$ denotes the conjugate transpose; by a standard result in linear algebra, there exists an orthonormal basis $\set{\mathbf{v}_j}_{j=1}^{n}$ for $\C^n$ and real numbers $\lambda_j$ such that $A\mathbf{v}_j = \lambda \mathbf{v}_j$.\newline

  When we enter infinite dimensions, things get a little more complicated; there are many self-adjoint operators that have no eigenvalues or eigenvectors. For instance, if $A$ is defined on $\mathbf{H} = L^{2}([0,1])$ as
  \begin{align*}
    (A\psi)(x) &= x\psi(x),
  \end{align*}
  or $A = M_x$, then $A$ is self-adjoint. However, the only way $x \psi(x) = \lambda \psi(x)$ works is if $\lambda = 0$ or $\psi(x)$ is only supported on $\{\lambda\}$ (which has measure zero). As a result, only the zero element of $L^{2}([0,1])$ is an eigenvector for $M_x$.\newline

  It is true that physicists would say that $M_x$ has the eigenvectors $\delta(x-\lambda)$, which do satisfy $x\delta(x-\lambda) = \lambda\delta(x-\lambda)$, but they aren't elements of $L^{2}([0,1])$. However, understanding these \textit{generalized} eigenvectors is a problem for the future.\newline

  What is valuable, though, is that we can create an orthonormal basis of eigenvectors for \textit{compact} self-adjoint operators (the operators whose image of every bounded set has compact closure); however, in quantum mechanics, operators aren't compact, let alone bounded.\newline

  For instance, the position operator $X$ acting on the indicator function $\mathbb{1}_{[n,n+1]}$ has operator norm at least $n$ (and thus, is not bounded on $L^{2}(\R)$).\newline

  It can be shown\footnote{That is, it will be shown in the future.} that if $\iprod{\phi}{A\psi} = \iprod{A\phi}{\psi}$ for all $\phi,\psi \in \mathbf{H}$, then $A$ must be bounded; thus, our unbounded operators cannot be defined on $\mathbf{H}$, only a subspace.\newline

  Specifically, unbounded operators in quantum mechanics are densely defined. The adjoint of the unbounded operator $A$ will be a different unbounded operator $A^{\ast}$ with its own domain (where $\iprod{A^{\ast}\phi}{\psi} = \iprod{\phi}{A\psi}$ for $\phi,\psi \in \text{Dom}(A^{\ast})$); if $A^{\ast}$ and $A$ have the same value, and the domains of $A^{\ast}$ and $A$ are identical, then we say $A$ is self-adjoint.
  \subsection{Goals of Spectral Theory}%
  In order to fully understand spectral theory, we need to take a step back and ponder what we expect the spectral theorem to do for us. Specifically, we want it to be able to apply functions to operators; for instance, the time-dependent SchrÃ¶dinger equation is solved by setting
  \begin{align*}
    \psi(t) &= \psi_0e^{-\frac{it}{\hbar}\hat{H}}.
  \end{align*}
  However, since $\hat{H}$ is unbounded, we can't use the power series representation of $e^{x}$ to define it; if $\hat{H}$ has an orthonormal basis of eigenvectors $\set{e_k}_{k}$ with eigenvalues $\set{\lambda_k}_k$, then we can define $\hat{H}$ to be the unique bounded operator with
  \begin{align*}
    e_ke^{-it\frac{\hat{H}}{\hbar}} &= e_ke^{-it\frac{\lambda_k}{\hbar}}
  \end{align*}
  for all $k$.\newline

  If $\hat{H}$ doesn't have an orthonormal basis of eigenvectors, we want the spectral theorem to provide a functional calculus that allows us to apply functions to $\hat{H}$ (and specifically, one that has similar properties to the case where $\hat{H}$ has an orthonormal basis).\newline

  We would also like the spectral theorem to provide a probability distribution for measurement. In the case where $A$ has an orthonormal basis $\set{e_j}_j$ of eigenvectors with eigenvalues $\set{\lambda_j}_j$, we compute probabilities as follows. \newline

  For Borel $E\subseteq \R$, we let $V_E = \overline{\text{span}\set{e_j\mid \lambda_j\in E}}$, and $P_E$ be the orthogonal projection onto $V_E$. For any unit vector $\psi$, we define
  \begin{align*}
    \text{prob}_{\psi}(A\in E) &= \iprod{\psi}{P_E\psi}.
  \end{align*}
  In particular, if the eigenvalues are distinct and $\psi = \sum_{j}c_je_j$, then the probability of observing $\lambda_j$ will be $|c_j|^2$.\newline

  If $A$ does not have an orthonormal basis of eigenvectors, the spectral theorem should be able to provide a family of projection operators $P_E$ for each Borel $E\subseteq \R$. These will be known as spectral projections, with $V_E$ the spectral subspaces.\newline

  In order to achieve these goals, we will create spectral projections using a projection-valued measure, and create the functional calculus via integration.\newline

  Meanwhile, to understand generalized eigenvectors, we need to create the \textit{direct integral} to decompose $\mathbf{H}$ with respect to a measure $\mu$ into generalized eigenspaces for the self-adjoint operator $A$.\footnote{Think about it as the ``continuous'' analogue of the direct sum of subspaces for a vector space.} The generalized eigenspace for $\lambda$ will not be a subspace of $\mathbf{H}$ unless $\mu\left(\set{\lambda}\right) > 0$. The direct integral will allow us to formally define ``eigenvectors'' that are not elements of the Hilbert space.
  \subsection{Spectral Theorem on the Position Operator}%
  Consider the position operator $X$. We have established that $X$ has no true Hilbert space eigenvectors. If we think that the generalized eigenvectors for $X$ are the distributions $\delta(x-\lambda)$ for $\lambda \in \R$, we can make a guess that $V_E$ should consist of functions whose support is $E$. The projection $P_E$ can then be calculated by
  \begin{align*}
    P_E\psi &= \mathbb{1}_{E}\psi.
  \end{align*}
  In this case, the probability is
  \begin{align*}
    \iprod{\psi}{P_E\psi} &= \int_{E}|\psi(x)|^2dx.
  \end{align*}
  Recall from earlier that we claimed that the probability distribution for the position of a particle with state $\psi$ is $|\psi(x)|^2$, meaning this construction holds.\newline

  Turning our attention the functional calculus, if $f(\lambda) = \lambda^m$, then we expect that $f(X)$ should be multiplication by $x^m$. Similarly, for any function $f$, $f(X)$ should be multiplication by $f(x)$; in particular, $e^{iaX}$ should be defined as multiplication by $e^{iax}$, itself a bounded operator on $L^2(\R)$.
  \subsection{Generalization: Multiplication Operators}%
  Consider the more general setting $\mathbf{H} = L^{2}(X,\mu)$, with $h$ a real-valued measurable function on $X$. The multiplication operator on $L^{2}(X,\mu)$ is then defined
  \begin{align*}
    M_h\psi = h\psi,
  \end{align*}
  with the spectral subspaces
  \begin{align*}
    V_E &= \set{\psi\mid \text{supp}(\psi) = h^{-1}(E)}.
  \end{align*}
  The functional calculus can then be defined by
  \begin{align*}
    f(A) &= M_{f\circ h}A.
  \end{align*}
  \begin{theorem}[Spectral Theorem, Multiplication Operator Form]
    A self-adjoint operator $A$ on a separable Hilbert space is unitarily equivalent to a multiplication operator.
  \end{theorem}
  This means there is some $\sigma$-finite measure space $(X,\mu)$ and a measurable function $h$ on $X$ such that $A = UM_hU^{\ast}$ for some unitary operator $U$.
  \subsection{Spectral Theorem on the Momentum Operator}%
  Consider, now, $P = -i\hbar \frac{d }{dx}$ on $L^{2}(\R)$. The eigenvectors of $P$ are $e^{ikx}$ for $k\in \R$, with eigenvalues $\hbar k$. While $e^{ikx}\notin L^{2}(\R)$, the Fourier transform shows that any function in $L^{2}(\R)$ can be expanded as a superposition of functions of the form $e^{ikx}$.\newline

  As a result, the Fourier coefficients of $\hat{\psi}(k)$ can be expressed in terms of a quasi-inner product of $\psi$ with $e^{ikx}$:
  \begin{align*}
    \hat{\psi}(k) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-ikx}\psi(x) dx\\
                  &= \frac{1}{\sqrt{2\pi}}\iprod{e^{ikx}}{\psi}_{L^{2}(\R)}.
  \end{align*}
  Physicists say\footnote{They have a tendency to do that.} that $\frac{1}{\sqrt{2\pi}}e^{ikx}$ forms a ``continuous orthonormal basis'' for $L^{2}(\R)$. The Kronecker delta is replaced by the Dirac $\delta$ distribution.
  \begin{align*}
    \iprod{\frac{1}{\sqrt{2\pi}}e^{ikx}}{\frac{1}{\sqrt{2\pi}}e^{ilx}} &= \delta{k-l},
  \end{align*}
  where for continuous $f$, $\delta$ satisfies
  \begin{align*}
    \int_{-\infty}^{\infty} f(k)\delta(k-l) dk &= f(l).
  \end{align*}
  The quasi-inner product on $e^{ikx}$ and $e^{ilx}$ can be more rigorously defined by approximation as
  \begin{align*}
    \frac{1}{\sqrt{2\pi}}\int_{-A}^{A} e^{-ikx}e^{ilx} dx &= \frac{A}{\pi}\frac{\sin\left(A(k-l)\right)}{A(k-l)}\\
                                                          &= f(k).
  \end{align*}
  Taking the limit as $A$ approaches infinity, it is possible to see that
  \begin{align*}
    \lim_{A\rightarrow\infty}\int_{-A}^{A} \psi(k)\frac{A}{\pi}\frac{\sin\left(A(k-l)\right)}{A(k-l)} dk &= \psi(l),
  \end{align*}
  meaning $f(k)$ behaves like the $\delta$ distribution.
  \begin{definition}[Momentum Wave Function]
    For any $\psi \in L^{2}(\R)$, define $\tilde{\psi}$ by
    \begin{align*}
      \tilde{\psi}(p) &= \frac{1}{\sqrt{\hbar}}\hat{\psi}\left(\frac{p}{\hbar}\right),
    \end{align*}
    meaning
    \begin{align*}
      \tilde{\psi}(p) &= \frac{1}{\sqrt{2\pi \hbar}}\int_{-\infty}^{\infty} e^{-ipx/\hbar}\psi(x) dx.
    \end{align*}
    We call $\tilde{\psi}(p)$ the momentum wave function.
  \end{definition}
  Since the Fourier transform is unitary, if $\psi$ is a unit vector then so too is $\hat{\psi}$ and $\tilde{\psi}$; we interpret $|\tilde{\psi}(p)|^2$ to be the probability density of the momentum just as $\left\vert \psi(x) \right\vert^2$ is the probability distribution of the position.\newline

  We can then verify that for nice $\psi$, $\widetilde{P\psi}(p) = p\tilde{\psi}(p)$, meaning the map $\psi \rightarrow \tilde{\psi}$ turns the momentum operator into multiplication by $p$.\newline

  The functional calculus for $P$ can be defined similar to the case for $X$. For Borel $E\subseteq \R$, define $P_E$ to be the orthogonal projection onto the space of functions $\psi$ for which $\text{supp}(\tilde{\psi}(p)) = E$. If $f$ is any bounded measurable function on $\R$, define $f(P)\psi$ to be the unique element of $L^{2}(\R)$ where
  \begin{align*}
    \widetilde{f(P)\psi}(p) &= f(p)\tilde{\psi}(p).
  \end{align*}
  \section{The Spectral Theorem for Bounded Self-Adjoint Operators}%
  \subsection{Preliminaries}%
  We will let $\mathbf{H}$ denote a separable complex Hilbert space. An operator $A$ on $\mathbf{H}$ is bounded if the operator norm of $A$ is finite:
  \begin{align*}
    \norm{A}_{\text{op}} &= \sup_{\norm{\psi} = 1}\norm{A\psi}\\
                         &< \infty.
  \end{align*}
  For any operator $A$, I will be denoting the operator norm $\norm{A}_{\text{op}} $ as $ \norm{A}$. The space of bounded operators on $\mathbf{H}$ forms a Banach algebra under the operator norm with multiplication; additionally, $\norm{AB}_{\text{op}}\leq \norm{A}_{\text{op}}\norm{B}_{\text{op}}$. The Banach algebra of bounded operators on $\mathbf{H}$ with respect to the operator norm is denoted $\mathcal{B}(\mathbf{H})$.\newline

  For any $A\in \mathcal{B}(\mathbf{H})$, there is a unique operator $A^{\ast}\in \mathcal{B}(\mathbf{H})$ such that
  \begin{align*}
    \iprod{\phi}{A\psi} &= \iprod{A^{\ast}\phi}{\psi}
  \end{align*}
  for all $\phi,\psi \in \mathbf{H}$. An operator $A\in \mathcal{B}(\mathbf{H})$ is self-adjoint if $A^{\ast} = A$, and $A$ is nonnegative if $\iprod{\psi}{A\psi} \geq 0$ for all $\psi \in \mathbf{H}$.
  \begin{proposition}[The $C^{\ast}$ Property]
    For all $A\in \mathcal{B}(\mathbf{H})$, we have
    \begin{align*}
      \norm{A^{\ast}} &= \norm{A}
    \end{align*}
    and
    \begin{align*}
      \norm{A^{\ast}A} &= \norm{A}^2.
    \end{align*}
    If $A$ is self-adjoint, we have $\norm{A^2} = \norm{A}^2$.
  \end{proposition}
  \begin{proof}
    For any vector $\phi \in \mathbf{H}$, $\norm{\phi} = \sup_{\norm{\chi} = 1}|\iprod{\chi}{\phi}|$ (by the Cauchy-Schwarz inequality and taking $\chi$ a multiple of $\phi$). Thus,
    \begin{align*}
      \norm{A} &= \sup_{\norm{\phi} = \norm{\psi} = 1} |\iprod{\phi}{A\psi}|.
    \end{align*}
    Thus, we get
    \begin{align*}
      \norm{A^{\ast}} &= \sup_{\norm{\phi} = \norm{\psi} = 1} |\iprod{\phi}{A^{\ast}\psi}|\\
                      &= \sup_{\norm{\phi} = \norm{\psi} = 1} |\iprod{A\phi}{\psi}|\\
                      &=\sup_{\norm{\phi} = \norm{\psi} = 1} |\iprod{\psi}{A\phi}|\\
                      &= \norm{A}.
    \end{align*}
    Since $\norm{A^{\ast}A} \leq \norm{A}^2$ (by submultiplication of the norm), we have
    \begin{align*}
      \norm{A^{\ast}A} &= \sup_{\norm{\phi} = \norm{\psi} = 1}|\iprod{\phi}{A^{\ast}A\psi}|\\
                       &= \sup_{\norm{\phi} = \norm{\psi} = 1} |\iprod{A\phi}{A\psi}|\\
                       &\geq \sup_{\norm{\psi} = 1}|\iprod{A\psi}{A\psi}|\\
                       &= \norm{A}^2.
    \end{align*}
  \end{proof}
  \begin{proposition}[Orthogonal Complements and Adjoints]
  For all $A\in \mathcal{B}(\mathbf{H})$, we have
  \begin{align*}
    \left(\text{Range}(A)\right)^{\perp} &= \ker(A^{\ast}).
  \end{align*}
  \end{proposition}
  \begin{proof}
    Let $\psi \in \left(\text{Range}(A)\right)^{\perp}$. Then, for all $\phi \in \mathbf{H}$, we have
    \begin{align*}
      0 &= \iprod{\psi}{A\phi}\\
        &= \iprod{A^{\ast}\psi}{\phi},
    \end{align*}
    meaning $A^{\ast}\psi = 0$, or $\psi \in \ker(A^{\ast})$.\newline

    Let $\psi \in \ker(A^{\ast})$. Then, for all $\phi \in \mathbf{H}$, the above relation holds (reading from bottom right upward), implying $\psi$ is orthogonal to every element of the form $A\phi$, so $\psi \in \left(\text{Range}(A)\right)^{\perp}$.
  \end{proof}
  \begin{definition}[Spectrum and Resolvent]
    For $A\in \mathcal{B}(\mathbf{H})$, the resolvent set of $A$, denoted $\rho(A)$, is the set of all $\lambda \in \C$ such that $(A-\lambda I)$ has a bounded inverse.\newline

    The spectrum of $A$, denoted $\sigma(A)$, is equal to $\C\setminus \rho(A)$.\newline

    For $\lambda \in \rho(A)$, the operator $(A - \lambda I)^{-1}$ is called the resolvent of $A$ at $\lambda$.
  \end{definition}
  By ``bounded inverse,'' it is meant that there exists a bounded operator $B$ such that $\left(A - \lambda I\right) B = B \left(A - \lambda I\right) = I$. If $A$ is bounded and $A - \lambda I$ is one-to-one and maps $\mathbf{H}$ to $\mathbf{H}$, then the closed graph theorem states that the inverse map must be bounded. With this in mind, we can also describe the resolvent set as the set of $\lambda \in \C$ such that $A - \lambda I$ is bijective.
  \begin{proposition}[Properties of the Spectrum and Resolvent]
    Let $A\in \mathcal{B}\left(\mathbf{H}\right)$. Then,
    \begin{itemize}
      \item $\sigma(A)\subseteq \C$ is closed, bounded, and non-empty.
      \item If $|\lambda| \geq \norm{A}$, then $\lambda \in \rho(A)$.
    \end{itemize}
  \end{proposition}
  \begin{lemma}[Property of a Contraction Operator]
    Let $X\in \mathcal{B}\left(\mathbf{H}\right)$ with $\norm{X} < 1$. Then, $I-X$ is invertible and its inverse is given by the series
    \begin{align*}
      \left(I-X\right)^{-1} &= \sum_{n=0}^{\infty}X^n.
    \end{align*}
  \end{lemma}
  \begin{proof}[Proof of Lemma]
    Since the operator norm is submultiplicative, $\norm{X^m}\leq \norm{X}^{m}$. The geometric series is, therefore, absolutely convergent, which implies convergence in $\mathcal{B}\left(\mathbf{H}\right)$.\newline

    Multiplying by $(I-X)$, everything on the right cancels except for $I$; therefore, the series is the inverse of $(I-X)$.
  \end{proof}
  \begin{proof}[Proof of Properties of Spectrum and Resolvent]
    Let $\lambda \in \C\setminus \{0\}$. Consider the operator
    \begin{align*}
      A - \lambda I &= -\lambda \left(I - \frac{A}{\lambda}\right).
    \end{align*}
    If $|\lambda| > \norm{A}$, then $\norm{A / \lambda} < 1$, meaning $I - A/\lambda$ is invertible, so $A - \lambda I$ is invertible, meaning $\lambda \in \rho(A)$.\newline

    Let $\lambda_0\in \rho(A)$. For any other value $\lambda$, we have
    \begin{align*}
      A - \lambda I &= A - \lambda_0 I - (\lambda - \lambda_0)I\\
                    &= (A-\lambda_0 I)\left(A - (\lambda - \lambda_0)(A-\lambda_0 I)^{-1}\right).
    \end{align*}
    If
    \begin{align*}
      |\lambda - \lambda_0| < \frac{1}{\norm{(A - \lambda_0 I)}^{-1}},
    \end{align*}
    both factors on the right will be invertible, so $A - \lambda_0$ is also invertible, meaning $\rho(A)$ is open and $\sigma(A)$ is closed.\newline

    To show $\sigma(A)$ is nonempty, we note that
    \begin{align*}
      \left(A - \lambda I\right)^{-1} &= \left(\sum_{m=0}^{\infty}\left(\lambda - \lambda_0\right)^m\left(A - \lambda_0 I\right)^{-m}\right)\left(A - \lambda_0 I\right)^{-1}.
    \end{align*}
    Near any point $\lambda_0$ in the resolvent set, the resolvent $(A-\lambda I)^{-1}$ can be computed by the series in powers of $\lambda - \lambda_0$, and the coefficients of the series are elements of $\mathcal{B}\left(\mathbf{H}\right)$. For any $\psi,\phi \in \mathbf{H}$, the map
    \begin{align*}
      \lambda \mapsto \iprod{\phi}{(A-\lambda I)^{1}\psi}
    \end{align*}
    will be given by a locally convergent power series with coefficients in $\C$, meaning the function is holomorphic on $\rho(A)$. We can also see that $\norm{\left(A-\lambda I\right)^{-1}}$ tends to zero as $|\lambda|$ tends to infinity.\newline

    If $\sigma(A)$ were empty, the function would be holomorphic on all of $\C$ and tend to zero at infinity, meaning it would be identical to $0$ for all $\phi,\psi$, which means $\left(A-\lambda I\right)^{-1}$ is the zero operator, which cannot happen.
  \end{proof}
  If $A\psi = \lambda \psi$ for some $\lambda \in \C$ and nonzero $\psi \in \mathbf{H}$, then $(A - \lambda I)$ has nonzero kernel, meaning $\lambda \in \sigma(A)$, so any eigenvalue of $A$ is contained in the spectrum of $A$.\newline

  In the infinite-dimensional case, though, points in the spectrum may not be eigenvalues; but, for a bounded self-adjoint operator, the spectrum of $A$ can be defined similar to the finite-dimensional case.
  \begin{proposition}[Properties of the Spectrum of a Bounded Self-Adjoint Operator]
  If $A\in \mathcal{B}\left(\mathbf{H}\right)$ is self-adjoint, then the following are true.
  \begin{itemize}
    \item $\sigma(A) \subseteq \R$.
    \item A number $\lambda \in \R$ belongs to the spectrum of $A$ if and only if there exists a sequence $\psi_n$ of nonzero vectors in $\mathbf{H}$ such that
      \begin{align*}
        \lim_{n\rightarrow\infty}\frac{\norm{A\psi_n - \lambda \psi_n}}{\norm{\psi_n}} = 0.
      \end{align*}
  \end{itemize}
  \end{proposition}
  Essentially, the spectrum contains all numbers where the error between $A\psi$ and $\lambda \psi$ is small relative to $\psi$.
  \begin{lemma}[Lower bound for $A - \lambda I$]
    Let $A\in \mathcal{B}\left(\mathbf{H}\right)$ be self-adjoint. Then, for all $\lambda = a + bi\in \C$, we have
    \begin{align*}
      \iprod{(A-\lambda I)\psi}{(A -\lambda I)\psi} \geq b^2 \iprod{\psi}{\psi}.
    \end{align*}
  \end{lemma}
  \begin{proof}[Proof of Lemma]
    We compute
    \begin{align*}
      \iprod{\left(A - \lambda I\right)\psi}{\left(A - \lambda\right)\psi} &= \iprod{(A - aI)\psi}{(A - aI)\psi} + bi\iprod{\psi}{(A - aI)\psi} \\
                                                                           & - bi\iprod{(A - aI)\psi}{\psi} + b^2\iprod{\psi}{\psi},
                                                                           \intertext{and since $A$ is self-adjoint,}
                                                                           &= \iprod{\left(A - aI\right)\psi}{\left(A - aI\right)\psi} + b^2\iprod{\psi}{\psi}.
    \end{align*}
  \end{proof}
  \begin{proof}[Proof of Properties of Spectrum of a Bounded Self-Adjoint Operator]
    To show the first item, we need to show that for any $\lambda \in \C$ with $\lambda = a + bi$, $b \neq 0$, $\lambda \in \rho(A)$.\newline

    To start, since $b\neq 0$, we know from the previous lemma that $A - \lambda I$ is injective.\footnote{$\ker(A - \lambda I) = \{0\}$ since $\norm{(A-\lambda I)\psi}^2 \geq b^2\norm{\psi}^2 \neq 0$ for all $\psi \neq 0$.} Since $\left(\text{Range}(A - \lambda I)\right)^{\perp} = \ker(A - \bar{\lambda}I)$, and $\text{Im}(\bar{\lambda}) \neq 0$, $A - \bar{\lambda} I$ is injective, meaning $\overline{\text{Range}(A - \lambda I)} = \mathbf{H}$.\newline

    To show that $\overline{\text{Range}(A - \lambda I)} = \mathbf{H}$, let $\phi \in \mathbf{H}$, and choose a sequence $\phi_n = (A - \lambda I)\psi_n$ in $\text{Range}(A - \lambda I)$ with $\phi_n \rightarrow \phi$. Applying the lemma, replacing $\psi$ with $\psi_n - \psi_m$, we see that $(\psi_n)_n$ is Cauchy, so $\psi_n \rightarrow \psi$ for some $\psi \in \mathbf{H}$. Additionally, since $A$ is bounded,
    \begin{align*}
      (A - \lambda I)\psi &= \lim_{n\rightarrow\infty}(A - \lambda I)\psi_n\\
                          &= \lim_{n\rightarrow\infty}\phi_n\\
                          &= \phi.
    \end{align*}
    Thus, $A - \lambda I$ is surjective, so $(A - \lambda I)^{-1}$ is bounded.\newline

    To show the second item in the forward direction, let $\lambda \in \sigma(A)$ and $\psi_n$ be a sequence such that
    \begin{align*}
      \lim_{n\rightarrow\infty}\frac{\norm{A\psi_n - \lambda \psi_n}}{\norm{\psi_n}} = 0.
    \end{align*}
    Suppose toward contradiction that $A - \lambda I$ has an inverse. Let $\phi_n = (A - \lambda I)\psi_n$, meaning $\psi_n = (A - \lambda I)^{-1}\phi_n$; thus, we see
    \begin{align*}
      \lim_{n\rightarrow\infty}\frac{\norm{\phi_n}}{\norm{(A - \lambda I)^{-1}\phi_n}} &= 0,
    \end{align*}
    meaning $A - \lambda I$ is unbounded.\newline

    To show the second item in the reverse direction, if for some $\lambda \in \R$, no such sequence exists, then for some $\varepsilon > 0$,
    \begin{align*}
      \norm{(A - \lambda I)\psi} \geq \varepsilon \norm{\psi},
    \end{align*}
    meaning $A - \lambda I$ is injective, so the range of $A$ is dense in $\mathbf{H}$; by the argument made in the first point, this means $A - \lambda I$ is also bijective, so $\lambda \in \rho(A)$.
  \end{proof}
  \begin{example}[Spectrum of $M_x$]
    Let $\mathbf{H} = L^{2}\left([0,1]\right)$, and let $A = M_x$. Then, $A$ is a bounded, self-adjoint operator whose spectrum is $\sigma(A) = [0,1]$.
  \end{example}
  \begin{proof}
    It is readily apparent that $\norm{A\psi} \leq \norm{\psi}$ (since $x\leq 1$ for all $x\in [0,1]$), and $\iprod{\phi}{A\psi} = \norm{A\phi}{\psi}$ for all $\phi,\psi \in \mathbf{H}$, meaning $A$ is bounded and self-adjoint.\newline

    Let $\lambda \in (0,1)$, consider $\psi_n = \mathbb{1}_{\lambda,\lambda + 1/n}$, where $\norm{\psi_n}^1 = \frac{1}{n}$. Additionally, since $|x - \lambda| \leq \frac{1}{n}$ on $[\lambda,\lambda + 1/n]$, we have
    \begin{align*}
      \norm{(A - \lambda I)\psi_n}^2 &\leq \frac{1}{n^3}.
    \end{align*}
    Thus, $\lambda \in \sigma(A)$; since this is true for all $\lambda \in (0,1)$ and $\sigma(A)$ is closed, $[0,1]\subseteq \sigma(A)$.\newline

    Meanwhile, if $\lambda \notin [0,1]$, the function $\frac{1}{x - \lambda}$ is bounded on $[0,1]$, meaning $A - \lambda I$ has a bounded inverse consisting of multiplication by $\frac{1}{x - \lambda}$. Therefore,$\sigma(A) = [0,1]$.
  \end{proof}
  \subsection{Spectral Subspaces}%
  Given a bounded self-adjoint operator $A$, we hope to associate each Borel $E\subseteq \sigma(A)$ with a closed subspace $V_E\subseteq \mathbf{H}$, where $V_E$  is effectively the closed span of the generalized eigenvectors of $A$ with eigenvalues in $E$. The subspaces should have the following properties:
  \begin{enumerate}[(1)]
    \item $V_{\sigma(A)} = \mathbf{H}$ and $V_{\emptyset} = \{0\}$.
    \item $E\cap F = \emptyset$ implies $V_E \perp V_F$.
    \item For any $E,F$, $V_{E\cap F} = V_E \cap V_F$.
    \item If $E_1,E_2,\dots$ are disjoint, $E = \bigsqcup_{j}E_j$, then
      \begin{align*}
        V_E = \bigoplus_{j} V_{E_j}.
      \end{align*}
    \item For any $E$, $V_E$ is invariant under $A$.
    \item If $E\subseteq [\lambda_0 - \varepsilon, \lambda_0 + \varepsilon]$, then for $\psi \in V_E$,
      \begin{align*}
        \norm{\left(A - \lambda_0 I\right)\psi} \leq \varepsilon \norm{\psi}.
      \end{align*}
  \end{enumerate}
  Combined, these properties essentially allow for an infinite-dimensional version of the finite-dimensional properties that the spectrum holds for self-adjoint matrices.
  \subsection{Projection-Valued Measure}%
  In order to describe closed subspaces of $\mathbf{H}$, we would like to use the projection operator to do so. Given a closed subspace $V\subseteq \mathbf{H}$, there exists a unique bounded, self-adjoint, idempotent operator $P$ that equals $I$ on $V$ and equals $0$ on $V^{\perp}$.\newline

  We can express the first four properties of spectral subspaces by defining measure that has properties similar to that of a measure.
  \begin{definition}[Projection-Valued Measure]
    Let $X$ be a set, with $\Omega$ a $\sigma$-algebra on $X$.\footnote{For more information on $\sigma$-algebras, measures, etc., consult my Real Analysis II notes.} A map $\mu: \Omega \rightarrow \mathcal{B}\left(\mathbf{H}\right)$ is called a projection-valued measure if the following hold:
        \begin{enumerate}[(1)]
          \item For each $E\in \Omega$, $\mu(E)$ is an orthogonal projection.
          \item $\mu(\emptyset) = 0$ and $\mu(X) = I$.
          \item If $E_1,E_2,\dots$ are disjoint, then for all $v\in \mathbf{H}$, we have
            \begin{align*}
              \mu\left(\bigsqcup_{j=1}^{\infty}E_j\right)v &= \sum_{j=1}^{\infty}\mu(E_j)v.
            \end{align*}
          \item For all $E_1,E_2\in \Omega$, we have $\mu(E_1\cap E_2) = \mu(E_1)\mu(E_2)$.
        \end{enumerate}
  \end{definition}
  If $E_1$ and $E_2$ are disjoint, then properties (2) and (4) tell us that $\mu(E_1)\mu(E_2) = 0$, from which it follows that the range of $\mu(E_1)$ and $\mu(E_2)$ are perpendicular; we can verify that $\mu(E_1)\mu(E_2)$ is the projection onto the intersection of the ranges of $\mu(E_1)$ and $\mu(E_2)$. For each $E\in \Omega$, we define a closed subspace $V_E = \text{Range}(\mu(E))$; the collection of $V_E$ then satisfy the first four properties we expect for spectral subspaces.\newline

  We will associate a projection-valued measure $\mu^{A}$ for each bounded self-adjoint operator $A$; then, $\mu^{A}(E)$ is a projection onto the spectral subspace corresponding to $E$. Then, we will introduce operator-valued integration with respect to a projection-valued measure $\mu^{A}$ for $A$, which will be known as a functional calculus for $A$.\newline

  Observe that for any projection-valued $\mu$ and $\psi\in \mathbf{H}$, we can define a positive, real-valued $\mu_{\psi}$ by
  \begin{align*}
    \mu_{\psi}(E) &= \iprod{\psi}{\mu(E)\psi}
  \end{align*}
  for $E\in \Omega$. This allows us to link between projection-valued measure and integration with respect to an ordinary measure.
  \begin{definition}[Operator-Valued Integration]
    Let $\Omega$ be a $\sigma$-algebra in $X$ and $\mu: \Omega \rightarrow \mathcal{B}\left(\mathbf{H}\right)$ a projection-valued measure. Then, there exists a unique linear map, $f \mapsto \int_{\Omega}fd\mu$ from the space of bounded, measurable, complex-valued functions on $\Omega$ into $\mathcal{B}(\mathbf{H})$, where
    \begin{align*}
      \iprod{\psi}{\left(\int_{X}f\:d\mu\right)\psi} &= \int_{X}f \:d\mu_{\psi}
    \end{align*}
    for all $f$ and all $\psi \in \mathbf{H}$, where $\mu_{\psi}$ is given as above. The integral has the following properties:
    \begin{enumerate}[(1)]
      \item For all $E\in \Omega$, we have
        \begin{align*}
          \int_{X}^{} \mathbb{1}_{E} \:d\mu &= \mu(E).
        \end{align*}
        In particular, the integral of $\mathbb{1}_{X}$ is $I$.
      \item For all $f$, we have
        \begin{align*}
          \norm{\int_{X}f\:d\mu} \leq \sup_{\lambda \in X}|f(\lambda)|.
        \end{align*}
      \item For all $f$ and $g$, we have
        \begin{align*}
          \int_{X}fg\:d\mu &= \left(\int_{X}f\:d\mu\right) \left(\int_{X}^{} g \:d\mu\right).
        \end{align*}
      \item For all $f$, we have
        \begin{align*}
          \int_{X}^{} \bar{f} \:d\mu &= \left(\int_{X}^{} f \:d\mu\right)^{\ast}.
        \end{align*}
        In particular, if $f$ is real-valued, then $\int_{X}f\:d\mu$ is self-adjoint.
    \end{enumerate}
  \end{definition}
  We can see from linearity that property (1) applies to all simple functions; as a result, we can integrate an arbitrary bounded measurable function by taking a limit of simple functions converging uniformly to $f$ --- the integral of $f$ is the limit in the operator norm topology of the integral of the simple functions.\newline

  In this context, multiplication is straightforward:
  \begin{align*}
    \left(\int_{X}^{} \mathbb{1}_{E_1}\:d\mu\right)\left(\int_{X}^{} \mathbb{1}_{E_2}\:d\mu\right) &= \mu(E_1)\mu(E_2)\\
                                                                                                   &= \mu(E_1 \cap E_2)\\
                                                                                                   &= \int_{X}^{} \mathbb{1}_{E_1}\mathbb{1}_{E_2}\:d\mu.
  \end{align*}
  One of the major reasons this can be the case is that there are many more idempotent elements in $\mathcal{B}(\mathbf{H})$ than in $\R$ (attempting a similar construction of a multiplicative integral in $\R$ yields effectively no benefit).
  \begin{proof}[Proof of Properties of Operator-Valued Integration]
    Given a projection-valued measure $\mu$ and a bounded measurable function $f$, define $Q_{f}: \mathbf{H}\rightarrow \C$ by
    \begin{align*}
      Q_f(\psi) &= \int_{X}^{}f \:d\mu_{\psi},
    \end{align*}
    where $\mu_{\psi}(E) = \iprod{\psi}{\mu(E)\psi}$. If $f$ is an indicator function, then $Q_f(\psi)$ is a bounded quadratic form --- as a result, by the boundedness of the form,
    \begin{align*}
      |Q_f(\psi)| \leq \left(\sup_{\lambda \in X}|f(\lambda)|\right)\norm{\psi}^2.
    \end{align*}
    Thus, there is a unique bounded operator $A_f$ such that $Q_f(\psi) = \iprod{\psi}{A_f\psi}$ for all $\psi \in \mathbf{H}$.\newline

    If $f = \mathbb{1}_E$, then $Q_f(\psi) = \mu_{\psi}(E) = \iprod{\psi}{\mu(E)\psi}$, meaning $A_f = \mu(E)$. Property (2) follows from the above inequality.\newline

    To show property (3), we see that multiplicativity of the integral for indicator functions is built into the definition of the projection-valued measure; we use linearity to extend the multiplicativity to simple functions, then limits for bounded measurable functions.\newline

    Finally, for $f$ real valued, $Q_f(\psi)$ is real for all $\psi \in \mathbf{H}$, meaning $A_f$ is self-adjoint. This yields property (4) via linearity.
  \end{proof}
  \subsection{The Spectral Theorem for Bounded Self-Adjoint Operators, Statement 1}%
  \begin{theorem}[Spectral Theorem, Projection Integral Form]
    Let $A\in \mathcal{B}\left(\mathbf{H}\right)$ be self-adjoint. Then, there exists a unique projection-valued measure $\mu^{A}$, defined on the Borel $\sigma$-algebra of $\sigma(A)$, with values of projections on $\mathbf{H}$, such that
    \begin{align*}
      \int_{\sigma(A)}^{} \lambda\:d\mu^{A}(\lambda) &= A.
    \end{align*}
    Since $\sigma(A)$ is bounded, the function $f(\lambda) := \lambda$ is bounded.\footnote{The integral is effectively integrating $\mu^{A}\left(\set{\lambda}\right)$ with respect to the spectrum.}
  \end{theorem}
  \begin{definition}[Functional Calculus]
    If $A\in \mathcal{B}\left(\mathbf{H}\right)$ is self-adjoint, and $f: \sigma(A)\rightarrow \C$ is an (essentially) bounded measurable function, then
    \begin{align*}
      f(A) &= \int_{\sigma(A)}^{} f(\lambda)\:d\mu^{A}(\lambda),
    \end{align*}
    where $\mu^{A}$ is the aforementioned unique projection-valued measure.
  \end{definition}
  To extend the projection-valued measure from $\sigma(A)$ to $\R$, we assign measure $0$ to $\R\setminus \sigma(A)$.\newline

  Since the integral with respect to $\mu^{A}$ is multiplicative, $f(\lambda) = \lambda^{m}$ yields $f(A) = A^{m}$, and since
  \begin{align*}
    e^{a\lambda} &= \sum_{m=0}^{\infty}\frac{(a\lambda)^{m}}{m!}
  \end{align*}
  converges on the compact set $\sigma(A)$, the operator $e^{aA}$ computed with the functional calculus on the function $f(\lambda) = e^{a\lambda}$ is equal to its power series.
  \begin{definition}[Spectral Subspace]
    For $A\in \mathcal{B}\left(\mathbf{H}\right)$, let $\mu^{A}$ be the projection-valued measure extended to $\R$ by assigning $\mu^{A}\left(\R\setminus \sigma(A)\right) = 0$. Then, for Borel $E\subseteq \R$, define the spectral subspace $V_E$ of $\mathbf{H}$ by
    \begin{align*}
      V_E &= \text{Range}\left(\mu^{A}\left(E\right)\right).
    \end{align*}
  \end{definition}
  \begin{proposition}[Properties of Spectral Subspaces]
    If $A\in \mathcal{B}\left(\mathbf{H}\right)$ is self-adjoint, the spectral subspaces associated with $A$ have the following properties:
    \begin{enumerate}[(1)]
      \item Each spectral subspace $V_E$ is invariant under $A$.
      \item If $E\subseteq [\lambda_0 - \varepsilon, \lambda_0 + \varepsilon]$, then for all $\psi \in V_E$,
        \begin{align*}
          \norm{\left(A-\lambda_0I\right)\psi}\leq \varepsilon \norm{\psi}
        \end{align*}
      \item $\displaystyle \sigma\left(A\rvert_{V_E}\right)\subseteq \overline{E}$.
      \item If $\lambda_0 \in \sigma(A)$, then for every neighborhood $U$ of $\lambda_0$, $V_U \neq \{0\}$ (or $\mu(U)\neq 0$).
    \end{enumerate}
  \end{proposition}
  \begin{proof}
    To show (1), we can see that for any bounded measurable functions $f$ and $g$ on $\sigma(A)$, $f(A)$ and $g(A)$ commute; in particular, $A$ is the integral of $f(\lambda) = \lambda$, which commutes with $\mu^{A}(E)$, which is the integral of the indicator function of $E$. As a result, for any $\mu^{A}(E)\phi \in \text{Range}\left(\mu^{A}(E)\right)$, we have
    \begin{align*}
      A\mu^{A}(E)\phi &= \mu^{A}(E)A\psi,
    \end{align*}
    which is in $\mu^{A}(E)$. Thus, $\text{Range}\left(\mu^{A}(E)\right)$ is invariant under the spectral subspace.\newline

    To show (2), let $\psi \in V_E$ with $E \subseteq [\lambda_0 - \varepsilon,\lambda_0 + \varepsilon]$. Then, $\psi\in \text{Range}\left(\mu^{A}(E)\right)$, meaning
    \begin{align*}
      (A - \lambda_0 I)\psi &= (A - \lambda_0 I)\mu^{A}(E)\psi.
    \end{align*}
    However, since $\mu^{A}(E) = \mathbb{1}_{E}(A)$, and $A - \lambda_0 I = f(A)$ for $f(\lambda) = \lambda - \lambda_0$, the integral's multiplicativity gives us
    \begin{align*}
      (A - \lambda_0 I)\psi &= \left(f\mathbb{1}_{E}\right)(A)\psi.
    \end{align*}
    We can see that $\left\vert f(\lambda) - \mathbb{1}_{E}(\lambda) \right\vert \leq \varepsilon$, meaning $\norm{(A - \lambda_0 I)\psi} \leq \varepsilon \norm{\psi}$.\newline

    To show (3), let $\lambda_0 \notin \overline{E}$. Then, $g(\lambda) = \mathbb{1}_{E}(\lambda)\frac{1}{\lambda - \lambda_0}$ is bounded, meaning $g(A)$ is a bounded operator with
    \begin{align*}
      g(A)\left(A - \lambda_0 I\right) &= (A - \lambda_0 I)g(A)\\
                                       &= \mathbb{1}_{E}(A).
    \end{align*}
    Therefore, the restriction to $V_E$ of $g(A)$ is the inverse of the restriction of $A$ to $V_E$, meaning $\lambda_0\notin \sigma\left(A\rvert_{V_E}\right)$.\newline

    To show (4), fix $\lambda_0\in \sigma(A)$, and suppose that for some $\varepsilon > 0$, $\mu\left((\lambda_0 - \varepsilon, \lambda_0 + \varepsilon)\right) = 0$. Consider the function
    \begin{align*}
      f(\lambda) &= \begin{cases}
        \frac{1}{\lambda - \lambda_0} & \left\vert \lambda - \lambda_0 \right\vert \geq \varepsilon\\
        0 & \left\vert \lambda - \lambda_0 \right\vert < \varepsilon
      \end{cases}.
    \end{align*}
    Since $f(\lambda)\left(\lambda - \lambda_0\right) = 1$ everywhere except $(\lambda_0 - \varepsilon,\lambda_0 + \varepsilon)$, the equation $f(\lambda)\left(\lambda - \lambda_0\right) = 1$ is true $\mu$-a.e., meaning the integral is equal to the integral of $\mathbb{1}_{\sigma(A)} = I$. Since integrals are multiplicative, we see
    \begin{align*}
      f(A)\left(A - lambda_0 I\right) &= \left(A - \lambda_0 I\right)f(A)\\
                                      &= I,
    \end{align*}
    meaning $f(A)$ is the inverse of $A - \lambda_0 I$, which contradicts $\lambda_0 \in \sigma(A)$.\footnote{Seems like fundamental construction we need for proofs involving the spectrum is $A - \lambda_0 I$.}
  \end{proof}
  \begin{proposition}[Commuting Operators]
    Let $A\in \mathcal{B}\left(\mathbf{H}\right)$ be self-adjoint, and $B\in \mathcal{B}\left(\mathbf{H}\right)$ commute with $A$. The following are true:
    \begin{enumerate}[(1)]
      \item For all bounded measurable functions $f$ on $\sigma(A)$, $f(A)$ commutes with $B$.
      \item Every spectral subspace for $A$ is invariant under $B$.
    \end{enumerate}
  \end{proposition}
  \begin{proof}
    To be shown later.
  \end{proof}
  As a result of the properties of the spectral theorem, we are able to generate the most important part of quantum mechanics: a probability measure to find the measurements of a self-adjoint operator with the state $\psi$.
  \begin{proposition}[Probability Distributions]
    Let $A\in \mathcal{B}\left(\mathbf{H}\right)$ be self-adjoint, and $\psi \in \mathbf{H}$ a unit vector. Then, there exists a unique probability measure such that
    \begin{align*}
      \int_{\R}^{} \lambda^{m}\:d\mu^{A}_{\psi}(\lambda) &= \iprod{\psi}{A^{m}\psi}.
    \end{align*}
  \end{proposition}
  \begin{proof}
    Define $\mu^{A}_{\psi}$ on $\sigma(A)$ by
    \begin{align*}
      \mu^{A}_{\psi}(E) &= \iprod{\psi}{\mu^{A}(E)\psi}.
    \end{align*}
    Then, by the properties of integration with respect to $\mu^{A}$, we get
    \begin{align*}
      \iprod{\psi}{A^{m}\psi} &= \iprod{\psi}{\left(\int_{\sigma(A)}^{} \lambda^{m}\:d\mu^{A}(\lambda)\right)\psi}\\
                              &= \int_{\sigma(A)}^{} \lambda^{m}\:d\mu^{A}_{\psi}(\lambda).
    \end{align*}
    To extend $\mu^{A}_{\psi}$ to be defined on $\R$, set $\mu^{A}_{\psi}$ to be $0$ on $\R\setminus \sigma(A)$.\newline

    To show uniqueness, we see that
    \begin{align*}
      \left\vert \iprod{\psi}{A^{m}\psi} \right\vert &\leq \norm{\psi}^2\norm{A^{m}}\\
                                                     &\leq \norm{\psi}^2\norm{A}^{m},
    \end{align*}
    meaning moments only grow exponentially with $m$. The uniqueness of the moment problem\footnote{The book's recommended resource here is Allan Gut's \textit{Probability Theory: A Graduate Course}, so I'll probably look there for more information and create a prelude.} yields the uniqueness of $\mu^{A}_{\psi}$.
  \end{proof}
  \subsection{Proof: The Spectral Theorem for Bounded Self-Adjoint Operators, Statement 1}%
  To come soon.
  \subsection{The Spectral Theorem for Bounded Self-Adjoint Operators, Statement 2}%
  One version of the spectral theorem states that every self-adjoint operator is unitarily equivalent to a multiplication operator. However, we can state a more general version of the spectral theorem by invoking the direct integral.\newline

  For every $\lambda \in X$, where $(X,\mu)$ is a measure space,\footnote{I presume this is with the Borel $\sigma$-algebra.} we associate a Hilbert space $\mathbf{H}_{\lambda}$; an element of the direct integral is a function $s$ on $X$ such that $s(\lambda)$ is in $\mathbf{H}_{\lambda}$ for each $\lambda \in X$. For a real-valued measurable function $h$ on $X$, we can multiply an element $s$ of the direct integral by $h$.\newline

  The direct integral version of the spectral theorem states that a bounded self-adjoint operator is unitarily equivalent to a multiplication on a direct integral. The benefits are several.
  \begin{itemize}
    \item The canonical set $X$ is $\sigma(A)$ and canonical function $h$ is $h(\lambda) = \lambda$.
    \item We can say $\mathbf{H}_{\lambda}$ is the space of generalized eigenvectors with eigenvalue $\lambda$. Thus, we can give a rigorous definition of the ``eigenvectors'' that are not in the Hilbert space.
    \item We can classify two self-adjoint operators as unitarily equivalent if their direct integral representations are equivalent.
  \end{itemize}
  \begin{definition}[Direct Integral]
    Let $\mu$ be a $\sigma$-finite measure on the $\sigma$-algebra $\Omega$ of $X$. Suppose that for each $\lambda \in X$, we have a separable Hilbert space $\mathbf{H}_{\lambda}$, with inner product $\iprod{\cdot}{\cdot}_{\lambda}$.\newline

    Elements of the integral will be sections, $s$, functions on $X$ with values in the union of $\mathbf{H}_{\lambda}$, meaning $s(\lambda)\in \mathbf{H}_{\lambda}$ for each $\lambda \in X$.\newline

    We would like to define the norm of a section by the formula
    \begin{align*}
      \norm{s}^{2} &= \int_{X}^{} \iprod{s(\lambda)}{s(\lambda)}\:d\mu(\lambda),
    \end{align*}
    provided the integral is finite. The inner product of $s_1$ and $s_2$ is
    \begin{align*}
      \iprod{s_1}{s_2} &= \int_{X}^{} \iprod{s_1(\lambda)}{s_2(\lambda)}\:d\mu(\lambda).
    \end{align*}
    In order to ensure measurability of the sections, we will need to choose a \textit{simultaneous} orthonormal basis for the $\mathbf{H}_{lambda}$.\newline

    Recall that an orthonormal basis for $\mathbf{H}$ is a family unit vectors $\set{e_j}_j$ where $\iprod{e_j}{e_k} = 0$ for $j\neq k$, and $\overline{\text{span}\set{e_j}_j} = \mathbf{H}$.\newline

    The simultaneous orthonormal basis of $\mathbf{H}_{\lambda}$ is a collection of sections $\set{e_j(\cdot)}$ such that for each $\lambda$, $\set{e_j(\lambda)}$ is an orthonormal basis for $\mathbf{H}_{\lambda}$. Provided that $\lambda \mapsto \text{dim}(\mathbf{H}_{\lambda})$ is measurable, it is possible to choose a simultaneous orthonormal basis such that $\iprod{e_j(\lambda)}{e_k(\lambda)}$ is measurable for all $j,k$.\newline

    The section $s$ is measurable if the function $\lambda \mapsto \iprod{e_j(\lambda)}{s(\lambda)}$ is measurable for each $j$; the $e_j$ are assumed to be measurable sections.\newline

    The choice of simultaneous orthonormal basis is known as a measurability structure on $\set{\mathbf{H}_{\lambda}}$. Given two measurable sections, the function
    \begin{align*}
      \lambda \mapsto \iprod{s_1(\lambda)}{s_2(\lambda)}_\lambda = \sum_{j=1}^{\infty}\iprod{s_1(\lambda)}{e_j(\lambda)}\iprod{e_j(\lambda)}{s_2(\lambda)}
    \end{align*}
    is also measurable.\newline

    Thus, for a $\sigma$-finite measure space $(X,\Omega,\mu)$ with a collection $\set{H_{\lambda}}$ with measurable dimension functions, and a measurability structure on $\set{H_{\lambda}}$, the direct integral of $\mathbf{H}_{\lambda}$ with respect to $\mu$ is denoted
    \begin{align*}
      \int_{X}^{\oplus} \mathbf{H}_{\lambda}\:d\mu(\lambda).
    \end{align*}
    The direct integral is the space of equivalence classes of $\mu$-a.e. equal measurable sections for which
    \begin{align*}
      \norm{s}^2 &= \int_{X}^{} \iprod{s(\lambda)}{s(\lambda)}\:d\mu(\lambda)
    \end{align*}
    is finite. The inner product is defined by
    \begin{align*}
      \iprod{s_1}{s_2} &= \int_{X}^{} \iprod{s_1(\lambda)}{s_2(\lambda)}\:d\mu(\lambda).
    \end{align*}
  \end{definition}
  It is possible to show that the direct integral is itself a Hilbert space.
  \begin{example}[Two Particular Direct Sums]
    If each $\mathbf{H}_{\lambda}$ is equal to $\C$, then the direct integral is $L^{2}\left(X,\mu\right)$.\newline

    If $X = \set{\lambda_1,\lambda_2,\dots}$, $\Omega$ consists of all subsets, and $\mu$ is the counting measure, then the direct integral is the Hilbert space direct sum.
  \end{example}
  \begin{theorem}[Spectral Theorem, Direct Integral Form]
    If $A \in \mathcal{B}\left(\mathbf{H}\right)$ is self-adjoint, then there exists a $\sigma$-finite measure $\mu$ on $\sigma(A)$, a direct integral
    \begin{align*}
      \int_{\sigma(A)}^{\oplus} \mathbf{H}_{\lambda}\:d\mu(\lambda),
    \end{align*}
    and a unitary map between $\mathbf{H}$ and the direct integral such that
    \begin{align*}
      UAU^{-1}(s) &= M_{\lambda}s
    \end{align*}
    for all $s$ in the direct integral.
  \end{theorem}
  \begin{theorem}[Spectral Theorem, Multiplication Operator Form]
    Let $A\in \mathcal{B}\left(\mathbf{H}\right)$ be self-adjoint. Then, there exists a $\sigma$-finite measure space $(X,\mu)$, a bounded, measurable, real-valued function $h$ on $X$, and a unitary map $U: \mathbf{H}\rightarrow L^{2}(X,\mu)$ such that
    \begin{align*}
      \left(UAU^{-1}\right)\psi &= M_{h}\psi
    \end{align*}
    for all $\psi \in L^{2}\left(X,\mu\right)$.
  \end{theorem}
  \begin{proposition}[Functional Calculus on Direct Integral]
  Let $A\in \mathcal{B}\left(\mathbf{H}\right)$ be self-adjoint, and $U$ the specific unitary map that satisfies the direct integral form of the spectral theorem. Then, for $f$ a bounded measurable function on $\sigma(A)$,
  \begin{align*}
    \left(Uf(A)U^{-1}\right)s &= M_{f}s
  \end{align*}
  \end{proposition}
  Roughly speaking, $f(A) = f(\lambda)I$ on each generalized eigenspace $\mathbf{H}_{\lambda}$; we can see that the proposition follows from the direct integral form applied to $\lambda^{m}$, then take limits over the compact set $\sigma(A)$.
\end{document}
