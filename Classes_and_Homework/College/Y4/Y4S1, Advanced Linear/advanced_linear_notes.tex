\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright}
%\usepackage{sfmath}
%\usepackage{bbold} %better blackboard bold

%serif font + different blackboard bold for serif font
\DeclareMathOperator*{\lcm}{lcm}
\usepackage{newpxtext,eulerpx}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\usepackage{epigraph}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushleft}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Alt}{Alt}
\DeclareMathOperator{\conj}{conj}
%\DeclareMathOperator{\Hom}{Hom}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Advanced Linear Algebra: Class Notes}
\cfoot{\thepage}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\section{Introduction}%
\epigraph{It is my experience that proofs involving matrices can be shortened by 50\% if one throws the matrices out.}{Emil Artin}
The goal of this course is to prove a lot of the essential results of linear algebra without basis dependence (as in, using the properties of the linear transformations themselves rather than matrices).
\tableofcontents
\section{Vector Spaces}%
\subsection{Vector Spaces and Linear Transformations}
\begin{remark}
We let $\F$ be either $\R,\Q,\C,\F_{p}$ (where $p$ is a prime). Primarily, we let $\F = \Q,\R,\C$.\newline
\end{remark}
\begin{example}[Our First Vector Space]
  The primary vector space we study in lower-division linear algebra is
  \begin{align*}
    V &= \R^n\\
      &= \set{ \left. \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_1,\dots,a_n\in \R }
  \end{align*}
  We know that for
  \begin{align*}
    v &= \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\\
    w &= \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix},
  \end{align*}
  that
  \begin{align*}
    v+w &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    cv &= \begin{pmatrix}ca_1 \\\vdots\\ca_n\end{pmatrix},
  \end{align*}
  where $c\in\R$ is some constant.
\end{example}
\begin{definition}[Vector Space]
  Let $V$ be a nonempty set with the following operations:
  \begin{itemize}
    \item $a: V\times V \rightarrow V$, $a(v,w)\mapsto v+w$ (vector addition);
    \item $m: F\times V \rightarrow V$, $m(c,v) \mapsto cv$ (scalar multiplication);
  \end{itemize}
  satisfying the following:
  \begin{enumerate}[(1)]
    \item there exists $0_v\in V$ such that $0_v + v = v = v + 0_v$ for all $v\in V$;
    \item for every $v\in V$, there exists $-v$ such that $v + (-v) = 0_v = (-v) + v$;
    \item for every $u,v,w\in V$, $(u+v) + w = u + (v+w)$;
    \item for every $v,w\in V$, $v+w = w+v$;
    \item for every $v,w\in V$ and $c\in \F$, $c(v+w) = cv + cw$;
    \item for every $c,d\in \F$, $v\in V$, $(c+d)v = cv + dv$;
    \item for every $c,d\in \F$, $v\in V$, $(cd)v = c(dv)$;
    \item for every $v\in V$, $\left(1_{\F}\right)v = v$.
  \end{enumerate}
  We say $V$ is a $\F$-vector space.
\end{definition}
\begin{example}[$\F^{n}$]
  Let $\F$ be a field, $V = \F^n$.
  \begin{align*}
    V &= \set{ \left.\begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_i\in \F }.
    \intertext{Define:}
    \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} + \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix} &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    c \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} &= \begin{pmatrix}ca_1 \\\vdots \\ ca_n\end{pmatrix}.
  \end{align*}
  We set
  \begin{align*}
    0_{\F^n} &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}.
  \end{align*}
  Let
  \begin{align*}
    v &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
    w &= \begin{pmatrix}w_1\\\vdots\\w_n\end{pmatrix}\\
    u &= \begin{pmatrix}u_1\\\vdots\\u_n\end{pmatrix},
  \end{align*}
  $c,d\in \F$. We observe that
  \begin{align*}
    0_{\F^n} + v &= \begin{pmatrix}0 + v_1\\\vdots\\0 + v_n\end{pmatrix}\\
                 &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}.
  \end{align*}
  Define
  \begin{align*}
    -v &= \begin{pmatrix}-v_1\\\vdots\\-v_n\end{pmatrix}.
  \end{align*}
  Then,
  \begin{align*}
    v + (-v) &= \begin{pmatrix}v_1 + \left(-v_1\right)\\\vdots\\v_n + \left(-v_n\right)\end{pmatrix}\\
             &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}\\
             &= 0_{\F^n}.
  \end{align*}
  Note that
  \begin{align*}
    (u + v) + w &= \begin{pmatrix}\left(u_1 + v_1\right) + w_1 \\\vdots\\\left(u_n + v_n\right) + w_n\end{pmatrix}\\
                &= \begin{pmatrix}u_1 + \left(v_1 + w_1\right) \\\vdots\\u_n + \left(v_n + w_n\right)\end{pmatrix}\\
                &= u + (v+w).
  \end{align*}
  We have
  \begin{align*}
    v +w &= \begin{pmatrix}v_1 + w_1 \\\vdots\\v_n + w_n\end{pmatrix}\\
         &= \begin{pmatrix}w_1 + v_1\\\vdots\\w_n + v_n\end{pmatrix}\\
         &= w + v.
  \end{align*}
  Observe
  \begin{align*}
    c\left(v+w\right) &= c \begin{pmatrix}v_1 + w_1\\\vdots\\v_n + w_n\end{pmatrix}\\
                      &= \begin{pmatrix}c\left(v_1 + w_1\right)\\\vdots\\c\left(v_n + w_n\right)\end{pmatrix}\\
                      &= \begin{pmatrix}cv_1 + cw_1 \\\vdots \\cv_n + cw_n\end{pmatrix}\\
                      &= cv + cw,\\
    (c+d)v &= (c+d) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(c+d)v_1\\\vdots\\(c+d)v_n\end{pmatrix}\\
          &= \begin{pmatrix}cv_1 + dv_1 \\\vdots\\cv_n + dv_n\end{pmatrix}\\
          &= cv + dv,
          \intertext{and}
    (cd)v &= (cd) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(cd)v_1 \\\vdots\\(cd)v_n\end{pmatrix}\\
          &= \begin{pmatrix}c\left(dv_1\right) \\\vdots\\c\left(dv_n\right)\end{pmatrix}\\
          &= c\left(dv\right).
  \end{align*}
  Finally,
  \begin{align*}
    1_{\mathbb{F}} &= 1_{\mathbb{F}} \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}1_{\mathbb{F}}v_1\\\vdots 1_{\F}\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= v.
  \end{align*}
\end{example}
\begin{example}[Polynomials]
  Let $n\in \Z_{\geq 0}$. We define
  \begin{align*}
    P_{n}\left(\mathbb{F}\right) &= \set{a_0 + a_1x + \cdots + a_nx^n\left|a_i\in \F\right.}.
  \end{align*}
  For $f(x) = \sum_{j=0}^{n}a_jx^j$ and $g(x) = \sum_{j=0}^{n}b_jx^j$ in $P_n(\mathbb{F})$, we have
  \begin{align*}
    f(x) + g(x) &= \sum_{j=0}^{n}\left(a_j + b_j\right)x^j\\
    cf(x) &= \sum_{j=0}^{n}\left(ca_j\right)x^j.
  \end{align*}
  Note that these are not functions \textit{per se}, we are only $f(x)$ and $g(x)$ to represent elements of $P_n\left(\mathbb{F}\right)$. We can verify that $P_n\left(\mathbb{F}\right)$ is a $\mathbb{F}$-vector space.\newline

  We define
  \begin{align*}
    \mathbb{F}[x] &= \bigcup_{n\geq 0}P_n\left(\F\right),
  \end{align*}
  which is also a $\F$-vector space.
\end{example}
\begin{example}[Matrices]
  Let $m,n\in \Z_{> 0}$. We set
  \begin{align*}
    V &= \text{Mat}_{m,n}\left(\F\right),
  \end{align*}
  which is the set of $m\times n$ matrices with entries in $\F$. This is an $\F$-vector space with matrix addition and scalar multiplication.\newline

  In the case where $m = n$, we write $\text{Mat}_{n}\left(\F\right)$ to denote $\text{Mat}_{n,n}\left(\F\right)$.
\end{example}
\begin{example}[Complex Numbers]
  Let $V = \C$. Then, $V$ is a $\C$-vector space, an $\R$-vector space, and a $\Q$-vector space.\newline

  Note that the properties of a vector space change with the underlying scalar field.
\end{example}
\begin{lemma}[Basic Properties of Vector Spaces]
  Let $V$ be a $\F$-vector space.
  \begin{enumerate}[(1)]
    \item $0_V$ is unique.
    \item $0_{\mathbb{F}}v = 0_V$.
    \item $\left(-1_{\mathbb{F}}\right)v = -v$.
  \end{enumerate}
\end{lemma}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Suppose toward contradiction that there exist $0,0'$ both satisfy 
      \begin{align*}
        0 + v &= v\tag*{(\textasteriskcentered)}\\
        0' + v &= v.\tag*{(\textasteriskcentered\textasteriskcentered)}
      \end{align*}
      Then,
      \begin{align*}
        0 + v &= v\\
        0 + 0' &= 0'\tag*{by (\textasteriskcentered) with $v = 0'$}\\
               &= 0' + 0\\
               &= 0. \tag*{by (\textasteriskcentered\textasteriskcentered) with $v = 0$}
      \end{align*}
    \item Note
      \begin{align*}
        0_{\mathbb{F}}v &= \left(0_{\mathbb{F}} + 0_{\F}\right) v\\
                        &= 0_{\F}v + 0_{\F}v.
      \end{align*}
      We subtract $0_{\F}v$ from both sides.
    \item
      \begin{align*}
        \left(-1_{\mathbb{F}}\right)v + v &= \left(-1_{\mathbb{F}} \right)v + 1_{\F}v\\
                                          &= \left(-1_{\F} + 1_{\F}\right)v\\
                                          &= 0_{\F}v.
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{definition}[Subspaces]
  Let $V$ be an $\F$-vector space. We say $W\subseteq V$ is an $\F$-subspace (henceforth subspace) if $W$ is an $\F$-vector space under the same addition and scalar multiplication.
\end{definition}
\begin{example}[Subspaces of $\R^2$]
  Let $V = \R^2$. 
  \begin{center}
    \begin{tikzpicture}[scale = 0.5]
      \draw (0,5) -- (0,-5);
      \draw (5,0) -- (-5,0);
      \draw[thick, color=orange,<->] (-5,-5) -- (5,5);
      \node[anchor = south west] at (5,5){$W_1$};
      \draw[thick, color=yellow!40!black,<->] (-5,-1) -- (1,5);
      \node[anchor = south west] at (1,5) {$W_2$};
    \end{tikzpicture}
  \end{center}
  Here, we see that $W_1$ is a subspace, and $W_2$ is not a subspace (as $W_2$ does not contain $0_{V}$).
\end{example}
\begin{example}[Subspaces of $\C$]
  Let $V = \C$, $W = \set{a + 0i\mid a\in \R}$.
  \begin{itemize}
    \item If $\F = \R$, then $W$ is a subspace of $V$.
    \item If $\F = \C$, then $W$ is not a subspace; we can see that $2\in W$, $i\in \C$, but $2i\notin W$.
  \end{itemize}
\end{example}
\begin{example}[Matrices]
  It is not the case that $\text{Mat}_2(\R)$ is a subspace of $\text{Mat}_4(\R)$, since $\text{Mat}_2(\R)$ is not a subset of $\text{Mat}_4(\R)$.
\end{example}
\begin{example}[Polynomials]
  For the spaces $P_{m}(\F)$ and $P_{n}\left(\F\right)$, if $m \leq n$, then $P_{m}\left(\F\right)$ is a subspace of $P_{n}\left(\F\right)$.
\end{example}
\begin{lemma}[Proving Subspace Relation]
  Let $V$ be a $\F$-vector space, $W\subseteq V$. Then, $W$ is a subspace of $V$ if
  \begin{enumerate}[(1)]
    \item $W$ is nonempty;
    \item $W$ is closed under addition;
    \item $W$ is closed under scalar multiplication.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof is an exercise.
\end{proof}
\begin{definition}[Linear Transformation]
  Let $V,W$ be $\F$-vector spaces. Let $T: V\rightarrow W$. We say $T$ is a linear transformation (or linear map) if for every $v_1,v_2\in V$, $c\in \F$, we have
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right).
  \end{align*}
  Note that on the left side, addition is in $V$, and on the right side, addition is in $W$.\newline

  The collection of all linear maps from $V$ to $W$ is denoted $\Hom_{\F}\left(V,W\right)$, or $\mathcal{L}\left(V,W\right)$.
\end{definition}
\begin{example}[Identity Transformation]
  Define
  \begin{align*}
    \id_{V}: V\rightarrow V,
  \end{align*}
  where $\id_V(v) = v$. We can see that $\id_V \in \Hom_{\F}\left(V,V\right)$, since
  \begin{align*}
    \id_V\left(v_1 + cv_2\right) &= v_1 + cv_2\\
                            &= \id_V\left(v_1\right) + (c)\left(\id_{V}\left(v_2\right)\right)
  \end{align*}
\end{example}
\begin{example}[Complex Conjugation]
  Let $V = \C$. Define $T: V\rightarrow V$ by $z\mapsto \overline{z}$.\newline

  We may ask whether $T\in \Hom_{\R}\left(\C,\C\right)$ or $T\in \Hom_{\C}\left(\C,\C\right)$.
  \begin{align*}
    T\left(z_1 + cz_1\right) &= \overline{z_1 + cz_2}\\
                             &= \overline{z_1} + \left(\overline{c} \right)\left(\overline{z_2}\right).
  \end{align*}
  We can see that $T\left(z_1 + cz_2\right) = T\left(z_1\right) cT\left(z_2\right)$ if and only if $c = \overline{c}$, meaning $c$ must be real. This means $T\in \Hom_{\R}\left(\C,\C\right)$, but $T\notin \Hom_{\C}\left(\C,\C\right)$.
\end{example}
\begin{example}[Matrices]
  Let $A \in \text{Mat}_{m,n}\left(\F\right)$. We define
  \begin{align*}
    T_{A}: \F^n \rightarrow \F^m\\
    x \mapsto Ax.
  \end{align*}
  Then, $T_A \in \Hom_{\F}\left(\F^n,\F^m\right)$.
\end{example}
\begin{example}[Linear Maps on Smooth Functions]
  Let $V = C^{\infty}\left(\R\right)$, which denotes the set of continuous functions with continuous derivatives at all orders. This is a vector space under pointwise addition and scalar multiplication.
  \begin{align*}
    (f+g)\left(x\right) &= f(x) + g(x)\\
    (cf)(x) &= (c)\left(f(x)\right).
  \end{align*}
  Let $a\in \R$.
  \begin{enumerate}[(1)]
    \item 
\begin{align*}
  E_a: V\rightarrow \R\\
  f \mapsto f(a).
\end{align*}
Then, $E_a \in \Hom_{\R}\left(V,\R\right)$.
\item
  \begin{align*}
    D: V\rightarrow V\\
    f\mapsto f'.
  \end{align*}
  Then, $D\in \Hom_{\R}\left(V,V\right)$.
\item 
  \begin{align*}
    I_a: V\rightarrow V\\
    f\mapsto \int_{a}^{x} f(t)\:dt.
  \end{align*}
  Then, $I_a\in \Hom_{\R}\left(V,V\right)$.
\item Treating $f(a)$ as a (constant) function,
  \begin{align*}
  \tilde{E}_a: V\rightarrow V\\
    f\mapsto f(a).
  \end{align*}
  Then, $\tilde{E}_{a}\in \Hom_{\R}\left(V,V\right)$.
  \end{enumerate}
  Additionally,
  \begin{itemize}
    \item $D\circ I_a = \text{id}_V$;
    \item $I_a\circ D = \text{id}_V - \tilde{E}_a$ for some $a\in \R$.
  \end{itemize}
\end{example}
\begin{exercise}
  Show $\Hom_{\mathbb{F}}\left(V,W\right)$ is an $F$-vector space.
\end{exercise}
\begin{exercise}
  Let $U,V,W$ be vector spaces. Let $S\in \Hom_{\mathbb{F}}\left(U,V\right)$ and $T\in \Hom_{\mathbb{F}}\left(V,W\right)$. Show $T\circ S \in \Hom_{\mathbb{F}}\left(U,W\right)$
\end{exercise}
\begin{lemma}[Image of Identity]
  Let $T\in \Hom_{V,W}$. Then, $T\left(0_V\right) = 0_W$.
\end{lemma}
\begin{definition}[Isomorphism]
  Let $T\in \Hom_{\mathbb{F}}\left(V,W\right)$ be invertible, meaning there exists $T^{-1}W\rightarrow V$ such that $T\circ T^{-1} = \text{id}_{W}$ and $T^{-1}\circ T = \text{id}_{V}$.\newline

  We say $T$ is an isomorphism, and $V,W$ are isomorphic.
\end{definition}
\begin{exercise}
  Show $T^{-1}\in \Hom_{\mathbb{F}}\left(W,V\right)$.
\end{exercise}
\begin{example}[$\R^2$ and $\C$]
  Let $V = \R^2$, $W = \C$. Define $T: \R^2\rightarrow \C$, $(x,y)\mapsto x + iy$.\newline

  We can verify that $T\in \Hom_{\R}\left(\R^2,\C\right)$. Let $\left(x_1,y_1\right),\left(x_2,y_2\right)\in \R^2$ and $r\in \R$. Then,
  \begin{align*}
    T\left(\left(x_1,y_1\right) + r\left(x_2,y_2\right)\right) &= T\left(\left(x_1 + rx_2,y_1 + ry_2\right)\right)\\
                                                               &= \left(x_1 + rx_2\right) + i\left(y_1 + ry_2\right)\\
                                                               &= x_1 + iy_1 + rx_2 + i\left(ry_2\right)\\
                                                               &= x_1 + iy_1 + r\left(x_2 + iy_2\right)\\
                                                               &= T\left(\left(x_1,y_1\right)\right) + rT\left(\left(x_2,y_2\right)\right).
  \end{align*}
  Define $T^{-1}\C \rightarrow \R^2$ by $x + iy \mapsto (x,y)$. We have $T\circ T^{-1}\left(x + iy\right) = x+iy$ is an inverse map and $T^{-1}\circ T\left(\left(x,y\right)\right) = \left(x,y\right)$. Thus, $\R^2\cong \C$ as $\R$-vector spaces.
\end{example}
\begin{example}[$P_{n}\left(\mathbb{F}\right)$ and $\F^{n+1}$]
  Set $V = P_{n}\left(\mathbb{F}\right)$ and $W = \mathbb{F}^{n+1}$.\newline

  Define $T: P_{n}\left(\F\right) \mapsto \F^{n+1}$,
  \begin{align*}
    a_0 + a_1x + \cdots + a_nx^n &\mapsto \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can verify that $T$ is linear, with inverse map $T^{-1}: \F^{n+1}\rightarrow P_{n}\left(\F\right)$
  \begin{align*}
    \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix} \mapsto a_0 + a_1x + \cdots + a_nx^n.
  \end{align*}
  Thus, $P_n(\F) \cong \F^{n+1}$.
\end{example}
\begin{definition}[Kernel]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \ker (T) &= \set{v\in V\mid T(v) = 0_W}.
  \end{align*}
  We call this the kernel of $T$.
\end{definition}
\begin{definition}[Image]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \img\left(T\right) &= T(V)\\
                      &= \set{w\in W\mid \exists v\in V\text{ such that }T(v) = w}
  \end{align*}
\end{definition}
\begin{lemma}[Kernel and Image are Subspaces]
  The kernel, $\ker(T)$, is a subspace of $V$, and the image, $\img\left(T\right)$, is a subspace of $W$.
\end{lemma}
\begin{proof}
  Since $T\left(0_V\right) = 0_W$, we know that both $\ker(T)$ and $\img\left(T\right)$ are nonempty.\newline

  Let $c\in \F$ and $v_1,v_2\in \ker(T)$. Then,
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right)\\
                             &= 0.
  \end{align*}
  Thus, $v_1 + cv_2 \in \ker(T)$.\newline

  Let $w_1,w_2\in \img\left(T\right)$. Then, there exist $u_1,u_2\in V$ such that $T\left(u_1\right) = w_1$ and $T\left(u_2\right) = w_2$. We have
  \begin{align*}
    T\left(u_1 + cu_2\right) &= T\left(u_1\right) + cT\left(u_2\right)\\
                             &= w_1 + cw_2,
  \end{align*}
  meaning $w_1 + cw_2\in \img\left(T\right)$, meaning $\img\left(T\right)$ is a subspace of $W$.
\end{proof}
\begin{lemma}[Injectivity of a Linear Transformation]
  $T$ is injective and only if $\ker(T) = \set{0_V}$.
\end{lemma}
\begin{proof}
  Suppose $T$ is injective. Let $v\in V$ be such that $T\left(v\right) = 0_W$. We also know that $T\left(0_V\right) = 0_W$. Since $T$ is injective, this means $v = 0_V$.\newline

  Let $\ker(T) = \set{0_V}$. Suppose $T\left(v_1\right) = T\left(v_2\right)$. Then,
  \begin{align*}
    T\left(v_1\right) - T\left(v_2\right) &= 0_W\\
    T\left(v_1 - v_2\right) &= 0_W,
  \end{align*}
  meaning $v_1 - v_2 \in \ker(T)$, meaning $v_1 - v_2 = 0_V$. Thus, $v_1 = v_2$.
\end{proof}
\begin{example}[Projection Map]
  Let $m > n$. Define $T: \F^{m}\rightarrow \F^n$ by
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix} \mapsto \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can see that $\img\left(T\right) = \F^n$.\newline

  To examine the kernel, let
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\in \ker(T).
  \end{align*}
  Then,
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\mapsto \begin{pmatrix}0\\\vdots\\0\end{pmatrix},
  \end{align*}
  with $n$ entries. Thus,
  \begin{align*}
    \ker(T) &= \set{\left. \begin{pmatrix}0\\0\\\vdots\\0\\a_{n+1}\\\vdots\\a_{m}\end{pmatrix}\right|a_i\in \F^m}\\
            &\cong \F^{m-n}.
  \end{align*}
\end{example}
\subsection{Bases and Dimension}%
For this section, we let $V $ be a $ \F$-vector space.
\begin{definition}[Linear Combination]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $v\in V$ is an $\F$-linear combination of $\mathcal{B}$ if there is a set $\set{a_i}_{i\in I}$ with $a_i = 0$ for all but finitely many $i$ such that
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  We write $v\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{definition}
\begin{example}
  Let $V = P_2\left(\F\right)$. Set $\mathcal{B} = \set{1,x,x^2}$. We have $\Span_{\F}\left(\mathcal{B}\right) = P_2\left(\F\right)$.
\end{example}
\begin{definition}[Linear Independence]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is $\F$-linearly independent if whenever
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V,
  \end{align*}
  we have $a_i = 0 $ for all $i\in I$. Note that these are finite sums.
\end{definition}
\begin{definition}[Hamel Basis]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is a $\F$-basis for $V$ if
  \begin{enumerate}[(1)]
    \item $\Span\left(\mathcal{B}\right) = V$
    \item $\mathcal{B}$ is linearly independent.
  \end{enumerate}
\end{definition}
\begin{example}[Standard Basis for $\F^n$]
Let $V = \F^n$. We let
\begin{align*}
  \mathcal{E}_n = \set{e_1,\dots,e_n},
\end{align*}
where
\begin{align*}
  e_1 &= \begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}\\
  e_2 &= \begin{pmatrix}0\\1\\\vdots\\0\end{pmatrix}\\
      &\vdots\\
  e_n &= \begin{pmatrix}0\\0\\\vdots\\1\end{pmatrix}.
\end{align*}
We have $\mathcal{E}_n$ is a basis of $\F^n$ referred to as the standard basis.
\end{example}
We wish to show that every vector space has a basis. In order to do so, we require Zorn's lemma.
\begin{theorem}[Zorn's Lemma]
  Let $X$ be a nonempty partially ordered set. If every totally ordered subset of $X$ has an upper bound, then there exists at least one maximal element in $X$.
\end{theorem}
\begin{theorem}
  Let $\mathcal{A}$ and $\mathcal{C}$ be subsets of $V$ with $\mathcal{A}\subseteq \mathcal{C}$. Assume $\mathcal{A}$ is linearly independent and $\Span_{\F}\left(\mathcal{C}\right) = V$. Then, there exists a basis $\mathcal{B}$ of $V$ with $\mathcal{A}\subseteq \mathcal{B}\subseteq \mathcal{C}$.
\end{theorem}
\begin{proof}
  Take
  \begin{align*}
    X &= \set{\mathcal{B}'\subseteq V\mid \mathcal{A}\subseteq \mathcal{B}'\subseteq \mathcal{C},\mathcal{B}\text{ linearly independent}}.
  \end{align*}
  We have $\mathcal{A}\in X$, meaning $X$ is nonempty. We know that $X$ is partially ordered with respect to inclusion, and has an upper bound of $\mathcal{C}$.\newline

  Thus, by Zorn's lemma, we have a maximal element in $X$. We call this maximal element $\mathcal{B}$. By the definition of $X$, $\mathcal{B}$ is linearly independent.\newline

  We claim that $\Span_{\F}\left(\mathcal{B}\right) = V$. If not, there exists some $v\in \mathcal{C}$ such that $v\notin \Span_{\F}\left(\mathcal{B}\right)$. However, if $v\notin \Span_{\F}\left(\mathcal{B}\right)$, then $\mathcal{B}\cup \set{v}\subseteq \mathcal{C}$ is linearly independent. However, since $\mathcal{B}\subsetneq \mathcal{B}\cup \set{v}$, this implies that $\mathcal{B}$ is not maximal, which is a contradiction. Thus, $\Span_{\F}\left(\mathcal{B}\right) = V$.
\end{proof}
\begin{remark}
This proof applies to all vector spaces, not just those with finite dimensions.
\end{remark}
\begin{lemma}
  A homogeneous system of $m$ linear equations in $n$ unknowns with $m < n$ has a nonzero solution.
\end{lemma}
\begin{corollary}
  Let $\mathcal{B}\subseteq V$ with $\Span_{\F}\left(\mathcal{B}\right) = V$ and $\left\vert \mathcal{B} \right\vert = m$.\newline

  Then, any set with more than $m$ elements cannot be linearly independent.
\end{corollary}
\begin{proof}
  Let $\mathcal{C} = \set{w_1,\dots,w_n}$ with $n > m$. We wish to show that $\mathcal{C}$ cannot be linearly independent.\newline

  Write $\mathcal{B} = \set{v_1,\dots,v_m}$ with $\Span_{\F}\left(\mathcal{B}\right) = V$. For each $i$, write $w_i = \sum_{j=1}^{m}a_{ji}v_j$ for some $a_{ji}\in \F$.\newline

  Consider the equations
  \begin{align*}
    \sum_{i=1}^{n}a_{ji}x_i = 0.
  \end{align*}
  We have a solution to this $\left(c_1,\dots,c_n\right) \neq \left(0,\dots,0\right)$.\newline

  We have
  \begin{align*}
    0 &= \sum_{j=1}^{m} \left(\sum_{i=1}^{n}a_{ji}c_i\right)v_j\\
      &= \sum_{i=1}^{n}c_i\left(\sum_{j=1}^{m}a_{ji}v_j\right)\\
      &= \sum_{i=1}^{n}c_iw_i.
  \end{align*}
  Thus, $\mathcal{C}$ is not linearly independent.
\end{proof}
\begin{corollary}
  If $\mathcal{B}$ and $\mathcal{C}$ are bases over $V$, with $\mathcal{B}$ and $\mathcal{C}$ finite, then $\Card \mathcal{B} = \Card \mathcal{C}$.
\end{corollary}
\begin{proof}
  Let $|\mathcal{B}| = m$, $|\mathcal{C}| = n$. Since $\mathcal{C}$ is linearly independent, we know that $n\leq m$. We reverse the roles to see that $m\leq n$.
\end{proof}
\begin{definition}[Dimension]
  Let $V$ be a $\F$-vector space with Hamel basis $\mathcal{B}$. Then, we define $\Dim_{\F} V = \Card \mathcal{B}$.
\end{definition}
\begin{theorem}
  Let $V$ be finite-dimensional with $\Dim_{\F} V = n$. Let $\mathcal{C} \subseteq V$ with $\Card \mathcal{C} = m$.
  \begin{enumerate}[(1)]
    \item If $m > n$, then $\mathcal{C}$ is not linearly independent.
    \item If $m < n$, then $\Span_{\F}\left(\mathcal{C}\right) \neq V$.
    \item If $m = n$, then the following are equal:
      \begin{itemize}
        \item $\mathcal{C}$ is a basis;
        \item $\mathcal{C}$ is linearly independent;
        \item $\Span_{\F}\left(\mathcal{C}\right) = V$.
      \end{itemize}
  \end{enumerate}
\end{theorem}
\begin{corollary}
  Let $W\subseteq V$ be a subspace. We have $\Dim_{\F}W \leq \Dim_{\F} V$.\newline

  If $\Dim_{\F} V < \infty$, then $V = W$ if and only if $\Dim_{\F} W = \Dim_{\F} V$.
\end{corollary}
\begin{example}
  Let $V = \C$.\newline

  If $\F = \C$, then $\mathcal{B} = \set{1}$, and $\Dim_{\C}\C = 1$.\newline

  If $\F = \R$, then $\mathcal{B} = \set{1,i}$, and $\Dim_{\R}\C = 2$.

  %If $\F = \Q$, then $\mathcal{B}$ is uncountable.
\end{example}
\begin{example}
  Let $V = \F[x]$, and let $f(x) \in \F[x]$ be fixed.\newline

  Define an equivalence relation $g(x) \equiv h(x) $ if $f(x)|\left(g(x) - h(x)\right)$.\newline

  Given $g(x) \in \F[x]$, write $\left[g(x)\right]$ for the equivalence class containing $g(x)$.\newline

  Define $W = \F[x] / \left(f(x)\right) = \set{\left[g(x)\right]\mid g(x)\in \F[x]}$.\newline

  Define
  \begin{align*}
    [g(x)] + [h(x)] &= [g(x) + h(x)]\\
    c[g(x)] &= [cg(x)].
  \end{align*}
  This makes $W$ into a vector space. Set $n = \deg f(x)$.\newline

  Then, we claim
  \begin{align*}
    \mathcal{B} = \set{[1],[x],\dots,\left[x^{n-1}\right]}.
  \end{align*}
  Suppose there exist $a_0,\dots,a_{n-1} \in \F$ with
  \begin{align*}
    a_0 [1] + a_1[x] + \cdots + a_{n-1}\left[x^{n-1}\right] = [0].
  \end{align*}
  Then,
  \begin{align*}
    \left[a_0 + a_1x + \cdots + a_{n-1}x^{n-1}\right] = [0].
  \end{align*}
  Therefore,
  \begin{align*}
    f(x) | \left(a_0 + a_1x + \cdots + a_{n-1}x^{n-1} - 0\right),
  \end{align*}
  which means we must have $a_0 = a_1 = \cdots = a_{n-1}$.\newline

  Let $\left[g(x)\right]\in W$. By the Euclidean algorithm,
  \begin{align*}
    g(x) &= f(x)q(x) + r(x)
  \end{align*}
  for some $q(x),r(x) \in \F[x]$ with $r(x) = 0$ or $\deg r(x) < n$. Thus, we have
  \begin{align*}
    \left[g(x)\right] &= \left[f(x)q(x)\right] + \left[r(x)\right]\\
                      &= \left[r(x)\right].
  \end{align*}
  Since $r(x) = 0$ or $\deg r(x) < n$, we must have $\left[g(x)\right] = \left[r(x)\right]\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{example}
\begin{lemma}
  Let $V$ be an $\mathbb{F}$-vector space, with $\mathcal{C} = \set{v_i}_{i\in I}$ be a subset of $V$.\newline

  Then, $\mathcal{C}$ is a basis if and only if each $v\in V$ can be uniquely written as a linear combination of elements of $\mathcal{C}$.
\end{lemma}
\begin{proof}
  Suppose $\mathcal{C}$ is a basis. Let $v\in V$, and suppose
  \begin{align*}
    v &= \sum_{i\in I}a_iv_i\\
      &= \sum_{i\in I}b_iv_i
  \end{align*}
  for some $a_i,b_i\in \mathbb{F}$. Then,
  \begin{align*}
    0_V &= \sum_{i\in I}\left(a_i - b_i\right)v_i.
  \end{align*}
  Since $\mathcal{C}$ is a basis, $a_i - b_i = 0$ for all $i$, meaning $a_i = b_i$, so the expression is unique.\newline

  Suppose every $v$ can be written as a unique linear combination of $\mathcal{C}$. Certainly, this means $\Span_{\mathbb{F}}\left(\mathcal{C}\right) = V$. Suppose
  \begin{align*}
    0_V &= \sum_{i\in I}a_iv_i
  \end{align*}
  for some $a_i\in \mathbb{F}$. It is also true that $0_V = \sum_{i\in I}0v_i$, meaning $a_i = 0$ for all $i$ by uniqueness; thus, $\mathcal{C}$ is linearly independent.
\end{proof}
\begin{proposition}
  Let $V,W$ be $\mathbb{F}$-vector spaces.
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,W\right)$. We have $T$ is uniquely determined by the image of the basis of $V$.
    \item Let $\mathcal{B}=\set{v_i}_{i\in I}$ be a basis of $V$, and let $\mathcal{C} = \set{w_i}$ be a subset of $W$. If $\Card(\mathcal{B}) = \Card\left(\mathcal{C}\right)$, there is a $T\in \Hom_{\F}\left(V,W\right)$ such that $T\left(v_i\right) = w_i$ for every $i$
  \end{enumerate}
\end{proposition}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Let $v\in V$, let $\mathcal{B} = \set{v_i}$ be a basis of $V$, and write $v = \sum_{i\in I}a_iv_i$. We have
  \begin{align*}
    T\left(v\right) &= T\left(\sum_{i\in I}a_iv_i\right)\\
                    &= \sum_{i\in I}a_iT\left(v_i\right).
  \end{align*}
    \item  Define $T$ by setting
      \begin{align*}
        T\left(v\right) &= \sum_{i\in I}a_iw_i,
      \end{align*}
      for $v = \sum_{i\in I}a_iv_i$. We can verify that $T$ is linear.
  \end{enumerate}
\end{proof}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$, with $\mathcal{B} = \set{v_i}$ a basis of $V$ and $\mathcal{C} = \set{w_i}\subseteq W$, with $w_i = T\left(v_i\right)$. Then, we have $\mathcal{C}$ is a basis of $W$ if and only if $T$ is an isomorphism.
\end{corollary}
\begin{proof}
  Let $\mathcal{C}$ be a basis for $W$. Since $\mathcal{C}$ is a basis of $W$, we use the proposition to define $S\in \Hom_{\F}\left(W,V\right)$ with $S\left(w_i\right) = v_i$. We can verify that $T\circ S = \text{id}_{W}$ and $S\circ T = \text{id}_V$, meaning $S = T^{-1}$ and $T$ is an isomorphism.\newline

  Suppose $T$ is an isomorphism. Let $w\in W$. Since $T$ is an isomorphism, $T$ is surjective, meaning there exists $v\in V$ such that $T(v) = w$. Since $\mathcal{B}$ is a basis of $V$, we expand $v$ to have
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  Combining these two facts, we have
  \begin{align*}
    w &= T(v)\\
      &= T\left(\sum_{i\in I}a_iv_i\right)\\
      &= \sum_{i\in I}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right).
  \end{align*}
  Thus, $W = \Span_{\F}\left(\mathcal{C}\right)$.\newline

  Suppose there exists $a_i\in \F$ with $\sum_{i\in I}a_iT\left(v_i\right) = 0_W$. Since $T$ is linear, we have
  \begin{align*}
    \sum_{i\in I}a_iT\left(v_i\right) &= T\left(\sum_{i\in I}a_iv_i\right).
  \end{align*}
  Since $T$ is injective, we have
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V.
  \end{align*}
  Since $\mathcal{B}$ is a basis, we have $a_i = 0$.
\end{proof}
\begin{theorem}[Rank--Nullity]
  Let $V$ be finite-dimensional vector space over $\mathbb{F}$. Let $T\in \Hom_{\F}\left(V,W\right)$. Then,
  \begin{align*}
    \Dim_{\F}(V) &= \Dim_{\F}\left(\ker (T)\right) + \Dim_{\F}\left(\img (T)\right)
  \end{align*}
\end{theorem}
\begin{proof}
  Let $\Dim_{\F}\left(\ker(T)\right) = k$ and $\Dim_{\F}\left(V\right) = n$. Let $\mathcal{A} = \set{v_1,\dots,v_k}$ be a basis of $\ker(T)$. We extend $\mathcal{A}$ to a basis $\mathcal{B} = \set{v_1,\dots,v_n}$ of $V$.\newline

  We want to show that $\mathcal{C} = \set{T\left(v_{k+1}\right),\dots,T\left(v_n\right)}$ is a basis of $\img(T)$.\newline

  Let $w\in \img(T)$. Then, there is $v\in V$ such that $T(v) = w$. We write
  \begin{align*}
    v &= \sum_{i=1}^{n}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    w &= T\left(v\right)\\
      &= T\left(\sum_{i=1}^{n}a_iv_i\right)\\
      &= \sum_{i=1}^{n}a_iT\left(v_i\right)\\
      &= \sum_{i=k+1}^{n}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right),
  \end{align*}
  since $\set{v_1,\dots,v_k}\subseteq \ker(T)$, meaning $\Span_{\F}\left(\mathcal{C}\right) = \im(T)$.\newline

  Suppose we have
  \begin{align*}
    \sum_{i=k+1}^{n}a_iT\left(v_i\right) = 0_W.
  \end{align*}
  Then, we have
  \begin{align*}
    T\left(\sum_{i=k+1}^{n}a_iv_i\right) &= 0_W,
  \end{align*}
  meaning $\sum_{i=k+1}^{n}a_iv_i\in \ker(T)$. This means there exist $a_1,\dots,a_k$ such that
  \begin{align*}
    \sum_{i=k+1}^{n}a_iv_i &= \sum_{i=1}^{k}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    \sum_{i=1}^{k}a_iv_i + \sum_{i=k+1}^{n}\left(-a_i\right)v_i = 0_V.
  \end{align*}
  Since $\set{v_i}$ are a basis, this means $a_i = 0$ for all $i$.
\end{proof}
\begin{corollary}
  Let $V,W$ be $\mathbb{F}$-vector spaces with $\Dim_{\F}\left(V\right) = n$. Let $V_1\subseteq V$ be a subspace with $\Dim_{\F}\left(V_1\right) = k$, and $W_1\subseteq W$ a subspace with $\Dim_{\F}\left(W_1\right) = n-k$. Then, there exists $T\in \Hom_{\F}\left(V,W\right)$ such that $\ker(T) = V_1$ and $\img(T) = W_1$.
\end{corollary}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$ with $\Dim_{\F}\left(V\right) = \Dim_{\F}\left(W\right) < \infty$. Then, the following are equivalent:
  \begin{enumerate}[(1)]
    \item $T$ is an isomorphism;
    \item $T$ is injective;
    \item $T$ is surjective.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $A\in \Mat_{n}\left(\F\right)$. The following are equivalent:
  \begin{enumerate}[(1)]
    \item $A$ is invertible;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $BA = I_{n}$;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $AB = I_n$.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $\Dim_{\F}(V) = m$ and $\Dim_{\F}(W) = n$.
  \begin{enumerate}[(1)]
    \item If $m < n$ and $T\in \Hom_{\F}\left(V,W\right)$, then $T$ is not surjective.
    \item If $m > n$ and $T\in \Hom_\F\left(V,W\right)$, then $T$ is not injective.
    \item We have $m = n$ if and only if $V\cong W$.
  \end{enumerate}
\end{corollary}
\subsection{Direct Sums and Quotient Spaces}%
\begin{definition}[Sum of Subspaces]
  Let $V$ be a vector space, and $V_1,\dots,V_k$ be subspaces. Then, the sum of $V_1,\dots,V_k$ is
  \begin{align*}
    V_1 + \cdots + V_k &= \set{\sum_{i=1}^{k}v_i\mid v_i\in V_i}.
  \end{align*}
  This is a subspace of $V$.
\end{definition}
\begin{definition}[Independence of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V_1,\dots,V_k$ are independent if whenever $v_1 + \cdots v_k = 0_V$, we have $v_i = 0_V$.
\end{definition}
\begin{definition}[Direct Sum of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V$ is the direct sum of $V_1,\dots,V_k$, and write
  \begin{align*}
    V = V_1 \oplus \cdots \oplus V_k,
  \end{align*}
  if the following conditions hold.
  \begin{enumerate}[(1)]
    \item $V = V_1 + \cdots V_k$;
    \item $V_1,\dots,V_k$ are independent.
  \end{enumerate}
\end{definition}
\begin{example}[A Very Simple Direct Sum]
  Let $V = \F^2$, with $V_1 = \set{\left(x,0\right)\mid x\in \F}$ and $V_2 = \set{\left(0,y\right)\mid y\in \F}$, we can see that
  \begin{align*}
    V_1 + V_2 &= \set{\left(x,0\right) + \left(0,y\right)\mid x,y\in \F}\\
              &= \set{\left(x,y\right)\mid x,y\in \F}\\
              &= \F^2.
  \end{align*}
  If $\left(x,0\right) + \left(0,y\right) = 0$, then $x = 0$ and $y = 0$, meaning $\F^2 = V_1\oplus V_2$.
\end{example}
\begin{example}[Direct Sum Constructions]
  Let $V = \F[x]$.\newline

  Define $V_1 = \F$, $V_2 = \F x = \set{\alpha x\mid \alpha \in \F}$, $V_3 = P_1\left(\F\right)$.\newline

  We can see that
  \begin{align*}
    P_1 &= V_1\oplus V_2.
  \end{align*}
  However, $V_1$ and $V_3$ are not independent, since $1_{\F}\in V_1$ and $-1_{\F}\in V_3$ with $1_{\F} + \left(-1_{\F}\right) = 0_\F$.
\end{example}
\begin{example}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$, with $V_i = \Span(v_i)$. Then,
  \begin{align*}
    V = V_1\oplus \cdots \oplus V_n.
  \end{align*}
\end{example}
\begin{lemma}
  Let $V$ be a vector space, $V_1,\dots,V_k$ subspaces. We have $V = V_1\oplus \cdots \oplus V_k$ if and only if every $v\in V$ can be written uniquely in the form
  \begin{align*}
    v = v_1 +\cdots + v_k
  \end{align*}
  for $v_i\in V_i$.
\end{lemma}
\begin{proof}
  Suppose $V = V_1\oplus\cdots\oplus V_k$. Let $v\in V$. Then, $v = v_1 + \cdots + v_k$ for some $v_i\in V_i$ since $V = V_1+\cdots+V_k$. Suppose
  \begin{align*}
    v &= v_1 + \cdots v_k\\
      &= \tilde{v}_1 + \cdots + \tilde{v}_k
  \end{align*}
  for $v_i,\tilde{v}_i\in V_i$. Then,
  \begin{align*}
    0_V &= \left(v_1 - \tilde{v}_1\right) + \cdots + \left(v_k - \tilde{v}_k\right).
  \end{align*}
  Since $V_1,\dots,V_k$ are linearly independent, $v_i - \tilde{v}_i\in V_i$, we have $v_i - \tilde{v}_i = 0_V$, meaning the expression for $v$ is unique.\newline

  Suppose that every $v\in V$ can be written uniquely in the form $v = v_1 + \cdots + v_k$ with $v_i\in V_i$. Then,
  \begin{align*}
    V = V_1 + \cdots V_k
  \end{align*}
  by the definition of $V_1 + \cdots + V_k$. If
  \begin{align*}
    0_V = v_1 + \cdots v_k
  \end{align*}
  for $v_i\in V_i$, and it is also the case that
  \begin{align*}
    0_V = 0_V + \cdots + 0_V,
  \end{align*}
  with $0_V \in V_i$, then it must be the case that $v_i = 0_V$ for all $i$ by uniqueness. Thus, the $V_i$ are independent, so
  \begin{align*}
    V = V_1\oplus\cdots\oplus V_k.
  \end{align*}
\end{proof}
\begin{exercise}
  Let $V_1,\dots,V_k$ be subspaces of $V$. For each $i$, let $\mathcal{B}_i$ be a basis for $V_i$. Let $\mathcal{B} = \bigcup_{i =1}^{k}\mathcal{B}_i$. Show
  \begin{enumerate}[(1)]
    \item $\mathcal{B}$ spans $V$ if and only if $V = V_1 + \cdots + V_k$;
    \item $\mathcal{B}$ is linearly independent if and only if $V_1,\dots,V_k$ are independent;
    \item $\mathcal{B}$ is a basis if and only if $V = V_1 \oplus \cdots \oplus V_k$.
  \end{enumerate}
\end{exercise}
\begin{lemma}[Existence of Complement]
Let $V$ be a vector space, and $U\subseteq V$ be a subspace. Then, $U$ has a complement $W$ such that $U\oplus W = V$.
\end{lemma}
\begin{proof}
  Let $\mathcal{A} $ be a basis for $U$. Extend $\mathcal{A}$ to a basis $\mathcal{B}$ of $V$. Let $\mathcal{C} = \mathcal{B}\setminus \mathcal{A}$, and $W = \Span\left(\mathcal{C}\right)$.
\end{proof}
\begin{example}[Constructing a Quotient Group]
To introduce quotient spaces, consider the construction of the quotient group.\newline

Let $n\in \Z_{>1}$. We say $a \equiv b$ modulo $n$ if and only if $n|(a-b)$. This is an equivalence relation; we form $\Z/n\Z = \set{\left[a\right]_n\mid a\in\Z} = \set{\left[0\right]_n,\dots,\left[n-1\right]_n}$.\newline

However, we also do this by defining $n\Z = \set{nk\mid k\in\Z}$, and taking $a\equiv b$ mod $n$ if and only if $a-b \in n\Z$. Our equivalence classes are now
\begin{align*}
  \left[a\right]_n &= \set{a + nk\mid k\in\Z}\\
                   &= a + n\Z.
\end{align*}
\end{example}
\begin{definition}[Quotient Space]
  Let $W\subseteq V$ be a subspace. We say $v_1\sim v_2$ if $v_1 - v_2 \in W$. Note that if $w\in W$, then $w\sim 0_V$ since $w-0_V\in W$.\newline

  This is an equivalence relation.
  \begin{itemize}
    \item Reflexivity: since $W$ is a subspace, $0_V\in W$, meaning $v-v\in W$ for all $v\in V$.
    \item Symmetry: if $v_1\sim v_2$, then $v_1 - v_2 \in W$, meaning $-\left(v_1 - v_2\right)\in W$, so $v_2 - v_1\in W$, or $v_2 \sim v_1$.
    \item Transitivity: Let $v_1\sim v_2$ and $v_2\sim v_3$. Then, $v_1 - v_2\in W$ and $v_2 - v_3\in W$. Since $W$ is a subspace, $\left(v_1 - v_2\right) + \left(v_2 - v_3\right)\in W$, meaning $v_1 - v_3 \in W$, so $v_1 \sim v_3$.
  \end{itemize}
  We denote the equivalence classes by
  \begin{align*}
    \left[v\right] &= \left[v\right]_W\\
                   &= v+W\\
                   &= \set{\tilde{v}\in V\mid v\sim \tilde{v}}\\
                   &= \set{v+w\mid w\in W}.
  \end{align*}
  We set
  \begin{align*}
    V/W &:= \set{v + W\mid v\in V}.
  \end{align*}
  We need to define vector addition and scalar multiplication on $V/W$. Let $v_1 + W,v_2 + W\in V/W$ and $c\in\F$. Define
  \begin{align*}
    \left(v_1 + W\right) + \left(v_2 + W\right) &= \left(v_1 + v_2\right) + W\\
    c\left(v_1 + W\right) &= cv_1 + W.
  \end{align*}
  We will show that addition and scalar-multiplication are well-defined.
  \begin{description}
    \item[Addition:] Let $v_1 + W = \tilde{v}_1 + W$, $v_2 + W = \tilde{v}_2 + W$, meaning $v_1 = \tilde{v}_1 + w_1$ and $v_2 = \tilde{v}_2 + w_2$ for some $w_1,w_2 \in W$. We have
      \begin{align*}
        \left(v_1 + W\right) + \left(v_2 + W\right) &= \left(v_1 + v_2\right) + W\\
                                                    &= \left(\tilde{v}_1 + w_1 + \tilde{v}_2 + w_2\right) + W\\
                                                    &= \left(\tilde{v}_1 + \tilde{v}_2\right) + W
      \end{align*}
    \item[Scalar Multiplication:] Let $v + W = \tilde{v} + W$. Then, we have $v = \tilde{v} + w$ for some $w\in W$. For $c\in\F$, we have
      \begin{align*}
        c\left(v + W\right) &= cv + W\\
                            &= c\left(\tilde{v} + w\right) + W\\
                            &= c\tilde{v} + W\\
                            &= c\left(\tilde{v} + W\right).
      \end{align*}
  \end{description}
  We say $V/W$ is the quotient space of $V$ by $W$.
\end{definition}
\begin{example}[Quotient Space of $\R^2$]
  Let $V = \R^2$, and $W = \set{\left(x,0\right)\mid x\in \R}$.\newline

  Let $\left(x_0,y_0\right)\in V$. We have
  \begin{align*}
  \left(x_0,y_0\right) \sim \left(x,y\right)
  \end{align*}
  if
  \begin{align*}
    \left(x_0 - x,y_0 - y\right)\in W.
  \end{align*}
  The only condition is thus that the $y$-coordinates in $\R^2$ must be equal. Therefore,
  \begin{align*}
    \left(x_0,y_0\right) + W &= \set{(x,y_0)\mid x\in\R}.
  \end{align*}
  Define $\tau: \R\rightarrow V/W$, $y\mapsto \left(0,y\right) + W$. We claim that $\tau$ is an isomorphism.\newline

  Let $y_1,y_2,c\in\R$. We have
  \begin{align*}
    \tau\left(y_1 + cy_2\right) &= \left(0,y_1 + cy_2\right) + W\\
                                &= \left(\left(0,y_1\right) + W\right) + c\left(\left(0,y_2\right) + W\right)\\
                                &= \tau\left(y_1\right) + c\tau\left(y_2\right).
  \end{align*}
  Thus, we see that $\tau$ is a linear map.\newline

  To show surjectivity, let $\left(x,y\right) + W\in V/W$. We have $\left(x,y\right) + W = \left(0,y\right) + W$. Thus, $\tau$ is surjective, since
  \begin{align*}
    \tau\left(y\right) &= \left(0,y\right) + W\\
                       &= \left(x,y\right) + W.
  \end{align*}
  Finally, to show injectivity, we let $y\in\ker\left(\tau\right)$. We have
  \begin{align*}
    \tau\left(y\right) &= \left(0,y\right) + W\\
                       &= \left(0,0\right) + W,
  \end{align*}
  implying that $y = 0$. Thus, $\tau$ is injective.
\end{example}
\begin{example}[Quotient Space of Polynomials]
  Let $V = \F[x]$, $f(x) \in V$, and
  \begin{align*}
    W &= \set{g(x)\in \F[x]\mid f(x)|g(x)}.
  \end{align*}
  We can see that $W$ is a subspace, which we refer to as $\left\langle f(x) \right\rangle$.\newline

  We defined an equivalence class $g(x) \sim h(x)$ if $f(x) | \left(g(x) - h(x)\right)$, where we then constructed a vector space from this set.\newline

  In particular, this construction is realized as $V/W$.\footnote{The ramifications of this construction are covered in depth in Algebra II.}
\end{example}
\begin{definition}[Canonical Projection]
  Let $W\subseteq V$ be a subspace. The canonical projection map $\pi_W$ is defined by
  \begin{align*}
    \pi_W: V\rightarrow V/W\\
    v\mapsto v + W.
  \end{align*}
  Note that $\pi_W\in \Hom_{\F}\left(V,V/W\right)$.
\end{definition}
\begin{remark}
  To define a map $T: V/W\rightarrow U$, one must always verify that $T$ is well-defined.
\end{remark}
\begin{theorem}[First Isomorphism Theorem for Vector Spaces]
  Let $T\in\Hom_{\F}\left(V,W\right)$. Define $\overline{T}: V/\ker(T)\rightarrow W$ by taking $v + \ker(T) \mapsto T(v)$. Then, $\overline{T}\in \Hom_{\F}\left(V/\ker(T),W\right)$. Moreover, $V/\ker(T)\cong \img(T)$.
\end{theorem}
\begin{proof}
  We will first show that $\overline{T}$ is well-defined. Let $v_1 + \ker(T) = v_2 + \ker(T)$. Then, for some $\tilde{v}\in \ker(T)$, we have $v_1 = v_2 + \tilde{v}$. Then,
  \begin{align*}
    \overline{T}\left(v_1 + \ker(T)\right) &= T\left(v_1\right)\\
                                           &= T\left(v_2 + \tilde{v}\right)\\
                                           &= T\left(v_2\right) + T\left(\tilde{v}\right)\\
                                           &= T\left(v_2\right)\\
                                           &= \overline{T}\left(v_2 + \ker(T)\right).
  \end{align*}
  Let $v_1 + \ker(T)$, $v_2 + \ker(T)\in V/\ker(T)$, and $c\in \F$. Then, we have
  \begin{align*}
    \overline{T}\left(\left(v_1 + \ker(T)\right) + c\left(v_2 + \ker(T)\right)\right) &= \overline{T}\left(\left(v_1 + cv_2\right) + \ker(T)\right)\\
                                                                                      &= T\left(v_1 + cv_2\right)\\
                                                                                      &= T\left(v_1\right) + cT\left(v_2\right)\\
                                                                                      &= \overline{T}\left(v_1 + \ker(T)\right) + c\overline{T}\left(v_2 + \ker(T)\right).
  \end{align*}
  Let $w\in \img(T)$. Then, $w = T(v)$ for some $v\in V$, meaning
  \begin{align*}
    w &= T\left(v\right)\\
    &= \overline{T} \left(v + \ker(T)\right).
  \end{align*}
  Thus, $\overline{T}$ is surjective onto $\img(T)$.\newline

  Let $v + \ker(T)\in \ker\left(\overline{T}\right)$. Then,
  \begin{align*}
    \overline{T}\left(v + \ker(T)\right) &= 0_W.
  \end{align*}
  This gives
  \begin{align*}
    T\left(v\right) = 0_W,
  \end{align*}
  meaning $v\in \ker(T)$, meaning $v + \ker(T) = 0_V + \ker(T)$. Thus, $\overline{T}$ is injective.
\end{proof}
\subsection{Dual Spaces}%
\begin{definition}[Dual Space]
  Let $V$ be an $\F$-vector space. The dual space, $V'$,\footnote{My professor denotes this as $V^{\vee}$, but it's too hard to type that out in real time, so I will use the $'$ to denote the algebraic dual, just as $V^{\ast}$ denotes the continuous dual of $V$.} is defined to be
  \begin{align*}
    V':= \Hom_{\F}\left(V,\F\right).
  \end{align*}
\end{definition}
\begin{theorem}
  We have $V$ is isomorphic to a subspace of $V'$. If $\Dim_{\F}\left(V\right) < \infty$, then $V\cong V'$.
\end{theorem}
\begin{remark}
  The isomorphism between $V$ and $V'$ in the finite-dimensional case is not canonical --- that is, it depends on a basis.
\end{remark}
\begin{proof}
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a basis for $V$.\newline

  For each $i\in I$, let $v_i'(v_j) = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta. We get $\set{v_i'}_{i\in I}$ are elements of $V'$. We obtain
  \begin{align*}
    T\in \Hom_{\F}\left(V,V'\right)
  \end{align*}
  by $T\left(v_i\right) = v_i'$.\newline

  To show $V$ is isomorphic to a subspace of $V'$, it suffices to show that $T$ is injective, since $V\cong \img(T)$, which is a subspace of $V'$.\newline

  Let $v\in V$ with $T(v) = 0_{V'}$. We write
  \begin{align*}
    v &= \sum_{i\in I}a_iv_i\\
    0_{V'}&= T(v)\\
                    &= \sum_{i\in I}a_iT\left(v_i\right)\\
                    &= \sum_{i\in I}a_iv_i'.
  \end{align*}
  Pick $j$ with $a_j\neq 0$. Note that
  \begin{align*}
    \sum_{i\in I}a_iv_i'(v_j) &= 0\\
                              &= a_j,
  \end{align*}
  which contradicts $a_j\neq 0$. Thus, $v = 0_V$, and $T$ is injective.\newline

  Suppose $\Dim_{\F}\left(V\right) = n$, with $\mathcal{B} = \set{v_1,\dots,v_n}$. Let $\in V'$. Define $a_i$ by
  \begin{align*}
    a_i &= \left(v_i\right).
  \end{align*}
  Set
  \begin{align*}
    v &= \sum_{i=1}^{n}a_iv_i.
  \end{align*}
  Define the map $S: V'\rightarrow V$ by taking
  \begin{align*}
    S\left(\right) &= \sum_{i=1}^{n}\left(v'\left(v_i\right)\right)v_i.
  \end{align*}
  We want to show that $S\in \Hom_{\F}\left(V',V\right)$, and $S$ is the inverse to $T$.\newline

  Let $,w'\in V'$, $c\in \F$. Set $a_i = v'\left(v_i\right)$ and $b_i = w'\left(v_i\right)$. Then,
  \begin{align*}
    S\left( + cw'\right) &= \sum_{i=1}^{n}\left(v' cw'\right)\left(v_i\right)v_i\\
                           &= \sum_{i=1}^{n}\left(\left(v_i\right) + cw'\left(v_i\right)\right)v_i\\
                           &= \sum_{i=1}^{n}\left(\left(v_i\right)\right)v_i + c\sum_{i=1}^{n}w'\left(v_i\right)\\
                           &= S\left(\right) + cS\left(w'\right).
  \end{align*}
  We compute $S\circ T\left(v_i\right)$.
  \begin{align*}
    S\circ T\left(v_j\right) &= S\left(T\left(v_j\right)\right)\\
                             &= S\left(v_j'\right)\\
                             &= \sum_{i=1}^{n}v_j'\left(v_i\right)v_i\\
                             &= \sum_{i=1}^{n}\delta_{ij}v_i\\
                             &= v_j.
  \end{align*}
  Note that for $T\circ S$, we have $T\circ S$ maps $V'$ to $V'$, meaning we need to check that $T\circ S$ is the identity map on $V'$. Let $\in V'$. Then,
  \begin{align*}
    \left(T\circ S\right)\left(\right)\left(v_j\right) &= T\left(S\left(v'\right)\right)\left(v_j\right)\\
                                                         &= T\left(\sum_{i=1}^{n}\left(v_i\right)v_i\right)\left(v_j\right)\\
                                                         &= \left(\sum_{i=1}^{n}\left(v_i\right)T\left(v_i\right)\right)\left(v_j\right)\\
                                                         &= \sum_{i=1}^{n}\left(v_i\right)\left(v_i'\left(v_j\right)\right)\\
                                                         &= \sum_{i=1}^{n}\left(v_i\right)\delta_{ij}\\
                                                         &= \left(v_j\right).
  \end{align*}
\end{proof}
\begin{definition}[Dual Basis]
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$. The dual basis for $V'$ is
  \begin{align*}
    \mathcal{B}' &= \set{v_i',\dots,v_n'}.
  \end{align*}
\end{definition}
\begin{remark}
  It is possible to continue taking duals; in the case of finite-dimensional $V$, we have
  \begin{align*}
    V &\cong V'\\
    V' &\cong V''.
  \end{align*}
  Despite the isomorphism between $V$ and $V'$ not being canonical, it is the case that the isomorphism between $V$ and $V''$ \textit{is} canonical (i.e., not dependent on a basis).
\end{remark}
\begin{proposition}
  There is a canonical injective linear map from $V$ to $V''$. If $\Dim_{\F}\left(V\right) < \infty$, this is an isomorphism.
\end{proposition}
\begin{proof}
  Let $v\in V$. Define $\hat{v}: V' \rightarrow \F$, $\varphi \mapsto \varphi(v)$.\footnote{This can be notated as $\text{eval}_v$, but $\hat{v}$ is faster to type (and it's used in functional analysis).} We can easily verify that $\hat{v}$ is a linear map.\newline

  Therefore, we have $\hat{v}\in \Hom_{\F}\left(V',\F\right) = V''$. We have a map
  \begin{align*}
    \Phi: V\rightarrow V''\\
    v\mapsto \hat{v}.
  \end{align*}
  We want to verify that $\Phi$ is a linear and injective map. Let $v_1,v_2\in V$, $c\in \F$. Let $\varphi\in V'$.
  \begin{align*}
    \Phi\left(v_1 + cv_2\right)\left(\varphi\right) &= \left(\hat{v}_1 + c\hat{v}_2\right)\left(\varphi\right)\\
                                                    &= \varphi\left(v_1 + cv_2\right)\\
                                                    &= \varphi\left(v_1\right) + c\varphi\left(v_2\right)\\
                                                    &= \hat{v}_1\left(\varphi\right) + c\hat{v}_2\left(\varphi\right)\\
                                                    &= \Phi\left(v_1\right)(\varphi) + c\Phi\left(v_2\right)\left(\varphi\right).
  \end{align*}
  We will show that $\Phi$ is injective. Let $v\in V$; suppose $v\neq 0_V$. We form a basis $\mathcal{B}$ of $V$ that contains $v$. Note that $\in V'$, with $v'(v) = 1$ and $v'(w) = 0$ for $w\in \mathcal{B}$ and $w\neq v$.\newline
  
  Assume $v\in \ker\left(\Phi\right)$. Then, for any $\varphi\in V'$,
  \begin{align*}
    \Phi\left(v\right)(\varphi) &= 0\\
    \varphi(v) &= 0.
  \end{align*}
  However, this is a contradiction, as we can take $\varphi = $, where $\varphi(v) = 1$. Thus, it must be the case that $\Phi$ is injective.
\end{proof}
\begin{definition}[Dual Operator]
  Let $T\in \Hom_{\F}\left(V,W\right)$. We get an induced map $T': W'\rightarrow V'$. We define $T'\left(\varphi\right) = \varphi\circ T$.
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEio4ePrNWiEAB1dAMW7iYUAObwioAGYAnCAFskZEDghIATNU1SdAFQByAAp9els0AAssAEoQagY6ACMYBgAFfmUhEFssMwicORt7J0QXNyRRCS02P0KQO0cK6nLELyrfPV0wyKxjLiA
\begin{tikzcd}
V \arrow[rd, "T'(\varphi)"'] \arrow[r, "T"] & W \arrow[d, "\varphi"] \\
                                            & \F                    
\end{tikzcd}
  \end{center}
\end{definition}
\section{Choosing Coordinates}%
\subsection{Linear Transformations and Matrices}%
Let $V$ be a finite-dimensional $\mathbf{F}$-vector space. Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis. This vector space fixes an isomorphism $V \cong \mathbf{F}^n$.\newline

Let $v\in V$. We can write $v = \sum_{i=1}^{n}a_iv_i$ for some $a_i\in \F$. We take the map
\begin{align*}
  T_{\mathcal{B}}\left(v\right) &= \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\in \F^n.
\end{align*}
It is easy to see that $T$ is an isomorphism. Given $v\in V$, we write $\left[v\right]_{\mathcal{B}} = T_{\mathcal{B}}\left(v\right)$. We refer to this process as choosing coordinates.
\begin{example}
  Let $V = \Q^2$, and $\mathcal{B} = \set{ \begin{pmatrix}1\\1\end{pmatrix}, \begin{pmatrix}1\\-1\end{pmatrix} }$. We can check that $\mathcal{B}$ is a basis of $V$.\newline

  Let $v\in V$, $v = \begin{pmatrix}a\\b\end{pmatrix}$. We have
  \begin{align*}
    v &= \frac{a+b}{2} \begin{pmatrix}1\\1\end{pmatrix} + \frac{a-b}{2} \begin{pmatrix}1\\-1\end{pmatrix}.
  \end{align*}
  To represent $v$ in terms of this basis, we have
  \begin{align*}
    \left[v\right]_{\mathcal{B}} &= \begin{pmatrix}\frac{a+b}{2}\\\frac{a-b}{2}\end{pmatrix}.
  \end{align*}
  If we chose a different basis, such as the standard basis $\mathcal{E}_2 = \set{ \begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix} }$. In that case, we have
  \begin{align*}
    \left[v\right]_{\mathcal{E}_2} &= \begin{pmatrix}a\\b\end{pmatrix}.
  \end{align*}
\end{example}
\begin{example}
  Let $V = P_2\left(\R\right)$. Let $\mathcal{C} = \set{1,\left(x-1\right),\left(x-1\right)^2}$. We  know that $\mathcal{C}$ is a basis of $V$.\newline

  Let $f(x) = a + bx + cx^2\in P_2\left(\R\right)$. We can write $f$ in terms of this basis by taking
  \begin{align*}
    f(x) &= \left(a+b+c\right) + \left(b+2c\right)\left(x-1\right) + c\left(x-1\right)^2.
  \end{align*}
  In this case, we then have
  \begin{align*}
    \left[f(x)\right]_{\mathcal{C}} &= \begin{pmatrix}a+b+c\\b+2c\\c\end{pmatrix}.
  \end{align*}
\end{example}
Recall that given $A\in \Mat_{m,n}\left(\F\right)$, we obtain a linear map $T_{A}\in \Hom_{\F}\left(\F^{n},\F^{m}\right)$ by $T_A\left(v\right) = Av$. The converse is true as well. Given any map $T\in \Hom_{\F}\left(\F^{n},\F^{m}\right)$, there is a matrix $A$ such that $T = T_A$.\newline

Let $\mathcal{E}_n = \set{e_1,\dots,e_n}$ be the standard basis of $\F^n$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be the standard basis of $\F^{m}$.\newline

We have $T\left(e_j\right)\in \F^m$ for each $j$, meaning we have $a_{ij}\in \F$ with $T\left(e_j\right) = \sum_{i=1}^{m}a_{ij}f_j$.\newline

Define $A = \left(a_{ij}\right)_{ij}\in \Mat_{m,n}\left(\F\right)$. We want to show that $T_A\left(e_j\right) = T\left(e_j\right)$ for every $j$.\newline

Then, we have
\begin{align*}
  T_A\left(e_j\right) &= Ae_j\\
                      &= \sum_{a_{ij}}f_i\\
                      &= T\left(e_j\right).
\end{align*}
Let $T\in \Hom_{\F}\left(V,W\right)$. Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$ and $\mathcal{C} = \set{w_1,\dots,w_m}$ be a basis for $W$.\newline

Define $P = T_{\mathcal{B}}: V\rightarrow \F^n$, $v\mapsto \left[v\right]_{\mathcal{B}}$, $Q =  T_{\mathcal{C}}: W\rightarrow \F^{m}$, $w \mapsto \left[w\right]_{\mathcal{C}}$. This yields the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEiZYePrNWiEAB1dAMQB6wMFzl8lgoqPXVNUnfuPAAtua7iYUAObwioABmAE4QrkhkIDgQSKISWmwAKhYgIWER1NFIAEz2ktogAAog1Ax0AEYwDIX8ykIgwVg+ABY4KWnhiHFZiADMeQk6AIolIGWV1bXWOgwwgW08QaGduVExfQOOIEP6AMZYwbsABIl7B8eFJgC0wualFVU1Vio6jS0LFFxAA
\begin{tikzcd}
  V \arrow[r, "T"] \arrow[d, "T_{\mathcal{B}}"']          & W \arrow[d, "T_{\mathcal{C}}"] \\
  \F^{n} \arrow[r, "T_{\mathcal{C}}\circ T\circ T_{\mathcal{B}}^{-1}"'] & \F^{m}          
\end{tikzcd}
\end{center}
In particular, this means $T$ is given by a matrix $A\in \Mat_{m,n}\left(\F\right)$, which we write as $\left[T\right]_{\mathcal{B}}^{\mathcal{C}} = A$.\newline

In particular, $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$ is the unique matrix that satisfies
\begin{align*}
  \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left(\left[v\right]_{\mathcal{B}}\right) &= \left[T(v)\right]_{\mathcal{C}}.
\end{align*}

To compute $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$, we have
  \begin{align*}
    T\left(v_j\right) &= \sum_{i=1}^{m}a_{ij}w_i \tag*{$a_{ij}\in\F$}\\
    \left[T\left(v_j\right)\right]_{\mathcal{C}} &= \left[\sum_{i=1}^{m}a_{ij}w_j\right]_{\mathcal{C}}\\
                                                 &= \begin{pmatrix}a_{1j}\\\vdots\\a_{mj}\end{pmatrix}.
  \end{align*}
  Similarly, since $\left[v\right]_{\mathcal{B}} = e_j$, we have
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left(e_j\right) &= \left[T\left(v_j\right)\right]_{\mathcal{C}}\\
                                                               &= \begin{pmatrix}a_{1j}\\\vdots\\a_{mj}\end{pmatrix},
  \end{align*}
  which is exactly the $j$th column of $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$.\newline

  We thus get a matrix of the form
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}} &= \begin{pmatrix}\left[T\left(v_1\right)\right]_{\mathcal{C}} & \cdots & \left[T\left(v_n\right)\right]_{\mathcal{C}}\end{pmatrix},
  \end{align*}
  where $\left[T\left(v_j\right)\right]_{\mathcal{C}}$ are column vectors.
\begin{example}
  Let $V = P_{3}\left(\R\right)$. Define $T\in \Hom_{\R}\left(V,V\right)$ by $T\left(f(x)\right) = f'(x)$.\newline

  We take $\mathcal{B} = \set{1,x,x^2,x^3}$ as our basis. Then, we have
  \begin{align*}
    T\left(1\right) &= 0\\
    T\left(x\right) &= 1\\
    T\left(x^2\right) &= 2x\\
    T\left(x^3\right) &= 3x^2.
  \end{align*}
  As we fill in our matrix, we have
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{B}} &= \begin{pmatrix}0 & 1 & 0 & 0\\  0 & 0 & 2 & 0\\ 0 & 0 & 0 & 3 \\ 0 & 0 & 0 & 0 \end{pmatrix}.
  \end{align*}
  We can view each column as a basis vector of $\mathcal{B}$ and each row as the corresponding representation in $\mathcal{C}$ (where, in this case, $\mathcal{C} = \mathcal{B}$).
\end{example}
\begin{example}
  Let $V = P_{3}\left(\R\right)$, $T\left(f(x)\right) = f'(x)$. Let $\mathcal{B} = \set{1,x,x^2,x^3}$ and $\mathcal{C} = \set{1,\left(x-1\right),\left(x-1\right)^2,\left(x-1\right)^3}$.
  \begin{align*}
    T\left(1\right) &= 0\\
    T\left(x\right) &= 1\\
    T\left(x^2\right) &= 2x = 2 + 2\left(x-1\right)\\
    T\left(x^3\right) &= 3x^2 = -9 - 6\left(x-1\right) + 3\left(x-1\right)^2.
  \end{align*}
  Thus, our matrix $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$ is
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}} &= \begin{pmatrix}0 & 1 & 2 & -9 \\ 0 & 0 & 2 & -6\\ 0&0&0&3\\ 0&0&0&0\end{pmatrix}
  \end{align*}
\end{example}
\begin{exercise}\hfill
  \begin{enumerate}[(1)]
    \item Let $\mathcal{A}$ be a basis of $U$, $\mathcal{B}$ a basis of $V$, and $\mathcal{C}$ a basis of $W$. Let $S\in\Hom_{\F}\left(U,V\right)$ and $T\in\Hom_{\F}\left(V,W\right)$.\newline

  Show that
  \begin{align*}
    \left[T\circ S\right]_{\mathcal{A}}^{\mathcal{C}} &= \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left[\mathcal{S}\right]_{\mathcal{A}}^{\mathcal{B}}.
  \end{align*}
  \item We know that given $A\in \Mat_{m,k}\left(\F\right)$ and $B\in \Mat_{n,m}\left(\F\right)$, we have corresponding $T_A$ and $T_B$ linear maps.\newline

  Show that you recover the definition of matrix multiplication by using Part 1 to define matrix multiplication.
  \end{enumerate}
  \end{exercise}
  \begin{note}
    To refer to $\left[T\right]_{\mathcal{B}}^{\mathcal{B}}$, we will write $\left[T\right]_{\mathcal{B}}$.
  \end{note}
  Let $V$ be a vector space, with $\mathcal{B}$ and $\mathcal{B}'$ bases of $V$. We want to be able to transfer information about $V$ in terms of $\mathcal{B}$ to information about $V$ in terms of $\mathcal{B}'$ (i.e., change the basis).\footnote{Note that $\mathcal{B}'$ does not refer to the algebraic dual.}\newline

Let $\mathcal{B} = \set{v_1,\dots,v_n}$ and $\mathcal{B}' = \set{v_1',\dots,v_n'}$. Define
\begin{align*}
  T: V\rightarrow \F^{n}\\
  v\mapsto \left[v\right]_{\mathcal{B}}\\
  S: V\rightarrow \F^{n}\\
  v\mapsto \left[v\right]_{\mathcal{B}'}.
\end{align*}
In terms of a diagram, we have
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbTjz7Y8BImWHj6zVohAAdLQDEAeoVkgM8wUVErqaqZp0Gj4mFADm8IqABmAJwgBbJDIQHAgkACZqBjoAIxgGAAV+BSEQbywXAAscEGtJDRAAFW5eEB9-JFFg0MQAZlz1NgBlYq9fAMQgkIr6220tLCgAfWB2LhbStvDqLtrImLjEs0VNNMzsnvzGnQBjLG9tgAIdAeHRnb3Dgv1gAFphMa4KLiA
\begin{tikzcd}
V \arrow[d, "T"'] \arrow[r, "\id_{V}"]        & V \arrow[d, "S"] \\
\F^n \arrow[r, "S\circ \id_{V}\circ T^{-1}"'] & \F^n            
\end{tikzcd}
\end{center}
In particular, the change of basis matrix is
\begin{align*}
  \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'}.
\end{align*}
\begin{exercise}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$. Show that
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}\left[v_1\right]_{\mathcal{B'}} & \cdots & \left[v_n\right]_{\mathcal{B}'}\end{pmatrix}.
  \end{align*}
\end{exercise}
\begin{example}
  Let $V = \Q^2$, $\mathcal{B} = \mathcal{E}_2 = \set{ \begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix} }$. Let
  \begin{align*}
    \mathcal{B}' &= \set{v_1 = \begin{pmatrix}1\\-1\end{pmatrix},v_2 = \begin{pmatrix}1\\1\end{pmatrix}}.
  \end{align*}
  Notice that
  \begin{align*}
    e_1 &= \frac{1}{2}v_1 + \frac{1}{2}v_2\\
    e_2 &= -\frac{1}{2}v_1 + \frac{1}{2}v_2.
  \end{align*}
  In particular, we have
  \begin{align*}
    \left[e_1\right]_{\mathcal{B}'} &= \begin{pmatrix}\frac{1}{2}\\\frac{1}{2}\end{pmatrix}\\
    \left[e_2\right]_{\mathcal{B}'} &= \begin{pmatrix}-\frac{1}{2}\\\frac{1}{2}\end{pmatrix}.
  \end{align*}
  Thus,
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1/2 & -1/2 \\ 1/2 & 1/2\end{pmatrix}.
  \end{align*}
  Let
  \begin{align*}
    v &= \begin{pmatrix}\frac{2}{3}\end{pmatrix}.
  \end{align*}
  We have
  \begin{align*}
    \left[v\right]_{\mathcal{E}_2} &= \begin{pmatrix}2\\3\end{pmatrix}\\
    \left[v\right]_{\mathcal{E}_2}^{\mathcal{B}} &= \begin{pmatrix}1/2 & -1/2 \\ 1/2 & 1/2\end{pmatrix} \begin{pmatrix}2\\3\end{pmatrix}\\
                                                 &= \begin{pmatrix}-1/2\\5/2\end{pmatrix}\\
                                                 &= -\frac{1}{2} \begin{pmatrix}1\\-1\end{pmatrix} + \frac{5}{2} \begin{pmatrix}1\\1\end{pmatrix}\\
                                                 &= \left[v\right]_{\mathcal{B}'}.
  \end{align*}
\end{example}
\begin{example}
  Let $V = P_2\left(\R\right)$, $\mathcal{B} = \set{1,x,x^2}$, $\mathcal{B}' = \set{1,\left(x-2\right),\left(x-2\right)^2}$.
\end{example}
We have
\begin{align*}
  1 &= (1)(1) + (0)\left(x-2\right) + (0)\left(x-2\right)^2\\
  x &= (2)(1) + (1)\left(x-2\right) + (0)\left(x-2\right)^2\\
  x^2 &= (4)(1) + (4)\left(x-2\right) + (1)\left(x-2\right)^2.
\end{align*}
Thus, we have
\begin{align*}
  \left[1\right]_{\mathcal{B}'} &= \begin{pmatrix}1\\0\\0\end{pmatrix}\\
  \left[x\right]_{\mathcal{B}'} &= \begin{pmatrix}2\\1\\0\end{pmatrix}\\
  \left[x^2\right]_{\mathcal{B}'} &= \begin{pmatrix}4\\4\\1\end{pmatrix}.
\end{align*}
Therefore,
\begin{align*}
  \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1 & 2 & 4\\0 & 1 & 4\\0 & 0 & 1\end{pmatrix}.
\end{align*}
For example, if we let $f(x) = -7 + 3x + 4x^2$, we have
\begin{align*}
  \left[f(x)\right]_{\mathcal{B}} &= \begin{pmatrix}-7\\3\\4\end{pmatrix}\\
  \left[f(x)\right]_{\mathcal{B}'} &= \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'}\left[f(x)\right]_{\mathcal{B}}\\
                                   &= \begin{pmatrix}1 & 2 & 4 \\ 0 & 1 & 4 \\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}-7\\3\\4\end{pmatrix}\\
                                   &= \begin{pmatrix}15\\19\\4\end{pmatrix}
                                   \intertext{meaning}
                                   f(x) &= 15 + 19\left(x-2\right) + 4\left(x-2\right)^2.
\end{align*}
\begin{exercise}[Group Work]
  Let $V = P_2\left(\R\right)$, $\mathcal{B} = \set{1,\left(x-1\right),\left(x-1\right)^{2}}$ and $\mathcal{B}' = \set{1,\left(x+1\right),\left(x+1\right)^2}$. Find the change of basis matrix, and find $\left[2 - 6\left(x-1\right) + 2\left(x-1\right)^2\right]_{\mathcal{B}'}$.
\end{exercise}
\begin{solution}
  We have
  \begin{align*}
    1 &= (1)(1) + (0)\left(x+1\right) + (0)\left(x+1\right)^2\\
    \left(x-1\right) &= -2\left(1\right) + (1)\left(x+1\right) + \left(0\right)\left(x+1\right)^2\\
    \left(x-1\right)^2 &=  4(1) -(4)\left(x+1\right) + (1)\left(x+1\right)^2
  \end{align*}
  Thus, the change of basis matrix is
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1 & -2 & 4 \\ 0 & 1 & -4 \\ 0 & 0 & 1\end{pmatrix}.
  \end{align*}
  Thus, we have
  \begin{align*}
    \left[2 - 6\left(x-1\right) + 2\left(x-1\right)^2\right]_{\mathcal{B}'} &= \begin{pmatrix}1 & -2 & 4\\ 0 & 1 & -4 \\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}2\\-6\\2\end{pmatrix}\\
                                                                            &= \begin{pmatrix}22\\-14\\2\end{pmatrix}
  \end{align*}
\end{solution}
%Recall that if $\Dim_{\F}\left(V\right) = n$, with $\mathcal{B} = \set{v_1,\dots,v_n}$, there is an isomorphism
%\begin{align*}
%  T_{\mathcal{B}}:V\rightarrow \F^n\\
%  v\mapsto \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix},
%\end{align*}
%where $v = \sum_{i=1}^{n}a_iv_i$.\newline
\begin{definition}[Similar Matrices]
  Given $A,B\in \Mat_{n}\left(\F\right)$, we say $A$ and $B$ are similar if there exists $P\in \text{GL}_{n}\left(\F\right)$\footnote{$\text{GL}_{n}\left(\F\right) = \set{C\in \Mat_{n}\left(\F\right)\mid C^{-1}\text{ exists}}$} such that $A = PBP^{-1}$.
\end{definition}
We wish to rephrase this definition in terms of matrices. Given $A\in \Mat_{n}\left(\F\right)$, there exists $T_A\in \Hom_{\F}\left(F^n,H^n\right)$ with $T_A(v) = Av$. Given a basis $\mathcal{B}$, we have the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbTjz7Y8BImWHj6zVohAAdLQDEAeoVkgM8wUVErqaqZp0Gj4mFADm8IqABmAJwgBbJDIQHAgkUQl1NgAVAH1gAEEubl4QH39A6hCkACZqBjoAIxgGAAV+BSEQbywXAAscEGtJDRBY4B0-OhxagGNGYAAhLiTjNIDEcKzEAGYmyM02jq7e-qGRlLGczNCZudttLQYYTxxkNsSdarqcCjil7r6GQeHGkHyi0vLzTSv67gouEA
\begin{tikzcd}
  \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{B}}"']    & \F^n \arrow[d, "T_{\mathcal{B}}"] \\
\F^n \arrow[r, "{\left[T_{A}\right]_{\mathcal{B}}}"'] & \F^n                          
\end{tikzcd}
\end{center}
If $\mathcal{E}_n$ is the standard basis, then $A = \left[T_{A}\right]_{\mathcal{E}_n}$, meaning we have the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDgMQD1CAvqXSZc+QigBMFanSat2XPoOEgM2PASJlishizaJOPfiCEiN4otN019Co0tPm1ozROQBmGXfmHjymaq6mJaKN62cgaKJioWoR7kPlEOAc7BblYoSZH2-k6CsjBQAObwRKAAZgBOEAC2SEkgOBBIZCn+ACoA+sAAggJBVbUNiE0tSNIgjPQARjCMAAqZYSDVWCUAFjggvtFGPcBcdfQ4mwDGTMAAogLdYIMuNfVtNBOI3h3sh8enF1e3e6PVTPUZTd6fPLsPq7aZzBbLSyrdZbHZPEavZqtRAAFj2qS4WCgvQKwOGLw+b2xAFZ8f5FrxgABaYgCAAEAF42VxGDBKjhkITiUdYgIuCjtpQSRwTmdLowbncHozfnKrgAhASDGgzeZLFYSNYbbZDECgpB4rFIWlfA7S2X-RhszWm82IABsVMadJiROlyjJZoxiAA7F7EFMoUYGczWZzuRxefzBRw-SKA+LjTgpSKHfLgJqBCqZX984CHrDdQiDewJWiQcHPVbQzr4fqkYa67CoyAfiW1QrC2ZKAIgA
  \begin{tikzcd}
\F^n \arrow[r, "\id_{\F^n}"] \arrow[d, "T_{\mathcal{B}}"']                          & \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{E}_n}"'] & \F^n \arrow[d, "T_{\mathcal{E}_n}"] \arrow[r, "\id_{\F^n}"]                         & \F^n \arrow[d, "T_{\mathcal B}"] \\
\F^n \arrow[r, "{P^{-1} = \left[\id_{\F^n}\right]_{\mathcal{B}}^{\mathcal{E}_n}}"'] & \F^n \arrow[r, "A"']                                    & \F^n \arrow[r, "{P^{-1} = \left[\id_{\F^n}\right]_{\mathcal{E}_n}^{\mathcal{B}}}"'] & \F^n                            
\end{tikzcd}
\end{center}
%\begin{center}
%  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDgMQD1CAvqXSZc+QigBMFanSat2XPoOEgM2PASJlishizaJOPfiCEiN4otN019Co0tPm1ozROQBmGXfmHjymaq6mJaKN62cgaKJioWoR7kPlEOAc7BblYoSZH2-k6CsjBQAObwRKAAZgBOEAC2SEkgOBBIZCn+ACoA+sAAggJBVbUNiE0tSNIgjPQARjCMAAqZYSDVWCUAFjggvtFGPcBcdfQ4mwDGTMAAogLdYIMuNfVtNBOI3h3sh8enF1e3e6PVTPUZTd6fPLsPq7aZzBbLSyrdZbHZPEavZqtRAAFj2qS4WCgvQKwOGLw+b2xAFZ8f5FrCZvMlisJGsNtshiBQUg8VikLSvgcSRwTmdLowAAQAITJ3IxiAAbFTGnSYkSRco5TzEAB2FWIKZQoyLXjAAC0xEGNCZCNZ7BRnPRFOV-L1NvhLKRbMdOzVwqOor+EuAsselAEQA
%\begin{tikzcd}
%\F^n \arrow[r, "\id_{\F^n}"] \arrow[d, "T_{\mathcal{B}}"'] & \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{E}_n}"'] & \F^n \arrow[d, "T_{\mathcal{E}_n}"] \arrow[r, "\id_{\F^n}"] & \F^n \arrow[d, "T_{\mathcal B}"] \\\F^n \arrow[r, "P^{-1} = \id_{\F^n}\right]"']                                  & \F^n \arrow[r, "A"']                                    & \F^n \arrow[r, "P = \id_{\F^n}"']                                        & \F^n                            
%\end{tikzcd}
%\end{center}
Thus, $A = P \left[T_{A}\right]_{\mathcal{B}} P^{-1}$. In other words, $A\sim B$ if and only if $A = \left[T_{A}\right]_{\mathcal{B}}$ for some basis $\mathcal{B}$ and $B = \left[T_{A}\right]_{\mathcal{C}}$.
\subsection{Row Operations, Column Space, and Null Space}%
\begin{definition}[Pivot]
  Let $A = \left(a_{ij}\right)\in \Mat_{m,n}\left(\F\right)$. We say $a_{k\ell}$ is a pivot of $A$ if and only if $a_{k\ell}\neq 0$ and $a_{ij} = 0$ if $i\geq k$ or $j\leq \ell$, with $\left(i,j\right)\neq \left(k,\ell\right)$.
\end{definition}
\begin{example}
  For the matrix
  \begin{align*}
    A &= \begin{pmatrix}\boxed{2} & 1 & 4 & 5 \\ 0 & 0 & \boxed{1} & 7 \\ 0 & 0 & 0 & \boxed{5}\end{pmatrix},
  \end{align*}
  the boxed entries are pivots.
\end{example}
\begin{definition}
  Let $A\in \Mat_{m,n}\left(\F\right)$. We say $A$ is in row echelon form if all its nonzero rows have a pivot and all its zero rows are located below the nonzero rows. We say the matrix is in reduced row echelon form if it is in row echelon form and the pivots are the nonzero elements in the columns containing the pivots.
\end{definition}
\begin{example}
  We have
  \begin{align*}
    A &= \begin{pmatrix}2 & 1 & 4 & 5 \\ 0 & 0 & 1 & 7 \\ 0 & 0 & 0 & 5 \\ 0 & 0 & 0 & 0\end{pmatrix}
  \end{align*}
  is in row echelon form, and
  \begin{align*}
    B &= \begin{pmatrix}2 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\\0 & 0 & 0 & 0\end{pmatrix}
  \end{align*}
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}3 & 4 & 5 & 6 \\ 1 & 2 & 3 & 4 \\ 1 & 1 & 2 & 3\end{pmatrix}.
  \end{align*}
  We are going to put this matrix into reduced row echelon form. We have $T_A: \F^4 \rightarrow \F^3$. Let $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$ and $\mathcal{F}_3 = \set{f_1,f_2,f_3}$. Then, $A = \left[T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3}$. We have
  \begin{align*}
    T_A\left(e_1\right) &= 3f_1 + f_2 + f_3\\
    T_A\left(e_2\right) &= 4f_1 + 2f_2 + f_3\\
    T_A\left(e_3\right) &= 5f_1 + 3f_2 + 2f_3\\
    T_A\left(e_4\right) &= 6f_1 + 4f_2 + 3f_3
  \end{align*}
  \begin{description}
    \item[Step 1:] We switch $R_1\leftrightarrow R_3$, yielding
      \begin{align*}
        \mathcal{F}_3^{(2)} &= \set{f_1^{(2)} = f_3,f_{2}^{(2)},f_{3}^{(2)} = f_1},
      \end{align*}
      yielding
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(2)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 1 & 2 & 3 & 4 \\ 3 & 4 & 5 & 6\end{pmatrix}\\
        \\
        T_A\left(e_1\right) &= f_1^{(2)} + f_{2}^{(3)} + 3f_{3}^{(2)}\\
        T_A\left(e_2\right) &= f_{1}^{(2)} + 2f_{2}^{(3)} + 4f_3^{(2)}\\
        T_A\left(e_3\right) &= 2f_1^{(2)} + 3f_2^{(2)} + 5f_3^{(2)}\\
        T_A\left(e_4\right) &= 3f_1^{(2)} + f_2^{(2)} + 6f_3^{(2)}.
      \end{align*}
    \item[Step 2:] Our next step is $-R_1 + R_2 \rightarrow R_2$, yielding
      \begin{align*}
        \mathcal{F}_3^{(3)} &= \set{f_1^{(3)} = f_1^{(2)} + f_{2}^{(2)},f_{3}^{(2)} = f_{2}^{(2)},f_{3}^{(3)} = f_{2}^{(3)}}.
      \end{align*}
      Our new matrix is
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(3)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 3 & 4 & 5 & 6\end{pmatrix}\\
        \\
        T_A\left(e_1\right) &= \left(f_1^{(2)} + f_{2}^{(2)}\right) + 3f_{3}^{(2)}\\
                            &= f_{1}^{(3)} + 3f_3^{(3)}\\
        T_{A}\left(e_2\right) &= \left(f_1^{(2)} + f_2^{(2)}\right) + f_{2}^{(2)} + 4f_3^{(2)}\\
                              &= f_1^{(3)} + f_2^{(2)} + 4f_3^{(3)}\\
                              &\vdots
      \end{align*}
    \item[Step 3:] Next, we have $-3R_1 + R_3\rightarrow R_3$, which yields
      \begin{align*}
        \mathcal{F}_3^{(4)} &= \set{f_1^{(4)} = f_{1}^{(3)} + 3f_{3}^{(3)},f_{2}^{(4)} = f_{2}^{(3)},f_{3}^{(4)} = f_{3}^{(3)}}.
      \end{align*}
      Our matrix is now
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_{4}}^{\mathcal{F}_{3}^{(4)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 1 & -1 & -3\end{pmatrix}
      \end{align*}
    \item[Step 4:] Next, we have $-R_2 + R_3 \rightarrow R_3$, which yields
      \begin{align*}
        \mathcal{F}_{3}^{(5)} &= \set{f_{1}^{(5)} = f_{1}^{(4)},f_{2}^{(5)} = f_{2}^{(4)} + f_{3}^{(4)},f_{3}^{(5)} = f_{3}^{(4)}},
      \end{align*}
      and a matrix of
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(5)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & -4\end{pmatrix}.
      \end{align*}
  \end{description}
\end{example}
\begin{theorem}
  Let $A\in \Mat_{m,n}\left(\F\right)$. The matrix $A$ can be put in row echelon form through a series of row operations of the form:
  \begin{itemize}
    \item switching two rows: $R_i\leftrightarrow R_j$;
    \item multiplying a row by a scalar: $\R_i\rightarrow cR_i$;
    \item replacing a row by adding a scalar multiple of another row: $aR_i + R_j\rightarrow R_j$.
  \end{itemize}
\end{theorem}
\begin{proof}[Sketch of a Proof]
  For any matrix, we switch rows such that the value of $a_{11}$ is nonzero. Then, we take
  \begin{align*}
    f_{1}^{(2)} &= \sum_{j=1}^{m}a_{ji}f_j\\
    f_k^{(2)} = f_{k}.
  \end{align*}
\end{proof}
Instead of directly changing the bases, we can use linear maps to change the bases.\newline

We define $T_{i,j}: W\rightarrow W$ to be
\begin{align*}
  T_{i,j}\left(w_{k}\right) &= w_k\tag*{$k\neq i,j$}\\
  T_{i,j}\left(w_i\right) &= w_j\\
  T_{i,j}\left(w_j\right) &= w_i.
\end{align*}
Thus,
\begin{align*}
  E_{i,j} &= \left[T_{i,j}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
is the identity matrix except for switching the $i$ and $j$ rows.\newline

Let $c\in \F$, define $T_i^{(c)}:W\rightarrow W$ by
\begin{align*}
  T_i^{(c)}\left(w_k\right) &= w_k\tag*{$k\neq i$}\\
  T_{i}^{(c)}\left(w_i\right) &= cw_i,
\end{align*}
with
\begin{align*}
  E_i^{(c)} &= \left[T_{i}^{(c)}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
being the identity matrix except for row $i$ multiplied by $c$.\newline

Finally, we define $T_{i,j}^{(c)}:W\rightarrow W$ by
\begin{align*}
  T_{i,j}^{(c)}\left(w_k\right) &= w_k\tag*{$k\neq j$}\\
  T_{i,j}^{(c)}\left(w_j\right) &= cw_i + w_j,
\end{align*}
with
\begin{align*}
  E_{i,j}^{(c)} &= \left[T_{i,j}^{(c)}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
as the identity map with $c$ in the $ij$th entry.
\begin{example}
  Let 
  \begin{align*}
    A &= \begin{pmatrix}3 & 4 & 5 & 5\\ 1 & 2 & 3 & 4 \\ 1 & 1 & 2 & 3\end{pmatrix}.
  \end{align*}
  Define $T_A: \F^4\rightarrow \F^3$, $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$, and $\mathcal{F}_3 = \set{f_1,f_2,f_3}$. We have
  \begin{align*}
    T_A\left(e_1\right) &= 3f_1 + f_2 + f_3\\
    T_A\left(e_2\right) &= 4f_1 + 2f_2 + f_3\\
    T_A\left(e_3\right) &= 5f_1 + 3f_2 + 2f_3\\
    T_A\left(e_4\right) &= 6f_1 + 4f_2 + 3f_3.
  \end{align*}
  First, we interchange the rows by $T_{1,3}:\F^3\rightarrow \F^3$, Then,
  \begin{align*}
    \left(T_{1,3}\circ T_A\right)\left(e_1\right) &= T_{1,3}\left(3f_1 + f_2 + f_3\right)\\
                                                  &= 3T_{1,3}\left(f_1\right) + T_{1,3}\left(f_1\right) + T_{1,3}\left(f_3\right).
  \end{align*}
  If we look at the matrix, we then have
  \begin{align*}
    \left[T_{1,3}\circ T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 1 & 2 & 3 & 4 \\ 3 & 4 & 5 & 6\end{pmatrix}.
  \end{align*}
  For the full reduced row echelon form, we would have the following series of transformations:
  \begin{align*}
    \left[T_{1,3}^{(-1)}\circ T_{2,3}^{(-1)}\circ T_{3}^{(-2)}\circ T_{3,1}^{(-3)}\circ T_{1,2}^{-1}\circ T_{1,3}\circ T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3} &= \begin{pmatrix}1 & 0 & 0 & 0 \\ 0 & 1 & 0  & -1\\ 0 & 0 & 1 & 2\end{pmatrix}.
  \end{align*}
\end{example}
\begin{definition}[Column Space, Null Space, and Rank]
  Let $A\in \Mat_{m,n}\left(\F\right)$. The column space of $A$ is the $\F$-span of the column vectors. This is denoted $\Col(A)$.\newline

  The null space, $\Null(A)$, is the $\F$-span of the vectors $v\in \F^n$ such that $Av = 0_{\F^m}$.\newline

  The rank of $A$, denoted $\Rank(A)$, is $\Rank(A) = \Dim_{\F}\left(\Col(A)\right)$.
\end{definition}
  Let $\mathcal{E}_n = \set{e_1,\dots,e_n}$ be the standard basis for $\F^n$, with $T_A \in \Hom_{\F}\left(\F^n,\F^n\right)$, and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ the standard basis of $\F^m$.\newline

  We have $\left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m} = A$. We know that
  \begin{align*}
    A &= \begin{pmatrix}T_A\left(e_1\right) & \cdots & T_A\left(e_n\right)\end{pmatrix}.
  \end{align*}
  Thus, $\Col(A) = \img\left(T_A\right)$, meaning $\Rank(A) = \Dim_{\F}\left(\img\left(T_A\right)\right)$.\newline

In order to calculate $\col(A)$, we put the matrix $A$ into row echelon form, look at the columns that have pivots, and those columns form the basis for $\col(A)$.\newline

We have an isomorphism $E: \F^m\rightarrow \F^m$ such that
\begin{align*}
  \left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m} &= \left[E\right]_{\mathcal{F}_m}^{\mathcal{F}_m}
\end{align*}
is in row echelon form. In particular, the column space of $\left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$ has as its basis the columns containing pivots:
\begin{align*}
  \underbrace{\overbrace{\left[E\circ T_A\left(e_{i_1}\right)\right]}^{w_1}_{\mathcal{F}_m},\dots,\overbrace{\left[E\circ T_A\left(e_{i_k}\right)\right]}^{w_k}_{\mathcal{F}_m}}_{\text{basis of }\col\left(\left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}\right)}
\end{align*}
We have an inverse $E^{-1}: \F^{m}\rightarrow \F^m$. In particular,
\begin{align*}
  \underbrace{E^{-1}\left(w_1\right),\dots,E^{-1}\left(w_k\right)}_{=\left[T_{A}\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots,\left[T_{A}\left(e_{i_k}\right)\right]_{\mathcal{F}_m}}
\end{align*}
are linearly independent since $E^{-1}$ is an isomorphism.\newline

If there is a vector $v\in \col(A)$ that is not in the span of $\left[T_{A}\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots,\left[T_{A}\left(e_{i_k}\right)\right]_{\mathcal{F}_m}$, then $E(v)$ cannot be in the span of $w_1,\dots,w_k$.\newline

Thus, the columns $\left[T_A\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots \left[T_A\left(e_{i_k}\right)\right]_{\mathcal{F}_m}$ give a basis for $\col(A)$.
\begin{example}
  Consider the matrix
  \begin{align*}
    A &= \begin{pmatrix}3&4&5&6\\1&2&3&4\\1&1&2&3\end{pmatrix}.
  \end{align*}
  We put $A$ into row echelon form as
  \begin{align*}
    B &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & -4\end{pmatrix}.
  \end{align*}
  Examining the pivots, we have the column space as
  \begin{align*}
    \col(B) &= \Span_{\F}\left( \begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}1\\1\\0\end{pmatrix}, \begin{pmatrix}2\\1\\-2\end{pmatrix}\right),
  \end{align*}
  implying the basis of the column space for $A$ is
  \begin{align*}
    \col(A) &= \Span_{\F}\left( \begin{pmatrix}3\\1\\1\end{pmatrix}, \begin{pmatrix}4\\2\\1\end{pmatrix}, \begin{pmatrix}5\\3\\2\end{pmatrix}\right).
  \end{align*}
\end{example}
We have $v\in \Null(A)$ if and only if $Av = 0_{\F^m}$. Since $Av = T_A(v)$, we have $\Null(A) = \ker\left(T_A\right)$.
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}4 & -4 & 2 \\ -4 & 4 & -2 \\ 2 & -1 & 1\end{pmatrix}.
  \end{align*}
  The reduced row echelon form of $A$ is
  \begin{align*}
    B &= \begin{pmatrix}1 & 0 & 1/2 \\ 0 & 1 & 0 \\ 0 & 0 & 0\end{pmatrix}.
  \end{align*}
  Thus,
  \begin{align*}
    \col(A) &= \Span_{\F}\left( \begin{pmatrix}4\\-4\\2\end{pmatrix}, \begin{pmatrix}-4\\4\\-1\end{pmatrix}\right).
  \end{align*}
  We know that $\null(A) = \ker\left(T_A\right) \subseteq \F^{3}$-domain of $T_A$. When we put a matrix into reduced row echelon form, we do not impact the basis vectors of the domain of $T_A$, implying that $\Null(A) = \Null(B)$.\newline

  In particular, we want
  \begin{align*}
    \begin{pmatrix}1 & 0 & 1/2\\0 & 1 & 0 \\ 0 & 0 & 0\end{pmatrix} \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} &= \begin{pmatrix}x_1 + \left(1/2\right) x_3\\x_2\\0\end{pmatrix}\\
                     &= \begin{pmatrix}0\\0\\0\end{pmatrix}.
  \end{align*}
  Therefore, we have $x_2 = 0$, $x_1 = -1/2 x_3$, meaning
  \begin{align*}
    \Null(A) &= \Span_{\F}\left( \begin{pmatrix}-1/2\\0\\1\end{pmatrix}\right).
  \end{align*}
\end{example}
\subsection{Transpose of a Matrix}%
Recall that, given a linear map $T\in \Hom_{\F}\left(V,W\right)$, there is an induced map $T'\in \Hom_{\F}\left(W',V'\right)$ on the dual space given by $T'\left(\varphi\right) = \varphi\circ T$.\newline

Let $A\in \Mat_{m,n}\left(\F\right)$, $\mathcal{E}_n = \set{e_1,\dots,e_n}$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be standard bases for $\F^n$ and $\F^m$ respectively. Let $T_A \in \Hom_\F\left(\F^n,\F^m\right)$, meaning $A = \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$.\newline

We have $\mathcal{E}_n' = \set{e_1',\dots,e_n'}$ and $\mathcal{F}_{m}' = \set{f_1',\dots,f_m'}$. The dual map $T_A'\in \Hom_{\F}\left(\F^m,\F^n\right)$, and the transpose of $A$ is defined by
\begin{align*}
  A^{T} &= \left[T_{A}'\right]_{\mathcal{F}_m'}^{\mathcal{E}_n'}.
\end{align*}
\begin{lemma}
  Let $A = \left(a_{ij}\right) \in \Mat_{m,n}\left(\F\right)$. Then,
  \begin{align*}
    A^{T} &= \left(b_{ij}\right)\in \Mat_{n,m}\left(\F\right)
  \end{align*}
  with $b_{ij} = a_{ji}$.
\end{lemma}
\begin{proof}
  Let $A\in \Mat_{m,n}\left(\F\right)$, $\mathcal{E}_n = \set{e_1,\dots,e_n}$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be standard bases for $\F^n$ and $\F^m$ respectively. Let $\mathcal{E}_n'$ and $\mathcal{F}_m'$ denote the dual bases.\newline

  Let $T_A \in \Hom_\F\left(\F^n,\F^m\right)$, meaning $A = \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$. In particular, we have
  \begin{align*}
    T_A\left(e_i\right) &= \sum_{k=1}^{m}a_{ki}f_k. \tag*{(\textasteriskcentered)}
  \end{align*}
  We have
  \begin{align*}
    A^{t} &= \left[T_{A}'\right]_{\mathcal{F}_m'}^{\mathcal{E}_n'} \tag*{(\textasteriskcentered\textasteriskcentered)}\\
          &= \left(b_{ij}\right)
  \end{align*}
  Now, we have
  \begin{align*}
    T_{A}' \left(f_j'\right) &= \sum_{j=1}^{n}b_{kj}e_{k}'.
  \end{align*}
  Apply $f_j'$ to (\textasteriskcentered). Then,
  \begin{align*}
    \left(f_j'\circ T_A\right)\left(e_i\right) &= f_j'\left(\sum_{k=1}^{m}a_{ki}f_k\right)\\
                                               &= \sum_{k=1}^{m}a_{ki}f_j'\left(f_k\right)\\
                                               &= a_{ji}.
  \end{align*}
  Apply (\textasteriskcentered\textasteriskcentered) to $e_i$. Then,
  \begin{align*}
    T_A'\left(f_j'\right)\left(e_i\right) &= \sum_{k=1}^{n}b_{kj}e_k'\left(e_i\right)\\
                                          &= b_{ij}.
  \end{align*}
  We have
  \begin{align*}
    \left(f_j'\circ T_A\right)\left(e_i\right) &= \left(T_A'\left(f_j'\right)\right)\left(e_i\right)
  \end{align*}
  by the definition of $T_A'$, meaning $b_{ij} = a_{ji}$.
\end{proof}
\begin{exercise}
  Let $A_1,A_2\in \Mat_{m,n}\left(\F\right)$, $c\in \F$. Use the definition of the transpose to show
  \begin{align*}
    \left(A_1 + A_2\right)^{T} &= A_1^T + A_2^T\\
    \left(cA_1\right)^T &= cA_1^T.
  \end{align*}
\end{exercise}
\begin{lemma}
  Let $A\in \Mat_{m,n}\left(\F\right)$, $B\in \Mat_{p,m}\left(\F\right)$. Then,
  \begin{align*}
    \left(BA\right)^T &= A^TB^T.
  \end{align*}
\end{lemma}
\begin{proof}
  Let $\mathcal{E}_m$, $\mathcal{E}_n$, and $\mathcal{E}_p$ be standard bases.\newline

  We have
  \begin{align*}
    \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_m} &= A\\
    \left[T_B\right]_{\mathcal{E}_m}^{\mathcal{E}_p} &= B.
  \end{align*}
  So,
  \begin{align*}
    BA &= \left[T_B\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_p}.
  \end{align*}
  Thus,
  \begin{align*}
    \left(BA\right)^{T} &= \left[\left(T_B\circ T_A\right)'\right]_{\mathcal{E}_p'}^{\mathcal{E}_n'}\\
                        &= \left[T_A'\circ T_B'\right]_{\mathcal{E}_p'}^{\mathcal{E}_n'}\\
                        &= \left[T_A'\right]_{\mathcal{E}_m'}^{\mathcal{E}_n'}\left[T_{B}'\right]_{\mathcal{E}_p'}^{\mathcal{E}_m'}\\
                        &= A^TB^T.
  \end{align*}
\end{proof}
\begin{lemma}
  Let $A\in \text{GL}_{n}\left(\F\right)$. Then,
  \begin{align*}
    \left(A^{-1}\right)^{T} &= \left(A^{T}\right)^{-1}.
  \end{align*}
\end{lemma}
\begin{proof}
  We will show that $A^{T} \left(A^{-1}\right)^{T} = I_n = \left(A^{-1}\right)^{T} A^T$, and use the fact that inverses are unique.\newline

  We have
  \begin{align*}
    A &= \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_n}\\
    A^{-1} &= \left[T_A^{-1}\right]_{\mathcal{E}_n}^{\mathcal{E}_n}
  \end{align*}
  We have
  \begin{align*}
    I_n &= \left[\id_{\F^n}'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\circ T_A\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[T_A'\circ \left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\left[\left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= A^T\left(A^{-1}\right)^T.\\
    I_n &= \left[\left(T_A\circ T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\right)'\circ T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\left[T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left(A^{-1}\right)^{T}A^{T}.
  \end{align*}
\end{proof}
\section{Generalized Eigenvectors and Jordan Canonical Form}%
\subsection{Eigenvalues and Eigenvectors}%
Recall that we say $A \sim B$ if $A = PBP^{-1}$ for some $P\in \text{GL}_{n}\left(\F\right)$. In particular, this means that $A = \left[T\right]_{\mathcal{A}}$ and $B = \left[T\right]_{\mathcal{B}}$ for some bases $\mathcal{A}$ and $\mathcal{B}$.
\begin{definition}[Diagonalizable]
  We say $A$ is diagonalizable if $A \sim D$ for some $D$ a diagonal matrix.\newline

  If $A = \left[T\right]_{\mathcal{A}}$, $A$ is diagonalizable if there is a basis $\mathcal{B}$ if $\left[T\right]_{\mathcal{B}} = D$ for $D$ a diagonal matrix.\newline

  If $A\sim B$, $A$ is diagonalizable if and only if $B$ is diagonalizable. If $A$ and $B$ are diagonalizable, they must be similar to the same diagonal matrix up to reordering the diagonals.
\end{definition}
\begin{example}
  Let $V = \F^2$, $T \in \Hom_{\F}\left(V,V\right)$. We take $T\left(e_1\right) = 3e_1$ and $T\left(e_2\right) = -2e_2$.\newline

  In particular, we can see that
  \begin{align*}
    \left[T\right]_{\mathcal{E}_2} &= \begin{pmatrix}3 & 0 \\ 0 & -2\end{pmatrix}.
  \end{align*}
  When we look at $V = V_1\oplus V_2$, with $V_1 = \Span_{\F}\left(e_1\right)$ and $V_2 = \Span_{\F}\left(e_2\right)$.\newline

  In this case, we have $T\left(V_1\right)\subseteq V_1$ and $T\left(V_2\right) \subseteq V_2$, which allows us to write $T$ as a diagonal matrix.
\end{example}
\begin{example}
  Let $V = \F^2$, $T \in \Hom_{\F}\left(V,V\right)$. We take $T\left(e_1\right) = 3e_1$ and $T\left(e_2\right) = e_1 + 3e_2$.\newline

  In particular, we can see that
  \begin{align*}
    \left[T\right]_{\mathcal{E}_2} &= \begin{pmatrix}3 & 1 \\ 0 & 3\end{pmatrix}.
  \end{align*}
  We still have $V = V_1\oplus V_2$ with $V_1 = \Span_{\F}\left(e_1\right)$ and $V_2 = \Span_{\F}\left(e_2\right)$.\newline

  While we have $T\left(V_1\right)\subseteq V_1$, we do not have $T\left(V_2\right)\subseteq V_2$. We will find a diagonalization (or lack thereof) of $T$.\newline

  Suppose we have $W_1,W_2\neq \set{0}$ with $V = W_1 \oplus W_2$ with $T\left(W_1\right)\subseteq W_1$ and $T\left(W_2\right)\subseteq W_2$.\newline

  Write $W_i = \Span_{\F}\left(w_i\right)$. In particular, this means we can write $T\left(w_1\right) = \alpha w_1$ and $T\left(w_2\right) = \beta w_2$. For $\mathcal{B} = \set{w_1,w_2}$, we would be able to write
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\alpha & 0 \\ 0 & \beta\end{pmatrix}.
  \end{align*}
  Write $w_1 = ae_1 + be_2$ and $w_2 = ce_1 + de_2$.
  \begin{align*}
    \alpha w_1 &= T\left(w_1\right)\\
               &= aT\left(e_1\right) + bT\left(e_2\right)\\
               &= a\left(3e_1\right) + b\left(e_1 + 3e_2\right)\\
               &= \left(3a+b\right)e_1 + 3be_2
  \end{align*}
  Thus, $\alpha\left(ae_1 + be_2\right) = \left(3a+b\right)e_1 + 3be_2$, meaning $\alpha a = 3a + b$, $\alpha b = 3b$. Either $b = 0$ or $\alpha = 3$, but we still end with $\alpha = 3$. Thus, $T\left(w_1\right) = 3w_1$.\newline

  Applying to $w_2$, we have
  \begin{align*}
    \beta w_2 &= \left(3c + d\right)e_1 + \left(3d\right)e_2,
  \end{align*}
  implying $\beta c = ec + d$ and $\beta d = 3d$, meaning either $\beta = 3$ (which contradicts the first equation)or $w_2 = ce_1$, which contradicts $w_1,w_2$ being a basis.
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1& 2\\ 3&4\end{pmatrix}.
  \end{align*}
  Let $\F = \Q$. Can we find $P \in \text{GL}_2\left(\Q\right)$ such that $P^{-1}AP = \begin{pmatrix}\alpha & 0\\ 0 & \beta\end{pmatrix}$.\newline

  If we write $P = \begin{pmatrix}a & b \\c & d\end{pmatrix}$, we have
  \begin{align*}
    P^{-1}AP &= \frac{1}{ad-bc} \begin{pmatrix}ad - 3ab + 2cd - 4bc & -3bd - 3b^2 + 2d^2\\ 3ac + 3a^2 - 2c^2 & -bc + 3ab - 2cd + 4ad\end{pmatrix}.
  \end{align*}
  By the definition of diagonal matrix, we must have
  \begin{align*}
    3a^2 + 3ac - 2c^2 &= 0.
  \end{align*}
  If $c = 0$, then $a = 0$, which is a contradiction since $P$ is invertible. We have $c\neq 0$, meaning we can divide by $c^2$ and set $x = a/c$
  \begin{align*}
    3x^2 + 3x - 2 &= 0\\
    x &= \frac{-3 \pm \sqrt{33}}{6}\\
    a &= \frac{-3 \pm \sqrt{33}}{6}c.
  \end{align*}
  Since $c\neq 0$, $\frac{-3 \pm \sqrt{33}}{6}c \notin \Q$. Thus, we cannot diagonalize $A$ over $\Q$.\newline

  If we take $\F = \Q\left(\sqrt{33}\right)$, then we take
  \begin{align*}
    \mathcal{B} &= \set{v_1 = \begin{pmatrix}1\\ \frac{3 + \sqrt{33}}{4}\end{pmatrix}, v_2 = \begin{pmatrix}1\\\frac{3-\sqrt{33}}{4}\end{pmatrix}},\\
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\frac{5 + \sqrt{33}}{2} & 0 \\ 0 & \frac{5-\sqrt{33}}{2}\end{pmatrix}.
  \end{align*}
\end{example}
\begin{recall}
  The fundamental question we are investigating is whether given a $A\in \Mat_{n}\left(\F\right)$, can we choose $P\in \text{GL}_{n}\left(\F\right)$ such that $PAP^{-1}$ is diagonal.\newline

  We saw that if $\F^2 = V_1\oplus V_2$ with $A\left(V_1\right) \subseteq V_1$, $A\left(V_2\right)\subseteq V_2$, then it is possible to diagonalize $A$.
\end{recall}
\begin{definition}
  Let $V$ be an $\F$-vector space with $T\in \Hom_{\F}\left(V,V\right)$. We say a subspace $W\subseteq V$ is $T$-invariant or $T$-stable if $T\left(W\right)\subseteq W$.
\end{definition}
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$, $W\subseteq V$ a $k$-dimensional subspace.\newline

Let $\mathcal{B}_{W} = \set{v_1,\dots,v_k}$ be a basis for $W$, and extend to a basis $\mathcal{B} = \set{v_1,\dots,v_n}$ of $V$.\newline

Let $T\in \Hom_{\F}\left(V,V\right)$.\newline

Then, $W$ is $T$-stable if and only if $\left[T\right]_{\mathcal{B}}$ is block-upper triangular of the form
\begin{align*}
  \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}A & B \\ 0 & D\end{pmatrix},
\end{align*}
where $A = \left[T\vert_{W}\right]_{\mathcal{B}_W}$.
\end{theorem}
\begin{example}
  Let $V = \Q^4$, $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$ the standard basis. Define $T$ by
  \begin{align*}
    T\left(e_1\right) &= 2e_1 + 3e_3\\
    T\left(e_2\right) &= e_1 + e_4\\
    T\left(e_3\right) &= e_1 - e_3\\
    T\left(e_4\right) &= 2e_1 - 2e_2 + 5e_3 - 4e_4.
  \end{align*}
  Notice that if we set $W = \Span_{\Q}\left(e_1,e_3\right)$, then $W$ is $T$-stable. We set $\mathcal{B}_W = \set{e_1,e_3}$, $\mathcal{B} = \set{e_1,e_2,e_3,e_4}$.
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}2 & 1 & 1 & 2 \\ 3 & -1 & 0 & 5 \\ 0 & 0 & 0 & -2 \\ 0 & 0 & 1 & -4\end{pmatrix}
  \end{align*}
\end{example}
A special case is when $\Dim_{\F}\left(W\right) = 1$. If $W = \Span_{\F}\left(w_1\right)$, and $W$ is $T$-stable, then $T\left(w_1\right) \in W$, meaning $T\left(w_1\right) = \lambda w_1$ for some $\lambda \in \F$.\newline

We can rewrite this as $T\left(w_1\right) - \lambda\left(w_1\right) = 0_V$, meaning $\left(T - \lambda \id_V\right)\left(w_1\right) = 0_V$, meaning $w_1\in \ker\left(T - \lambda\id_V\right)$.
\begin{definition}
  Let $T\in \Hom_{\F}\left(V,V\right)$, and $\lambda \in F$. If $\ker\left(T - \lambda\id_V\right) \neq \set{0_V}$, we say $\lambda$ is an eigenvalue of $T$.\newline

  Any nonzero vector in $\ker\left(T - \lambda \id_V\right)$ is called an eigenvector.\newline

  The set $E^{1}_{\lambda} = \ker\left(T - \lambda \id_V\right)$ is called the eigenspace associated with $\lambda$.
\end{definition}
\begin{exercise}
  Show $E^{1}_{\lambda}$ is a subspace of $V$.
\end{exercise}
\begin{exercise}
  Let $T\in \Hom_{\F}\left(V,V\right)$. If $\lambda_1,\lambda_2 \in \F$ with $\lambda_1\neq \lambda_2$, then $E^{1}_{\lambda_1} \cap E^{1}_{\lambda_2} = \set{0_V}$.
\end{exercise}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix}\in \Mat_{2}\left(\Q\right),
  \end{align*}
  with $T_A \in \Hom_{\Q}\left(\Q^2,\Q^2\right)$ the associated linear map.\newline

  We have
  \begin{align*}
    \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix} \begin{pmatrix}1\\2/5\end{pmatrix} &= 2 \begin{pmatrix}1\\2/5\end{pmatrix}\\
    \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix} \begin{pmatrix}1\\3/7\end{pmatrix} &= 3 \begin{pmatrix}1\\3/7\end{pmatrix}.
  \end{align*}
  Therefore, $T_A$ has eigenvalues of $2$ and $3$, with
  \begin{align*}
    E_2 &= \Span_{\Q} \left(\begin{pmatrix}1\\2/5\end{pmatrix}\right) = \Span_{\Q}\left(v_1\right)\\
    E_3 &= \Span_{\Q}\left( \begin{pmatrix}1\\3/7\end{pmatrix}\right) = \Span_{\Q}\left(v_2\right),
  \end{align*}
  meaning
  \begin{align*}
    \left[T_{A}\right]_{\set{v_1,v_2}} &= \begin{pmatrix}2 & 0 \\ 0 & 3\end{pmatrix}.
  \end{align*}
\end{example}
\begin{notation}
  Let $T\in \Hom_{\F}\left(V,V\right)$. We write $T^m = \underbrace{T\circ \cdots \circ T}_{m\text{ times}}$.\newline

  If $f(x) \in \F[x]$, $f(x) = a_mx^m + \cdots + a_1 x + a_0$, then
  \begin{align*}
    f\left(T\right) &= a_mT^m + \cdots + a_1T + a_0 \id_V\\
                    &\in \Hom_{\F}\left(V,V\right).
  \end{align*}
  If $f(x) = g(x)h(x)$, then
  \begin{align*}
    f(T) &= g(T)\circ h(T)
  \end{align*}
\end{notation}
\begin{example}
  If $g(x) = 2x^2 + 3$, then
  \begin{align*}
    g\left(T\right) &= 2T^2 + 3\id_V\\
    g\left(T\right)\left(v\right) &= 2T\left(T\left(v\right)\right) + 3v.
  \end{align*}
\end{example}
Let $\Dim_{\F}\left(V\right) = n$. Recall that $\Hom_{\F}\left(V,V\right)$ is an $\F$-vector space, meaning $\Hom_{\F}\left(V,V\right) \cong \Mat_{n}\left(\F\right)$. Thus, $\Dim_{\F}\left(\Hom_{\F}\left(V,V\right)\right) = n^2$.\newline

Given $T\in \Hom_{\F}\left(V,V\right)$, consider
\begin{align*}
  \set{\id_V,T,T^2,\dots,T^{n^2}}\subseteq \Hom_{\F}\left(V,V\right).
\end{align*}
Since this set contains $n^2 + 1$ elements, it must be linearly dependent. Let $m$ be the smallest integer such that $a_m T^{m} + \cdots + a_1 T + a_0\id_V = 0_{\Hom_{\F}\left(V,V\right)}$. Since $m$ is minimal, $a_m \neq 0$.\newline

Define $f(x) = x^m + b_{m-1}x^{m-1} + \cdots + b_1 x + b_0\in \F[x]$, where $b_i = \frac{a_i}{a_m}$.\newline

Observe that $f(T) = 0_{\Hom_{\F}\left(V,V\right)}$. In other words, $f\left(T\right)\left(v\right) = 0_V$ for all $v\in V$.
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$. There is a unique monic polynomial $m_T(x) \in \F[x]$ of lowest degree such that
  \begin{align*}
    m_T\left(T\right)\left(v\right) &= 0_V
  \end{align*}
  for every $v\in V$. Moreover, $\deg\left( m_T\left(x\right)\right)\leq n^2$
\end{theorem}
\begin{proof}[Proof of Uniqueness]
  Suppose $f(x) \in \F[x]$ satisfies $f(T)(v) = 0$ for all $v\in V$.\newline

  We write
  \begin{align*}
    f(x) &= m_T\left(x\right)q(x) + r(x),
  \end{align*}
  for some $q(x),r(x) \in \F[x]$, with $r(x) = 0$ or $\deg r(x) < \deg m_T(x)$.\newline

  Plugging in $T$, we have for all $v\in V$,
  \begin{align*}
    0_V &= f(T)(v)\\
        &= q(T)m_T(T)(v) + r(T)(v)\\
        &= q(T)\left(0_V\right) + r(T)(v)\\
        &= r(T)(v)
  \end{align*}
  Thus, $r(T) (v) = 0$ for all $v\in V$; thus, it must be the case that $r(T) = 0$.\newline

  Thus, $m_T(x)|f(x)$. However, if $m_T(x)$ and $f(x)$ are monic and of minimal degree, with $m_T(x)|f(x)$, then $m_T(x) = f(x)$.
\end{proof}
\begin{definition}
  The unique monic polynomial $m_T(x)$ is called the minimal polynomial.
\end{definition}
\begin{corollary}
  If $f(x)\in \F[x]$ satisfies $f(T)(v) = 0$ for all $v\in V$, then $m_T(x)|f(x)$.
\end{corollary}
\begin{example}
  Let $F = \Q$,
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 \\ 3 & 4\end{pmatrix}.
  \end{align*}
  We can see that for any $a_0\in \Q$,
  \begin{align*}
    A - a_0I_{2} \neq 0_{\Mat_{2}\left(\Q\right)}.
  \end{align*}
  However, for
  \begin{align*}
    A^2 &= \begin{pmatrix}7 & 10 \\ 15 & 22\end{pmatrix},
  \end{align*}
  we have
  \begin{align*}
    A^2 - 5A - 2I_{2} &= 0_{\Mat_{2}\left(\Q\right)},
  \end{align*}
  yielding $m_{A}\left(x\right) = x^2 - 5x - 2$.\newline

  The roots of $m_A(x)$ are $\frac{5\pm \sqrt{33}}{2}$.
\end{example}
\begin{example}
  Let $V = \Q^3$, $\mathcal{E}_3 = \set{e_1,e_2,e_3}$, with $T_A$ given by
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  We can find
  \begin{align*}
    A^2 &= \begin{pmatrix}1 & 4 & 8 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{pmatrix}\\
    A^3 &= \begin{pmatrix}1 & 6 & 11 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  Thus, we find
  \begin{align*}
    A^3 - A^2 - A + I &= 0,\\
    \left(x-1\right)^2 \left(x+1\right) &= m_{T_A}\left(x\right)
  \end{align*}
\end{example}
\begin{theorem}
  Let $V$ be an $\F$-vector space, and let $T\in \Hom_{\F}\left(V,V\right)$. We have $\lambda$ is an eigenvalue if and only if $\lambda$ is a root of $m_{T}\left(x\right)$.\newline

  In particular, if $\left(x-\lambda\right)\vert m_T(x)$, then $E^{1}_{\lambda}\neq \set{0_V}$.
\end{theorem}
\begin{proof}
  Let $\lambda$ be an eigenvalue with eigenvector $v$, and write $m_{T}\left(x\right) = x^m + \cdots + a_1x + a_0$. Notice that $T^{k}\left(v\right) = \lambda^{k}\left(v\right)$.\newline

  We have 
  \begin{align*}
    0_V &= m_T(T)(v)\\
        &= \left(T^{m} + a_{m-1}T^{m-1} + \cdots + a_1T + a_0\id_V\right)\left(v\right)\\
        &= T^{m}\left(v\right) + a_{m-1}T^{m-1}\left(v\right) + \cdots + a_1T\left(v\right) + a_0 v\\
        &= \lambda^{m}v + a_{m-1}\lambda^{m-1}v + \cdots + a_1\lambda v + a_0 v\\
        &= \left(\lambda^{m} + a_{m-1}\lambda^{m-1} + \cdots + a_1\lambda + a_0\right)v\\
        &= m_{T}\left(\lambda\right)v,
  \end{align*}
  meaning $m_T\left(\lambda\right)v = 0_V$. Since $m_T\left(\lambda\right)\in \F$ and $v \neq 0_V$, it is the case that $m_T\left(\lambda\right) = 0$, meaning $\lambda$ is a root of $m_T(x)$.\newline

  Suppose $m_T\left(\lambda\right) = 0$. This gives
  \begin{align*}
    m_T\left(x\right) &= \left(x-\lambda\right)f(x)
  \end{align*}
  for some $f(x)\in \F[x]$. Therefore, $\deg\left(f(x)\right) < \deg \left(m_{T}\left(x\right)\right)$. There must exist a nonzero vector $v\in V$ such that $f(T)(v) \neq 0_V$. Set $w = f(T)(v)$. Observe that $m_T(T)(v) = 0_V$, so $\left(T-\lambda\id_V\right)f(T)(v) = 0_V$, meaning $\left(T-\lambda\id_V\right)\left(w\right) = 0_V$, so $T(w) = \lambda w$. Thus, $\lambda$ is an eigenvalue.
\end{proof}
\begin{corollary}
  Let $\lambda_1,\dots,\lambda_m\in \F$ be distinct eigenvalues of $T$. For each $i$, let $v_i$ be an eigenvector with eigenvalue $\lambda_i$. Then, $\set{v_1,\dots,v_m}$ is linearly independent 
\end{corollary}
\begin{proof}
  We can write
  \begin{align*}
    m_T(x) &= \left(x-\lambda_1\right)\cdots\left(x-\lambda_m\right)f(x).
  \end{align*}
  Suppose $a_1v_1 + \cdots + a_mv_m = 0_V$ for some $a_i\in \F$.\newline

  Define $g_1(x) = \left(x-\lambda_2\right)\cdots\left(x-\lambda_m\right)f(x)$. Note that $g_1(T)(v_i) = 0_V$ for all $2\leq i \leq m$. Then,
  \begin{align*}
    0_V &= g_1(T)\left(0_V\right)\\
        &= \sum_{j=1}^{m}a_jg_1(T)\left(v_j\right)\\
        &= a_1g_1(T)\left(v_1\right)\\
        &= a_1g_1\left(\lambda_1\right)v_1.
  \end{align*}
  Since $g_1\left(\lambda_1\right)\neq 0$, and $v_1\neq 0$, it must be the case that $a_1 = 0$. Symmetry provides the case for $2,\dots,m$.
\end{proof}
\begin{corollary}
  If $\deg m_{T}\left(x\right) = \dim_{\F}\left(V\right)$, and $m_{T}\left(x\right)$ has distinct roots, all of which are in $\F$, then we can find a basis $\mathcal{B}$ for $V$ such that $\left[T\right]_{\mathcal{B}}$ is diagonal.
\end{corollary}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2\end{pmatrix}\\
    B &= \begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2\end{pmatrix}.
  \end{align*}
  These matrices are not similar. However, $m_{A}(x) = m_{B}\left(x\right) = \left(x-1\right)\left(x-2\right)$.\newline

  Therefore, the minimal polynomial does not provide enough information about a matrix's similarity class.
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  We found that the minimal polynomial for $A$ was $m_A(x) = \left(x-1\right)^2 \left(x+1\right)$.\newline

  We can see that $Ae_{1} = e_1$, meaning $\Span_{\F}\left(e_1\right) = E_{1}^{1}$. Note that
  \begin{align*}
    Ae_2 &= \begin{pmatrix}2\\1\\0\end{pmatrix},
  \end{align*}
  meaning $e_2 \notin E_{1}^{1}$.\newline

  We can see that
  \begin{align*}
    \left(A-I_{3}\right)^{2} &= \begin{pmatrix}0 & 0 & 2 \\ 0 & 0 & -8 \\ 0 & 0 & 4\end{pmatrix}.
  \end{align*}
  However,
  \begin{align*}
    \left(A-I_{3}\right)^2\left(e_2\right) &= \begin{pmatrix}0\\0\\0\end{pmatrix},
  \end{align*}
  meaning $e_1,e_2\in \ker\left(\left(T_A - \id_{\F^3}\right)^2\right)$.\newline

  Though we do not have distinct eigenvectors, we \textit{kinda} have them.
\end{example}
\begin{definition}[Generalized Eigenvector]
  Let $T\in \Hom_{\F}\left(V,V\right)$. For $k\geq 1$, the $k$th generalized eigenspace of $T$ with eigenvalue $\lambda$ is
  \begin{align*}
    E_{\lambda}^{k} &= \ker\left(\left(T_A - \lambda\id_{V}\right)^{k}\right)\\
                    &= \set{v\in V \mid \left(T-\lambda \id_{V}\right)^{k}v = 0_V}.
  \end{align*}
  Elements in $E_{\lambda}^{k}$ are called generalized $\lambda$-eigenvectors.\newline

  We set
  \begin{align*}
    E_{\lambda}^{\infty} &= \bigcup_{k\geq 1}E_{\lambda}^{k}.
  \end{align*}
\end{definition}
\begin{example}
  In the previous example, we saw that $\Span_{\F}\left(e_1,e_2\right)\subseteq E_{1}^{2}$, and we have $-1$ is an eigenvalue of $A$ with eigenvector
  \begin{align*}
    v_3 &= \begin{pmatrix}1/2\\-1/2\\1\end{pmatrix}.
  \end{align*}
  We can verify that $v_3 \notin E_{1}^{2}$.\newline

  Thus, $\dim_{F}E_{1}^{2} \leq 2$, meaning $E_{1}^{2} = \Span_{\F}\left(e_1,e_2\right)$.
\end{example}
\begin{example}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$, and $T\in \Hom_{\F}\left(V,V\right)$, $\lambda \in \F$ such that
  \begin{align*}
    A = \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\lambda & 1 & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & \lambda\end{pmatrix},
  \end{align*}
  which is a matrix of $\lambda$ along the diagonal and $1$ along the superdiagonal. In particular, we can see that $A - \lambda I_{n}$ is the matrix with $1$ along the superdiagonal and $0$ everywhere else.\newline

  Notice that $\left(A - \lambda I_{n}\right)(v_1) = 0$, $\left(A - \lambda I_{n}\right)\left(v_2\right) = v_1$, etc.\newline

  Thus, we get that $E_{\lambda}^{1} = \Span_{\F}\left(v_1\right)$, $E_{\lambda}^{2} = \Span_{\F}\left(v_1,v_2\right)$, etc.\newline

  In general, $E_{\lambda}^{k} = \Span_{\F}\left(v_1,\dots,v_k\right)$ for $1 \leq k \leq n$.\newline

  Thus, $E_{\lambda}^{\infty} = E_{\lambda}^{n} = V$.
\end{example}
\begin{exercise}
  Describe the generalized eigenspaces of
  \begin{align*}
    \begin{pmatrix}\lambda_1 & 1 & 0 & 0 \\ 0 & \lambda_1 & 0 & 0 \\ 0 & 0 & \lambda_2 & 0 \\ 0 & 0 & 0 & \lambda_3\end{pmatrix}
  \end{align*}
\end{exercise}
We can see that we used $E_{\lambda}^{i} \subseteq E_{\lambda}^{i+1}$; this is true more generally.\newline

More generally, let $T\in \Hom_{\F}\left(V,V\right)$. We claim that if $i\geq j$, then $\ker\left(T^j\right)\subseteq \ker\left(T^i\right)$.\newline

Write $i = j + k$. Let $v\in \ker\left(T^j\right)$. Then, 
\begin{align*}
  T^i\left(v\right) &= T^{j+k}\left(v\right)\\
                    &= T^{k}\left(T^j\left(v\right)\right)\\
                    &= T^{k}\left(0_V\right)\\
                    &= 0_V.
\end{align*}
This gives $E_{\lambda}^{1}\subseteq E_{\lambda}^{2}\subseteq \cdots\subseteq E_{\lambda}^{\infty}$.
\begin{lemma}
  Let $V$ be a finite dimensional vector space with $\Dim_{\F}\left(V\right) = n$, and $T\in \Hom_{\F}\left(V,V\right)$. Then, there exists $m$ with $1 \leq m \leq n$ such that
  \begin{align*}
    \ker\left(T^{m}\right) = \ker\left(T^{m+1}\right).
  \end{align*}
  Moreover, for such an $m$, $\ker\left(T^{m}\right) = \ker\left(T^{m+j}\right)$ for all $j\geq 0$.
\end{lemma}
\begin{proof}
  We have
  \begin{align*}
    \ker\left(T^{1}\right)\subseteq \ker\left(T^{2}\right)\subseteq \cdots \subseteq \ker\left(T^{\infty}\right).
  \end{align*}
  If these containments are strict, then the dimension goes up indefinitely, contradicting $\Dim_{\F}\left(V\right) = n$.\newline

  Thus, we have $1 \leq m \leq n$ with
  \begin{align*}
    \ker\left(T^{m}\right) = \ker\left(T^{m+1}\right).
  \end{align*}
  Let $m$ be the smallest value such that $\ker\left(T^{m}\right) = \ker\left(T^{m+1}\right)$.\newline

  We use induction on $j$. The base case of $j = 1$ is what defines $m$. Assume $\ker\left(T^{m}\right) = \ker\left(T^{m+j}\right)$ for all $1 \leq j \leq N$.\newline

  Let $v\in \ker\left(T^{m+N+1}\right)$. This gives 
  \begin{align*}
    0_V &= T^{m+N+1}\left(v\right)\\
        &= T^{m+1}\left(T^{N}\left(v\right)\right),
  \end{align*}
  meaning $T^N(v)\in \ker\left(T^{m+1}\right)$. However, $\ker\left(T^{m+1}\right) = \ker\left(T^{m}\right)$, meaning $T^{N}\left(v\right) \in \ker\left(T^{m}\right)$, hence
  \begin{align*}
    0_V &= T^{m}\left(T^{N}(v)\right)\\
        &= T^{m+N}\left(v\right),
  \end{align*}
  meaning $v\in \ker\left(T^{m+N}\right)$. The inductive hypothesis gives $\ker\left(T^{m+N}\right) = \ker\left(T^{m}\right)$, meaning $v\in \ker\left(T^{m}\right)$. Thus, $\ker\left(T^{m+N+1}\right)\subseteq \ker\left(T^{m+N}\right)$, meaning $\ker\left(T^{m+N+1}\right) = \ker\left(T^{m+N}\right)$.
\end{proof}
\begin{corollary}
  If $\Dim_{\F}\left(V\right) = n$, and $T\in \Hom_{\F}\left(V,V\right)$, there exists $m$ with $1 \leq m \leq n$ such that for any $\lambda \in \F$,
  \begin{align*}
    E_{\lambda}^{\infty} &= E_{\lambda}^{m}.
  \end{align*}
\end{corollary}
\begin{theorem}
  Let $T\in \Hom_{\F}\left(V,V\right)$, $\lambda \in \F$, with $\left(x-\lambda\right)^{j}\mid m_{T}\left(x\right)$. We have
  \begin{align*}
    \Dim_{\F}\left(E_{\lambda}^{j}\right) \geq j.
  \end{align*}
\end{theorem}
\begin{proof}
  Write $m_T(x) = \left(x-\lambda\right)^{k}f(x)$, $f(x)\in \F[x]$, $f(x)\neq 0$.\newline

  Define $g_j\left(x\right) = \left(x-\lambda\right)^{j}$. We have $g_{k-1}f(x)$ is not the minimal polynomial, meaning there is $v\in V$ such that
  \begin{align*}
    g_{k-1}\left(T\right)f(T)(v) \neq 0_V.
  \end{align*}
  Set $v_k = f(T) v$. Note that $v_k \neq 0_V$.\newline

  Observe that 
  \begin{align*}
    \left(T - \lambda \id_{V}\right)^k\left(v_k\right) &= \left(T-\lambda \id_V\right)^{k}f(T) \left(v_k\right)\\
                                                       &= m_{T}\left(T\right)\left(v_k\right)\\
                                                       &= 0_V.
  \end{align*}
  Thus, $v\in E_{\lambda}^{k}$.\newline

  Moreover, by construction, 
  \begin{align*}
    \left(T-\lambda\id_{V}\right)^{k-1}\left(v_k\right) &= g_{k-1}(T)\left(v_k\right)\\
                                                        &= g_{k-1}(T)f(T)(v)\\
                                                        &\neq 0_{V}.
  \end{align*}
  Thus, $v_k\notin E_{\lambda}^{k-1}$.\newline

  Define
  \begin{align*}
    v_{k-1} &= \left(T - \lambda \id_V\right)\left(v_k\right)\\
            &= \left(T - \lambda \id_V\right)f(T)(v).
  \end{align*}
  Note that
  \begin{align*}
    \left(T - \lambda \id_V\right)^{k-1}\left(v_{k-1}\right) &= \left(T - \lambda\id_V\right)\left(v_k\right)\\
                                                             &= m_T\left(T\right)(v)\\
                                                             &= 0_V,
  \end{align*}
  meaning $v_{k-1}\in E_{\lambda}^{k-1}$.\newline

  Additionally,
  \begin{align*}
    \left(T - \lambda \id_{V}\right)^{k-1}\left(v_{k-1}\right) &= \left(T -\lambda \id_V\right)^{k-2}\left(v_{k}\right)\\
                                                               &\neq 0_{V},
  \end{align*}
  meaning $v_{k-1}\in E_{\lambda}^{k-1}\setminus E_{\lambda}^{k-2}$.\newline

  Continuing the process, we construct $\set{v_1,\dots,v_k}$ linearly independent.
\end{proof}
\begin{example}
  Let $T_A\in \Hom_{\F}\left(\F^3,\F^3\right)$ given by
  \begin{align*}
    A &= \begin{pmatrix}2 & 1 & 3 \\ 0 & 2 & 4 \\ 0 & 0 & 2\end{pmatrix}.
  \end{align*}
  We can verify that $m_{T}\left(x\right) = \left(x-2\right)^3$.\newline

  Observe that
  \begin{align*}
    \left(A - 2 I_{3}\right)^2 &= \begin{pmatrix}0 & 0 & 4 \\ 0 & 0 & 0 \\ 0 & 0 & 0\end{pmatrix}.
  \end{align*}
  Notice that $\left(A - 2I_3\right)^{3}\left(e_3\right) = 4e_3\neq 0$, meaning we set $v_3 = e_3$.\newline

  Note that $\left(T - 2\id_V\right)^{3}\left(e_3\right) = 0$, meaning $e_3\in E_{2}^{3}$.\newline

  We find $v_2 = \left(A - 2I_3\right)\left(v_3\right)$, meaning
  \begin{align*}
    v_2 &= \begin{pmatrix}0 & 1 & 3 \\ 0 & 0 & 4 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix}0 \\ 0 \\ 1\end{pmatrix}\\
        &= \begin{pmatrix}3\\4\\0\end{pmatrix}.
  \end{align*}
  Finally,
  \begin{align*}
    v_1 &= \left(A - 2I_{3}\right)\left(v_2\right)\\
        &= \begin{pmatrix}4\\0\\0\end{pmatrix}.
  \end{align*}
  Thus, our generalized eigenvectors are
  \begin{align*}
    E_{2}^{3} &= \Span\left( \begin{pmatrix}0\\0\\1\end{pmatrix}, \begin{pmatrix}3\\4\\0\end{pmatrix}, \begin{pmatrix}4\\0\\0\end{pmatrix}\right).
  \end{align*}
  If we say $\mathcal{B} = \set{v_1,v_2,v_3}$, then our matrix $\left[T_{A}\right]_{\mathcal{B}}$ is
  \begin{align*}
    \left[T_{A}\right]_{\mathcal{B}} &= \begin{pmatrix}2 & 1 & 0 \\ 0 & 2 & 1\\ 0 & 0 & 2\end{pmatrix}.
  \end{align*}
  \begin{remark}
    This matrix is in what is known as Jordan canonical form.
  \end{remark}
\end{example}
\subsection{Characteristic Polynomials and the Cayley--Hamilton Theorem}%
\begin{definition}
  Let $A \in \Mat_{n}\left(\F\right)$. The characteristic polynomial is $c_{A}\left(x\right) = \det\left(xI_n - A\right)$.
\end{definition}
\begin{remark}
  The Cayley--Hamilton theorem states that
  \begin{align*}
    c_A(A) &=0_{n}.
  \end{align*}
\end{remark}
\begin{definition}
  Let $f(x) = x^n + a_{n-1}x^{n-1} +  \cdots + a_1 x + a_0\in \F[x]$. The companion matrix of $f(x)$ is given by $C\left(f(x)\right)$, which consists of $-a_{n-1}$ through $-a_0$ along the first column, $0$ on the rest of the diagonal, and $1$ along the superdiagonal.
\end{definition}
\begin{lemma}
  If $A = C\left(f(x)\right)$, then $c_{A}\left(x\right) = f(x)$.
\end{lemma}
\begin{lemma}
  Let $A,B\in \Mat_{n}\left(\F\right)$ be similar matrices. Then, $c_{A}\left(x\right) = c_{B}\left(x\right)$.
\end{lemma}
\begin{proof}
  Let $A = PBP^{-1}$ for some $P\in \text{GL}_{n}\left(\F\right)$. Then, we have
  \begin{align*}
    c_{A}\left(x\right) &= \det\left(xI_{n} - A\right)\\
                        &= \det\left(xI_{n}- PBP^{-1}\right)\\
                        &= \det\left(P\left(xI_n\right)P^{-1}-PBP^{-1}\right)\\
                        &= \det\left(P\left(xI_n - B\right)P^{-1}\right)\\
                        &= \det\left(P\right)\det\left(xI_n-B\right)\det\left(P^{-1}\right)\\
                        &= \det\left(xI_n - B\right)\\
                        &= c_B\left(x\right).
  \end{align*}
\end{proof}
\begin{definition}[Characteristic Polynomial of Linear Transformation]
  For $T\in \Hom_{\F}\left(V,V\right)$, let $\mathcal{B}$ be a basis of $V$ and set
  \begin{align*}
    c_T(x) &= c_{\left[T\right]_{\mathcal{B}}}(x).
  \end{align*}
\end{definition}
\begin{theorem}
  Let $v\in V$, $v\neq 0$. Let $\Dim_{\F}\left(V\right) < \infty$. Then, there is a unique monic polynomial $m_{T,v}(x)\in \F[x]$ of minimal degree such that $m_{T,v}\left(T\right)\left(v\right) = 0_V$.\newline

  Moreover, if $f(x)\in \F[x]$ with $f(T)(v) = 0$, then $m_{T,v}(x)|f(x)$.
\end{theorem}
\begin{proof}
  Consider the set $\set{v,T(v),\dots,T^n(v)}$. This collection consists of $n+1$ elements of $V$, meaning it is linearly dependent. Let
  \begin{align*}
    a_mT^{m}\left(v\right) + \cdots + a_1T(v) + a_0 = 0_V
  \end{align*}
  for some $m \leq n$ of minimal degree with not all $a_i = 0$. Set
  \begin{align*}
    p(x) &= x^m + \frac{a_{m-1}}{a_m}x^{m-1} + \cdots + \frac{a_1}{a_m}x + \frac{a_0}{a_m}.
  \end{align*}
  Thus, $p(t)(v) = 0_V$ by construction.\newline

  Set
  \begin{align*}
    I_v &= \set{g(x)\in \F[x]\mid g(T)(v) = 0_V}.
  \end{align*}
  We know $p(x)\in I_v$, and $p(x)\neq 0$. We have $p(x)$ is a nonzero monic polynomial in $I_v$ of minimal degree.\newline

  Set $m_{T,v}(x) = p(x)$.\newline

  Let $f(x)\in I_v$. We want to show that $m_{T,v}(x)|f(x)$.\newline

  Write $f(x) = q(x)m_{T,v}(x) + r(x)$ for some $q(x),r(x)\in \F[x]$, with $r(x) = 0$ or $\deg \left(r(x)\right) < \deg m_{T,v}(x)$. We have $r(x) = f(x) - q(x)m_{T,v}(x)$, implying
  \begin{align*}
    r(T)(v) &= f(T)(v) - q(T)\left(m_{T,v}\left(T\right)(v)\right)\\
            &= 0_v- q(T)\left(0_v\right)\\
            &= 0_v,
  \end{align*}
  implying $r(x)\in I_v$. Since $m_{T,v}(x)$ was defined to have minimal degree, it has to be the case that $r(x) = 0$.\newline

  If $h(x) \in I_{v}$ with $\deg \left(h(x)\right) = \deg\left(m_{T,v}(x)\right)$ with $h(x)$ monic, then $m_{T,v}(x)|h(x)$ implies $h(x) = m_{T,v}(x)$.
\end{proof}
We will refer to $m_{T,v}(x)$ as the $T$-annihilator of $v$.
\begin{example}
  Let $V = \F^n$, $\mathcal{B} = \set{e_1,\dots,e_n}$. Define $T\in \Hom_{\F}\left(V,V\right)$ by
  \begin{align*}
    T\left(e_1\right) &= 0\\
    T\left(e_{j}\right) &= e_{j-1} \tag*{$2\leq j\leq n$}.
  \end{align*}
  Let $f(x) = x$. Then, $f(T)\left(e_1\right) = T\left(e_1\right) = 0_V$, implying that $m_{T,e_1}\left(x\right) | x$; thus, $m_{T,e_1}(x) = 1$ or $m_{T,e_1}(x) = x$, but $\id\left(e_1\right) = e_1 \neq 0_V$, meaning $m_{T,e_1}\left(x\right) = x$.\newline

  Let $g(x) = x^2$. Then,
  \begin{align*}
    g(T)\left(e_2\right) &= T^{2}\left(e_2\right)\\
                         &= T\left(T\left(e_1\right)\right)\\
                         &= T\left(0_V\right)\\
                         &= 0_V.
  \end{align*}
  This gives $m_{T,e_2}(x) | x^2$, so $m_{T,e_2}(x) = 1,x,x^2$. If $m_{T,e_2}(x) = 1$, then $\id_V\left(e_2\right) = e_2 = 0_V$, which is not the case. Similarly, if $m_{T,e_2}(x) = x$, then $T(e_2) = e_1 = 0_V$, so $m_{T,e_2}(x) = x^2$.\newline

  For each $1 \leq j\leq n$, $m_{T,e_j}(x) = x^j$.
\end{example}
\begin{example}
  Let $V = \Q^2$, $T\in \Hom_{\Q}\left(\Q^2,\Q^2\right)$, with
  \begin{align*}
    T\left(e_1\right) &= e_1 + 3e_2\\
    T\left(e_2\right) &= 2e_1 + 4e_2.
  \end{align*}
  We wish to find the annihilating polynomial for $e_1$.\newline

  We know that $m_{T,e_1}(x)$ has degree $1$ or $2$. Additionally, $m_{T,e_1}(x)$ cannot have degree $1$, as if $m_{T,e_1}\left(x\right) = x+a$, then
  \begin{align*}
    m_{T,e_1}\left(T\right)\left(e_1\right) &= T\left(e_1\right) + ae_1\\
                                            &= e_1 + 3e_2 + ae_1\\
                                            &\neq 0.
  \end{align*}
  Thus, $m_{T,e_1}$ is of degree $2$.
  \begin{align*}
    T^2\left(e_1\right) &= T\left(e_1 + 3e_2\right)\\
                        &= T\left(e_1\right) + 3T\left(e_2\right)\\
                        &= e_1 + 3e_2 + 3\left(2e_1 + 4e_2\right)\\
                        &= 7e_1 + 15 e_2.
  \end{align*}
  We want to find $b,c\in \Q$ such that
  \begin{align*}
    T^2\left(e_1\right) + bT\left(e_1\right) + ce_1 &= 0_V.
  \end{align*}
  Solving the resulting system of linear equation yields $b = -5$ and $c = -2$. The annihilating polynomial is, thus,
  \begin{align*}
    m_{T,e_1}(x) &= x^2 - 5x - 2.
  \end{align*}
\end{example}
\begin{exercise}\hfill
  \begin{enumerate}[(1)]
    \item Show that $m_{T,e_2}(x) = x^2 - 5x - 2$.
    \item Calculate $m_{T,e_1}(x)$ and $m_{T,e_2}(x)$ for $\F = \F_3 = \Z/3\Z$.
  \end{enumerate}
\end{exercise}
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$, and $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$. Let $T\in \Hom_{\F}\left(V,V\right)$. We have
  \begin{align*}
    m_{T}\left(x\right) &= \lcm_{1 \leq i \leq n}m_{T,v_i}\left(x\right).
  \end{align*}
\end{theorem}
\begin{proof}
  Let $f(x) = \lcm_{1\leq i\leq n} m_{T,v_i}\left(x\right)$. Then,
  \begin{align*}
    m_{T}\left(T\right)\left(v_i\right) &= 0
  \end{align*}
  meaning $m_{T,v_i}|m_{T}(x)$ for each $i$, so $f(x) | m_{T}(x)$.\newline

  Let $v\in V$; write $v = \sum_{i=1}^{n}a_iv_i$. Then,
  \begin{align*}
    f(T)(v) &= f(T)\left(\sum_{i=1}^{n}a_iv_i\right)\\
            &= \sum_{i=1}^{n}a_if(T)\left(v_i\right)\\
            &= 0,
  \end{align*}
  since $m_{T,v_i}(x)|f(x)$ for all $i$. Thus, $m_{T}(x) | f(x)$.
\end{proof}
\begin{lemma}
  Let $T \in \Hom_{\F}\left(V,V\right)$. Let $v_1,\dots,v_k\in V$, and set $p_i(x) = m_{T,v_i}(x)$. Suppose $p_i(x)$ are pairwise relatively prime. Set
  \begin{align*}
    v &= v_1 + \cdots v_k.
  \end{align*}
  Then,
  \begin{align*}
    m_{T,v}\left(x\right) &= \prod_{j=1}^{k}p_j(x).
  \end{align*}
\end{lemma}
\begin{proof}
  We will prove this for $k = 2$.\newline

  Since $p_1(x)$ and $p_2(x)$ are relatively prime, we can write
  \begin{align*}
    1 &= p_1(x)q_1(x) + p_2(x)q_2(x).
  \end{align*}
  Particularly,
  \begin{align*}
    \id_V &= p_1(T)q_1(T) + p_2(T)q_2(T).
  \end{align*}
  Set $v = v_1 + v_2$. Then,
  \begin{align*}
    v &= \id_V(v)\\
      &= \left(p_1(T)q_1(T) + p_2(T)q_2(T)\right)(v)\\
      &= p_1(T)q_2(T)(v) + p_2(T)q_2(T)(v)\\
      &= p_1(T)q_2(T)\left(v_1 + v_2\right) + p_2(T)q_2(T)\left(v_1 + v_2\right)\\
      &= \underbrace{p_1(T)q_2(T)\left(v_2\right)}_{w_2} + \underbrace{p_2(T)q_2(T)\left(v_2\right)}_{w_1}
      \intertext{meaning}
    v &= w_1 + w_2.
  \end{align*}
Note that
\begin{align*}
  p_1(T)(w_1) &= p_1(T)p_2(T)q_2(T)\left(v_1\right)\\
              &= q_2(T)p_2(T)p_1(T)\left(v_1\right)\\
              &= 0_V,
\end{align*}
meaning $w_1\in \ker \left(p_1(T)\right)$, and similarly, $w_2\in \ker \left(p_2(T)\right)$.\newline

Let $r(x)\in \F[x]$ with $r(T)(v) = 0$. We have $v = w_1 + w_2$ and $w_2\in \ker\left( p_2(T)\right)$, meaning
\begin{align*}
  p_2(T)(v) &= p_2(T)\left(w_1 + w_2\right)\\
            &= p_2(T)\left(w_1\right).
\end{align*}
Thus,
\begin{align*}
  0_V &= p_2(T)q_2(T)\left(0_V\right)\\
      &= p_2(T)q_2(T)r(T)(v)\\
      &= r(T)q_2(T)p_2(T)(v)\\
      &= r(T)q_2(T)p_2(T)\left(w_1\right).
\end{align*}
Similarly, $r(T)q_1(T)p_1(T)\left(w_1\right) = 0_V$ since $w_1\in \ker\left(p_1(T)\right)$. Hence,
\begin{align*}
  0_V &= r(T)p_2(T)q_2(T)\left(w_1\right) + r(t)p_1(T)q_1(T)\left(w_1\right)\\
      &= r(T)\underbrace{\left(p_2(T)q_2(T) + p_1(T)q_1(T)\right)}_{\id_V}\left(w_1\right)\\
      &= r(T)\left(w_1\right).
\end{align*}
This gives
\begin{align*}
  0_V &= r(T)\left(w_1\right)\\
      &= r(T)p_2(T)q_2(T)\left(v_1\right).
\end{align*}
Thus, $p_1(x)|r(x)p_2(x)q_2(x)$. Additionally,
\begin{align*}
  1 &= p_1(x)q_1(x) + p_2(x)q_2(x)\\
  \gcd\left(p_1(x),p_2(x)q_2(x)\right) &= 1,
\end{align*}
implying $p_1(x)|r(x)$, and similarly for $p_2(x) | r(x)$.\newline

Since $\gcd\left(p_1(x),p_2(x)\right) = 1$, we have
\begin{align*}
  \lcm\left(p_1(x),p_2(x)\right) &= p_1(x)p_2(x),
\end{align*}
so $p_1(x)p_2(x) | r(x)$. If we take $r(x) = m_{T,v}(x)$, implying $p_1(x)p_2(x)|m_{T,v}(x)$. Thus, $m_{T,v}(x) = p_1(x)p_2(x)$.
\end{proof}
\begin{exercise}
  Prove for $k > 2$.
\end{exercise}
\begin{theorem}
  Let $T\in \Hom_{\F}\left(V,V\right)$. There exists $v\in V$ such that $m_{T,v}\left(x\right) = m_{T}(x)$. In particular, $\deg m_{T}(x) \leq n$.
\end{theorem}
\begin{proof}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$.\newline

  We know that
  \begin{align*}
    m_{T}\left(v\right) &= \lcm_{1 \leq i \leq n} m_{T,v_i}(x).
  \end{align*}
  Factor
  \begin{align*}
    m_T(x) &= p_1(x)^{e_1} \cdots p_{k}(x)e^{k},
  \end{align*}
  with $p_i$ relatively prime, $e_i \geq 1$.\newline

  For $1 \leq j \leq k$, there exists $i_j\in \set{1,\dots,n}$ and $q_{i_j}(x)\in \F[x]$ with
  \begin{align*}
    m_{T,v_{i_j}}(x) &= p_j(x)^{e_j}q_{i_j}(x).
  \end{align*}
  Define $w_j = q_{i_j}(T)\left(v_{i_j}\right)$. This gives
  \begin{align*}
    M_{T,w_j} &= p_j(x)^{e_j}.
  \end{align*}
  Set $w = w_1 + \cdots + w_k$. The previous result gives
  \begin{align*}
    m_{T,w}(x) &= \prod_{j=1}^{k}p_j(x)^{e_j}\\
               &= m_T(x).
  \end{align*}
\end{proof}
\begin{recall}
  We defined $m_{T,v}(x)$, and that $m_T(x)$ is $m_{T,v}(x)$ for some $v\in V$, meaning $\deg\left(m_T(x)\right) < n$.
\end{recall}
\begin{lemma}
  Let $W$ be a $T$-invariant subspace. We get a map $\overline{T}\in\Hom_{\F}\left(V/W,V/W\right)$ defined by
  \begin{align*}
    \overline{T}\left(v+W\right) = T(v) + W.
  \end{align*}
  Let $v\in V$. Then,
  \begin{align*}
    m_{\overline{T},v+W}(x)|m_{T,v}(x)
  \end{align*}
  and similarly,
  \begin{align*}
    m_{\overline{T}}(x) | m_T(x).
  \end{align*}
\end{lemma}
\begin{proof}
  We have
  \begin{align*}
    m_{T,v}\left(\overline{T}\right)\left(v+W\right) &= m_{T,v}\left(T\right)(v) + W\\
                                                     &= 0_V + W\\
                                                     &= 0_{V/W}.
  \end{align*}
  Thus, $m_{\overline{T},v+W}|m_{T,v}(x)$.
\end{proof}
\begin{definition}
  Let $T\in \Hom_{\F}\left(V,V\right)$, $\mathcal{A} = \set{v_1,\dots,v_k}$ a set of vectors. The $T$-span of $\mathcal{A}$ is
  \begin{align*}
    W &= \set{\sum_{i=1}^{k}p_i(T)\left(v_i\right)\mid p_i(x)\in \F[x]}.
  \end{align*}
\end{definition}
\begin{exercise}
  Show that $W$ is a $T$-invariant subspace of $V$. Moreover, show it is the smallest with respect to inclusion $T$-invariant subspace of $V$ that contains $\mathcal{A}$.
\end{exercise}
\begin{example}
  Let $V = \Q^4$. Take $T\in \Hom_{\F}\left(V,V\right)$ by
  \begin{align*}
    T\left(e_1\right) &= 2e_1 + 3e_3\\
    T\left(e_2\right) &= e_1 + e_4\\
    T\left(e_3\right) &= e_1 - e_3\\
    T\left(e_4\right) &= 2e_1 + 2e_2 + 5e_3 - 4e_4.
  \end{align*}
  Let $\mathcal{A} = \set{e_1}$. We want the $T$-span of $\mathcal{A}$. Set $p(x) = 1$, meaning $p(T)(e_1) = \id(e_1) = e_1$.\newline

  Set $q(x) = \frac{1}{3}\left(x-2\right)$. If we take $q(T)\left(e_1\right)$, we have
  \begin{align*}
    q(T)\left(e_1\right) &= \frac{1}{3}\left(T - 2\id_v\right)\left(e_1\right)\\
                         &= \frac{1}{3}\left(T\left(e_1\right) - 2e_1\right)\\
                         &= e_3.
  \end{align*}
  Thus, $\Span_{\F}\left(e_1,e_3\right)\subseteq T$-span of $\mathcal{A}$.\newline

  However, we also know that $\Span_{\F}\left(e_1,e_3\right)$ is $T$-invariant and contains $\mathcal{A}$.\newline

  Thus, the $T$-span of $\mathcal{A}$ is $\Span_{\F}\left(e_1,e_3\right)$.\newline

  If we set $f(x) = x^2 - 5x - 1$, then $f(T)\left(e_1\right) = 0_V$, meaning $m_{T,e_1}(x)|f(x)$. However, $f$ is irreducible over $\Q$, so $m_{T,e_1}(x) = x^2 - 5x - 1$. Note that $\deg\left(m_{T,e_1}(x)\right) = \Dim_{\F}\left(T\text{-span }\set{e_1}\right)$.
\end{example}
\begin{lemma}
  Let $T\in \Hom_{\F}\left(V,V\right)$, $w\in V$, and $W$ the subspace of $V$ that is generated by $T$ on $\set{w}$.\newline

  Then, $\Dim_{\F}\left(W\right) = \deg\left(m_{T,w}(x)\right)$.
\end{lemma}
\begin{proof}
  Let $\deg\left(m_{T,w}(x)\right) = k$. Consider the set $\set{w,T(w),\dots,T^{k-1}(w)}$. This has to be a basis for the $T$-span of $\set{w}$.
\end{proof}
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$.
  \begin{enumerate}[(1)]
    \item We have $m_{T}(x)|c_{T}(x)$.
    \item Every irreducible factor of $c_T(x)$ is a factor of $m_{T}(x)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Let $\deg\left(m_{T}(x)\right) = k \leq n$. Let $v\in V$ with $m_{T}(x) = m_{T,v}(x)$.\newline

  Let $W_1$ be the $T$-span of $\set{v}$, with $\Dim_{\F}\left(W_1\right) = K$\newline

  Set $v_k = v$, $v_{k-i} = T^{i}\left(v\right)$. We have
  \begin{align*}
    \mathcal{B} = \set{v_1,dots,v_k}
  \end{align*}
  is a basis of $W_1$, and
  \begin{align*}
    \left[T\bigr\vert_{W_1}\right]_{\mathcal{B}_1} &= c\left(m_{T}(x)\right).
  \end{align*}
  If $k = n$, then $W_1 = V$, so $\left[T\right]_{\mathcal{B}_1} = c\left(m_T(x)\right)$ which has characteristic polynomial $m_T(x)$,  meaning $m_T(x) = c_T(x)$.\newline

  Suppose $k < n$. Expand $\mathcal{B}_1$ to a full basis of $V$, $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$, with $\mathcal{B}_2 = \set{v_{k+1},\dots,v_{n}}$. In the upper triangular matrix
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}A & B \\ 0 & D\end{pmatrix},
  \end{align*}
  we have $A = c\left(m_{T}(x)\right)$, so
  \begin{align*}
    c_{T}(x) &= \det\left(xI_n - \left[T\right]_{\mathcal{B}}\right)\\
             &= \det\begin{pmatrix}xI_k - A & B \\ 0 & xI_{n-k}-D\end{pmatrix}\\
             &= \det\left(xI_{k} - A\right)\det\left(xI_{n-k}-D\right)\\
             &= c_{A}(x)\det\left(xI_{n-k}-D\right)\\
             &= m_{T}(x)\det\left(xI_{n-k}-D\right),
  \end{align*}
  meaning $m_{T}(x)|c_T(x)$.\newline

  To prove (2), we induct on $\Dim_{\F}\left(V\right) = n$. If $n=1$, then both characteristic polynomials are monic of degree $1$, so they are equal.\newline

  If $\deg\left(m_T(x)\right) = n$, then $ m_T(x)|c_T(x)$, and both have degree $n$ and are monic, so $c_T(x) = m_T(x)$.\newline

  Suppose $\deg\left(m_T(x)\right) = k < n$. Pick $v$ such that $m_T(x) = m_{T,v}(x)$. Define $W_1$ to be the $T$-span of $\set{v}$, with $\mathcal{B}_1 = \set{v_1,\dots,v_k}$ defined as above. Extend $\mathcal{B}_1$ to $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$ as above.\newline

  Consider $\overline{T}: V/W_1 \rightarrow V/W_1$, and $\overline{\mathcal{B}} = \set{v_{k_1}+W_1,\dots,v_{n}+W_1}=  \pi_{W_1}\left(\mathcal{B}\right)$.\newline

  In the upper triangular matrix
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}A & B \\ 0 & D\end{pmatrix},
  \end{align*}
  the matrix $\left[\overline{T}\right]_{\overline{\mathcal{B}}} = D$.\newline

  Since $\Dim_{\F}\left(V/W_1\right) < \Dim_{\F}\left(V\right)$, the induction hypothesis holds that $m_{\overline{T}}(x)$ and $c_{\overline{T}}(x)$ have the same irreducible factors.\newline

  Earlier, we had
  \begin{align*}
    c_{T}(x) &= m_{T}\det\left(xI_{n-k}-D\right),
  \end{align*}
  yielding
  \begin{align*}
    c_{T}(x) &= m_{T}(x)c_{\overline{T}}(x).
  \end{align*}
  Let $p(x)$ be an irreducible factor of $c_{T}(x)$. If $p(x)|m_{T}(x)$, we are done. Else, $p(x)|c_{\overline{T}}(x)$. However, $c_{\overline{T}}(x)$ and $m_{\overline{T}}(x)$ have the same irreducible factors, so $p|m_{\overline{T}}(x)$. However, $m_{\overline{T}}(x)|m_{T}(x)$, so $p(x)|m_{T}(x)$.
\end{proof}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 & 0 & 0 & 0 & 0 \\ 3 & 4 & 0 & 0 & 0 & 0 \\ 0 & 0 & 3 & 7 & 0 & 0 \\ 0 & 0 & -1 & 2 & 0 & 0 \\ 0 & 0 & 0 & 0 & -5 & 6 \\ 0 & 0 & 0 & 0 & 2 & -3\end{pmatrix}\in \Mat_{6}\left(\Q\right).
  \end{align*}
  We can verify that
  \begin{align*}
    c_A(x) &- \left(x^{2}-5x-2\right)\left(x^2 - x - 1\right)\left(x^2 + 8x + 3\right),
  \end{align*}
  implying that
  \begin{align*}
    m_{A}\left(x\right) &= \left(x^{2}-5x-2\right)\left(x^2 - x - 1\right)\left(x^2 + 8x + 3\right).
  \end{align*}
\end{example}
\begin{theorem}[Cayley--Hamilton]\hfill
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,V\right)$, $\dim_{\F}\left(V\right) < \infty$. Then,
      \begin{align*}
        c_T(T) &= 0_{\Hom_{\F}\left(V,V\right)}
      \end{align*}
    \item Let $A\in \Mat_{n}\left(\F\right)$. Then,
      \begin{align*}
        c_A(A) &= 0_{n}.
      \end{align*}
  \end{enumerate}
\end{theorem}
\begin{proof}
  Write $c_T(x) = f(x)m_T(x)$. Then, for any $v\in V$, we have
  \begin{align*}
    c_T(T)(v) &= f(T)m_T(T)(v)\\
              &= f(T)\left(0_V\right)\\
              &= 0_V.
  \end{align*}
\end{proof}
\subsection{Jordan Canonical Form}%
For the purposes of this section, $V$ is always finite dimensional, and all polynomials split into linear factors over their respective fields.
\begin{definition}
  Let $T\in \Hom_{\F}\left(V,V\right)$. A Jordan basis for $V$ with regard to $T$ is a basis $\mathcal{B}$ such that $\left[T\right]_{\mathcal{B}}$ has some $\lambda \in \F$ along the diagonal and $1$ along the superdiagonal.\newline

  More generally, if $V = V_1\oplus \cdots \oplus V_k$ is a decomposition into $T$-invariant subspaces, then each $V_i$ has Jordan basis $\mathcal{B}_i$, and we say $\mathcal{B} = \bigcup_{i=1}^{k}\mathcal{B}_i$ is a Jordan basis for $V$.
\end{definition}
\begin{definition}
  A matrix with $\lambda$ along the diagonal and $1$ along the superdiagonal is called a Jordan block associated with eigenvalue $\lambda$.
  \begin{align*}
    J_i &= \begin{pmatrix}
\lambda_i & 1            & \;     & \;  \\
\;        & \lambda_i    & \ddots & \;  \\
\;        & \;           & \ddots & 1   \\
\;        & \;           & \;     & \lambda_i       
\end{pmatrix}
  \end{align*}
\end{definition}
\begin{definition}
  We say a matrix is in Jordan canonical form if it is block diagonal with Jordan blocks.
\end{definition}
\begin{theorem}\hfill
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,V\right)$. Suppose $c_T(x) = \left(x-\lambda_1\right)^{e_1}\cdots \left(x-\lambda_k\right)^{e_k}$ over $\F$. Then, $V$ has a Jordan basis $\mathcal{B}$. Moreover, $J = \left[T\right]_{\mathcal{B}}$ is unique up to the order of the blocks.
    \item Let $A\in \Mat_{n}\left(\F\right)$ with $c_A = \left(x-\lambda_1\right)^{e_1}\cdots \left(x-\lambda_k\right)^{e_k}$ over $\F$. Then $A$ is similar to a matrix in Jordan canonical form that is unique up to the order of the blocks.
  \end{enumerate}
\end{theorem}
\begin{lemma}
  Let $T\in \Hom_{\F}\left(V,V\right)$. We have $\ker\left(\left(T-\lambda \id_V\right)^j\right)$ and $\img\left(\left(T-\lambda \id_V\right)^j\right)$ are $T$-invariant subspaces for all $j\geq 0$.
\end{lemma}
\begin{proof}
  Note that
  \begin{align*}
    T\circ \left(T - \lambda\id_{V}\right)^j &= \left(T - \lambda\id_{V}\right)^j\circ T.
  \end{align*}
  Let $v\in \ker\left(\left(T-\lambda \id_V\right)^j\right)$. We have
  \begin{align*}
    \left(T-\lambda\id_V\right)^j\left(T(v)\right) &= T\left(\left(T-\lambda\id_V\right)^j(v)\right)\\
                                                   &= T\left(0_V\right)\\
                                                   &= 0_V.
  \end{align*}
  Thus, $T(v)\in \ker\left(\left(T-\lambda\id_V\right)^j\right)$.\newline

  Let $w\in \img\left(\left(T-\lambda\id_V\right)^j\right)$. We can write
  \begin{align*}
    w &= \left(T-\lambda\id_V\right)^j(v)
  \end{align*}
  for some $v\in V$. Applying $T$ to both sides, we have
  \begin{align*}
    T(w) &= T\left(\left(T-\lambda\id_V\right)^j(v)\right)\\
         &= \left(T-\lambda\id_V\right)^j\left(T(v)\right),
  \end{align*}
  meaning $T(w)\in \img\left(\left(T-\lambda\id_V\right)^j\right)$.
\end{proof}
We know there exists $m$ such that $E_{\lambda}^{\infty} = E_{\lambda}^{m}$. We also know that if $\left(x-\lambda\right)^k|m_T(x)$, then $\dim_{\F}\left(E_{\lambda}^{k}\right)\geq k$.
\begin{lemma}
  Suppose $m_T(x) = \left(x-\lambda\right)^mp(x)$ with $p\left(\lambda\right)\neq 0$. Then,
  \begin{align*}
    E_{\lambda}^{\infty} &= E_{\lambda}^{m}.
  \end{align*}
\end{lemma}
\begin{proof}
  Let $v\in E_{\lambda}^{\infty}$ and $e$ be the least positive integer such that 
  \begin{align*}
    \left(T-\lambda\id_V\right)^{e}\left(v\right) = 0_V.
  \end{align*}
  Suppose toward contradiction that $e > m$. We have $m_{T,v}(x) | \left(x-\lambda\right)^e$, but $m_{T,v}\left(x\right) \nmid \left(x-\lambda\right)^{e-1}$. In particular, $m_{T,v}(x) = \left(x-\lambda\right)^{e}$. However, $m_{T,v}(x) | m_{T}(x)$.
\end{proof}
\begin{lemma}
  Let $\dim_{\F}\left(V\right) = n$. Let $m_T(x) = \left(x-\lambda\right)^mp(x)$ with $p\left(\lambda\right)\neq 0$. Then, we have
  \begin{align*}
    V &= E_{\lambda}^{m} \oplus \img\left(\left(T-\lambda\id_V\right)^m\right).
  \end{align*}
\end{lemma}
\begin{proof}
  Recall that
  \begin{align*}
    E_{\lambda}^{m} &= \ker\left(\left(T-\lambda\id_{V}\right)^m\right).
  \end{align*}
  Therefore, the dimensions line up. All we need show is that $E_{\lambda}^{m}\cap \img\left(\left(T-\lambda\id_V\right)^m\right) = \set{0_V}$.\newline

  Let $v\in E_{\lambda}^{m}\cap \img\left(\left(T-\lambda\id_V\right)^m\right)$. We have
  \begin{align*}
    v &= \left(T-\lambda\id_V\right)^{m}\left(w\right)
  \end{align*}
  for some $w\in V$. Applying $\left(T-\lambda\id_V\right)^{m}$ to both sides, we have
  \begin{align*}
    \left(T-\lambda\id_V\right)^{m}\left(v\right) &= \left(T-\lambda\id_V\right)^{2m}(w)\\
                                                  &=0_V,
  \end{align*}
  since $v\in \ker\left(\left(T-\lambda\id_V\right)^m\right)$. Thus,
  \begin{align*}
    \left(T-\lambda\right)^{2m}(w) &= 0_V.
  \end{align*}
  Thus, $w\in E_{\lambda}^{2m}$. However, since $E_{\lambda}^{\infty} = E_{\lambda}^{m}$, so too is $E_{\lambda}^{2m}$, so $w\in E_{\lambda}^{m}$, meaning
  \begin{align*}
    \left(T-\lambda\right)^{m}\left(w\right) &= 0_V,
  \end{align*}
  so $v = 0_V$.
\end{proof}
\begin{theorem}
  Assume $m_T(x) = \left(x-\lambda_1\right)^{m_1}\cdots\left(x-\lambda_k\right)^{m_k}$ with $\lambda_j\in \F$, $\lambda_j$ distinct, $m_j\geq 1$.\newline

  Then, 
  \begin{align*}
    V &= E_{\lambda_1}^{m_1} \oplus \cdots \oplus E_{\lambda_k}^{m_k}.
  \end{align*}
\end{theorem}
\begin{proof}
  We will use induction on $k$.\newline

  If $k = 1$, then $m_T(x) = \left(x-\lambda_1\right)^{m_1}$. Since $m_{T}\left(T\right)\left(v\right) = 0_V$ for all $v\in V$, we have
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}.
  \end{align*}
  Assume the result is true for any vector space $W$ and $S\in \Hom_{\F}\left(W,W\right)$, where $m_S(x)$ splits completely over $\F$ and has fewer than $k$ distinct roots.\newline

  We can break our vector space $V$ to be
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}\oplus \img\left(\left(T-\lambda_1\id_{V}\right)^{m_1}\right).
  \end{align*}
  Set $W = \img\left(\left(T-\lambda_1\right)^{m_1}\right)$. We have $W$ is $T$-invariant. Thus, $T_W := T\vert_{W}\in \Hom_{\F}\left(W,W\right)$.\newline

  We claim that $m_{T_W}(x) = \left(x-\lambda_2\right)^{m_2}\cdots \left(x-\lambda_k\right)^{m_k}$.\newline

  Set $p(x) = \left(x-\lambda_2\right)^{m_2}\cdots \left(x-\lambda_k\right)^{m_k}$. Suppose $w\in W$ satisfies $p\left(T_W\right)\left(w\right) \neq 0_V$. At the same time, we have $m_{T}(T)(w) = 0_V$. Thus,
  \begin{align*}
    \left(T-\lambda_1\id_V\right)^{m_1}\left(p(T)(w)\right) = 0_V,
  \end{align*}
  meaning $p(T)(w) \in E_{\lambda_1}^{m_1}$. This is a contradiction, since $p(T)(w) = p\left(T_W\right)(w)\in W$.\newline

  Thus, $m_{T_W}|p(x)$.\newline

  Suppose $m_{T_W}$ is a proper divisor of $p(x)$. If we set $f(x) = m_{T_W}(x)\left(x-\lambda_1\right)^{m_1}$. For $v\in V$, write
  \begin{align*}
    v &= v_1 + w
  \end{align*}
  with $v_1\in E_{\lambda_1}^{m_1}$ and $w\in W$. Notice that
  \begin{align*}
    f(T)(v) &= f(T)\left(v_1\right) + f(T)\left(w\right)\\
            &= m_{T_W}\left(\left(T-\lambda_1\id_V\right)^{m_1}\right)(v) + \left(T-\lambda_1\id_V\right)^{m_1}M_{T_W}(w)\\
            &= 0_V + 0_V\\
            &= 0_V.
  \end{align*}
  Thus, $m_{T}(x)|f(x)$, which is a contradiction if $m_{T_W}$ is a proper divisor of $p(x)$.\newline

  Thus, $m_{T_W}(x) = p(x)$ as claimed.\newline

  We have that
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}\oplus W,
  \end{align*}
  and we apply the induction hypothesis to $W$ to yield
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}\oplus \left(E_{\lambda_2}^{m_2}\oplus \cdots \oplus E_{\lambda_k}^{m_k}\right).
  \end{align*}
\end{proof}
If $T$ has minimal polynomial of the form $m_T(x) = \left(x-\lambda\right)^mp(x)$ with $p\left(\lambda\right) \neq 0$, then we get at least one Jordan block with size $m$.
\begin{lemma}
  Let $m_T(x) = c_T(x) = \left(x-\lambda\right)^n$, with $\dim_{\F}\left(V\right) = n$. Then, a Jordan basis for $V$ exists.
\end{lemma}
\begin{proof}
  Let $w_1\in V$ with $m_{T,w_1}(x) = m_{T}(x) = c_T(x)$. Let $W_1$ be the space generated by $T$ on $\set{w_1}$. We claim $W_1 = V$.\newline

  Set $v_n = w_1$ and
  \begin{align*}
    v_i &= \left(T-\lambda\id_V\right)^{n-i}\left(v_n\right).
  \end{align*}
  Note that
  \begin{align*}
    v_i &= \left(T-\lambda\id_V\right)^{n-i}\left(v_n\right)\\
        &= \left(T-\lambda\id_V\right)\left(T-\lambda\id_V\right)^{n-i-1}\left(v_n\right)\\
        &= \left(T-\lambda\id_V\right)\left(v_{i+1}\right),
  \end{align*}
  meaning $T\left(v_{i+1}\right) = v_i + \lambda v_{i+1}$.\newline

  We claim that $\set{v_1,\dots,v_n}$ is a basis of $V$.\newline

  Suppose
  \begin{align*}
    c_1 v_1 + \cdots + c_nv_n = 0_V
  \end{align*}
  for some $c_i\in \F$> This gives
  \begin{align*}
    c_1\left(T-\lambda\id_V\right)^{n-1} + \cdots + c_{n}v_n = 0_V.
  \end{align*}
  Set $p(x) = c_1(x-\lambda)^{n-1} + \cdots + c_{n-1}\left(x-\lambda\right) + c_n$.\newline

  Then,
  \begin{align*}
    p(T)\left(v_n\right) &= 0_V\\
                         &= p(T)\left(w_1\right),
  \end{align*}
  meaning
  \begin{align*}
    m_{T,w_1}\left(x\right)|p(x),
  \end{align*}
  but $\deg\left(m_{T,w_1}(x)\right) = n$, meaning $p(x) = 0$, so $c_i = 0$ for all $i$.\newline

  Thus, $\set{v_1,\dots,v_n}$ is a Jordan basis.
\end{proof}
\begin{proposition}
  Let $\dim_{\F}\left(V\right) = n$ and $m_T(x) = \left(x-\lambda\right)^k$ for some $1\leq k \leq n$. Then, a Jordan basis for $V$ exists.
\end{proposition}
\begin{proof}
  We have $V = E_{\lambda}^{\infty} = E_{\lambda}^{k}$. We know the result if $k = n$. Assume $k < n$.\newline

  We claim that given any subspace $W_1$ of $V$ with $W_1\cap \ker\left(\left(T-\lambda\id_{V}\right)^{k-1}\right) = \set{0_V}$, there is a $T$-stable subspace $U$ of $V$ with 
  \begin{align*}
    V =\underbrace{\left( W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}(W_1)\right)}_{\text{$k\times k$ Jordan block}}\oplus U.
  \end{align*}
  We know there exists $v_k\in V$ with $\left(T-\lambda\id_V\right)^{k-1}\left(v_k\right) \neq 0_V$. Set $W_1 = \Span_{\F}\left(v_k\right)$. We have
  \begin{align*}
    W_1\cap \ker\left(\left(T-\lambda\id_V\right)^{k01}\right) &= \set{0_V}.
  \end{align*}
  Write
  \begin{align*}
    V &= W_1\oplus \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)\oplus W_2.
  \end{align*}
  Note that $W_2$ consists of other $k\times k$ Jordan block generators, though it can also be $0_V$.\newline

  Set $W = W_1\oplus W_2$. We have 
  \begin{align*}
    \left(T-\lambda\id_V\right)(W)\subseteq \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right).
  \end{align*}
  We also have
  \begin{align*}
    \left(T-\lambda\id_V\right)\left(W\right)\cap \ker\left(\left(T-\lambda\id_V\right)^{k-2}\right) &= \set{0_V}.
  \end{align*}
  If $w\in \left(T-\lambda\id_V\right)\left(W\right)\cap \ker\left(\left(T-\lambda\id_V\right)^{k-2}\right)$, then
  \begin{align*}
    w = \left(T-\lambda\id_V\right)\left(w_1 + w_2\right)
  \end{align*}
  for $w_i\in W_i$, and
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-2}\left(w\right) &= 0_V,
  \end{align*}
  meaning
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-2}\left(T-\lambda\id_V\right)\left(w_1 + w_2\right) &= 0_V\\
    \left(T-\lambda\id_V\right)^{k-1}\left(w_1\right) + \left(T-\lambda\id_V\right)^{k-1}w_2 &= 0_V,
  \end{align*}
  implying $w_1 = w_2 = 0_V$, since
  \begin{align*}
    V &= W_1 \oplus W_2 \oplus \underbrace{\ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)}_{\tilde{V}}.
  \end{align*}
  Note that $\dim_{\F}\left(\tilde{V}\right) < n$. We also know that $\tilde{V}$ is $T$-stable.\newline

  Let $\tilde{W} = \left(T-\lambda\id_V\right)(W)$. We have
  \begin{align*}
    \tilde{W} \cap \ker\left(\left(T-\lambda\id_V\right)^{k-2}\right) = \set{0_V}.
  \end{align*}
  We apply the induction hypothesis to $\tilde{V}$ and $\tilde{W}$ to get a $T$-stable subspace $\tilde{U}$ such that
  \begin{align*}
    \tilde{V} = \left(\tilde{W} + \left(T - \lambda\id_V\right)\left(\tilde{W}\right) + \cdots + \left(T-\lambda\id_V\right)^{k-2}\left(\tilde{W}\right)\right) \oplus \tilde{U}.
  \end{align*}
  Define
  \begin{align*}
    U &= \left(W_2 + \left(T-\lambda\id_V\right)\left(W_2\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_2\right)\right) + \tilde{U}.
  \end{align*}
  We have $U$ is $T$-stable. We need to show that
  \begin{align*}
    V &= \left(W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right)\right)\oplus U.
  \end{align*}
  We have
  \begin{align*}
    V &= W_1 + W_2 + \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)\\
      &= W_1 + W_2 + \tilde{V}\\
      &= W_1 + W_2 + \left(\tilde{W} + \left(T - \lambda\id_V\right)\left(\tilde{W}\right) + \cdots + \left(T-\lambda\id_V\right)^{k-2}\left(\tilde{W}\right)\right) + \tilde{U}\\
      &= W_1 + W_2 + \left(\left(W_1 + W_2\right) + \left(T - \lambda\id_V\right)\left(W_1 + W_2\right) + \cdots + \left(T-\lambda\id_V\right)^{k-2}\left(W_1 + W_2\right)\right) + \tilde{U}\\
      &= W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right) + U.
  \end{align*}
  Let $v\in \left(W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right)\right)\cap U$. Then,
  \begin{align*}
    v &= \sum_{j=0}^{k-1}\left(T-\lambda\id_V\right)^j\left(w_j\right)
  \end{align*}
  for $w_0,\dots,w_{k-1}\in W_1$. Additionally,
  \begin{align*}
    v &= \sum_{j=0}^{k-1}\left(T-\lambda\id_V\right)^{j}\left(w_j'\right) + \tilde{u}
  \end{align*}
  for $w_0',\dots,w_{k-1}'\in W_2$ and $\tilde{u}\in \tilde{U}$.\newline

  Applying $\left(T-\lambda\id_V\right)^{k-1}$ to both expressions for $v$, yielding
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-1}\left(w_0\right) &= \left(T-\lambda\id_V\right)\left(w_0'\right)
  \end{align*}
  since $\tilde{u}\in \ker\left(T-\lambda\id_V\right)^{k-1}$. Thus,
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-1}\left(w_0-w_0'\right) &= 0_V,
  \end{align*}
  meaning $w_0 - w_0'\in \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)$, and $w_0 - w_0'\in W$, so $w_0 = w_0'\in W_1\cap W_2 = \set{0_V}$.\newline

  To extract the basis, let $W_1 = \Span\left(v_k\right)$, $v_j = \left(T-\lambda\id_V\right)^{k-j}\left(v_k\right)$, and
  \begin{align*}
    \mathcal{B}_{\mathcal{W}} = \set{v_1,\dots,v_k}
  \end{align*}
  is a Jordan basis for $\mathcal{W} :=W_1 + \left(T-\lambda\id_V\right)W_1 + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right)$. Thus, we have
  \begin{align*}
    V &= \mathcal{W}\oplus U,
  \end{align*}
  with $U$ having Jordan basis $\mathcal{B}_U$ by induction. Thus,
  \begin{align*}
    \mathcal{B} &= \mathcal{B}_{\mathcal{W}}\cup \mathcal{B}_U
  \end{align*}
  is a Jordan basis for $V$.
\end{proof}
Thus, we have that for
\begin{align*}
  m_T(x) &= \left(x-\lambda\right)^k,
\end{align*}
then $V$ has has a Jordan basis with respect to $T$.
\begin{theorem}\hfill
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,V\right)$. Suppose $m_T(x) = \left(x-\lambda_1\right)^{m_1}\cdots \left(x-\lambda_k\right)^{m_k}$ over $\F$. Then, $V$ has a basis $\mathcal{B}$ such that $J = \left[T\right]_{\mathcal{B}}$ is in Jordan canonical form. Moreover, $J$ is unique up to order of the Jordan blocks.
    \item Let $A\in \Mat_{n}\left(\F\right)$. Suppose $m_A(x) = \left(x-\lambda_1\right)^{m_1}\cdots \left(x-\lambda_k\right)^{m_k}$ over $\F$. Then, $A$ is similar to a matrix $J$ in Jordan canonical form. The matrix $J$ is unique up to the order of the Jordan blocks.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We can write
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}\oplus\cdots\oplus E_{\lambda_k}^{m_k}.
  \end{align*}
  We know that $\displaystyle m_{T|_{E_{\lambda_j}^{m_j}}} = \left(x-\lambda_j\right)^{m_j}$, meaning we have a Jordan basis for $\displaystyle T_|{E_{\lambda_j}^{m_j}}$, $\mathcal{B}_j$.\newline

  Set
  \begin{align*}
    \mathcal{B} &= \bigcup_{j=1}^{k}\mathcal{B}_j.
  \end{align*}
  To show uniqueness, we know that the generators of the $j\times j$ Jordan blocks are
  \begin{align*}
    \ker\left(\left(T-\lambda_i\id_V\right)^{j-1}\right)\setminus \ker\left(\left(T-\lambda_i\id_V\right)^{j-2}\right).
  \end{align*}
\end{proof}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}
6 & 1 &  &  &  &  &  &  \\
 & 6 & 1 &  &  &  &  &  \\
 &  & 6 &  &  &  &  &  \\
 &  &  & 6 &  &  &  &  \\
 &  &  &  & 6 &  &  &  \\
 &  &  &  &  & 7 & 1 &  \\
 &  &  &  &  &  & 7 &  \\
 &  &  &  &  &  &  & 7 
\end{pmatrix} 
  \end{align*}
  We have $c_A(x) = \left(x-6\right)^5\left(x-7\right)^3$.\newline

  We have 
  \begin{align*}
    E_6^1 &= \Span_{\F}\left(e_1,e_4,e_5\right)\\
    E_6^2 &= \Span_{\F}\left(e_1,e_2,e_3,e_5\right)\\
    E_6^{3} &= \Span_{\F}\left(e_1,e_2,e_3,e_4,e_5\right).
  \end{align*}
  \begin{center}
    \begin{tikzpicture}
      \filldraw (0,0) circle (2pt)
        (0,2) circle (2pt)
        (0,4) circle (2pt)
        (2,0) circle (2pt)
        (4,0) circle (2pt);
      \draw (0,4) -- (0,0) -- (4,0);
      \node[anchor = north] at (0,0) {$e_1$};
      \node[anchor = north] at (2,0) {$e_4$};
      \node[anchor = north] at (4,0) {$e_5$};
      \node[anchor = west] at (0,2) {$e_2$};
      \node[anchor = west] at (0,4) {$e_3$};
      \node[anchor = east] at (-0.25,0) {$E_{6}^{1}$};
      \node[anchor = east] at (-0.25,2) {$E_{6}^{2}\setminus E_{6}^{1}$};
      \node[anchor = east] at (-0.25,4) {$E_{6}^{3}\setminus E_{6}^{2}$};
    \end{tikzpicture}
  \end{center}
  Reading this diagram, the first vertical line denotes the Jordan block of size $3$ over $e_1,e_2,e_3$, then the other two points at $e_4$ and $e_5$ denote the Jordan blocks of size $1$.
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &=  \begin{pmatrix}
3 & 3 & 0 & 0 & 0 & -1 & 0 & 2 \\
-3 & 4 & 1 & -1 & -1 & 0 & 1 & -1 \\
0 & 6 & 3 & 0 & 0 & -2 & 0 & 4 \\
-2 & 4 & 0 & 1 & -1 & 0 & 2 & -5 \\
-3 & 2 & 1 & -1 & 2 & 0 & 1 & -2 \\
-1 & 1 & 0 & -1 & -1 & 3 & 1 & -1 \\
-5 & 10 & 1 & -3 & -2 & -1 & 6 & -10 \\
-3 & 2 & 1 & -1 & -1 & 0 & 1 & 1 
\end{pmatrix}  
  \end{align*}
  We have $c_A(x) = \left(x-2\right)\left(x-3\right)^5\left(x^2-6x+21\right)$. Notice that this does not split over $\Q$, so ith as no Jordan canonical form over $\Q$. However, if we take $c_A$ into the extension field $\Q\left(\sqrt{-3}\right)$, we have
  \begin{align*}
    c_A(x) &= \left(x-2\right)\left(x-3\right)^5\left(x-\left(3 + 2\sqrt{-3}\right)\right)\left(x-\left(3-2\sqrt{-3}\right)\right).
  \end{align*}
  From this characteristic polynomial, we then obtain
  \begin{align*}
    E_2^{\infty} &= E_{2}^{1}\\
    E_{3\pm2\sqrt{-3}}^{\infty} &= E_{3\pm2\sqrt{-3}}^{2}.
  \end{align*}
  Calculating $E_{3}^{\infty}$, we get
  \begin{align*}
    \Dim_{\C}\left(E_3^{1}\right) &= \Dim_{\C}\left(\ker\left(A - 3I_{8}\right)\right)\\
    &= 2,
  \end{align*}
  we have
  \begin{align*}
    \Dim_{\C}\left(E_{3}^{2}\right) &= 4,
  \end{align*}
  meaning our diagram is
  \begin{center}
    \begin{tikzpicture}
      \filldraw (0,0) circle (2pt)
        (0,2) circle (2pt)
        (0,4) circle (2pt)
        (2,0) circle (2pt)
        (2,2) circle (2pt);
      \draw (0,4)-- (0,0) -- (2,0) -- (2,2);
      \node[anchor = north] at (0,0) {$v_1$};
      \node[anchor = north] at (2,0) {$v_1$};
      \node[anchor = west] at (0,2) {$v_3$};
      \node[anchor = west] at (2,2) {$v_4$};
      \node[anchor = west] at (0,4) {$v_5$};
    \end{tikzpicture}
  \end{center}
  This means there is one Jordan block of size $3$ and one Jordan block of size $2$ for the generalized eigenvalue $3$.
\end{example}
\subsection{Diagonalization}%
\begin{theorem}
  If $c_T(x)$ does not split into a product of linear factors over $\F$, $T$ is not diagonalizable.\newline

  If $c_T(x)$ does split into linear factors, the following are equivalent.
  \begin{enumerate}[(1)]
    \item $T$ is diagonalizable;
    \item for every eigenvalue $\lambda$, $E_{\lambda}^{\infty} = E_{\lambda}^{1}$;
    \item $m_{T}(x)$ splits into a product of (distinct) linear factors;
    \item for every eigenvalue $\lambda$, if $c_T(x) = \left(x-\lambda\right)^{e_{\lambda}}p(x)$ with $p\left(\lambda\right)\neq 0$, then $e_{\lambda} = \Dim_{\F}\left(E_{\lambda}^{1}\right)$;
    \item if we set set $d_{\lambda} = \Dim_{\F}\left(E_{\lambda}^{1}\right)$, then $\sum_{\lambda} d_{\lambda} = \Dim_{\F}\left(V\right)$;
    \item if $\lambda_1,\dots,\lambda_m$ are the distinct eigenvalues of $T$, then
      \begin{align*}
        V &= E_{\lambda_1}^{1}\oplus\cdots\oplus E_{\lambda_m}^{1}.
      \end{align*}
  \end{enumerate}
\end{theorem}
\section{Tensor Products and Determinants}%
\subsection{Extension of Scalars}%
If $V$ is a $\C$-vector space, then $V$ is also an $\R$-vector space, as we can restrict the scalars of $\C$.\newline

However, we may be interested in the opposite direction. If $V$ is an $\R$-vector space, can we ``extend'' $V$ to be a $\C$-vector space?
\begin{example}[Our First Complexification]
Let's start with $V = \R$. We cannot make $\R$ into a $\C$-vector space. However, we do have $V\hookrightarrow \C$ by $x\mapsto x + 0i$, and $\C$ is a $\C$-vector space.\newline

Turning our attention to $\C$, we have $z\in \C$ can be written as $z = x + yi$. We can see that $\C$ is isomorphic to $\R^2 = \R\oplus \R$ as $\R$-vector spaces by
\begin{align*}
  x + yi \mapsto \begin{pmatrix}x\\y\end{pmatrix}.
\end{align*}
If we take $v = x + yi\in \C$ to be a vector, and $a + bi\in \C$ to be a scalar, we have
\begin{align*}
  \left(a + ib\right)\left(x + iy\right) &= \left(ax-by\right) + \left(ay+bx\right)i,
\end{align*}
meaning in $\R^2$, we define
\begin{align*}
  \left(a + bi\right) \begin{pmatrix}x\\y\end{pmatrix} &= \begin{pmatrix}ax-by\\ay+bx\end{pmatrix}.
\end{align*}
With the scalar multiplication as defined above, we have $\R\oplus\R$ is a $\C$-vector space, and $\R\oplus \R\cong \C$ as a $\C$-vector space. We denote this version of $\R\oplus\R$ as $\R_{\C}$. This is known as the complexification of $\R$.
\end{example}
\begin{example}[Complexification of a Real Vector Space]
Given a real vector space $V$, we define $V_{\C} = V\oplus V$, defining the complex scalar product by taking
\begin{align*}
  \left(a + bi\right) \begin{pmatrix}v_1\\v_2\end{pmatrix} &= \begin{pmatrix}av_1 - bv_2 \\ av_2 + bv_1\end{pmatrix}.
\end{align*}
In particular, via the complexification, this yields $V_{\C}$ as a $\C$-vector space. Notice that
\begin{align*}
  i \begin{pmatrix}v_1\\v_2\end{pmatrix} &= \begin{pmatrix}-v_2\\v_1\end{pmatrix},
\end{align*}
meaning that
\begin{align*}
  \begin{pmatrix}v_1\\v_2\end{pmatrix} &= \begin{pmatrix}v_1\\0_V\end{pmatrix} + \begin{pmatrix}0_V\\v_2\end{pmatrix}\\
                                       &=  \begin{pmatrix}v_1\\0_V\end{pmatrix} + i\begin{pmatrix}v_2\\0_V\end{pmatrix},
\end{align*}
which looks like $v_1 + iv_2$.\newline

Let $z_1 = x_1 + y_1i$, $z_2 = x_2 + y_2i$, $v = \begin{pmatrix}v_1\\v_2\end{pmatrix}\in V_{\C}$. We want to show that
\begin{align*}
  \left(z_1z_2\right)v &= z_1\left(z_2v\right).
\end{align*}
This yields
\begin{align*}
  \left(z_1z_2\right)v &= \left(\left(x_1 + y_1i\right)\left(x_2 + y_2i\right)\right)v\\
                       &= \left(\left(x_1x_2 - y_1y_2\right) + \left(x_1y_2 + x_2y+1\right)i\right) \begin{pmatrix}v_1\\v_2\end{pmatrix}\\
                       &= \begin{pmatrix}\left(x_1x_2-y_1y_2\right)v_1 - \left(x_1y_2 + x_2y_1\right)v_2 \\ \left(x_1x_2-y_1y_2\right)v_2 + \left(x_1y_2 + x_2y_1\right)v_1\end{pmatrix}\\
  z_1\left(z_2v\right) &= z_1\left(x_1 + y_1i\right) \begin{pmatrix}v_2\\v_2\end{pmatrix}\\
                       &= z_1 \begin{pmatrix}x_2v_1 - y_2v_2\\x_2v_1 + y_2v_1\end{pmatrix}\\
                       &= \left(x_1 + y_1i\right) \begin{pmatrix}x_2v_1 - y_2v_2\\x_2v_1 + y_2v_1\end{pmatrix}\\
                       &= \begin{pmatrix}x_1\left(x_2v_1 - y_2v_2\right) - y_1\left(x_2v_2 + y_2v_1\right) \\ x_1\left(x_2v_2 + y_2v_1\right) + y_1\left(x_2v_1 - y_2v_2\right)\end{pmatrix}.
\end{align*}
Upon simplification, we see that these two expressions are equal.
\begin{exercise}
  Verify that $V_{\C}$ is a $\C$-vector space.\footnote{Don't actually do this exercise.}
\end{exercise}
We have an embedding $V\hookrightarrow V_{\C}$ by taking $v\mapsto \begin{pmatrix}v\\0_V\end{pmatrix}$. The set
\begin{align*}
  \set{ \begin{pmatrix}v\\0_V\end{pmatrix}\mid v\in V }
\end{align*}
is a real subspace of $V_{\C}$.
\end{example}
This method works great for the particular case of $\R$ and $\C$, but we need a different method for more arbitrary vector spaces. Eventually, we will show that for any linear map $T$, there is a unique map $T_{\C}$.
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEiZYePrNWiDgH1gAHQMBhLnL5LBRUeuqapO6fqOnu4mFADm8IqABmAE4QALZIZCA4EEgATDz+QaGI4ZFIohJabAAq5iCBIanUKYgAzHG5CTGFUSV2ktogmc4mZlwUXEA
\begin{tikzcd}
V \arrow[d] \arrow[r, "T"] & W \arrow[d] \\
V_{\C} \arrow[r, "T_{\C}"] & W_{\C}     
\end{tikzcd}
\end{center}
\begin{proposition}
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be an $\R$-basis of $V$. The set $\mathcal{B}_{\C} = \set{\left(v_i,0_V\right)}_{i\in I}$ is a $\C$-basis of $V_{\C}$.
\end{proposition}
\begin{proof}
  Let $\left(w_1,w_2\right)\in V_{\C}$. We can write
  \begin{align*}
    w_1 = \sum_{j\in I}a_jv_j\\
    w_1 = \sum_{j\in I}b_jv_j
  \end{align*}
  for some $a_i,b_i\in \R$. We have
  \begin{align*}
    \left(w_1,w_2\right) &= \left(\sum_{j\in I}a_jv_j,\sum_{j\in I} b_jv_j\right)\\
                         &= \left(\sum_{j\in I}a_jv_j,0_V\right) + \left(0_V,\sum_{j\in I}b_jv_j\right)\\
                         &= \sum_{j\in I}a_j\left(v_j,0_V\right) + \sum_{j\in I}b_j\left(0_V,v_j\right)\\
                         &= \sum_{j\in I}a_j\left(v_j,0_V\right) + \sum_{j}ib_j\left(v_j,0_V\right)\\
                         &\in \Span_{\C}\set{\left(v_i,0_V\right)}_{i\in I}.
  \end{align*}
  Suppose we have
  \begin{align*}
    \left(0_V,0_V\right) &= \sum_{j\in I}\left(a_j + ib_j\right)\left(v_j,0_V\right).
    \intertext{Then,}
                         &= \sum_{j\in I}a_j\left(v_j,0_V\right) + \sum_{j\in I}ib_j\left(v_j,0_V\right)\\
                         &= \left(\sum_{j\in I}a_jv_j,0_v\right) + i\left(\sum_{j\in I}b_jv_j,0_V\right)\\
                         &= \left(\sum_{j\in I}a_jv_j,\sum_{j\in I}b_j0_V\right),
  \end{align*}
  meaning
  \begin{align*}
    \sum_{j\in I}a_jv_j &= 0_V\\
    \sum_{j\in I}b_jv_j &= 0_V,
  \end{align*}
  so $a_j=0$ for all $j$ and $b_j = 0$ for all $j$. Thus $\set{\left(v_j,0_V\right)}_{j\in I}$ are linearly independent.
\end{proof}
\begin{proposition}
  Let $V,W$ be $\R$-vector spaces, and let $T\in \Hom_{\R}\left(V,W\right)$. There is a unique $T_{\C}\in \Hom_{\C}\left(V_{\C},W_{\C}\right)$ that makes the following diagram commute.
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEiZYePrNWiDgH1gAHQMBhLnL5LBRUeuqapO6fqOnu4mFADm8IqABmAE4QALZIZCA4EEgATDz+QaGI4ZFIohJabAAq5iCBIanUKYgAzHG5CTGFUSV2ktogmc4mZlwUXEA
\begin{tikzcd}
V \arrow[d,"\iota_V",hookrightarrow] \arrow[r, "T"] & W \arrow[d,"\iota_W",hookrightarrow] \\
V_{\C} \arrow[r, "T_{\C}"] & W_{\C}     
\end{tikzcd}
\end{center}
\end{proposition}
\begin{proof}
  We define
  \begin{align*}
    T_{\C}\left(v_1,v_2\right) &= \left(T\left(v_1\right),T\left(v_2\right)\right).
  \end{align*}
  Let $v\in V$. We have $\iota_V(v) = \left(v,0_V\right)$, meaning
  \begin{align*}
    T_{\C}\left(\iota_V(v)\right) &= T_{\C}\left(\left(v,0_V\right)\right)\\
                                  &= \left(T(v),T\left(0_V\right)\right)\\
                                  &= \left(T(v),0_W\right),
  \end{align*}
  and
  \begin{align*}
    \iota_W\left(T(v)\right) &= \left(T(v),0_W\right).
  \end{align*}
  We claim that $T_{\C}$ is $\C$-linear. Let $x + iy\in \C$, $\left(v_1,v_2\right), \left(v_1',v_2'\right)\in V_{\C}$. Then,
  \begin{align*}
    T_{\C}\left(\left(v_1,v_2\right) + \left(x + iy\right)\left(v_1',v_2'\right)\right) &= T_{\C}\left(\left(v_1,v_2\right) + \left(xv_1' - yv_2',xv_2' + yv_1'\right)\right)\\
                                                                                        &= T_{\C}\left(\left(v_1 + xv_1' - yv_2',v_2 + xv_2' + yv_1'\right)\right)\\
                                                                                        &= \left(T\left(v_1 + xv_1' - yv_2'\right),T\left(v_2 + xv_2' + yv_1'\right)\right)\\
                                                                                        &= \left(T\left(v_1\right),T\left(v_2\right)\right) + x\left(T\left(v_1'\right),T\left(v_2'\right)\right) + y\left(-T\left(v_2'\right),T\left(v_1'\right)\right)\\
                                                                                        &= \left(T\left(v_1\right),T\left(v_2\right)\right) + \left(x + iy\right)\left(T\left(v_1'\right),T\left(v_2'\right)\right)\\
                                                                                        &= T_{\C}\left(v_1,v_2\right) + \left(x + iy\right)T_{\C}\left(v_1',v_2'\right).
  \end{align*}
  Suppose we have $S\in \Hom_{\C}\left(V_{\C},W_{\C}\right)$ such that the following diagram commutes.
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEiZYePrNWiDgH1gAHQMBhLnL5LBRUeuqapO6fqOnu4mFADm8IqABmAE4QALZIZCA4EEgATDz+QaGI4ZFIohJabAAq5iCBIanUKYgAzHG5CTGFUSV2ktogmc4mZlwUXEA
\begin{tikzcd}
V \arrow[d,"\iota_V",hookrightarrow] \arrow[r, "T"] & W \arrow[d,"\iota_W",hookrightarrow] \\
V_{\C} \arrow[r, "S"] & W_{\C}     
\end{tikzcd}
\end{center}
Let $v_1,v_2\in V_{\C}$. Then,
\begin{align*}
  S\left(\left(v_1,v_2\right)\right) &= S\left(\left(v_1,0_V\right) + \left(0_V,v_2\right)\right)\\
                                     &= S\left(\left(v_1,0_V\right) + i\left(v_2,0_V\right)\right)\\
                                     &= S\left(\left(v_1,0_V\right)\right) + iS\left(\left(v_2,0_V\right)\right)\\
                                     &= S\left(\iota_V\left(v_1\right)\right) + iS\left(\iota_V\left(v_2\right)\right)\\
                                     &= \iota_W\left(T\left(v_1\right)\right) + i\iota_W\left(T\left(v_2\right)\right)\\
                                     &= \left(T\left(v_1\right),0_W\right) + i\left(T\left(v_2\right),0_W\right)\\
                                     &= \left(T\left(v_1\right),0_W\right) + \left(0_W,T\left(v_2\right)\right)\\
                                     &= \left(T\left(v_1\right),T\left(v_2\right)\right).
\end{align*}
Thus, $S = T_{\C}$, so $T_{\C}$ is unique.
\end{proof}
We are aware that every vector space has a basis. However, we may ask if, given a set $\Gamma$, can we build a vector space that has $\Gamma$ as a basis element?\newline

The answer is yes.
\begin{theorem}[Existence of a Free Vector Space]
  Let $\F$ be a field, and $\Gamma$ a set. There is an $\F$-vector space, $\F\left(\Gamma\right)$, that has $X$ as a basis.\newline

  Moreover, $\F\left(\Gamma\right)$ has the following universal property: if $W$ is any $\F$-vector space, and $t: \Gamma\rightarrow W$ is a map of sets, there is a unique $T\in \Hom_{\F}\left(\F(\Gamma),W\right)$ such that $T(x) = t(x)$ for every $x\in \Gamma$ --- i.e., the following diagram commutes.
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12BxOgW17ogAvqXSZc+QigCM5KrUYs2nAGIAKTj350AlMNEgM2PASKzp8+s1aIQAdWHyYUAObwioAGYAnCLyRkIDgQSABM1FZKtjgg1Ax0AEYwDAAK4iZSIN5YLgAWMSJevv6IgcFIsgrWyuz4OIKFID5+FdTliOFVUSAAKo5CQA
\begin{tikzcd}
\Gamma \arrow[rd, "t"'] \arrow[r, "\iota",hookrightarrow] & \F(\Gamma) \arrow[d, "T"] \\
                                           & W                        
\end{tikzcd}
  \end{center}
\end{theorem}
\begin{proof}
  If $\Gamma$ is the empty set, we take $\F\left(\Gamma\right) = \set{0}$.\newline

  Let $\Gamma \neq \emptyset$. Define
  \begin{align*}
    \F\left(\Gamma\right) &= \set{f: \Gamma\rightarrow \F\mid f(x)\text{ finitely supported}}.
  \end{align*}
  Let $c\in \F$, $f,g\in \F(\Gamma)$. Then, $\left(f+g\right)(x) = f(x) + g(x)$. Since $f,g$ are finitely supported, so too is $f+g$, and similarly $\left(cf\right)(x) = cf(x)$ is finitely supported. We can verify that $\F\left(\Gamma\right)$ is a vector space with the zero element $f(x) = 0_{\F\left(\Gamma\right)}$.\newline

  Given any $y\in \Gamma$, define $f_y$ by $f_y(y) = 1$ and $f_y(x) = 0$ for $x\neq y$. Thus, $\Gamma\hookrightarrow\F\left(\Gamma\right)$ by $x\mapsto f_{x}$. We let $\mathcal{X} = \set{f_x\mid x\in \Gamma}$; we let $\iota = \Gamma \xrightarrow{\text{bijection}}\mathcal{X}$.\newline

  We claim that $\mathcal{X}$ is a basis for $\F\left(\Gamma\right)$. For any $f\in \F\left(\Gamma\right)$, we claim $f = \sum_{x\in \Gamma}f(x)f_x$. This gives $\Span_{\F}\left(\mathcal{X}\right) = \F\left(\Gamma\right)$.\newline

  Note that
  \begin{align*}
    f(y) &= f(y)f_y(y)\\
         &= f(y)f_y(y) + \sum_{x\neq y}f(x)f_x(y)\\
         &= \sum_{x\in\Gamma}f(x)f_x(y).
  \end{align*}
  Suppose
  \begin{align*}
    \sum_{i=1}^{n}a_if_{x_i} &= 0_{\F\left(\Gamma\right)}.
  \end{align*}
  In particular,
  \begin{align*}
    \sum_{i=1}^{n}a_if_{x_i}(y) = 0
  \end{align*}
  for all $y\in \Gamma$. Thus,
  \begin{align*}
    0 &= \sum_{i=1}^{n}a_if_{x_i}\left(x_j\right)\\
      &= a_j,
  \end{align*}
  meaning $\set{f_{x}}_{x\in \Gamma}$ is a basis for $\F\left(\Gamma\right)$.\newline

  Suppose we have $t: X\rightarrow W$. Define
  \begin{align*}
    T: \F\left(\Gamma\right) \rightarrow W
  \end{align*}
  by
  \begin{align*}
    T\left(\sum_{i=1}^{n}a_if_{x_i}\right) &= \sum_{i=1}^{n}a_it\left(f^{-1}\left(x_i\right)\right)\\
                                           &= \sum_{i=1}^{n}a_it\left(x_i\right).
  \end{align*}
  This is clearly linear, and makes the diagram commute, and is unique (since $T$ is determined uniquely by the basis elements).
\end{proof}
\begin{example}
  If $\Gamma = \R$, we can form $\F_{\R}\left(\R\right)$. Then, we can write any element of $\F_{\R}\left(\R\right)$ by $2\cdot \pi + 3\cdot 2$, where $\pi$ and $2$ are basis elements and $2,3$ are scalars.
\end{example}
\begin{exercise}
  Show that if $\Gamma = \set{x_1,\dots,x_n}$, then $\F\left(\Gamma\right) \cong \F^{n}$.
\end{exercise}
\begin{definition}[Constructing $K\otimes V$]
  Let $K/F$ be an extension of fields. Let $X = \set{\left(a,v\right)\mid a\in K,v\in V}\in K\times V$, where $V$ is an $F$-vector space.\newline

  Form $\F_{K}\left(K\times V\right)$. Elements of $\F_{K}\left(K\times V\right)$ look like finite sums
  \begin{align*}
    \sum_{i=1}^{n}c_i\left(a_i,v_i\right)
  \end{align*}
  with $c_i\in K$, $\left(a_i,v_i\right)\in K\times V$.
  Our goal is, given $V$ an $F$-vector space, construct a $K$-vector space that contains $V$ as an $F$-subspace. We want to shrink $\F_{K}\left(K\times V\right)$ so that we still have the $F$-structure. We construct $\operatorname{Rel}_{K}\left(K\times V\right)$ such that
  \begin{itemize}
    \item $\left(a_1 + a_2\right)\ast v \sim a_1\ast v + a_2\ast v$ for $a_1,a_2\in K$, $v\in V$;
    \item $a\ast \left(v_1 + v_2\right) \sim a\ast v_1 + a\ast v_2$ for $a\in K$, $v_1,v_2\in V$;
    \item $\left(ac\right)\ast v \sim a\ast \left(cv\right)$ for $a\in K$, $c\in F$, and $v\in V$.
  \end{itemize}
  Thus, we define $\operatorname{Rel}_{K}\left(K\times V\right)$ to be the $K$-span of the following elements of $K\times V$
  \begin{enumerate}[(1)]
    \item $\left(a_1 + a_2,v\right) - \left(a_1,v\right) - \left(a_2,v\right)$ for $a_1,a_2\in K$, $v\in V$;
    \item $\left(a,v_1 + v_2\right) - \left(a,v_1\right) - \left(a,v_2\right)$ for $a\in K$, $v_1,v_2\in V$;
    \item $a_1\left(a_2,v\right) - \left(a_1a_2,v\right)$ for $a_1,a_2\in K$, $v\in V$;
    \item $\left(ac,v\right)-\left(a,cv\right)$ for $c\in F$, $a\in K$, and $v\in V$.
  \end{enumerate}
  This allows $\operatorname{Rel}_{K}\left(K\times V\right)$ to contain all such elements that we expect to be equal under the tensor product. Since $\operatorname{Rel}_{K}\left(K\times V\right)$ is the $K$-span of a subset of $\F\left(K\times V\right)$, the set of relations is a $K$-subspace of $\F_{K}\left(K\times V\right)$. We define
  \begin{align*}
    K\otimes_{F}V &= \F_{K}\left(K\times V\right)/\operatorname{Rel}_{K}\left(K\times V\right).
  \end{align*}
  Given $\left(a,v\right)\in \F_{K}\left(K\times V\right)$, we write $a\otimes v = \left(a,v\right) + \operatorname{Rel}_{K}\left(K\times V\right)$.\newline

  We convert the relation $\operatorname{Rel}_{K}\left(K\times V\right)$ into the language of the tensor product.
  \begin{enumerate}[(1)]
    \item $\left(a_1 + a_2\right)\otimes v = a_1\otimes v + a_2\otimes v$ for $a_1,a_2\in K$, $v\in V$;
    \item $a\otimes \left(v_1 + v_2\right) = a\otimes v_1 + a\otimes v_2$ for $a\in K$, $v_1,v_2\in V$;
    \item $c\left(a\otimes v\right) = ca\otimes v$ for $c,a\in K$ and $v\in V$;
    \item $ca\otimes v = a\otimes cv$ for $a\in K$, $c\in F$, and $v\in V$.
  \end{enumerate}
  Elements of $K\otimes_{F}V$ are of the form
  \begin{align*}
    \sum_{i\in I}c_i\left(a_i\otimes v_i\right) &= \sum_{i\in I}b_i\otimes v_i,
  \end{align*}
  where $b_i\in K$ and $v_i\in V$. A pure tensor is of the form $a\otimes v$.\newline

  The element $0_{K\otimes_{F}V}$ is of the form $0\otimes 0_{V}$.\newline

  Finally, we need to verify that $V$ is a $F$-subspace of $K\otimes V$.
\end{definition}
\begin{proposition}
  Let $K/F$ be a field extension, and $V$ an $F$-vector space. The $K$-vector space $K\otimes_{F}V$ contains a subspace isomorphic to $V$ as an $F$-vector space.
\end{proposition}
\begin{proof}
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a basis of $V$. Define $T: V\rightarrow K\otimes_{F}V$ by taking $v_i \mapsto 1\otimes v_i$. It is the case that $T$ is a $F$-linear map.\newline

  Let $W = T\left(V\right)$. Then, $T: V\rightarrow W$ is a surjection.\newline

  Let $v\in \ker\left(T\right)$, meaning $1\otimes v = 1\otimes 0_V$. This means $\left(1,v\right) - \left(0,0_V\right) \in \operatorname{Rel}_{K}\left(K\times V\right)$. Thus, $\left(1,v\right) - \left(1,0_V\right)\in \operatorname{Rel}_{K}\left(K\times V\right)$. This is only true if $v = 0_V$. Thus, $T$ is also injective.
\end{proof}

  We refer to $K\otimes_{F}V$ as the extension of scalars of $V$ from $F$ to $K$.
  %\begin{example}
  %  Let $K = \C$, $F = \R$, and $V$ be an $\R$-vector space. Then, for $v\in V$
  %  \begin{align*}
  %    i\left(\left(2+i\right)\otimes v\right) + 6\otimes v &= \left(2i-1\right)\otimes v + 6\otimes v\\
  %                                                         &= \left(5 + 2i\right)\otimes v.
  %  \end{align*}
  %\end{example}
  \begin{proposition}
    Let $K/F$ be a field extension, and $V$ an $F$-vector space with basis $\mathcal{B} = \set{v_i}_{i\in I}$.\newline

    Then, $\Span_{K}\set{1\otimes v_i}_{i\in I} = K\otimes_{F}V$.
  \end{proposition}
  \begin{proof}
    Let $a\otimes v\in K\otimes_{F}V$. Write
    \begin{align*}
      v &= \sum_{i\in I}c_i v_i
    \end{align*}
    for $c_i\in F$. Note that we have
    \begin{align*}
      a\otimes v &= a\otimes \left(\sum_{i\in I}c_iv_i\right)\\
                 &= \sum_{i\in I}a\otimes \left(c_iv_i\right)\\
                 &= \sum_{i\in I}\left(ac_i\right)\otimes v_i\\
                 &= \sum_{i\in I}ac_i\left(1\otimes v_i\right).
    \end{align*}
    If we take
    \begin{align*}
      \sum_{j\in I}a_j\otimes v_j' \in K\otimes_{F}V,
    \end{align*}
    then each $a_j\otimes v_j'$ can be written as a finite linear combination
    \begin{align*}
      a_j\otimes v_j'&= \sum_{i\in I}b_{j_i}\left(1\otimes v_i\right),
    \end{align*}
    so
    \begin{align*}
      \sum_{j\in I}\left(\sum_{i\in I}b_{j_i}\left(1\otimes v_i\right)\right)\in \Span_{K}\set{1\otimes v_i}_{i\in I}.
    \end{align*}
  \end{proof}
  \begin{theorem}
    Let $\iota_{V}: V\rightarrow K\otimes_{F}V$ be defined by $\iota_{V}\left(v\right) = 1\otimes v$.\newline

    Let $W$ be any $K$-vector space, and let $S\in \Hom_{F}\left(V,W\right)$. Then, there is a unique $T\in \Hom_{K}\left(K\otimes_{F}V,W\right)$ such that $S = T\circ \iota_{V}$. The following diagram commutes.
    \begin{center}
      % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbANIAdWRDwBbeAH1gAMS6cefbHgJFRw8fWatEIAOrdxMKAHN4RUADMAThCVIyIHBCQAJmoGOgAjGAYABX4DIRB3LAcACxwQajMpSwBlbl4QDy8fan8kUQlzNnl8HDpVHXzC70Ry0sRgiqyQABVbLiA
\begin{tikzcd}
V \arrow[rd, "S"'] \arrow[r, "\iota_V"] & K\otimes_{F}V \arrow[d, "T"] \\
                                        & W                           
\end{tikzcd}
    \end{center}
    Conversely, if $T\im \Hom_{K}\left(K\otimes F V,W\right)$, then $T\circ\iota_V \in \Hom_{F}\left(V,W\right)$.
  \end{theorem}
  \begin{proof}
    Let $S\in \Hom_{F}\left(V,W\right)$. Recall that we constructed $K\otimes_{F}V$ as a quotient of $\F\left(K\times V\right)$.\newline

    Define $t: K\times V \rightarrow W$, $\left(a,v\right) \mapsto aS(v)$.\newline

    The universal property for $\F\left(K\times V\right)$ gives a unique linear map $T: \F\left(K\times V\right) \rightarrow W$, given by $T\left(a,v\right) = t\left(a,v\right)$.\newline

  Since $T$ is linear, we have
  \begin{align*}
    T\left(\sum_{i\in I}c_i\left(a_iv_i\right)\right) &= \sum_{i\in I}T_i\left(c_i\left(a_i,v_i\right)\right)\\
                                                      &= \sum_{i\in I}c_iT\left(\left(a_i,v_i\right)\right)\\
                                                      &= \sum_{i\in I}c_ia_iS\left(v_i\right).
  \end{align*}
  All we need to do now is show that $T$ vanishes on $\operatorname{Rel}_{K}\left(K\times V\right)$. For instance, we need to show that $T$ vanishes on
  \begin{align*}
    \left(a+b,v\right) - \left(a,v\right)-\left(b,v\right),
  \end{align*}
  or
  \begin{align*}
    T\left(\left(a + b,v\right)-\left(a,v\right) - \left(b,v\right)\right) &= T\left(\left(a+b,v\right)\right) - T\left(a,v\right) - T\left(b,v\right)\\
                                                                           &= \left(a+b\right)S(v) - aS(v) - bS(v)\\
                                                                           &= 0.
  \end{align*}
  Thus, the diagram commutes.\newline

  We have shown that $\set{1\otimes v}$ spans $K\otimes_{F}V$. To determine a $K$-linear map on $K\otimes_{F}V$, it is enough to see what $T$ does to $1\otimes v$.\newline

  Since $T\left(1\otimes v\right) = S(v)$, if $\tilde{T}$ also made the diagram commute, then $S(v) = \tilde(1\otimes v)$, meaning $T = \tilde{T}$. Thus, $T$ is unique.
  \end{proof}
  \begin{proposition}
    Let $K/F$ be an extension of fields. Then,
    \begin{align*}
      K\otimes F \cong K
    \end{align*}
  \end{proposition}
  \begin{proof}
    We have $i: F\hookrightarrow K$ by inclusion. Our diagram is
    \begin{center}
      % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADEQBfU9TXfIRQBGclVqMWbANIAdWRDwBbeAH1g7Lpx59seAkVHDx9Zq0Qhp3cTCgBzeEVAAzAE4QlSMiBwQkAJmoGOgAjGAYABX59IRBXLDsACxwQalMpCyxuXhA3Dy9qXyRRCTM2eXwcOnVNbJd3T0QSosRA0oyQABVrLiA
  \begin{tikzcd}
  F \arrow[rd, "i"',hookrightarrow] \arrow[r, "\iota_{F}"] & K\otimes_{F}F \arrow[d, "T"] \\
                                            & K                           
  \end{tikzcd}
    \end{center}
  The universal property gives $T\in \Hom_{K}\left(K\otimes_{F}F,K\right)$ such that for $x\in F$, $T\left(1\otimes x\right) = i(x) = x$.\newline

  For any
  \begin{align*}
    \sum a_i\otimes x_i\in K\otimes_{F}F,
  \end{align*}
  since $T$ is $K$-linear, we have
  \begin{align*}
    T\left(\sum a_i\otimes x_i\right) &= \sum T\left(a_i\otimes x_i\right)\\
                                      &= \sum T\left(a_i\left(1\otimes x_i\right)\right)\\
                                      &= \sum a_iT\left(1\otimes x_i\right)\\
                                      &= \sum a_ix_i.
  \end{align*}
  Define $S: K\rightarrow K\otimes F$, $y\mapsto y\otimes 1$. For $a\in K$, $y_1,y_2\in K$, we have
  \begin{align*}
    S\left(y_1 + ay_2\right) &= \left(y_1 + ay_2\right)\otimes 1\\
                             &= y_1\otimes 1 + a\left(y_2\otimes 1\right)\\
                             &= S\left(y_1\right) + aS\left(y_2\right),
  \end{align*}
  meaning $S$ is a $K$-linear map. Thus, $S\in \Hom_{K}\left(K\otimes_{F}F\right)$. We have
  \begin{align*}
    T\circ S(y) &= T\left(y\otimes 1\right)\\
                &= yT\left(1\otimes 1\right)\\
                &= y,
  \end{align*}
  and
  \begin{align*}
    S\circ T\left(a\otimes x\right) &= S\left(T\left(a\otimes x\right)\right)\\
                                    &= S\left(aT\left(1\otimes x\right)\right)\\
                                    &= S\left(ax\right)\\
                                    &= ax\otimes 1\\
                                    &= a\otimes x.
  \end{align*}
  Thus, $T$ is an isomorphism.
  \end{proof}
  \begin{remark}
    This shows that $\C\otimes_{\R}\R \cong \C$.
  \end{remark}
  \begin{proposition}
    Let $K/F$ be an extension of fields, and let $V$ be an $F$-vector space with $\Dim_{F}\left(V\right) = n$. Then,
    \begin{align*}
      K\otimes_{F}V \cong K^{n}
    \end{align*}
    as $K$-vector spaces.
  \end{proposition}
  \begin{proof}
    We want a map $K\otimes_{F}V \rightarrow K^{n}$.\newline

    We will define a map $T: V\rightarrow K^{n}$ as follows. Set $\mathcal{B} = \set{v_1,\dots,v_n}$ to be a basis for $V$, and $\mathcal{E} = \set{e_1,\dots,e_n}$ as the standard basis for $K^n$. Define $t: \mathcal{B}\rightarrow \mathcal{E}$ by taking $v_i\mapsto e_i$. Since $t$ is defined on the bases, $t$ extends to a linear map $T: V\rightarrow K^n$. Thus, $T\in \Hom_{F}\left(V,K^{n}\right)$. The universal property for tensor products gives a $K$-linear map $\overline{T}\in \Hom_{K}\left(K\otimes_{F}V,K^{n}\right)$, where
    \begin{align*}
      \overline{T}\left(1\otimes v_i\right) = e_i.
    \end{align*}
    We can define $S\in \Hom_{K}\left(K^{n},K\otimes_{F}V\right)$ by taking
    \begin{align*}
      S\left(e_i\right) = 1\otimes v_i.
    \end{align*}
    Since $S$ and $\overline{T}$ are inverses of each other, $\overline{T}$ is an isomorphism, so $K\otimes_{F}V\cong K^n$.\newline

    Moreover, since $S$ is an isomorphism, and $\set{e_i}_{i=1}^{n}$ is a basis for $K^n$, the collection $\set{1\otimes v_i}_{i=1}^{n}$ forms a basis for $K\otimes_{F}V$.
  \end{proof}
  \begin{proposition}
    Let $K/F$ be an extension of fields. Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a $F$-basis for $V$. Then, $\mathcal{B}_{K} = \set{1\otimes v_{i}}_{i\in I}$ is a basis for $K\otimes_{F}V$.
  \end{proposition}
  \begin{proof}
    We know that $\set{1\otimes v_i}_{i\in I}$ is spanning for $K\otimes_{F}V$. Suppose
    \begin{align*}
      \sum_{i\in I}c_i\left(1\otimes v_i\right) = 0_{K\otimes_{F}V}.
    \end{align*}
    for some $c_i\in K$.\newline

    Fix $i_0\in I$. Define
    \begin{align*}
      t_{i_0}: V \rightarrow K
    \end{align*}
    by $v\mapsto a_{i_0}$. This is a $F$-linear map.
    \begin{align*}
      t_{i_0}\left(v + c\right) &= t_{i_0}\left(\sum_{i\in I}\left(a_ica_i'\right) v_i\right)\\
                                  &= a_{i_0} + ca_{i_0}'\\
                                  &= t_{i_0}\left(v\right) + ct_{i_0}\left(\right).
    \end{align*}
    Thus, $t\in \Hom_{F}\left(V,K\right)$. By the universal property of tensor products, we get a map $T_{i_0}\in \Hom_{K}\left(K\otimes_{F}V,K\right)$ such that
    \begin{align*}
      T_{i_0}\left(1\otimes v\right) &= t_{i_0}\left(v\right)\\
                                     &= a_{i_0}.
    \end{align*}
    Thus, we have
    \begin{align*}
      0_{K} &= T_{i_0}\left(0_{K\otimes_{F}V}\right)\\
            &= T_{i_0}\left(\sum_{i\in I}c_i\left(1\otimes v_i\right)\right)\\
            &= \sum_{i\in I}c_iT_{i_0}\left(1\otimes v_i\right)\\
            &= \sum_{i\in I}c_it_{i_0}\left(v_i\right)\\
            &= c_{i_0}.
    \end{align*}
    Thus, $\set{1\otimes v_{i}}_{i\in I}$ is linearly independent.
  \end{proof}
  \begin{theorem}
    Let $K/F$ be an extension of fields, and let $V,W$ be $F$-vector spaces. Let $T\in \Hom_{F}\left(V,W\right)$. There is a map $T_{K}\in \Hom_{K}\left(K\otimes_{F}V,K\otimes_{F}W\right)$ such that the following diagram commutes.
    \begin{center}
      % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEiZYePrNWiEAGkAOvoh4AtvAD6wAGJdOPPksFFR66pqk6DR0xetdZXOIwUADm8ESgAGYAThAmSGQgOBBIohJabAAqclGx8YgATNTJSADMbpLaIJmWulw5IDFxCcUphRUZOob4OHSW7PXUDHQARjAMAAr8ykIg0VghABY4DU35aSWI5ekeIN3GfcDS9YFcQA
\begin{tikzcd}
V \arrow[r, "T"] \arrow[d, "\iota_{V}"',hookrightarrow] & W \arrow[d, "\iota_{W}",hookrightarrow] \\
K\otimes_{F}V \arrow[r, "T_{K}"]         & K\otimes_{F}W           
\end{tikzcd}
    \end{center}
  \end{theorem}
  \begin{proof}
    Define a map $t: V\hookrightarrow K\otimes_{F}W$ by $v\mapsto 1\otimes T(v)$. We have
    \begin{align*}
      t\left(v_1 + cv_2\right) &= 1\otimes T\left(v_1 + cv_2\right)\\
                               &= 1\otimes \left(T\left(v_1\right) + cT\left(v_2\right)\right)\\
                               &= 1\otimes T\left(v_1\right) + c\left(1\otimes T\left(v_2\right)\right)\\
                               &= t\left(v_1\right) + ct\left(v_2\right).
    \end{align*}
    This gives $t\in \Hom_{F}\left(V,K\otimes_{F}W\right)$. The universal property for tensor products gives $T_K\in \Hom_{K}\left(K\otimes_{F}V,K\otimes_{F}W\right)$ satisfying
    \begin{align*}
      T_{K}\left(1\otimes v\right) &= t\left(v\right)\\
                                   &= 1\otimes T\left(v\right).
    \end{align*}
    Let $v\in V$. Then,
    \begin{align*}
      T_{K}\left(\iota_{V}\left(v\right)\right) &= T_{K}\left(1\otimes v\right)\\
                                                &= 1\otimes T\left(v\right)\\
                                                &= \iota_{W}\left(T(v)\right).
    \end{align*}
    So, the diagram commutes.
  \end{proof}
  \begin{remark}
    This shows that $\C\otimes_{\R}V \cong V_{\C}$.
  \end{remark}
  \subsection{Tensor Products of Vector Spaces and the Trace}%
  \begin{example}
    In multivariable calculus,
    \begin{align*}
      \R^n\times\R^n \xrightarrow{\cdot}\R\\
      \left( \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}, \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix}\right) \mapsto \sum_{i=1}^{n}a_ib_i.
    \end{align*}
    Some of the properties we like about the dot product are as follows.
    \begin{itemize}
      \item $\left(v_1 + v_2\right)\cdot w = v_1\cdot w + v_2\cdot w$;
      \item $v\cdot \left(w_1 + w_2\right) = v\cdot w_1 + v\cdot w_2$;
      \item $c\left(v\cdot w\right) = cv\cdot w = v\cdot \left(cw\right)$.
    \end{itemize}
    This is an example of a bilinear form.
  \end{example}
  \begin{example}
    In the case of
    \begin{align*}
      \R^3\times \R^3\xrightarrow{\times}\R^3\\
      \left(v,w\right) \mapsto v\times w,
    \end{align*}
    we have the following properties.
    \begin{itemize}
      \item $\left(v_1 + v_2\right)\times w = v_1\times w + v_2\times w$;
      \item $v\times \left(w_1 + w_2\right) = v\times w_1 + v\times w_2$;
      \item $c\left(v\times w\right) = \left(cv\right)\times w = v\times \left(cw\right)$.
    \end{itemize}
    This is an example of a bilinear form, this time not mapping to the scalar field.\newline

  Rephrasing the above two examples, if we let $t:\R^3\times\R^3 \rightarrow \R^3$ be defined by $t(v,w) = v\times w$, the above properties become the following.
  \begin{itemize}
    \item $t\left(v_1 + v_2,w\right) = t\left(v_1,w\right) + t\left(v_2,w\right)$;
    \item $t\left(v,w_1 + w_2\right) = t\left(v,w_1\right) + t\left(v,w_2\right)$;
    \item $ct\left(v,w\right) = t\left(cv,w\right) = t\left(v,cw\right)$.
  \end{itemize}
  \end{example}
  \begin{example}
    If we let $V$ be an $F$-vector space, and define $t: F\times V\rightarrow V$, $\left(a,v\right)\mapsto av$, this is also a bilinear form.
  \end{example}
  \begin{definition}[Bilinear Map]
    Let $U,V,W$ be $F$-vector spaces. Let $t: V\times W\rightarrow U$ be a map satisfying
    \begin{itemize}
      \item $t\left(v_1 + v_2,w\right) = t\left(v_1,w\right) + t\left(v_2,w\right)$;
      \item $t\left(v,w_1 + w_2\right) = t\left(v,w_1\right) + t\left(v,w_2\right)$;
      \item $ct\left(v,w\right) = t\left(cv,w\right) = t\left(v,cw\right)$.
    \end{itemize}
    We call such a map a bilinear map. The collection of bilinear maps is denoted $\Hom_{F}\left(V,W;U\right)$.
  \end{definition}
  We want to construct a vector space that contains $V\times W$, but treats $V$ and $W$ as separate vector spaces to ``linearize'' the bilinear map.\newline

  Let $X = V\times W$ as a set. We will form the vector space $\F\left(V\times W\right)$. We form $\operatorname{Rel}_{F}\left(V\times W\right)$ to be the $F$-span of $E = E_1 \cup E_2 \cup E_3 \cup E_4$, with
  \begin{align*}
    E_1 &= \set{\left(v_1 + v_2,w\right) - \left(v_1,w\right) - \left(v_2,w\right)\mid v_1,v_2\in V,w\in W}\\
    E_2 &= \set{\left(v,w_1+w_2\right) - \left(v,w_1\right) - v\left(w_2\right)\mid v\in V,w_1,w_2\in W}\\
    E_3 &= \set{\left(cv,w\right) - \left(v-cw\right)\mid c\in F,v\in V,w\in W}\\
    E_4 &= \set{c\left(v,w\right)-\left(cv,w\right)\mid c\in F,v\in V,w\in W}.
  \end{align*}
  We define $V\otimes W = \F\left(V\times W\right)/\operatorname{Rel}_{F}\left(V\times W\right)$. We write $v\otimes w = \left(v,w\right) + \operatorname{Rel}_{F}\left(V\times W\right)$. We have
  \begin{itemize}
    \item $\left(v_1 + v_2\right)\otimes w = v_1\otimes w + v_2\otimes w$;
    \item $v\otimes \left(w_1 + w_2\right) = v\otimes w_1 + v\otimes w_2$;
    \item $\left(cv\right)\otimes w = v\otimes \left(cw\right)$;
    \item $c\left(v\otimes w\right) = \left(cv\right)\otimes w = v\otimes \left(cw\right)$.
  \end{itemize}
  Elements of $V\otimes W$ look like
  \begin{align*}
    \sum_{i\in I}c_i\left(v_i\otimes w_i\right) &= \sum_{i\in I}\left(cv_i\right)\otimes w_i\\
                                                &= \sum_{i\in I}v_i\otimes w_i.
  \end{align*}
  We have
  \begin{align*}
    \iota: V\times W \hookrightarrow V\otimes W\\
    \left(v,w\right) \mapsto v\otimes w.
  \end{align*}
  \begin{exercise}
    Show $\iota\in \Hom_{F}\left(V,W;V\otimes W\right)$.
  \end{exercise}
  Let $U$ be another $F$-vector space, and let $T\in \Hom_{F}\left(V\otimes W,U\right)$. We have
  \begin{align*}
    T\left(\left(v_1 + v_2\right)\otimes w\right) &= T\left(v_1\otimes w + v_2\otimes w\right)\\
                                                  &= T\left(v_1\otimes w\right) + T\left(v_2\otimes w\right)\\
    T\left(v\otimes \left(w_1 + w_2\right) \right) &= T\left(v\otimes w_1 + v\otimes w_2\right)\\
                                                   &= T\left(v\otimes w_1\right) + T\left(v\otimes w_2\right)\\
    cT\left(v\otimes w\right) &= T\left(c\left(v\otimes w\right)\right)\\
                              &= T\left(\left(cv\right)\otimes w\right)\\
                              &= T\left(v\otimes \left(cw\right)\right).
  \end{align*}
  \begin{theorem}[Universal Property for Tensor Products]
    Let $U,V,W$ be $F$-vector spaces. 
    \begin{enumerate}[(1)]
      \item If $T\in \Hom_{F}\left(V\otimes W,U\right)$, then $T\circ\iota \in \Hom_{F}\left(V,W;U\right)$.
      \item If $t\in \Hom_{F}\left(V,W;U\right)$, there is a unique $T\in \Hom_{F}\left(V\otimes W,U\right)$ such that $t = T\circ \iota$ --- i.e., the following diagram commutes.
        \begin{center}
          % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUAdTvAW3gAEAdRABfUuky58hFAEZScqrUYs2AVTESQGbHgJEFlavWatEHbhD6CRo5TCgBzeEVAAzAE4ReSMiBwIJAUQBjoAIxgGAAUpfVkQTywnAAscEBNVcwCtD29fRAAmakDgzLM2bhgADyw4HDgAQgEAFVyQLx8-EqCi8rULbnwcOjEKUSA
\begin{tikzcd}
V\times W \arrow[rd, "t"'] \arrow[r, "\iota",hookrightarrow] & V\otimes W \arrow[d, "\exists! T"] \\
                                              & U                                 
\end{tikzcd}
        \end{center}
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    Let $T\in \Hom_{F}\left(V\otimes W,U\right)$, and set $t = T\circ \iota$. Let $v_1,v_2\in V$, $w\in W$, $c\in F$. We have
    \begin{align*}
      t\left(v_1+cv_2,w\right) &= T\left(\iota\left(v_1 + cv_2,w\right)\right)\\
                               &= T\left(\left(v_1 + cv_2\right)\otimes w\right)\\
                               &= T\left(v_1\otimes w + \left(cv_2\right)\otimes w\right)\\
                               &= T\left(v_1\otimes w\right) + T\left(\left(cv_2\right)\otimes w\right)\\
                               &= T\left(v_1\otimes w\right) + T\left(c\left(v_2 \otimes w\right)\right)\\
                               &= T\left(v_1\otimes w\right) + cT\left(v_2\otimes w\right)\\
                               &= t\left(v_1,w\right) + ct\left(v_2,w\right).
    \end{align*}
    The same arguments works in the second variable. Thus, $t\in \Hom_{F}\left(V,W;U\right)$.\newline

    Let $t\in \Hom_{F}\left(V,W;U\right)$. This says $t: V\times W\rightarrow U$; the universal property for $\F\left(V\times W\right)$ gives $t$ extends to a unique $F$-linear map $T: \F\left(V\times W\right)\rightarrow U$ that satisfies $T\left(v,w\right) = t\left(v,w\right)$.\newline

    Taking the canonical projection $\pi: \F\left(V\times W\right) \rightarrow \F\left(V\times W\right)/\operatorname{Rel}_{F}\left(V\times W\right)$, all we need to show is that $T$ vanishes on $\operatorname{Rel}_{F}\left(V\times W\right)$.\newline

    Since $T$ does vanish on $\operatorname{Rel}_{F}\left(V\times W\right)$, we have
    \begin{center}
      % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBpiBdUkANwEMAbAVxiRADUAdTvAW3gAEAdRABfUuky58hFACZyVWoxZsunCH0Ejxk7HgJEFcpfWatEIAKpiJIDPplEyABlMqLIABQ1SAdwBKWz1pQ3lSN2ozVUsabk0sfjgBP2D7KQNZZABmUhMojzYAFR94rWTAtIdQrIVs93M2HB9-IN10xzDkF0UCxssWytElGCgAc3giUAAzACcIXiQekBwIJAVlfpWQagY6ACMYBgAFDKdLWawxgAscNLmFpepVpDJNmJBufBw6e-nFxBvF6IDbRTxFP6PRAAdmea0QADZnnQsAw2Lw6Gg4C92g8AbkVvCACzI1HozHYta4-5IEmEpAAVmohzAUCQ2WWPzJlgxWJxFFEQA
\begin{tikzcd}
                              & {(v,w)} \arrow[r, maps to]                    & v\otimes w \arrow[rdd, maps to, bend left] &               \\
{(v,w)} \arrow[rrdd, maps to] & V\times W \arrow[rd, "t"'] \arrow[r, "\iota",hookrightarrow] & V\otimes W \arrow[d, "T"]                  &               \\
                              &                                               & U                                          & T(v\otimes w) \\
                              &                                               & {t(v,w)}                                   &              
\end{tikzcd}
    \end{center}
    Since the diagram commutes, we must have $T\left(v\otimes w\right) = t\left(v,w\right)$.
  \end{proof}
  \begin{corollary}
    Let $U,V,W$ be $F$-vector spaces.
    \begin{enumerate}[(1)]
      \item $V\otimes_{F} V \cong W\otimes_{F} V$;
      \item $\left(U\otimes_{F} V\right)\otimes_{F}W = U\otimes_{F}\left(V\otimes_{F} W\right)$.
    \end{enumerate}
  \end{corollary}
  \begin{proof}
    We will prove (2).\newline

    Fix $w\in W$. Define $t: U\times V \rightarrow U\otimes_{F}\left(V\otimes_{F} W\right)$, $\left(u,v\right)\mapsto u\otimes\left(v\otimes w\right)$. We claim that $t$ is bilinear.\newline

    Let $u_1,u_2\in U$, $v\in V$, $c\in F$. Then, we have
    \begin{align*}
      t\left(\left(u_1 + cu_2,v\right)\right) &= \left(u_1 + cu_2\right)\otimes \left(v\otimes w\right)\\
                                              &= u_1\otimes \left(v\otimes w\right) + c\left(u_2\otimes \left(v\otimes w\right)\right)\\
                                              &= t\left(\left(u_1,v\right)\right) + ct\left(\left(u_2,v\right)\right).
    \end{align*}
    Linearity in the second variable follows similarly.\newline

    Since $t$ is bilinear, the universal property gives a linear map $T: U\otimes_{F} V\rightarrow U\otimes_{F}\left(V\otimes_{F}W\right)$ by $u\otimes v \mapsto u\otimes \left(v\otimes w\right)$.\newline

    Define $s: \left(U\otimes_{F} V\right)\times W \rightarrow U\otimes_{F}\left(V\otimes_{F}W\right)$ by taking $\left(u\otimes v,w\right)\mapsto u\otimes \left(v\otimes w\right)$. Let $u_1 \otimes v_1 = u_2\otimes v_2$. Then,
    \begin{align*}
      s\left(\left(u_1\otimes v_1,w\right)\right) &= u_1\otimes \left(v_2\otimes w\right)\\
                                                  &= T\left(u_1\otimes v_1\right)\\
                                                  &= T\left(u_2\otimes v_2\right)\\
                                                  &= u_2\otimes \left(v_2\otimes w\right)\\
                                                  &= S\left(\left(u_2\otimes v_2,w\right)\right).
    \end{align*}
    Additionally, $s$ is a bilinear map, meaning that the universal property of tensor products gives a unique linear map $S: \left(U\otimes_{F}V\right)\otimes_{F}W \rightarrow U\otimes_{F}\left(V\otimes_{F}W\right)$, mapping $\left(u\otimes v\right)\otimes w \mapsto u\otimes \left(v\otimes w\right)$.\newline

    We do the same in the opposite direction. Fix $u\in U$.
    \begin{align*}
      \tilde{t}: V\times W \rightarrow \left(U\otimes_{F}V\right)\otimes_{F}W\\
      \left(v,w\right) \mapsto \left(u\otimes v\right)\otimes w.
    \end{align*}
    Continuing the process, we get $\tilde{S}: U\otimes_{F} \left(V\otimes_{F} W\right)\rightarrow \left(U\otimes_{F} V\right)\otimes_{F} W$, mapping $u\otimes \left(v\otimes w\right)\mapsto \left(u\otimes v\right)\otimes w$. These are inverses of each other.
  \end{proof}
  \begin{theorem}
    Let $U,V,W$ be $F$-vector spaces. There is an isomorphism
    \begin{align*}
      \left(U\oplus V\right)\otimes_{F} W &\xrightarrow{\cong} \left(U\otimes_{F}W\right)\oplus \left(V\otimes_{F}W\right)\\
      \left(u,v\right)\otimes w &\mapsto \left(u\otimes w,v\otimes w\right)
    \end{align*}
  \end{theorem}
  \begin{proof}
    Define $t: \left(U\oplus V\right)\times W \rightarrow \left(U\otimes_{F} W\right)\oplus \left(V\otimes_{F}W\right)$ by taking
    \begin{align*}
      \left(\left(u,v\right),w\right)\mapsto \left(u\otimes w,v\otimes w\right).
    \end{align*}
    This is a bilinear map.\newline

    Thus, there is a unique linear map
    \begin{align*}
      T: \left(U\oplus V\right)\otimes_{F} W \rightarrow \left(U\otimes_{F}W\right)\oplus \left(V\otimes_{F}W\right)
    \end{align*}
    mapping $\left(u,v\right)\otimes w \mapsto \left(u\otimes w,v\otimes w\right)$.\newline

    To find an inverse map, we define two maps
    \begin{align*}
      s_1: U\times W &\rightarrow \left(U\oplus V\right)\otimes_{F} W\\
      \left(u,w\right) &\mapsto \left(u,0_V\right)\otimes w\\
      \\
      s_2: V\times W &\rightarrow \left(U\oplus V\right)\otimes_{F} W\\
      \left(v,w\right) &\mapsto \left(0_{U},v\right)\otimes w.
    \end{align*}
    Let $u_1,u_2\in U, w\in W, c\in F$. Then,
    \begin{align*}
      s_1\left(\left(u_1 + cu_2,w\right)\right) &= \left(u_1 + cu_2,0_V\right) \otimes w\\
                                                &= \left(\left(u_1,0_V\right)+ c\left(u_{2},0_{V}\right)\right)\otimes w\\
                                                &= \left(u_1,0_V\right)\otimes w+ c\left(u_{2},0_{V}\right)\otimes w\\
                                                &= s_1\left(\left(u_1,w\right)\right) + c s_1\left(\left(u_2,w\right)\right)
    \end{align*}
    Similarly, $s_2$ is bilinear, meaning we have well-defined linear maps
    \begin{align*}
      S_1: U\otimes_{F} W&\rightarrow \left(U\oplus V\right)\otimes_{F} W\\
      u\otimes w &\mapsto \left(u,0_V\right)\otimes w\\
      \\
      S_2: V\otimes_{F} W &\rightarrow \left(U\oplus V\right)\otimes_{F} W\\
      v\otimes w &\mapsto \left(0_{U},v\right)\otimes_{F} W.
    \end{align*}
    Define
    \begin{align*}
      S: \left(U\otimes_{F}W\right)\oplus \left(V\otimes_{F} W\right) &\rightarrow \left(U\oplus V\right)\otimes_{F}W\\
      \left(u\otimes w_1,v\otimes w_2\right) &\mapsto S_1\left(u\otimes w_1\right) + S_2\left(v\otimes w_2\right)
    \end{align*}
    We have
    \begin{align*}
      S\circ T\left(\left(u,v\right)\otimes w\right) &= S\left(\left(u\otimes w, v\otimes w\right)\right)\\
                                                     &= \left(u,0_V\right)\otimes w + \left(0_U,v\right)\otimes w\\
                                                     &= \left(\left(u,0_V\right) + \left(0_U,v\right)\right)\otimes w\\
                                                     &= \left(u,v\right)\otimes w.
    \end{align*}
    Similarly,
    \begin{align*}
      T\circ S\left(\left(u\otimes w_1,v\otimes w_2\right)\right) &= T\left(\left(u,0_V\right)\otimes w_1 + \left(0_U,v\right)\otimes w_2\right)\\
                                                                  &= T\left(\left(u,0_V\right)\otimes w_1\right) + T\left(\left(0_U,v\right)\otimes w_2\right)\\
                                                                  &= \left(u\otimes w_1,0_V\otimes w_1 \right) + \left(0_U\otimes w_2 + v\otimes w_2\right)\\
                                                                  &= \left(u\otimes w_1,v\otimes w_2\right).
    \end{align*}
    
  \end{proof}
  \begin{corollary}[Bases of Tensor Products]
    Let $V,W$ be finite-dimensional $F$-vector spaces with bases $\mathcal{B} = \set{v_1,\dots,v_m}$ in $V$ and $\mathcal{C} = \set{w_1,\dots,w_n}$ respectively.\newline

    Then, the collection
    \begin{align*}
      \mathcal{D} &= \set{v_{i}\otimes w_j}_{\substack{1\leq i \leq m\\ 1\leq j \leq n}}
    \end{align*}
    is a basis for $V\otimes_{F} W$. In particular, $\Dim\left(V\otimes_{F} W\right) = \Dim_{F}(V)\Dim_{F}(W)$.
  \end{corollary}
  \begin{proof}
    We define $t: V\times W\rightarrow \Mat_{m,n}(F)$, mapping
    \begin{align*}
      \left(v_i,w_j\right) \mapsto e_{ij},
    \end{align*}
    where $e_{ij}$ is the matrix with $1$ in the $ij$ position and $0$ everywhere else.\newline

    Let $v\in V$ and $w\in W$. We write
    \begin{align*}
      v &= \sum_{i=1}^{m}a_iv_i\\
      w &= \sum_{j=1}^{n}b_jw_j,
    \end{align*}
    and define
    \begin{align*}
      t\left(v,w\right) &= \sum_{i=1}^{m}\sum_{j=1}^{n}a_ib_je_{ij}.
    \end{align*}
    This is, by definition, bilinear. Thus, there is a unique linear map
    \begin{align*}
      T: V\otimes_{F} W \rightarrow \Mat_{m,n}\left(F\right)
    \end{align*}
    such that $v_i\otimes w_j \mapsto e_{ij}$.\newline

    Define $S: \Mat_{m,n}\left(F\right)\rightarrow V\otimes_{F} W$ by $e_{ij}\mapsto v_i\otimes w_j$. Since $\set{e_{ij}}$ is a basis for $\Mat_{m,n}(F)$, it is the case that $T$ is an isomorphism, so $\Dim\left(T\right) = \Dim\left(\Mat_{m,n}(F)\right) = mn$.\newline

    Since $S$ is an isomoprhism, and $S\left(\set{e_{ij}}\right) = \set{v_i\otimes w_j}$, it is the case that $\set{v_{i}\otimes w_j}$ is a basis for $V\otimes_{F} W$.
  \end{proof}
  \begin{example}
    We have
    \begin{align*}
      \C\otimes_{\R}\C \cong \R^{4},
    \end{align*}
    with basis $\set{1\otimes 1,1\otimes i, i\otimes 1,i\otimes i}$. Meanwhile,
    \begin{align*}
      \C\otimes_{\C}\C &\cong \C,
    \end{align*}
    with basis $\set{1\otimes 1}$.
  \end{example}
  \begin{theorem}
    Let $V,W$ be $F$-vector spaces. Let $\mathcal{B}_{V} = \set{v_i}_{i\in I}$ and $\mathcal{B}_{W} = \set{w_{j}}_{j\in I}$ be bases. The set $\mathcal{B} = \set{v_i\otimes w_j}_{i,j\in I}$ is a basis for $V\otimes_{F} W$.
  \end{theorem}
  \begin{proof}
    Let $v\in V$ and $w\in W$. We can write
    \begin{align*}
      v &= \sum_{i\in I}a_iv_i\\
      w &= \sum_{j\in I}b_jw_j,
    \end{align*}
    where the sums are finite.\newline

    Then,
    \begin{align*}
      v\otimes w &= \sum_{i,j\in I} a_ib_j\left(v_i\otimes w_j\right),
    \end{align*}
    so $\mathcal{B} = \set{v_i\otimes w_j}_{i,j\in I}$ is spanning for $V\otimes_{F} W$.\newline

    Suppose we can write
    \begin{align*}
      \sum_{i,j\in I}c_{i,j}\left(v_i\otimes w_j\right) &= 0_{V\otimes_{F} W}
    \end{align*}
    for some $c_{i,j}\in F$ as a finite sum.\newline

    Fix $\left(i_0,j_0\right)\in I\times I$. Define
    \begin{align*}
      t_{i_0,j_0}: V\times W &\rightarrow F\\
      \left(v_i,w_j\right) &\mapsto \delta_{(i,j)\left(i_0,j_0\right)}.
    \end{align*}
    Note that
    \begin{align*}
      t_{i_0,j_0}\left(\sum_{i\in I}a_iv_i,\sum_{j\in I}b_jw_j\right) &= \sum_{i\in I}\sum_{j\in I}a_ib_jt_{i_0,j_0}\left(v_{i},w_j\right)\\
                                                                      &= a_{i_0}b_{i_0}.
    \end{align*}
    Therefore, there is $T_{i_0,j_0}\in \Hom_{F}\left(V\otimes_{F}W,F\right)$ with
    \begin{align*}
      T_{i_0,j_0}\left(v_{i}\otimes w_j\right) &= \delta_{\left(i_j\right)\left(i_{0},j_0\right)}.
    \end{align*}
    Therefore, we have
    \begin{align*}
      0 &= T_{i_0,j_0}\left(0_{V\otimes_{F}W}\right)T\\
        &= _{i_0,j_0}\left(\sum_{i,j\in I}c_{i,j}v_0\otimes w_j\right)\\
        &= c_{i_0,j_0}\\
                                                                   &= 0,
    \end{align*}
    for each $\left(i_0,j_0\right)$ in the sum, so $\set{v_i\otimes w_j}_{i,j\in I}$ is linearly independent.
  \end{proof}
  \begin{definition}[Trace]
    Let $A\in \Mat_{n}\left(F\right)$, $A = \left(a_{ij}\right)$. Then,
    \begin{align*}
      \operatorname{Tr}(A) &= \sum_{i=1}^{n}a_{ii}.
    \end{align*}
  \end{definition}
  Recall that
  \begin{align*}
    A &= \left[T_{A}\right]_{\mathcal{B}}
  \end{align*}
  for some $\mathcal{B}$. If trace is to mean anything, we should be able to define the trade to be basis-independent --- that is,
  \begin{align*}
    \tr(T) &= \tr\left(\left[T\right]_{\mathcal{B}_1}\right)\\
           &= \tr\left(\left[T\right]_{\mathcal{B}_2}\right)
  \end{align*}
  for different bases $\mathcal{B}_1$ and $\mathcal{B}_2$.\newline

  It may seem suspect that ``summing the diagonal'' is independent of choice of basis. Therefore, we want to define the trace to be basis-independent, then we will show that the ``summing the diagonal'' definition yields.\newline

  The trace should be defined
  \begin{align*}
    \tr: \Hom_{F}\left(V,V\right)\rightarrow F.
  \end{align*}
  We need to bring the tensor product into this, defining a map on $\Hom_{F}\left(V,V\right)'$ is difficult.
  \begin{lemma}
    Let $V$ be a finite-dimensional $F$-vector space. Then, $V\otimes_{F}V'\cong \Hom_{F}\left(V,V\right)$.
  \end{lemma}
  \begin{proof}
    Let
    \begin{align*}
      t: V\times V' &\rightarrow \Hom_{F}\left(V,V\right)\\
      \left(v,\varphi\right) &\mapsto \left(w \mapsto \varphi\left(w\right)v\right).
    \end{align*}
    Let $v_1,v_2\in V$, $c\in F$. Then,
    \begin{align*}
      t\left(v,\varphi\right)\left(v_1 + cv_2\right) &= \varphi\left(v_1 + cv_2\right)v\\
                                                  &= \varphi\left(v_1\right)v + c\varphi\left(v_2\right)v\\
                                                  &= t\left(v,\varphi\right)\left(v_1\right) + ct\left(v,\varphi\right)\left(v_2\right),
    \end{align*}
    meaning that the map $ \mapsto \varphi\left(w\right)v$ is indeed a linear map.\newline

    We want to show that $t$ is bilinear.
    \begin{align*}
      t\left(v_1 + cv_2,\varphi\right) &= t\left(v_1,\varphi\right) + ct\left(v_2,\varphi\right),
    \end{align*}
    and similarly,
    \begin{align*}
      t\left(v,\varphi_1 + c\varphi_2\right) &= t\left(v,\varphi_1\right) + ct\left(v,\varphi_2\right).
    \end{align*}
    We really need to show that
    \begin{align*}
      t\left(v_1 + cv_2,\varphi\right)\left(w\right) &= t\left(v_1,\varphi\right)\left(w\right) + ct\left(v_2,\varphi\right)\left(w\right)\\
      t\left(v,\varphi_1 + c\varphi_2\right)\left(w\right) &= t\left(v,\varphi_1\right)\left(w\right) + ct\left(v,\varphi_2\right)\left(w\right).
    \end{align*}
    Computing, we have
    \begin{align*}
      t\left(v_1 + cv_2,\varphi\right)\left(w\right) &= \varphi\left(w\right)\left(v_1 + cv_2\right)\\
                                                      &= \varphi\left(w\right)v_1 + c\varphi\left(w\right)v_2\\
                                                      &= t\left(v_1,\varphi\right)\left(w\right) + ct\left(v_2,\varphi\right)\left(w\right).
    \end{align*}
    Similarly,
    \begin{align*}
      t\left(v,\varphi_1 + c\varphi_2\right)\left(w\right) &= t\left(v,\varphi_1\right)\left(w\right) + ct\left(v,\varphi_2\right)\left(w\right).
    \end{align*}
    Thus, there is a well-defined map 
    \begin{align*}
      \mathcal{T}: V\otimes_{F}V' &\rightarrow \Hom_{F}\left(V,V\right)\\
      v\otimes \varphi &\mapsto \left(w\mapsto \varphi(w) v\right).
    \end{align*}
    In particular, we have
    \begin{align*}
      \mathcal{T}\left(v\otimes \varphi\right)\left(w\right) &= \varphi\left(w\right)v.
    \end{align*}
    Since both $V\otimes V'$ and $\Hom_{F}\left(V,V\right)$ have dimension $n^2$, it is enough to show that $\mathcal{T}$ is injective.\newline

    Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$, and $\mathcal{B}' = \set{v_1',\dots,v_n'}$ a basis for $V'$. Suppose we have
    \begin{align*}
      \mathcal{T}\left(\sum_{i,j}a_{i,j}\left(v_{i}\otimes v_{j}'\right)\right) &= 0_{\Hom_{F}\left(V,V\right)}
    \end{align*}
    for some $a_{i,j}\in \F$. Take $v_m\in \mathcal{B}$. Then, we have
    \begin{align*}
      0_{V} &= \mathcal{T}\left(\sum_{i,j}a_{i,j}\left(v_i\otimes v_j'\right)\right)\left(v_{m}\right)\\
            &= \sum_{i,j}a_{i,j}\mathcal{T}\left(v_{i}\otimes v_{j}'\right)\left(v_{m}\right)\\
            &= \sum_{i,j}a_{i,j}v_{j}'\left(v_m\right)v_i\\
            &= \sum_{i} a_{i,m}v_i,\\
      a_{i,m} &= 0.
    \end{align*}
    However, since $m$ was arbitrary, we have $a_{i,j} = 0$ for each $i,j$, so $\mathcal{T}$ is injective, hence $\mathcal{T}$ is an isomorphism.
  \end{proof}
  Recall that we have
  \begin{align*}
    \Hom_{F}\left(V,V\right) \times \Hom_{F}\left(V,V\right) &\rightarrow \Hom_{F}\left(V,V\right)\\
    \left(S,T\right) \xmapsto{\text{comp}} S\circ T.
  \end{align*}
  Therefore, we have the following diagram
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAAoA1AHW4jwC28AAScA5AEpegkV178sQuKMkgAvqXSZc+QijIAmKrUYs2vABIQBAfWAAxNV1Kcp3GcsvW7j56-WaIBjYeAREBuTG9MysiCA8fB4qAVohuuGkRtTRZnFetg5OnC4S6sYwUADm8ESgAGYATtZIAIzUOBBIAMzZprEg0jAAHjjAAMbWaGopII3NiGQgHa3UDHQARjAMAAraoXogDViVABY4IL0x5twCdDgnY4zAACpq0ooivLf3jwwv0xp6k0BEhFstEBEQAwsGB+lA6HAThULiYrnluNtsMIALzCL53B5PV4APWAAFoWm9uGMsA0xnj3MNRhMBFMGTS6cJnjM5iCIe1OogeqjcgMbgTfv8ymogA
\begin{tikzcd}
  \left(V\otimes V'\right)\times \left(V\otimes V'\right) \arrow[dd, "\mathcal{T}\times \mathcal{T}"'] \arrow[rr, "\Phi", dashed] &  & V\otimes V' \arrow[dd, "\mathcal{T}"] \\
                                                                                                                                                      &  &                                       \\
{\Hom_{F}\left(V,V\right)\times \Hom_{F}\left(V,V\right)} \arrow[rr, "\text{comp}"]                                                                                         &  & {\Hom_{F}\left(V,V\right)}                      
\end{tikzcd}
  \end{center}
  We define
  \begin{align*}
    \Phi: \left(V\otimes_{F}V'\right)\times \left(V\otimes_{F}V'\right) &\longrightarrow V\otimes_{F}V'\\
    \left(v\otimes \varphi, w\otimes \psi\right) &\longmapsto \varphi\left(w\right)v \otimes \psi.
  \end{align*}
  We need to verify that this map allows the diagram to commute. Let $x\in V$. Then, we have
  \begin{align*}
    \mathcal{T}\left(v\otimes \varphi\right)\circ \mathcal{T}\left(w\otimes \psi\right)\left(x\right) &= \mathcal{T}\left(v\otimes \varphi\right)\left(\psi(x) w\right)\\
                                                                                                      &= \psi(x)\mathcal{T}\left(v\otimes \varphi\right)\left(w\right)\\
                                                                                                      &= \psi(x)\varphi(w)v.
  \end{align*}
  In the other direction, we have
  \begin{align*}
    \mathcal{T}\circ \Phi \left(v\otimes \varphi,w\otimes \psi\right)(x) &= \mathcal{T}\left(\varphi(w)v \otimes \psi\right)(x)\\
                                                                         &= \varphi(w)\mathcal{T}\left(v\otimes \psi\right)\left(x\right)\\
                                                                         &= \varphi(w)\psi(x).
  \end{align*}
  Indeed, the diagram does commute, and $\Phi$ is our map that corresponds to composition of functions.\newline

  Returning to the trace, let $T\in \Hom_{F}\left(V,V\right)$, with $\mathcal{B} = \set{v_1,\dots,v_n}$ a basis for $V$, and we write
  \begin{align*}
    A &= \left[T\right]_{\mathcal{B}}.
  \end{align*}
  Note that
  \begin{align*}
    a_{ij} &= v_{i}'\left(T\left(v_j\right)\right).
  \end{align*}
  In particular, we have
  \begin{align*}
    \operatorname{Tr}\left(A\right) &= \sum_{i=1}^{n}a_{ii}\\
                      &= \sum_{i=1}^{n}v_i'\left(T\left(v_i\right)\right)
  \end{align*}
  Let
  \begin{align*}
    s: V\times V' &\rightarrow F\\
    \left(v,\varphi\right) &\mapsto \varphi(v).
  \end{align*}
  This map is bilinear, meaning we have
  \begin{align*}
    S: V\otimes V' &\rightarrow F\\
    v\otimes \varphi &\mapsto \varphi(v).
  \end{align*}
  Thus, we have the map
  \begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12AJCAWwH1gAMQC+ACgBqpCQEoQI0uky58hFGQBMVWoxZsJnCHl7wABBIDk8xSAzY8BIhvLb6zVohBD52mFADm8ESgAGYATnxIZCA4EEjOOu5snDhhpgC8pgDKnADGWGG5pgAqAHrAALQAjCLWoRG8SFXUsVHUbnqexXUg4ZGIzTFxiAkMdABGMAwACsoOaiBhWP4AFjgg7boeIFk+IkA
\begin{tikzcd}
{\Hom_{F}(V,V)} \arrow[rr, "\tr = S\circ T^{-1}"] &  & F \\
                                                  &  &   \\
V\otimes V' \arrow[uu, "T"] \arrow[rruu, "S"']    &  &  
\end{tikzcd}
  \end{center}
  We know that $\mathcal{T}\left(v_{i}\otimes v_{j}'\right) = T_{ij}\in \Hom_{F}\left(V,V\right)$. Since $\mathcal{T}$ is an isomorphism, we know that $\set{T_{ij} | T_{ij} = \mathcal{T}\left(v_{i}\otimes v_j\right)}_{i,j}$ is a basis of $\Hom_{F}\left(V,V\right)$.\newline

  We take
  \begin{align*}
    \operatorname{Tr}\left(T_{k,\ell}\right) &= \operatorname{Tr}\left(\mathcal{T}\left(v_{k}\otimes v_{\ell}'\right)\right)\\
                                             &= \sum_{i=1}^{n}v_{i}'\left(\mathcal{T}\left(v_{k}\otimes v_{\ell}\right)\left(v_i\right)\right)\\
                                             &= \sum_{i=1}^{n}v_{i}'\left(v_{\ell}'\left(v_i\right) v_k\right)\\
                                             &= \sum_{i=1}^{n}v_{\ell}'\left(v_i\right)v_{i}'\left(v_k\right)\\
                                             &= v_{\ell}'\left(v_k\right)\\
                                             &= \begin{cases}
                                               1 & k = \ell\\
                                               0 & \text{else}
                                             \end{cases}.
  \end{align*}
  We also have
  \begin{align*}
    \tr\left(T_{k,\ell}\right) &= \left(S\circ \mathcal{T}^{-1}\right)\left(T_{k,\ell}\right)\\
                               &= \left(S\circ \mathcal{T}^{-1}\right)\left(\mathcal{T}\left(v_{k}\otimes v_{\ell}'\right)\right)\\
                               &= S\left(v_{k}\otimes v_{\ell}'\right)\\
                               &= v_{\ell}'\left(v_k\right)\\
                               &= \begin{cases}
                                 1 & k = \ell\\
                                 0 & \text{else}
                               \end{cases}.
  \end{align*}
  Thus, $\operatorname{Tr} = \tr$ as they agree on the basis elements.\newline

  We immediately see that $\operatorname{Tr}$ is a linear map.
  \begin{corollary}
    Let $A,B\in \Mat_{n}\left(F\right)$. We have $\Tr\left(AB\right) = \Tr\left(BA\right)$.
  \end{corollary}
  \begin{proof}
    Define 
    \begin{align*}
      t_1: \Hom_{F}\left(V,V\right)\times \Hom_{F}\left(V,V\right) &\rightarrow F\\
      \left(S,T\right) &\mapsto \tr\left(S\circ T\right)\\
      \\
      t_{2}:\Hom_{F}\left(V,V\right)\times \Hom_{F}\left(V,V\right) &\rightarrow F\\
      \left(S,T\right) &\mapsto \tr\left(T\circ S\right).
    \end{align*}
    These are both bilinear maps, meaning we have maps
    \begin{align*}
      T_1: \Hom_{F}\left(V,V\right)\otimes \Hom_{F}\left(V,V\right) &\rightarrow F\\
      \left(S\otimes T\right) &\mapsto \tr\left(S\circ T\right)\\
      \\
      T_2: \Hom_{F}\left(V,V\right)\otimes \Hom_{F}\left(V,V\right) &\rightarrow F\\
      \left(S\otimes T\right) &\mapsto \tr\left(T\circ S\right).
    \end{align*}
    Let $v\otimes \varphi,w\otimes \psi\in V\otimes V'$. We need to show that
    \begin{align*}
      \tr\left(\mathcal{T}\left(v\otimes \varphi\right)\circ\mathcal{T}\left(w\otimes \psi\right)\right) &= \tr\left(\mathcal{T}\left(w\otimes \psi\right)\circ \mathcal{T}\left(v\otimes \varphi\right)\right).
    \end{align*}
    Recall that $\mathcal{T}\left(v\otimes \varphi\right)\circ \mathcal{T}\left(w\otimes \psi\right) \leftrightarrow \varphi(w)v\otimes \psi$, so we have
    \begin{align*}
      \tr\left(\mathcal{T}\left(v\otimes \varphi\right)\circ \mathcal{T}\left(w\otimes \psi\right)\right) &= \tr\left(\varphi(w) v\otimes \psi\right)\\
                                                                                                          &= \varphi(w)\tr\left(v\otimes \psi\right)\\
                                                                                                          &= \varphi(w)\psi(v)\\
      \tr\left(\mathcal{T}\left(w\otimes \psi\right)\circ \mathcal{T}\left(v\otimes \varphi\right)\right) &= \tr\left(\psi(v) w\otimes \varphi\right)\\
                                                                                                          &= \psi(v)\tr\left(w\otimes \varphi\right)\\
                                                                                                          &= \varphi(w)\psi(v).
    \end{align*}
  \end{proof}
  \subsection{Tensor Algebras, Exterior Algebras, and the Determinant}%
  Now that we understand the trace, we want to build some more structure to understand the determinant.\newline


    Recall that we have
    \begin{align*}
      \left(U\otimes V\right) \otimes W &\cong U\otimes \left(V\otimes W\right),
    \end{align*}
    so we may write
    \begin{align*}
      U\otimes V \otimes W.
    \end{align*}
  By induction, given $V_1,\dots,V_n$, we may write
  \begin{align*}
    V_1\otimes \cdots \otimes V_n
  \end{align*}
  as a unique vector space up to isomorphism.\newline

  Elements in $V_1\otimes \cdots \otimes V_n$ look like
  \begin{align*}
    \sum_{}a_{i_1,\dots,i_{n}}\left(v_{i_1}\otimes\cdots\otimes v_{i_n}\right),
  \end{align*}
  where $v_{i_j}\in V_j$.\newline

  We write
  \begin{align*}
    \mathcal{T}^{k}\left(V\right) &= \underbrace{V\otimes \cdots \otimes V}_{\text{$k$ copies}}
  \end{align*}
  \begin{definition}
    Let $V_1,\dots,V_n$ be $F$-vector spaces. A map
    \begin{align*}
      t: V_1\times\cdots\times V_n\rightarrow W
    \end{align*}
    is said to be multilinear if it is linear in each variable separately.\newline

    The collection of multilinear maps is denoted $\Hom_{F}\left(V_1,\dots,V_n;W\right)$.
  \end{definition}
  \begin{exercise}
    Show that $\Hom_{F}\left(V_1,\dots,V_n;W\right)$ is a vector space.
  \end{exercise}
  \begin{theorem}
    Let $V_1,\dots,V_n,W$ be $F$-vector spaces, and let $\iota: V_1\times\cdots\times V_n\rightarrow V_1\otimes \cdots \otimes V_n$ by $\iota\left(v_1,\dots,v_n\right) = v_1\otimes\cdots\otimes v_n$.
    \begin{enumerate}[(1)]
      \item Given $T\in \Hom_{F}\left(V_1\otimes\cdots\otimes V_n,W\right)$, then $T\circ \iota \in \Hom_{F}\left(V_1,\dots,V_n;W\right)$.
      \item Given $t\in \Hom_{F}\left(V_1,\dots,V_n;W\right)$, there is a unique linear map $T\in \Hom_{F}\left(V_1\otimes\cdots\otimes V_n,W\right)$ such that $t = T\circ \iota$.
    \end{enumerate}
  \end{theorem}
  \begin{proof}
    Proof is an exercise. Adapt the proof from $V_1\otimes V_2$.
  \end{proof}
  \begin{corollary}
    Let $V_1,\dots,V_k$ be vector spaces of dimension $n_1,\dots,n_k$. Let
    \begin{align*}
      \mathcal{B}_i &= \set{e_{1}^{i},\dots,e_{n_i}^{i}}
    \end{align*}
    be bases for $V_i$. Then,
    \begin{align*}
      \mathcal{B} &= \set{e_{i_1}^{1}\otimes e_{i_2}^{2}\otimes\cdots\otimes e_{i_k}^{k}}
    \end{align*}
    is a basis for $V_1\otimes\cdots V_k$.\newline

    In particular,
    \begin{align*}
      \Dim_{F}\left(V_1\otimes\cdots\otimes V_k\right) &= \prod_{j=1}^{k}\Dim_{F}\left(V_j\right).
    \end{align*}
    
  \end{corollary}
  \begin{example}
    Let $V = V_1 = V_2 = V_3 = \C$, $F = \R$.\newline

    We have $\mathcal{B}_1 = \mathcal{B}_2 =\mathcal{B}_3 = \set{1,i}$. For the basis of $V_1\otimes V_2\otimes V_3$, we then have
    \begin{align*}
      \mathcal{B} &= \set{1\otimes 1\otimes 1,i\otimes 1\otimes 1,1\otimes i \otimes 1, 1\otimes 1\otimes i,\dots,i\otimes i\otimes i}.
    \end{align*}
  \end{example}
  \begin{definition}[Exterior Product]
    Let $k\geq 1$ We define the $k$th exterior product of $V$, denoted $\Lambda^{k}\left(V\right)$, by
    \begin{align*}
      \Lambda^{k}\left(V\right) &= \mathcal{T}^{k}\left(V\right) / \mathcal{A}_{k}\left(V\right),
    \end{align*}
    where 
    \begin{align*}
      \mathcal{A}_k\left(V\right) &= \Span\set{v_1\otimes\cdots\otimes v_k | \text{ $v_i = v_j$ for some $i\neq j$}}
    \end{align*}
    We write
    \begin{align*}
      v_1\otimes \cdots \otimes v_k + \mathcal{A}_k\left(V\right) &= v_1\wedge\cdots\wedge v_k.
    \end{align*}
    We call this an elementary wedge product. Elements in $\Lambda^{k}\left(V\right)$ are finite sums of elementary wedge products.\newline

    We have
    \begin{align*}
      \left(v_1 + \tilde{v}_1\right)\wedge v_2\wedge\cdots\wedge v_k &= v_1\wedge v_2\wedge \cdots \wedge v_k + \tilde{v}_1 \wedge v_2\wedge \cdots \wedge v_k\\
      v_1\wedge \cdots \wedge v_{j-1}\wedge cv_j \wedge v_{j+1}\wedge \cdots v_{k} &= c\left(v_1\wedge\cdots\wedge v_k\right).
    \end{align*}
    We also have
    \begin{align*}
      v_1\cdots\wedge v_k = 0_{\Lambda^{k}\left(V\right)}
    \end{align*}
    if $v_i = v_j$ for some $i\neq j$.\newline

    Let $v,w\in V$. Then, we have
    \begin{align*}
      0_{\Lambda^{2}\left(V\right)} &= \left(v+w\right)\wedge \left(v+w\right)\\
                                    &= v\wedge v + w\wedge w + w\wedge v + v\wedge w\\
                                    &= w\wedge v + v\wedge w,
    \end{align*}
    meaning
    \begin{align*}
      v\wedge w = -w\wedge v.
    \end{align*}
    More generally, we have
    \begin{align*}
      v_1\wedge \cdots \wedge v_i \wedge v_{i+1}\wedge \cdots \wedge v_k &= -v_1\wedge\cdots\wedge v_{i+1}\wedge v_i\wedge\cdots \wedge v_k
    \end{align*}
  \end{definition}
  \begin{definition}[Alternating Maps]
    Let $V,W$ be $F$-vector spaces. Let $t\in \Hom_{F}\left(V,\dots,V;W\right)$. If
    \begin{align*}
      t\left(v_1,\dots,v_k\right) = 0_{W}
    \end{align*}
    whenever $v_i = v_j$ for some $i\neq j$, then we say $t$ is alternating.\newline

    We denote the set of alternating maps
    \begin{align*}
      \Alt^{k}\left(V;W\right).
    \end{align*}
    We set $\Alt^{0}\left(V;W\right) = F$.
  \end{definition}
  \begin{example}[Cross Product]
    Let $V = W = \R^3$, and define $t: V\times V \rightarrow W$ by $t\left(v_1,v_2\right) = v_1\times v_2$. We saw before that $t\in \Hom_{F}\left(V,V;W\right)$, and we remember from calculus that $v\times v = 0$. Thus, we also have $t\in \Alt^{2}\left(V,W\right)$.
  \end{example}
  \begin{example}[Determinant]
    Let $\det: \Mat_{n}\left(F\right)\rightarrow F$ be the regular determinant map.\newline

    Given $A\in \Mat_{n}\left(F\right)$, we can write
    \begin{align*}
      A &= \begin{pmatrix}a_{11} & \cdots & \cdots & a_{1n} \\ a_{21} & \cdots &\cdots & \vdots \\ \vdots & \ddots & \ddots & \vdots \\ a_{n1} & \cdots & \cdots & a_{nn}\\ \end{pmatrix}\\
        &= \begin{pmatrix}v_{1} & v_{2} & \cdots & v_{n}\end{pmatrix},
    \end{align*}
    where $v_{i}$ is the $i$th column of $A$. We can identify $\Mat_{n}\left(F\right)$ with $F^n\times\cdots\times F^n$, meaning we can imagine $\det: F^{n}\times\cdots\times F^{n}\rightarrow F$.\newline

    We have $\det$ is multilinear; for instance,
    \begin{align*}
      \det\left(v_1,\dots,v_{j} + c\tilde{v}_j,\cdots v_n\right) &= \det\left(v_1,\dots,v_n\right) + c\det\left(v_1,\dots,\tilde{v}_{j},\dots,v_n\right).
    \end{align*}
    \begin{exercise}
      Prove using induction.
    \end{exercise}
    It is also the case that $\det$ is alternating --- i.e., that $\det\left(v_1,\dots,v_n\right) = 0$ if $v_i = v_j$ for some $i\neq j$.\newline

    This shows that $\det$ is an alternating map.
  \end{example}
  \begin{exercise}\hfill
    \begin{enumerate}[(a)]
      \item Show that $\Alt^{k}_{F}\left(V;W\right)$ is an $F$-subspace of $\Hom_{F}\left(V,\dots,V;W\right)$.
      \item Show that $\Alt^{1}\left(V;F\right) = \Hom_{F}\left(V,F\right) = V'$.
      \item If $\Dim_{F}\left(V\right) = n$, show that $\Alt^{k}\left(V;F\right) = 0$ for all $k > n$.
    \end{enumerate}
  \end{exercise}
  \begin{theorem}[Universal Property for Alternating Linear Maps]
    Let $V,W$ be $F$-vector spaces, $k > 0$. Define
    \begin{align*}
      \iota: V\times\cdots\times V &\rightarrow \Lambda^{k}\left(V\right)\\
      \left(v_1,\dots,v_k\right) &\mapsto v_1\wedge\cdots\wedge v_k.
    \end{align*}
    \begin{enumerate}[(1)]
      \item $\iota\in \Alt^{k}\left(V,\Lambda^{k}\left(V\right)\right)$
      \item If $T\in \Hom_{F}\left(\Lambda^{k}\left(V\right),W\right)$, then $T\circ\iota\in \Alt^{k}\left(V,W\right)$
      \item If $t\in \Alt^{k}\left(V,W\right)$, then there is a unique $T\in \Hom_{F}\left(\Lambda^{k}\left(V\right),W\right)$ such that $t = T\circ \iota$ making the following diagram commute.
    \end{enumerate}
    \begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUAdTvAW3m4BjKBBxxufeAAJ2IAL6l0mXPkIoATOSq1GLNtwAydXgCModAHrAA1nIAU7AJTzFIDNjwEimgIzb6zKyIIADq8towUADm8ESgAGYAThC8SGQgOBBImiAMdCYwDAAKyp5qIIlYUQAWOCDUAXrBdQoJyamIPtSZ2Q26QSAAKi5tKWndWZ19gfqc+Dh04XJAA
\begin{tikzcd}
V\times\cdots\times V \arrow[rrd, "t"'] \arrow[rr, "\iota"] &  & \Lambda^{k}(V) \arrow[d, "T"] \\
                                                            &  & W                            
\end{tikzcd}
    \end{center}
  \end{theorem}
  \begin{proof}\hfill
    \begin{enumerate}[(1)]
      \item We have the composition
        \begin{center}
          % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUAdTvAW3m4BjKBBxxufeAAJ2IAL6l0mXPkIoAjOSq1GLNt150cAC0GNgAFTkA9YAGs58xSAzY8BIgCYt1es1aIINwAMnS8AEZQdLYOABTsAJROSm6qRGTq2n56gdwMMABmOLE0APqa3CJipGVg3ABOWADmxjhJCikqHhqkmb66AbTl3KJY-OKcwqITo+NSZXbJLsruasjefTr+bGXq3ADuMFBNMEJVE4fHMPOli3LaRycIKKAF9RC8SGQgOBBI6h0QG8Pv9qL8kJ5AcDPogAMxgv6IAAsYLoWAYbEMaDg4Kh7xhKJ+iIArKj0Zi6Njcc5oUh4USkKSQOEYGAoEgALSw74MOgshgABRWaUCjRaOBA-W2uU4+BwdElPzRGMCWJxf3uciAA
\begin{tikzcd}
V\times\cdots\times V \arrow[r]                                                           & \mathcal{T}^{k} \arrow[r]                      & \Lambda^{k}(V)            \\
{\left(v_1,\dots,v_n\right)} \arrow[r, maps to] \arrow[rr, "\iota"', maps to, bend right] & v_1\otimes\cdots\otimes v_k \arrow[r, maps to] & v_1\wedge\cdots\wedge v_k
\end{tikzcd}
        \end{center}
    \end{enumerate}
    implying that $\iota$ is multilinear.
  \item We have
    \begin{align*}
      t\left(v_1 + c\tilde{v}_1,\dots,v_k\right) &= T\circ \iota \left(v_1 + c\tilde{v}_1,\dots,v_k\right)\\
                                                     &= T\left(\left(v_1 + c\tilde{v}_1\right)\wedge v_2\wedge\cdots\wedge v_k\right)\\
                                                     &= T\left(v_1\wedge v_2\wedge\cdots\wedge v_k\right) + cT\left(\tilde{v}_1\wedge v_2\wedge\cdots\wedge v_k\right)\\
                                                     &= T\left(\iota\left(v_1,\dots,v_k\right)\right) + cT\left(\iota\left(\tilde{v}_1,\dots,v_k\right)\right)\\
                                                     &= t\left(v_1,\dots,v_k\right) + ct\left(\tilde{v}_1,\dots,v_k\right).
    \end{align*}
    Let $v_i = v_j$ with $i\neq j$. Then,
    \begin{align*}
      t\left(v_1,\dots,v_k\right) &= T\left(\iota\left(v_1,\dots,v_k\right)\right)\\
                                  &= T\left(0\right)\\
                                  &= 0,
    \end{align*}
    as $\iota$ is alternating, meaning $t$ is also alternating.
  \item Let $t\in \Alt^{k}\left(V;W\right)$. We have
    \begin{align*}
      t: V\times\cdots\times V\rightarrow W
    \end{align*}
    and $t\in \Hom_{F}\left(V,\dots,V;W\right)$. By the universal property of the tensor product, we have
    \begin{align*}
      T: \mathcal{T}^{k}\left(V\right)\rightarrow W,
    \end{align*}
    with $T\left(v_1\otimes\cdots\otimes v_k\right) = t\left(v_1,\dots,v_k\right)$.\newline

    We have
    \begin{align*}
      T|_{\mathcal{A}_{k}\left(V\right)} = 0
    \end{align*}
    because $T$ agrees with $t$ and $t$ is alternating.
  \end{proof}
  \begin{example}
    Let $V$ be a $F$-vector space with $\Dim_{F}\left(V\right) = 1$. Let $v\neq 0_V$, so $\mathcal{B} = \set{v}$ is a basis for $V$.\newline

    Consider $\Lambda^{k}\left(V\right)$. Elements in this set are finite sums
    \begin{align*}
      \omega &= \sum_{i\in I}\omega_i,
    \end{align*}
    where
    \begin{align*}
      \omega_i &= a_{1i}v\wedge\cdots\wedge a_{ki} v
    \end{align*}
    for some $a_{ji}\in F$, or
    \begin{align*}
      \omega_i &= a_{1i}\cdots a_{ki}\left(v\wedge\cdots\wedge v\right).
    \end{align*}
    If $k\geq 2$, then we have $v\wedge v\in \omega_i$, so $\omega_i = 0$. We have
    \begin{align*}
      \Lambda^{k}\left(V\right)&= 0 \tag*{$k\geq 2$}\\
      \lambda^{k}\left(V\right) &= V \tag*{$k=1$}\\
      \lambda^{k}\left(V\right) &= F. \tag*{$k=0$}
    \end{align*}
    Now, we let $V$ be a $2$-dimensional $F$-vector space, with basis $\mathcal{B} = \set{v_1,v_2}$.\newline

    Let $k = 2$. A typical element in $\Lambda^{2}\left(V\right)$ is a finite sum
    \begin{align*}
      \omega &= \sum_{i\in I}\omega_i\\
             \omega_i &= \left(a_iv_1 + b_iv_2\right)\wedge \left(c_iv_1 + d_iv_2\right)
    \end{align*}
    for some $a_i,b_i,c_i,d_i\in F$. Then, we have
    \begin{align*}
      \omega_i &= a_iv_1\wedge c_iv_1 + a_iv_1 \wedge d_iv_2 + b_iv_2\wedge c_iv_1 + b_iv_2 \wedge d_iv_2\\
               &= \left(a_id_i - b_ic_i\right)\left(v_1\wedge v_2\right).
    \end{align*}
    Thus, we have
    \begin{align*}
      \Lambda^{2}\left(V\right) &= \Span\set{v_1\wedge v_2}
    \end{align*}
    is a $1$-dimensional vector space.\newline

    Likewise, if $\Dim_{F}\left(V\right) = 3 $, then
    \begin{align*}
      \Lambda^{0}\left(V\right) \cong F\\
      \Lambda^{1}\left(V\right) \cong V\\
      \Lambda^{2}\left(V\right)\text{ has basis } \set{v_1\wedge v_2, v_1\wedge v_3,v_2\wedge v_3}\\
      \Lambda^{3}\left(V\right)\text{ has basis } \set{v_1\wedge v_2\wedge v_3}\\
      \lambda^{k}\left(V\right) = 0 \text{ for all $k \geq 4$}
    \end{align*}
    
%    We will show that
%    \begin{align*}
%      \Lambda^{k}\left(V\right)\hookrightarrow \mathcal{T}^{k}\left(V\right).
%    \end{align*}
%    Note that $\Lambda^{k}\left(V\right)$ is a quotient, so we have the following diagram
%    \begin{center}
%      % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12AZOgWwCModAHrAA1gF8AFADUAlCAml0mXPkIoAjOSq1GLNp150cACwDGjYABUJoybIVKV2PASIAmHdXrNWiDm4+QRFxaXkAAgBeCKMTCytbe3C5AHo4s0sGYABBCQB9MMdFXRgoAHN4IlAAMwAnCF4kMhAcCCRtVrosBjZTCAgxRWUQesaO6jakD2dRhqbEFqnELxB+GDAoJABmYgkKCSA
%\begin{tikzcd}
%\Lambda^{k}(V) \arrow[r, hook] \arrow[rr, bend left] & \mathcal{T}^{k}(V) \arrow[r] & \Lambda^{k}(V) = \mathcal{T}^{k}(V)/\mathcal{A}_{k}(V)
%\end{tikzcd}
%    \end{center}
  \end{example}
    \begin{theorem}
      Let $\Dim_{F}\left(V\right) = n$, with basis $\set{v_1,\dots,v_n}$. For $1 \leq k \leq n$, then
      \begin{align*}
        \mathcal{B}_{k} &= \set{v_{i_1}\wedge\cdots\wedge v_{i_k} | 1 \leq i_1 < \cdots < i_k \leq n}
      \end{align*}
      forms a basis for $\Lambda^{k}\left(V\right)$, and for $k > n$, $\Lambda^{k}\left(V\right) = 0$.\newline

      In particular, for $1 \leq k \leq n$,
      \begin{align*}
        \Dim_{F}\left(\Lambda^{k}\left(V\right)\right) &= {n\choose k}.
      \end{align*}
    \end{theorem}
    \begin{proof}
      Recall that
      \begin{align*}
        \mathcal{C} &= \set{v_{i_1}\otimes\cdots\otimes v_{i_k}}
      \end{align*}
      is a basis for $\mathcal{T}^{k}\left(V\right)$. The projection of these vectors, $\set{v_{i_1}\wedge\cdots\wedge v_{i_k}}$ is a spanning set for $\Lambda^{k}\left(V\right)$.\newline

      However, we can order the indices using the property that $v\wedge w = -w\wedge v$. Thus, $\mathcal{B}_k$ is spanning.\newline

      We want to show that $\Lambda^{k}\left(V\right) \hookrightarrow \mathcal{T}^{k}\left(V\right)$, and elements of $\mathcal{B}_k$ map to basis elements.\newline

      In order to do this, we must find an alternating map
      \begin{align*}
        t: V\times\cdots\times V \rightarrow \mathcal{T}^{k}\left(V\right).
      \end{align*}
      Suppose we have
      \begin{align*}
        0_{\Lambda^{k}\left(V\right)} &= \sum_{i_1,\dots,e_k\in I}c_{i_{1},\dots,i_{k}}\left(v_{i_1}\wedge\cdots\wedge v_{i_k}\right).
      \end{align*}
      Define
      \begin{align*}
        t_k: V\times\cdots V &\rightarrow \mathcal{T}^{k}\left(V\right)\\
        \left(\tilde{v}_1,\dots,\tilde{v}_k\right) &\mapsto \sum_{\sigma\in S_{k}}\sgn\left(\sigma\right) \tilde{v}_{\sigma(1)}\otimes\cdots\otimes \tilde{v}_{\sigma(k)}.
      \end{align*}
      Note that for
      \begin{align*}
        \Delta_{k} &= \prod_{1\leq i\leq j \leq k}\left(x_{i} - x_{j}\right)\\
        \sigma\left(\Delta_{k}\right) &= \prod_{1i\leq j \leq k}\left(x_{\sigma(i)} - x_{\sigma(j)}\right)\\
                                      &= \pm \Delta_{k},
      \end{align*}
      where $\sgn\left(\sigma\right)$ denotes the plus or minus sign on $\Delta_k$. Then, we have
      \begin{align*}
        t_{k}\left(\tilde{v}_1 + c\tilde{v}_1',\tilde{v}_2,\dots,\tilde{v}_k\right) &= \sum_{\sigma\in S_k}\sgn\left(\sigma\right)\left(\tilde{v}_{\sigma(1)} + c\tilde{v}_{\sigma(1)}'\right)\otimes \tilde{v}_{\sigma(2)}\otimes\cdots\otimes \tilde{v}_{\sigma(k)}\\
        &= \sum_{\sigma\in S_k}\sgn\left(\sigma\right)\left(\left(\tilde{v}_{\sigma(1)}\right)\otimes \tilde{v}_{\sigma(2)}\otimes\cdots\otimes \tilde{v}_{\sigma(k)}  + c\left(\tilde{v}_{\sigma(1)}'\otimes \tilde{v}_{\sigma(2)}\otimes\cdots\otimes \tilde{v}_{\sigma(k)}\right)\right)\\
        &= \sum_{\sigma\in S_k}\sgn\left(\sigma\right)\left(\tilde{v}_{\sigma(1)}\right)\otimes \tilde{v}_{\sigma(2)}\otimes\cdots\otimes \tilde{v}_{\sigma(k)} \\
        &+ c\sum_{\sigma\in S_k}\sgn\left(\sigma\right)\left(\tilde{v}_{\sigma(1)}'\otimes \tilde{v}_{\sigma(2)}\otimes\cdots\otimes \tilde{v}_{\sigma(k)}\right)\\
        \\
        &= t\left(\tilde{v}_{1},\dots,\tilde{v}_{k}\right) + ct\left(\tilde{v}_1',\dots,\tilde{v}_{k}\right).
      \end{align*}
      Suppose $\tilde{v}_{i} = \tilde{v}_{i+1}$. We want to show that $t_{k}\left(\tilde{v}_{1},\dots\tilde{v}_{k}\right) = 0$.\newline

      Set $N_i = \left\langle\left(i,i+1\right)\right\rangle\leq S_k$. We can write
      \begin{align*}
        S_k &= \bigsqcup_{\sigma\in S_k}N_i\sigma\\
            &= \bigsqcup \set{\left(i,i+1\right),\left(i,i+1\right)\sigma}.
      \end{align*}
      Thus, we have
      \begin{align*}
        \sum_{\sigma\in S_{k}}\sgn\left(\sigma\right)t\left(\tilde{v}_{\sigma(1)},\dots,\tilde{v}_{\sigma(k)}\right) &= \sum_{\sigma\in S_k/N_{i}}\sgn\left(\sigma\right)t\left(\tilde{v}_{\sigma(1)},\dots,\tilde{v}_{\sigma(k)}\right) \\
                                                                                                                     &+ \sgn\left(\left(i,i+1\right)\sigma\right)t\left(\tilde{v}_{\left(i,i+1\right)\sigma(1)},\dots,\tilde{v}_{\left(i,i+1\right)\sigma(k)}\right)\\
                                                                                                                     \\
                             &= \sum_{\sigma\in S_k/N_{i}}\sgn\left(\sigma\right)t\left(\tilde{v}_{\sigma(1)},\dots,\tilde{v}_{\sigma(k)}\right) \\
                             &- \sgn\left(\sigma\right)t\left(\tilde{v}_{\left(i,i+1\right)\sigma(1)},\dots,\tilde{v}_{\left(i,i+1\right)\sigma(k)}\right)
      \end{align*}
      Note that
      \begin{align*}
        \left(i,i+1\right)\sigma(j) &= \begin{cases}
          \sigma(j) & \sigma(j)\neq i,i+1\\
          i+1 & \sigma(j) = i\\
          i & \sigma(j) = i+1
        \end{cases}.
      \end{align*}
      Suppose $\sigma(1) = i$ and $\sigma(2) = i+1$. Then,
      \begin{align*}
        t\left(\tilde{v}_{\left(i,i+1\right)\sigma(1)},\dots,\tilde{v}_{\left(i,i+1\right)\sigma(k)}\right) &= t\left(\tilde{v}_{i+1},\tilde{v}_{i},\dots,\tilde{v}_{\sigma(k)}\right)\\
                                                                                                            &= t\left(\tilde{v}_{i},\tilde{v}_{i+1},\dots,\tilde{v}_{k}\right)\\
                                                                                                            &= t\left(\tilde{v}_{\sigma(1)},\dots,\tilde{v}_{\sigma(k)}\right).
      \end{align*}
      In other words, we get the subtraction equal to zero for each $\sigma$.\newline

      Since we have an alternating map, we can use the universal property for exterior products to get our linear map
      \begin{align*}
        T_{k}:\Lambda^{k}\left(V\right)&\rightarrow \mathcal{T}^{k}\left(V\right)\\
        \tilde{v}_{1}\wedge\cdots\wedge\tilde{v}_{k} &\mapsto \sum_{\sigma\in S_k}\sgn\left(\sigma\right)\left(\tilde{v}_{\sigma(1)}\otimes\cdots\otimes \tilde{v}_{\sigma(k)}\right).
      \end{align*}
      Thus, 
      \begin{align*}
        0_{\Lambda^{k}\left(V\right)} &= \sum_{i_1,\dots,i_k \in I}c_{i_1,\dots,i_k}\left(v_{i_1}\wedge\cdots\wedge v_{i_k}\right)\\
        0_{\mathcal{T}^{k}\left(V\right)} &= \sum_{i_1,\dots,i_k\in I}c_{i_1,\dots,i_k}T_k\left(v_{i_1}\wedge\cdots\wedge v_{i_k}\right)\\
                                          &= \sum_{i_1,\dots,i_k\in I}c_{i_1,\dots,i_k}\sum_{\sigma\in S_k}\sgn\left(\sigma\right)v_{\sigma\left(i_1\right)}\otimes\cdots\otimes v_{\sigma\left(i_k\right)}
      \end{align*}
      is a sum of a subset of a basis for $\mathcal{T}^{k}\left(V\right)$, so $c_{i_1,\dots,i_k} = 0$ for each $k$.
    \end{proof}
    We want to make use of the fact that $\Dim_{F}\left(\Lambda^{n}\left(V\right)\right) = 1$ for $\Dim_{F}\left(V\right) = 1$, with basis $v_1\wedge\cdots\wedge v_n$ for $\mathcal{B} = \set{v_1,\dots,v_n}$ a basis for $V$.
    \begin{proposition}
      Let $T\in \Hom_{F}\left(V,W\right)$. There is a unique linear map $\Lambda^{k}\left(T\right) \in \Hom_{F}\left(\Lambda^{k}\left(V\right),\Lambda^{k}\left(W\right)\right)$ such that
      \begin{align*}
        \Lambda^{k}\left(T\right)\left(v_1\wedge\cdots\wedge v_k\right) &= T\left(v_1\right)\wedge\cdots\wedge T\left(v_k\right)
      \end{align*}
      for $v_1,\dots,v_k\in V$. Moreover,
      \begin{align*}
        \Lambda^{k}\left(\id_{V}\right) &= \id_{\Lambda^{k}\left(V\right)},
      \end{align*}
      and if $S\in \Hom_{F}\left(U,V\right)$, then
      \begin{align*}
        \Lambda^{k}\left(T\circ S\right) &= \Lambda^{K}\left(T\right)\circ \Lambda^{k}\left(S\right).
      \end{align*}
    \end{proposition}
    \begin{proof}
      Define $t: V\times\cdots\times V\rightarrow \Lambda^{k}\left(W\right)$ by $\left(v_1,\dots,v_k\right)\mapsto T\left(v_1\right)\wedge\cdots T\left(v_{k}\right)$. Since $T$ is linear, $t$ is multilinear, and is alternating by the definition of the wedge product.\newline

      By the universal property, we have a linear map
      \begin{align*}
        \Lambda^{k}\left(T\right): \Lambda^{k}\left(V\right)&\rightarrow  \Lambda^{k}\left(W\right)\\
        v_1\wedge\cdots\wedge v_k &\mapsto T\left(v_1\right)\wedge\cdots\wedge T\left(v_k\right).
      \end{align*}
    \end{proof}
    \begin{example}
      Let $V = F^3$, $\mathcal{E}_3 = \set{e_1,e_2,e_3}$. Let $T$ be the linear map such that
      \begin{align*}
        \left[T\right]_{\mathcal{E}_3} &= \begin{pmatrix}1 & 0 & 2 \\ 0 & 2 &1 \\ 3 & -1 & 1\end{pmatrix}.
      \end{align*}
      Consider the map
      \begin{align*}
        \Lambda^{2}\left(T\right): \Lambda^{2}\left(F^3\right)\rightarrow \Lambda^{2}\left(F^3\right).
      \end{align*}
      We know a basis for $\Lambda^{2}\left(F^3\right)$ is
      \begin{align*}
        \mathcal{B} &= \set{e_1\wedge e_2,e_1\wedge e_3,e_2\wedge e_3}.
      \end{align*}
      We consider the matrix
      \begin{align*}
        \left[\Lambda^{2}\left(T\right)\right]_{\mathcal{B}}.
      \end{align*}
      Consider
      \begin{align*}
        \Lambda^{2}\left(T\right)\left(e_1\wedge e_2\right) &= T\left(e_1\right)\wedge T\left(e_{2}\right)\\
                                                            &= \left(e_1\wedge 3e_3\right) \wedge \left(2e_2 - e_3\right)\\
                                                            &= 2\left(e_1\wedge e_2\right) - \left(e_1\wedge e_3\right) - 6\left(e_2\wedge e_3\right).\\
        \Lambda^{2}\left(T\right)\left(e_1\wedge e_3\right) &= T\left(e_1\right)\wedge T\left(e_3\right)\\
                                                            &= \left(e_1 + 3e_3\right)\wedge \left(2e_1 + e_2 + e_3\right)\\
                                                            &= e_1\wedge e_2 - 5 e_1\wedge e_3 - 3e_2\wedge e_3.\\
        \Lambda^{2}\left(T\right)\left(e_2\wedge e_3\right) &= \left(2e_2 - e_3\right)\wedge \left(2e_1 + e_2 + e_3\right)\\
                                                            &= -4e_1\wedge e_2 +2e_1\wedge e_3 + 3e_2\wedge e_3
      \end{align*}
      Thus, we have
      \begin{align*}
        \left[\Lambda^{2}\left(T\right)\right]_{\mathcal{B}} &= \begin{pmatrix}2 & 1&-4 \\ -1 & -5&2 \\ 6 &-3 & 3\end{pmatrix}
      \end{align*}
      
    \end{example}
    Given $T\in \Hom_{F}\left(V,V\right)$, we have 
    \begin{align*}
      \Lambda^{n}\left(T\right):\Lambda^{n}\left(V\right)&\rightarrow \Lambda^{n}\left(V\right)\\
      v_1\wedge\cdots\wedge v_n &\mapsto \underbrace{\alpha \left(v_1\wedge\cdots\wedge v_n\right)}_{T\left(v_1\right)\wedge\cdots T\left(v_n\right)}
    \end{align*}
    \begin{definition}
      For $T\in \Hom_{F}\left(V,V\right)$, we define $\det(T)$ to be such that
      \begin{align*}
        \Lambda^{n}\left(T\right)\left(\omega\right) &= \det(T) \omega
      \end{align*}
      for any $\omega\in \Lambda^{n}\left(V\right)$.
    \end{definition}
    \begin{example}
      Let $V = F^2$, $\mathcal{E}_2 = \set{e_1,e_2}$. Let $T\in \Hom_{F}\left(V,V\right)$, we have
      \begin{align*}
        \left[T\right]_{\mathcal{E}_2} &= \begin{pmatrix}a & b \\ c & d\end{pmatrix}.
      \end{align*}
      Then,
      \begin{align*}
        \Lambda^{2}\left(T\right)\left(e_1\wedge e_2\right) &= \left(ae_1 + ce_2\right)\wedge \left(be_1 + de_2\right)\\
                                                            &= \underbrace{\left(ad-bc\right)}_{\det(T)}\left(e_1\wedge e_2\right).
      \end{align*}
    \end{example}
    \begin{exercise}
      Verify that we recover the cofactor expansion on a $3\times 3$ matrix.
    \end{exercise}
    \begin{lemma}
      Let $S,T\in \Hom_{F}\left(V,V\right)$. We have $\det\left(T\circ S\right) = \det(T)\det(S)$.
    \end{lemma}
    \begin{proof}
      We have
      \begin{align*}
        \det(T\circ S)\left(v_1\wedge\cdots\wedge v_n\right) &= \Lambda^{n}\left(T\circ S\right)\left(v_1\wedge\cdots\wedge v_n\right)\\
                                                             &= \Lambda^{n}\left(T\right)\circ \Lambda^{n}\left(S\right)\left(v_1\wedge\cdots\wedge v_n\right)\\
                                                             &= \Lambda^{n}\left(T\right)\left(\det(S)\left(v_1\wedge\dots\wedge v_n\right)\right)\\
                                                             &= \det(S) \Lambda^{n}\left(T\right)\left(v_1\wedge\cdots\wedge v_n\right)\\
                                                             &= \det(S)\det(T) \left(v_1\wedge\cdots\wedge v_n\right),
      \end{align*}
      meaning $\det\left(T\circ S\right) = \det(T)\det(S)$.
    \end{proof}
    Our ultimate goal is to prove that, for a given $A\in \Mat_{n}\left(F\right)$, that
    \begin{align*}
      \det\left(T_{A}\right) &= \det\left(A\right),
    \end{align*}
    where the determinant of the matrix defined by the cofactor expansion.\newline

    We view
    \begin{align*}
      A &= \begin{pmatrix}a_{11} & \cdots & a_{nn} \\ a_{21} & \cdots & a_{2n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn}\end{pmatrix}\\
        &\leftrightarrow \left( \begin{pmatrix}a_{11}\\a_{21}\\\vdots\\a_{n1}\end{pmatrix},\dots, \begin{pmatrix}a_{1n}\\a_{2n}\\\vdots\\a_{nn}\end{pmatrix}\right)
    \end{align*}
    as an element of $F^{n}\times\cdots \times F^{n}$, where $F^{n}$ are the column vectors of $A$.
    \begin{theorem}
      We have $\det\in \operatorname{Alt}^{n}\left(F^{n},F\right)$, with $\det\left(I_n\right) = 1$.
    \end{theorem}
    \begin{proof}
      Let $\mathcal{E}_{n} = \set{e_1,\dots,e_n}$ be the standard basis of $F^n$, and write
      \begin{align*}
        w_i &= a_{1i}e_{1} + \cdots + a_{ni}e_{n}
      \end{align*}
      for $i=1,\dots,n$. We let
      \begin{align*}
        w &= b_{11}e_1 + \cdots + b_{n1}e_n.
      \end{align*}
      Let $c\in F$.\newline

      We want to show that
      \begin{align*}
        \det\left(w_1 + cw,w_2,\dots,w_n\right) &= \det\left(w_1,w_2,\dots,w_n\right) + c\det\left(w,w_1,\dots,w_n\right).
      \end{align*}
      Define
      \begin{align*}
        T_1: F^n &\rightarrow F^n\\
        e_i&\mapsto w_i,
      \end{align*}
      with
      \begin{align*}
        \left[T_1\right]_{\mathcal{E}_n} &= \begin{pmatrix}a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn}\end{pmatrix}.
      \end{align*}
      We define
      \begin{align*}
        T_{2}: F^{n} &\rightarrow F^n\\
        e_{1} &\mapsto w\\
        e_{i} &\mapsto w_{i},
      \end{align*}
      with
      \begin{align*}
        \left[T_2\right]_{\mathcal{E}_n} &= \begin{pmatrix}b_{11} & a_{12} & \cdots & a_{1n} \\ b_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots &\vdots \\ b_{n1} & a_{n2} & \cdots & a_{nn}\end{pmatrix}.
      \end{align*}
      Finally, we define
      \begin{align*}
        T_3: F^n &\rightarrow F^n\\
        e_{1} &\mapsto w_1 + cw_2\\
        e_{i} &\mapsto w_i,
      \end{align*}
      with
      \begin{align*}
        \left[T_3\right]_{\mathcal{E}_n} &= \begin{pmatrix}a_{11} + cb_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} + cb_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots &\vdots \\ a_{n1} + cb_{n1} & a_{n2} & \cdots & a_{nn}\end{pmatrix}.
      \end{align*}
      In particular,
      \begin{align*}
        \det\left(T_1\right) &= \det\left(w_1,\dots,w_n\right)\\
        \det\left(T_2\right) &= \det\left(w,w_2,\dots,w_n\right)\\
        \det\left(T_3\right) &= \det\left(w_1 + cw,w_2,\dots,w_n\right).
      \end{align*}
      We want to show that
      \begin{align*}
        \det\left(T_3\right) &= \det\left(T_1\right) + c\det\left(T_2\right).
      \end{align*}
      Note that
      \begin{align*}
        \det\left(T_3\right)\left(e_1\wedge\cdots\wedge e_n\right) &= \Lambda^{n}\left(T_3\right)\left(e_1\wedge\cdots\wedge e_n\right)\\
                                                                   &= T_3\left(e_1\right)\wedge T_3\left(e_2\right)\cdots\wedge T_{3}\left(e_n\right)\\
                                                                   &= \left(w_1 + cw_2\right)\wedge w_2\wedge \cdots \wedge w_n\\
                                                                   &= w_1\wedge w_2\wedge\cdots\wedge w_n + c\left(w\wedge w_2 \wedge \cdots \wedge w_n\right)\\
                                                                   &= T_1\left(e_1\right)\wedge T_1\left(e_2\right)\wedge\cdots T_1\left(e_n\right) + c\left(T_2\left(e_1\right)\wedge T_2\left(e_2\right)\wedge\cdots T_2\left(e_n\right)\right)\\
                                                                   &= \Lambda^{n}\left(T_1\right)\left(e_1\wedge\cdots\wedge e_n\right) + c\Lambda^{n}\left(T_2\right)\left(e_1\wedge\cdots\wedge e_n\right)\\
                                                                  &= \det\left(T_1\right)\left(e_1\wedge\cdots\wedge e_n\right) + c\det\left(T_2\right)\left(e_1\wedge\cdots\wedge e_n\right)\\
                                                                  &= \left(\det\left(T_1\right) + c\det\left(T_2\right)\right)\left(e_1\wedge\cdots\wedge e_n\right).
      \end{align*}
      Thus, $\det\left(T_3\right) = \det\left(T_1\right) + c\det\left(T_2\right)$.\newline

      Let $w_i = w_j$ for some $i\neq j$.
      \begin{align*}
        \det\left(w_1,\dots,w_n\right)\left(e_1\wedge\cdots \wedge e_n\right) &= \det\left(T_1\right)\left(e_1\wedge\cdots\wedge e_n\right)\\
                                                                              &= \Lambda^{n}\left(T_1\right)\left(e_1\wedge\cdots\wedge e_n\right)\\
                                                                              &= T_1\left(e_1\right)\wedge\cdots\wedge T_1\left(e_n\right)\\
                                                                              &= w_1\wedge\cdots\wedge w_n\\
                                                                              &= 0_{\Lambda^{n}\left(V\right)}.
      \end{align*}
      Thus, $\det$ is an alternating map.\newline

      We have
      \begin{align*}
        \det\left(I_n\right)\left(e_1\wedge\cdots\wedge e_n\right) &= \Lambda^{n}\left(I_n\right)\left(e_1\wedge\cdots\wedge e_n\right)\\
                                                                   &= e_1\wedge\cdots\wedge e_n,
      \end{align*}
      so $\det\left(I_n\right) = 1$.
    \end{proof}
    \begin{lemma}
      Let $t\in \operatorname{Alt}^{k}\left(V,F\right)$. Then,
      \begin{align*}
        t\left(v_1,\dots,v_k\right) &= -t\left(v_{1},\dots,v_{i+1},v_{i},\dots,v_{k}\right).
      \end{align*}
    \end{lemma}
    \begin{proof}
      Define
      \begin{align*}
        \psi\left(x,y\right) &= T\left(v_1,\dots,v_{i-1},x,y,v_{i+2},\dots,v_{k}\right).
      \end{align*}
      It is enough to show that $\psi\left(x,y\right) = -\psi\left(y,x\right)$. Since $t$ is alternating,
      \begin{align*}
        0 &= \psi\left(x+y,x+y\right)\\
          &= \psi\left(x,x\right) + \psi\left(x,y\right) + \psi\left(y,x\right) + \psi\left(y,y\right)\\
          &= \psi\left(x,y\right) + \psi\left(y,x\right)\\
        \psi\left(x,y\right) &= -\psi\left(y,x\right).
      \end{align*}
    \end{proof}
    \begin{lemma}\hfill
      \begin{enumerate}[(1)]
        \item Let $t\in \Alt^{k}\left(V,F\right)$. Then,
        \begin{align*}
          t\left(v_{\sigma(1)},v_{\sigma(2)},\dots,v_{\sigma(n)}\right) &= \sgn\left(\sigma\right)t\left(v_1,\dots,v_k\right).
        \end{align*}
        \item If $v_i$ is replaced by $v_i + cv_j$ for any $i\neq j$ and $c\in F$, then the value of $t$ is unchanged.
      \end{enumerate}
    \end{lemma}
  \begin{proposition}
    Let $t\in \Alt^{k}\left(V,F\right)$. Suppose for some $v_1,\dots,v_n\in V$ and $w_1,\dots,w_n\in V$, $a_{ij}\in F$, we have
    \begin{align*}
      w_{i} &= a_{1i}v_i + \cdots + a_{ni}v_n.
    \end{align*}
    Then,
    \begin{align*}
      t\left(w_1,\dots,w_n\right) &= \sum_{\sigma\in S_n} \sgn\left(\sigma\right)a_{\sigma(1)1}\cdots a_{\sigma(n)n}t\left(v_{\sigma(1)},\dots,v_{\sigma(n)}\right).
    \end{align*}
  \end{proposition}
  \begin{proof}
    Expanding,
    \begin{align*}
      t\left(w_1,\dots,w_n\right) &= t\left(\sum_{j=1}^{n}a_{j1}e_j,\dots,a_{jn}e_{j}\right)\\
                                  &= a_{i_1 1}\cdots a_{i_nn}t\left(v_{i_1},\dots,v_{i_n}\right),
    \end{align*}
    where all the $i_j$ are distinct. We have a bijection between the possible tuples $\left(i_1,\dots,i_n\right)$ and all possible tuples $\left(\sigma(1),\dots,\sigma\left(n\right)\right)$ with $\sigma\in S_n$. From this, we get
    \begin{align*}
      t\left(w_1,\dots,w_n\right) &= \sum_{\sigma\in S_n} a_{\sigma(1)1}\cdots a_{\sigma(n)n}t\left(v_{\sigma(1)},\dots,v_{\sigma(n)}\right)\\
                                  &= \sum_{\sigma\in S_n}a_{\sigma(1)1}\cdots a_{\sigma(n)n}\sgn\left(\sigma\right)t\left(v_1,\dots,v_n\right).
    \end{align*}
  \end{proof}
  \begin{corollary}
    The determinant is the unique function in $\Alt^{n}\left(F^n,F\right)$ with $\det\left(I_n\right) = 1$.
  \end{corollary}
  \begin{proof}[]
    Let $\mathcal{E}_n = \set{e_1,\dots,e_n}$ be the standard basis, $t\in \Alt^{n}\left(F^n,F\right)$ with $t\left(I_n\right) = 1$. This is the same as saying $t\left(e_1,\dots,e_n\right) = 1$.\newline

    Let $v_1,\dots,v_n\in V$, and write
    \begin{align*}
      v_i &= a_{1i}e_1 + \cdots a_{ni}e_n.
    \end{align*}
    We have
    \begin{align*}
      t\left(v_1,\dots,v_n\right) &= \sum_{\sigma\in S_n}\sgn\left(\sigma\right)a_{\sigma(1)1}\cdots a_{\sigma(n)n}t\left(e_1,\dots,e_n\right)\\
                                  &= \sum_{\sigma\in S_n}\sgn\left(\sigma\right)a_{\sigma(1)1}\cdots a_{\sigma(n)n}\\
                                  &= \sum_{\sigma\in S_n}\sgn\left(\sigma\right)a_{\sigma(1)1}\cdots a_{\sigma(n)n}\det\left(e_1,\dots,e_n\right)\\
                                  &= \det\left(v_1,\dots,v_n\right),
    \end{align*}
    meaning $t = \det$.
  \end{proof}
  We can now determine the trace through the exterior product.\newline

  Let $T\in \Hom_{F}\left(V,V\right)$. Define
  \begin{align*}
    t: V\times\cdots\times V &\rightarrow \Lambda^{n}\left(V\right)\\
    \left(v_1,\dots,v_n\right) &\mapsto \sum_{j=1}^{n}\left(v_{1}\wedge\cdots\wedge v_{j-1}\wedge T\left(v_j\right)\wedge v_{j+1} \wedge \cdots \wedge v_n\right).
  \end{align*}
  \begin{exercise}[]
    Show this map is multilinear.
  \end{exercise}
  We want to show this map is alternating. Suppose $v_1 = v_2$.
  \begin{align*}
    t\left(v_1,v_2,\dots,v_n\right) &= T\left(v_1\right)\wedge v_1\wedge \cdots \wedge v_n + v_1\wedge T\left(v_2\right)\wedge\cdots\wedge v_n + \sum_{j=3}^{n}v_1\wedge v_2\wedge\cdots\wedge T\left(v_j\right)\wedge \cdots \wedge v_n\\
                                    &= T\left(v_1\right)\wedge v_2\wedge\cdots\wedge v_n + v_1\wedge T\left(v_2\right)\wedge\cdots\wedge v_n\\
                                    &= T\left(v_1\right)\wedge v_2\wedge\cdots\wedge v_n  - T\left(v_2\right)\wedge v_1\wedge\cdots\wedge v_n\\
                                    &= 0.
  \end{align*}
  Thus, $t\in \Alt^{n}\left(V,\Lambda^{n}\left(V\right)\right)$, meaning we have a map $\varphi_T\in \Hom_{f}\left(\Lambda^{n}\left(V\right),\Lambda^{n}\left(F\right)\right)$ such that
  \begin{align*}
    \varphi_{T}\left(v_1,\dots,v_n\right) &= \sum_{j=1}^{n}v_1\wedge\cdots\wedge T\left(v_j\right)\wedge\cdots\wedge v_n.
  \end{align*}
  Since $\Dim_{F}\left(\Lambda^{n}\left(V\right)\right) = 1$, we know that $\varphi_{T}$ is multiplication by a scalar.\newline

  We claim this scalar is $\tr\left(T\right)$. Let $A\in \Mat_{n}\left(F\right)$, with $T_A$ the corresponding linear transformation. Then,
  \begin{align*}
    \varphi_{T_A}\left(e_1\wedge\cdots\wedge e_n\right) &= \sum_{j=1}^{n}e_1\wedge\cdots\wedge T_{A}\left(e_j\right)\wedge\cdots\wedge e_n\\
                                                        &= \sum_{j=1}^{n} e_1\wedge\cdots \wedge \left(\sum_{i=1}^{n}a_{ij}e_i\right)\wedge\cdots \wedge e_n\\
                                                        &= \sum_{j=1}^{n}e_1\wedge\cdots\wedge \left(a_{jj}e_j\right)\wedge\cdots\wedge e_n\\
                                                        &= \sum_{j=1}^{n}a_{jj}\left(e_{1}\wedge\cdots\wedge e_n\right)\\
                                                        &= \left(\sum_{j=1}^{n}a_{jj}\right)\left(e_{1}\wedge\cdots\wedge e_{n}\right)\\
                                                        &= \tr\left(T_{A}\right)\left(e_1\wedge\cdots\wedge e_n\right).
  \end{align*}
  \section{Bilinear and Sesquilinear Forms}%
  Consider $V = \R^3$. We know that
  \begin{align*}
    \varphi: V\times V &\rightarrow \R\\
    \left(v,w\right) &\mapsto v\cdot w
  \end{align*}
  is such that $v\cdot v = \norm{v}^2$ and
  \begin{align*}
    \varphi\left(v_1 + cv_2,w\right) &= \varphi\left(v_1,w\right) + c\varphi\left(v_2,w\right),
  \end{align*}
  meaning $\varphi\in \Hom_{\R}\left(\R^3,\R^3;\R\right)$.\newline

  Turning our attention to $V = \C$ with $F = \C$, we have for $z = x + iy$,
  \begin{align*}
    \norm{z}^2 &= x^2 + y^2\\
               &= z \overline{z}.
  \end{align*}
  We define
  \begin{align*}
    \varphi: \C \times \C &\rightarrow \C\\
    \left(z,w\right) &\mapsto z\overline{w}.
  \end{align*}
  This allows us to have $\varphi\left(z,z\right) = z\overline{z} = \norm{z}^2$. We have
  \begin{align*}
    \varphi\left(z_1 + cz_2,w\right) &= \varphi\left(z_1,w\right) + c\varphi\left(z_2,w\right),
  \end{align*}
  but
  \begin{align*}
    \varphi\left(z,w_1 + cw_2\right) &= \varphi\left(z,w_1\right) + \overline{c}\varphi\left(z,w_2\right).
  \end{align*}
  This is not bilinear per se, but it's close.\newline

  In this case, we say $\varphi$ is an example of a sesquilinear form.
  \subsection{Basic Definitions and Facts}%
  \begin{definition}[Bilinear Form]
    A bilinear form is an element of the space $\Hom_{F}\left(V,V;F\right)$.
  \end{definition}
  \begin{example}[Bilinear Forms]\hfill
    \begin{enumerate}[(1)]
      \item The dot product on $F^n$.
      \item Let $A\in \Mat_{n}\left(F\right)$. Define
        \begin{align*}
          \varphi_{A}\left(v,w\right) &= v^{T}Aw
        \end{align*}
        for $v,w\in F^n$. We have $\varphi_{A}\in \Hom_{F}\left(F^n,F^n;F\right)$.
  \begin{exercise}
    Let $B\in \Mat_{n}\left(F\right)$. Define $\varphi$ on $V = F^n$ by taking
    \begin{align*}
      \varphi\left(v,w\right) &= \left(Bv\right)\cdot w.
    \end{align*}
    Show $\varphi\in \Hom_{F}\left(F^n,F^n;F\right)$. What is the relationship between $\varphi$ and $\varphi_{B}$.
  \end{exercise}
    \end{enumerate}
  \end{example}
  \begin{example}
    Let $V = \R^3$. Then, for $x,y,Z\in V$, recall that
    \begin{align*}
      \left\vert x\cdot \left(y\times z\right) \right\vert
    \end{align*}
    is the volume of the parallelepiped defined by $x,y,z$.\newline

    Fixing $x$, the map
    \begin{align*}
      \varphi_{x}\left(y,z\right) &= x\cdot \left(y\times z\right)
    \end{align*}
    is bilinear.
  \end{example}
  \begin{example}
    Let $p,q\in \Z_{\geq 0}$ with $p+q = n$. Set $V = F^n$.\newline

    Let $x,y\in V$. Define
    \begin{align*}
      \varphi_{p,q}\left(x,y\right) &= \sum_{j=1}^{p}x_jy_j - \sum_{j=p+1}^{n}x_jy_j.
    \end{align*}
    This is a bilinear form.\newline

    We denote the vector space with this bilinear form as $F^{p,q}$.\newline

    For instance, $\R^{3,1}$ is known as Minkowski space in the theory of relativity.
  \end{example}
  \begin{example}
    Let $V = F^{2n}$. Let $x,y\in V$.\newline

    We define
    \begin{align*}
      \varphi\left(x,y\right) &= \sum_{j=1}^{n}\left(x_{2j-1}y_{2j} - x_{2j}y_{2j-1}\right).
    \end{align*}
    This is a bilinear form.
  \end{example}
  \begin{definition}
    Let $\varphi\in \Hom_{F}\left(V,V;F\right)$. We say $\varphi$ is right non-degenerate if, given $w_0\in V$ such that
    \begin{align*}
      \varphi\left(v,w_0\right) = 0
    \end{align*}
    for every $v\in V$, then $w_0 = 0$.\newline

    Similarly, $\varphi$ is left non-degenerate if, given $v_0\in V$ such that
    \begin{align*}
      \varphi\left(v_0,w\right) = 0
    \end{align*}
    for every $w\in V$, then $v_0 = 0$.\newline

    If $\varphi$ is both left non-degenerate and right non-degenerate, we say $\varphi$ is non-degenerate.
  \end{definition}
  \begin{example}
    Let $\varphi\in \Hom_{F}\left(F^n,F^n;F\right)$ be the usual dot product. Suppose $\varphi$ is right degenerate --- i.e., there exists $w_0\in F^n$ such that $\varphi\left(v,w_0\right) = 0$ for all $v\in F^n$. In particular, we have 
    \begin{align*}
      \varphi\left(e_i,w_0\right) &= w_{0,i}\\
                                  &= 0,
    \end{align*}
    so $w_{0,i} = 0$ for each $i$. Thus, we must have $w_0 = 0$, so $\varphi$ is right non-degenerate.\newline

    Similarly, $\varphi$ is left non-degenerate.
  \end{example}
  \begin{exercise}
    Show that $\varphi_{p,q}$ is left and right non-degenerate.
  \end{exercise}
  \begin{example}
    Let $V = F^{3}$ and define $\varphi\left(x,y\right) = x_1y_1 + x_2y_2$ for $x = \left(x_1,x_2,x_3\right)$ and $y = \left(y_1,y_2,y_3\right)$.\newline

    It is the case that $\varphi$ is a bilinear form, but $\varphi$ is both left and right degenerate, as selecting either $x$ or $y$ to be $ v = \left(0,0,1\right)$, $\varphi\left(x,v\right) = 0$ for all $x$ and $\varphi\left(v,y\right) = 0$ for all $y$.
  \end{example}
  If $V$ is finite-dimensional, we will see that left and right non-degenerate are equivalent. However, if $V$ is infinite-dimensional, they are not equivalent.\footnote{In the sequence space $\ell_2$, using the left and right shift operators allows us to find this.}\newline

  Recall that we had $V\cong V'$, but that this isomorphism is not canonical. We will see that the isomorphisms for $V$ and $V'$ are in bijection with non-degenerate forms. This doesn't work in the infinite-dimensional case since $V$ is not necessarily isomorphic to $V'$.\newline

  Let $\varphi\in \Hom_{F}\left(V,V;F\right)$. Fix some element $v_0\in V$. The map
  \begin{align*}
    \varphi\left(\cdot,v_0\right)\in V',
  \end{align*}
  as $\varphi$ is bilinear, so $\varphi\left(\cdot,v_0\right)$ is linear. This gives a map
  \begin{align*}
    R_{\varphi}: V &\rightarrow V'\\
    v_0 &\mapsto \varphi\left(\cdot,v_0\right).
  \end{align*}
  We write this as $R_{\varphi}\left(v_0\right) = \varphi\left(\cdot,v_0\right)$.\newline

  Let $v_1,v_2\in V$, and $a\in F$. Then,
  \begin{align*}
    R_{\varphi}\left(v_1 + av_2\right)\left(w\right) &= \varphi\left(w,v_1 + av_2\right)\\
                                                     &= \varphi\left(w,v_1\right) + a\varphi\left(w,v_2\right)\\
                                                     &= R_{\varphi}\left(v_1\right)\left(w\right) + aR_{\varphi}\left(v_2\right)\left(w\right).
  \end{align*}
  Thus, we see that $R_{\varphi}\in \Hom_{F}\left(V,V'\right)$.\newline

  Similarly, we can have $L_{\varphi}\left(v\right) = \varphi\left(v,\cdot\right)$, so by a similar argument, we get
  \begin{align*}
    L_{\varphi}\in \Hom_{F}\left(V,V'\right).
  \end{align*}
  \begin{lemma}
    A bilinear form $\varphi$ is non-degenerate if and only if $L_{\varphi}$ and $R_{\varphi}$ are injections.
  \end{lemma}
  \begin{proof}
    Suppose $L_{\varphi}$ and $R_{\varphi}$ are injections. Suppose we have $w_0$ such that $\varphi\left(v,w_0\right) = 0$ for all $v\in V$. Thus,
    \begin{align*}
      0 &= \varphi\left(v,w_0\right)\\
        &= R_{\varphi}\left(w_0\right)\left(v\right)
    \end{align*}
    for all $v\in V$, so $R_{\varphi}\left(w_0\right)$ is the zero map. However, we said that $R_{\varphi}\left(w_0\right)$ is injective, $w_0 = 0$. Thus, $\varphi$ is right non-degenerate.\newline

    Similarly, if $\varphi\left(v_0,w\right) = 0$ for all $w\in V$, then $L_{\varphi}\left(v_0\right)$ is the zero map, so $v_0 = 0$. Thus, $\varphi$ is left non-degenerate.\newline

    Assume $L_{\varphi}$ or $R_{\varphi}$ is not injective. Let $R_{\varphi}$ be not injective. Then, there exists $w_0\in V$ such that
    \begin{align*}
      R_{\varphi}\left(w_0\right) &= 0_{V'},
    \end{align*}
    so $R_{\varphi}\left(w_0\right)\left(v\right) = 0$, so $\varphi\left(v,w_0\right) = 0$ for all $v\in V$, so $\varphi$ is right degenerate.
  \end{proof}
  \begin{corollary}
    If $V$ is finite-dimensional, then $\varphi$ is non-degenerate if and only if $L_{\varphi}$ and $R_{\varphi}$ are isomorphisms.
  \end{corollary}
  \begin{proof}
    If $\Dim_{F}\left(V\right) < \infty$, then $\Dim_{F}\left(V\right) = \Dim_{F}\left(V'\right)$, so injective is the same as bijective.
  \end{proof}
  \begin{definition}
    We define the left and right kernels of $\varphi$ to be
    \begin{align*}
      \ker_{R}\left(\varphi\right) &= \set{w\in V | \varphi\left(v,w\right) = 0 \text{ for all $v\in V$}}\\
      \ker_{L}\left(\varphi\right) &= \set{w\in V | \varphi\left(w,v\right) = 0\text{ for all $v\in V$}}
    \end{align*}
  \end{definition}
  \begin{theorem}
    Let $\Dim_{F}\left(V\right) < \infty$, $\varphi\in \Hom_{F}\left(V,V;F\right)$. The maps $R_{\varphi}$ and $L_{\varphi}$ are dual to each other. In other words, given
    \begin{align*}
      L_{\varphi}: V \rightarrow V',
    \end{align*}
    we consider the dual map
    \begin{align*}
      L_{\varphi}': V'' \rightarrow V'.
    \end{align*}
    If we identify $V\cong V''$ (where each $\hat{v}$ is identified with the element $v$), then $L_{\varphi}' = R_{\varphi}$, and similarly, $R_{\varphi}' = L_{\varphi}$.
  \end{theorem}
  \begin{proof}
    Recall that given $T\in \Hom_{F}\left(V,W\right)$, we have a dual map $T': W'\rightarrow V'$ defined by
    \begin{align*}
      T'\left(\varphi\right)\left(v\right) &= \varphi\left(T\left(v\right)\right).
    \end{align*}
    Recall the canonical isomorphism given by $v\mapsto \hat{v}$, where $\hat{v}\left(\varphi\right) = \varphi\left(v\right)$.\newline

    Let $v,w\in V$. We have
    \begin{align*}
      L_{\varphi}'\left(\hat{v}\right)\left(w\right) &= \hat{v}\left(L_{\varphi}\left(w\right)\right)\\
                                                     &= \hat{v}\left(L_{\varphi}\left(w,\cdot\right)\right)\\
                                                     &= L_{\varphi}\left(w,v\right)\\
                                                     &= \varphi\left(w,v\right)\\
                                                     &= R_{\varphi}\left(v\right)\left(w\right).
    \end{align*}
    Thus, $L_{\varphi}' = R_{\varphi}$.\newline

    Thus, if $V$ is finite dimensional, then $\varphi$ being non-degenerate is equivalent to $\varphi$ being left non-degenerate or right non-degenerate.
  \end{proof}
  \begin{lemma}
    Let $\Dim_{F}\left(V\right) < \infty$. There is a bijection between isomorphisms $V \rightarrow V'$ and non-degenerate bilinear forms.
  \end{lemma}
  \begin{proof}
    Given such a $\varphi$, we have $R_{\varphi}\in \Hom_{F}\left(V,V'\right)$. Since $\varphi$ is non-degenerate, $R_{\varphi}$ is an injective, so $R_{\varphi}$ is an isomorphism.\newline

    Suppose we have $T: V\rightarrow V'$ is an isomorphism. Define $\varphi\left(v,w\right) = T(w)(v)$. This is non-degenerate, as $T$ is an isomorphism.
  \end{proof}
  \begin{definition}[Conjugation]
    Let $F$ be a field, $\conj: F\rightarrow F$ a map such that
    \begin{enumerate}[(1)]
      \item $\conj\left(\conj\left(x\right)\right) = x$
      \item $\conj\left(x+y\right) = \conj\left(x\right) + \conj\left(y\right)$
      \item $\conj\left(xy\right) = \conj\left(x\right)\conj\left(y\right)$.
    \end{enumerate}
    We call $\conj$ a conjugation map. We say it is nontrivial if it is not the identity map.
  \end{definition}
  \begin{example}
    If $F = \C$, and we define $\conj\left(z\right) = \overline{z}$ is a conjugation map.
  \end{example}
  \begin{example}
    Let $F + \Q\left(\sqrt{d}\right) = \set{a + b\sqrt{d} | a,b\in \Q}$ with $d$ not a perfect square. The map
    \begin{align*}
      a + b\sqrt{d} &\xmapsto{\conj} a-b\sqrt{d}
    \end{align*}
    is a conjugation map.
  \end{example}
  Henceforth, we refer to conjugation maps by taking $x \mapsto \overline{x}$.
  \begin{lemma}
    Let $F$ be a field with nontrivial conjugation. Assume $\operatorname{char}\left(F\right) \neq 2$.
    \begin{enumerate}[(1)]
      \item Let $F_0 = \set{z\in F | z = \overline{z}}$. Then, $F_0$ is a proper subfield of $F$.
      \item There is a nonzero element $j\in F$ such that $\overline{j} = -j$.
      \item Every element of $F$ can be written as a $z = x + yj$ for some $x,y\in F_0$.
    \end{enumerate}
  \end{lemma}
  \begin{proof}
    Exercise.
  \end{proof}
  
  \begin{definition}
    Let $V$ be an $F$-vector space, where $F$ is a field with conjugation. A conjugation map on $V$ is a map $\conj: V\rightarrow V$ such that
    \begin{enumerate}[(1)]
      \item $\conj\left(\conj\left(v\right)\right) = v$
      \item $\conj\left(v + w\right) = \conj\left(v\right) + \conj\left(w\right)$.
      \item $\conj\left(av\right) = \overline{a}\conj(v)$.
    \end{enumerate}
  \end{definition}
  \begin{example}
    If $V$ is an $F$-vector space of dimension $n$ and $F$ has conjugation, we can define conjugation on $V$ by taking
    \begin{align*}
      \left[v\right]_{\mathcal{B}}\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix}  &\xmapsto{\conj} \begin{pmatrix}\overline{x_1}\\\vdots\\\overline{x_n}\end{pmatrix} = \left[w\right]_{\mathcal{B}}.
    \end{align*}
    We set $\conj(v) = w$.
  \end{example}
  \begin{definition}
    Let $V,W$ be $F$-vector spaces, where $F$ has conjugation. We say $T: V\rightarrow W$ is conjugate linear if
    \begin{enumerate}[(1)]
      \item $T\left(v_1 + v_2\right) = T\left(v_1\right) + T\left(v_2\right)$
      \item $T\left(av\right) = \overline{a}T\left(v\right)$.
    \end{enumerate}
    We say $T$ is a conjugate isomorphism if it is conjugate linear and bijective.\newline

    We can define a new vector space $\overline{V}$ by having $\overline{V} = V$ as a set, but with
    \begin{align*}
      m: F\times \overline{V} &\rightarrow \overline{V}\\
      \left(a,v\right) &\mapsto a\cdot v = \overline{a}v.
    \end{align*}
    
  \end{definition}
  \begin{exercise}
    Verify that $\overline{V}$ is a $F$-vector space.
  \end{exercise}
  We may ask what linear maps look like in $\Hom_{F}\left(\overline{V},W\right)$. 
  \begin{align*}
    T\left(v_1 + v_2\right) &= T\left(v_1\right) + T\left(v_2\right)\\
    T\left(a\cdot v\right) &= aT\left(v\right)\\
    T\left(a\overline{v}\right) &= \overline{a}T\left(v\right).
  \end{align*}
  Thus, we have $T$ is conjugate linear on $V$ if and only if it is linear on $\overline{V}$.
  \begin{definition}[Sesquilinear Form]
    Let $\varphi: V\times V \rightarrow F$ be a map that is linear in the first variable and conjugate linear in the second variable. Their collection is denoted $\Hom_{F}\left(V,\overline{V};W\right)$.
  \end{definition}
  \begin{example}
    Let $V = \C^n$. Define
    \begin{align*}
      \varphi: \C^n\times \C^n
    \end{align*}
    by
    \begin{align*}
      \varphi\left(v,w\right) &= v^{T}\overline{w}\\
                              &= \sum_{i=1}^{n}v_i\overline{w_i}.
    \end{align*}
  \end{example}
  \begin{example}
    Let $V\in F^{n}$, where $F$ has conjugation. Let $A\in \Mat_{n}\left(F\right)$. Define
    \begin{align*}
      \varphi_{A}: V\times V\rightarrow F
    \end{align*}
    by
   \begin{align*}
     \varphi_{A}\left(v,w\right) &=v^{T} A \overline{w}.
   \end{align*}
  \end{example}
  
\end{document}
