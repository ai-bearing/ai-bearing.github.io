\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright}
%\usepackage{sfmath}
%\usepackage{bbold} %better blackboard bold

%serif font + different blackboard bold for serif font
\usepackage{newpxtext,eulerpx}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\usepackage{epigraph}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushleft}
%\DeclareMathOperator{\Hom}{Hom}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Advanced Linear Algebra: Class Notes}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\section{Introduction}%
\epigraph{It is my experience that proofs involving matrices can be shortened by 50\% if one throws the matrices out.}{Emil Artin}
The goal of this course is to prove a lot of the essential results of linear algebra without basis dependence (as in, using the properties of the linear transformations themselves rather than matrices).
\section{Vector Spaces}%
\subsection{Vector Spaces and Linear Transformations}
\begin{remark}
We let $\F$ be either $\R,\Q,\C,\F_{p}$ (where $p$ is a prime). Primarily, we let $\F = \Q,\R,\C$.\newline
\end{remark}
\begin{example}[Our First Vector Space]
  The primary vector space we study in lower-division linear algebra is
  \begin{align*}
    V &= \R^n\\
      &= \set{ \left. \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_1,\dots,a_n\in \R }
  \end{align*}
  We know that for
  \begin{align*}
    v &= \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\\
    w &= \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix},
  \end{align*}
  that
  \begin{align*}
    v+w &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    cv &= \begin{pmatrix}ca_1 \\\vdots\\ca_n\end{pmatrix},
  \end{align*}
  where $c\in\R$ is some constant.
\end{example}
\begin{definition}[Vector Space]
  Let $V$ be a nonempty set with the following operations:
  \begin{itemize}
    \item $a: V\times V \rightarrow V$, $a(v,w)\mapsto v+w$ (vector addition);
    \item $m: F\times V \rightarrow V$, $m(c,v) \mapsto cv$ (scalar multiplication);
  \end{itemize}
  satisfying the following:
  \begin{enumerate}[(1)]
    \item there exists $0_v\in V$ such that $0_v + v = v = v + 0_v$ for all $v\in V$;
    \item for every $v\in V$, there exists $-v$ such that $v + (-v) = 0_v = (-v) + v$;
    \item for every $u,v,w\in V$, $(u+v) + w = u + (v+w)$;
    \item for every $v,w\in V$, $v+w = w+v$;
    \item for every $v,w\in V$ and $c\in \F$, $c(v+w) = cv + cw$;
    \item for every $c,d\in \F$, $v\in V$, $(c+d)v = cv + dv$;
    \item for every $c,d\in \F$, $v\in V$, $(cd)v = c(dv)$;
    \item for every $v\in V$, $\left(1_{\F}\right)v = v$.
  \end{enumerate}
  We say $V$ is a $\F$-vector space.
\end{definition}
\begin{example}[$\F^{n}$]
  Let $\F$ be a field, $V = \F^n$.
  \begin{align*}
    V &= \set{ \left.\begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_i\in \F }.
    \intertext{Define:}
    \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} + \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix} &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    c \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} &= \begin{pmatrix}ca_1 \\\vdots \\ ca_n\end{pmatrix}.
  \end{align*}
  We set
  \begin{align*}
    0_{\F^n} &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}.
  \end{align*}
  Let
  \begin{align*}
    v &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
    w &= \begin{pmatrix}w_1\\\vdots\\w_n\end{pmatrix}\\
    u &= \begin{pmatrix}u_1\\\vdots\\u_n\end{pmatrix},
  \end{align*}
  $c,d\in \F$. We observe that
  \begin{align*}
    0_{\F^n} + v &= \begin{pmatrix}0 + v_1\\\vdots\\0 + v_n\end{pmatrix}\\
                 &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}.
  \end{align*}
  Define
  \begin{align*}
    -v &= \begin{pmatrix}-v_1\\\vdots\\-v_n\end{pmatrix}.
  \end{align*}
  Then,
  \begin{align*}
    v + (-v) &= \begin{pmatrix}v_1 + \left(-v_1\right)\\\vdots\\v_n + \left(-v_n\right)\end{pmatrix}\\
             &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}\\
             &= 0_{\F^n}.
  \end{align*}
  Note that
  \begin{align*}
    (u + v) + w &= \begin{pmatrix}\left(u_1 + v_1\right) + w_1 \\\vdots\\\left(u_n + v_n\right) + w_n\end{pmatrix}\\
                &= \begin{pmatrix}u_1 + \left(v_1 + w_1\right) \\\vdots\\u_n + \left(v_n + w_n\right)\end{pmatrix}\\
                &= u + (v+w).
  \end{align*}
  We have
  \begin{align*}
    v +w &= \begin{pmatrix}v_1 + w_1 \\\vdots\\v_n + w_n\end{pmatrix}\\
         &= \begin{pmatrix}w_1 + v_1\\\vdots\\w_n + v_n\end{pmatrix}\\
         &= w + v.
  \end{align*}
  Observe
  \begin{align*}
    c\left(v+w\right) &= c \begin{pmatrix}v_1 + w_1\\\vdots\\v_n + w_n\end{pmatrix}\\
                      &= \begin{pmatrix}c\left(v_1 + w_1\right)\\\vdots\\c\left(v_n + w_n\right)\end{pmatrix}\\
                      &= \begin{pmatrix}cv_1 + cw_1 \\\vdots \\cv_n + cw_n\end{pmatrix}\\
                      &= cv + cw,\\
    (c+d)v &= (c+d) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(c+d)v_1\\\vdots\\(c+d)v_n\end{pmatrix}\\
          &= \begin{pmatrix}cv_1 + dv_1 \\\vdots\\cv_n + dv_n\end{pmatrix}\\
          &= cv + dv,
          \intertext{and}
    (cd)v &= (cd) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(cd)v_1 \\\vdots\\(cd)v_n\end{pmatrix}\\
          &= \begin{pmatrix}c\left(dv_1\right) \\\vdots\\c\left(dv_n\right)\end{pmatrix}\\
          &= c\left(dv\right).
  \end{align*}
  Finally,
  \begin{align*}
    1_{\mathbb{F}} &= 1_{\mathbb{F}} \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}1_{\mathbb{F}}v_1\\\vdots 1_{\F}\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= v.
  \end{align*}
\end{example}
\begin{example}[Polynomials]
  Let $n\in \Z_{\geq 0}$. We define
  \begin{align*}
    \mathcal{P}_{n}\left(\mathbb{F}\right) &= \set{a_0 + a_1x + \cdots + a_nx^n\left|a_i\in \F\right.}.
  \end{align*}
  For $f(x) = \sum_{j=0}^{n}a_jx^j$ and $g(x) = \sum_{j=0}^{n}b_jx^j$ in $\mathcal{P}_n(\mathbb{F})$, we have
  \begin{align*}
    f(x) + g(x) &= \sum_{j=0}^{n}\left(a_j + b_j\right)x^j\\
    cf(x) &= \sum_{j=0}^{n}\left(ca_j\right)x^j.
  \end{align*}
  Note that these are not functions \textit{per se}, we are only $f(x)$ and $g(x)$ to represent elements of $\mathcal{P}_n\left(\mathbb{F}\right)$. We can verify that $\mathcal{P}_n\left(\mathbb{F}\right)$ is a $\mathbb{F}$-vector space.\newline

  We define
  \begin{align*}
    \mathbb{F}[x] &= \bigcup_{n\geq 0}\mathcal{P}_n\left(\F\right),
  \end{align*}
  which is also a $\F$-vector space.
\end{example}
\begin{example}[Matrices]
  Let $m,n\in \Z_{> 0}$. We set
  \begin{align*}
    V &= \text{Mat}_{m,n}\left(\F\right),
  \end{align*}
  which is the set of $m\times n$ matrices with entries in $\F$. This is an $\F$-vector space with matrix addition and scalar multiplication.\newline

  In the case where $m = n$, we write $\text{Mat}_{n}\left(\F\right)$ to denote $\text{Mat}_{n,n}\left(\F\right)$.
\end{example}
\begin{example}[Complex Numbers]
  Let $V = \C$. Then, $V$ is a $\C$-vector space, an $\R$-vector space, and a $\Q$-vector space.\newline

  Note that the properties of a vector space change with the underlying scalar field.
\end{example}
\begin{lemma}[Basic Properties of Vector Spaces]
  Let $V$ be a $\F$-vector space.
  \begin{enumerate}[(1)]
    \item $0_V$ is unique.
    \item $0_{\mathbb{F}}v = 0_V$.
    \item $\left(-1_{\mathbb{F}}\right)v = -v$.
  \end{enumerate}
\end{lemma}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Suppose toward contradiction that there exist $0,0'$ both satisfy 
      \begin{align*}
        0 + v &= v\tag*{(\textasteriskcentered)}\\
        0' + v &= v.\tag*{(\textasteriskcentered\textasteriskcentered)}
      \end{align*}
      Then,
      \begin{align*}
        0 + v &= v\\
        0 + 0' &= 0'\tag*{by (\textasteriskcentered) with $v = 0'$}\\
               &= 0' + 0\\
               &= 0. \tag*{by (\textasteriskcentered\textasteriskcentered) with $v = 0$}
      \end{align*}
    \item Note
      \begin{align*}
        0_{\mathbb{F}}v &= \left(0_{\mathbb{F}} + 0_{\F}\right) v\\
                        &= 0_{\F}v + 0_{\F}v.
      \end{align*}
      We subtract $0_{\F}v$ from both sides.
    \item
      \begin{align*}
        \left(-1_{\mathbb{F}}\right)v + v &= \left(-1_{\mathbb{F}} \right)v + 1_{\F}v\\
                                          &= \left(-1_{\F} + 1_{\F}\right)v\\
                                          &= 0_{\F}v.
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{definition}[Subspaces]
  Let $V$ be an $\F$-vector space. We say $W\subseteq V$ is an $\F$-subspace (henceforth subspace) if $W$ is an $\F$-vector space under the same addition and scalar multiplication.
\end{definition}
\begin{example}[Subspaces of $\R^2$]
  Let $V = \R^2$. 
  \begin{center}
    \begin{tikzpicture}[scale = 0.5]
      \draw (0,5) -- (0,-5);
      \draw (5,0) -- (-5,0);
      \draw[thick, color=orange,<->] (-5,-5) -- (5,5);
      \node[anchor = south west] at (5,5){$W_1$};
      \draw[thick, color=yellow!40!black,<->] (-5,-1) -- (1,5);
      \node[anchor = south west] at (1,5) {$W_2$};
    \end{tikzpicture}
  \end{center}
  Here, we see that $W_1$ is a subspace, and $W_2$ is not a subspace (as $W_2$ does not contain $0_{V}$).
\end{example}
\begin{example}[Subspaces of $\C$]
  Let $V = \C$, $W = \set{a + 0i\mid a\in \R}$.
  \begin{itemize}
    \item If $\F = \R$, then $W$ is a subspace of $V$.
    \item If $\F = \C$, then $W$ is not a subspace; we can see that $2\in W$, $i\in \C$, but $2i\notin W$.
  \end{itemize}
\end{example}
\begin{example}[Matrices]
  It is not the case that $\text{Mat}_2(\R)$ is a subspace of $\text{Mat}_4(\R)$, since $\text{Mat}_2(\R)$ is not a subset of $\text{Mat}_4(\R)$.
\end{example}
\begin{example}[Polynomials]
  For the spaces $\mathcal{P}_{m}(\F)$ and $\mathcal{P}_{n}\left(\F\right)$, if $m \leq n$, then $\mathcal{P}_{m}\left(\F\right)$ is a subspace of $\mathcal{P}_{n}\left(\F\right)$.
\end{example}
\begin{lemma}[Proving Subspace Relation]
  Let $V$ be a $\F$-vector space, $W\subseteq V$. Then, $W$ is a subspace of $V$ if
  \begin{enumerate}[(1)]
    \item $W$ is nonempty;
    \item $W$ is closed under addition;
    \item $W$ is closed under scalar multiplication.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof is an exercise.
\end{proof}
\begin{definition}[Linear Transformation]
  Let $V,W$ be $\F$-vector spaces. Let $T: V\rightarrow W$. We say $T$ is a linear transformation (or linear map) if for every $v_1,v_2\in V$, $c\in \F$, we have
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right).
  \end{align*}
  Note that on the left side, addition is in $V$, and on the right side, addition is in $W$.\newline

  The collection of all linear maps from $V$ to $W$ is denoted $\Hom_{\F}\left(V,W\right)$, or $\mathcal{L}\left(V,W\right)$.
\end{definition}
\begin{example}[Identity Transformation]
  Define
  \begin{align*}
    \text{id}_{V}: V\rightarrow V,
  \end{align*}
  where $\text{id}_V(v) = v$. We can see that $\text{id}_V \in \Hom_{\F}\left(V,V\right)$, since
  \begin{align*}
    \text{id}_V\left(v_1 + cv_2\right) &= v_1 + cv_2\\
                            &= \text{id}_V\left(v_1\right) + (c)\left(\text{id}_{V}\left(v_2\right)\right)
  \end{align*}
\end{example}
\begin{example}[Complex Conjugation]
  Let $V = \C$. Define $T: V\rightarrow V$ by $z\mapsto \overline{z}$.\newline

  We may ask whether $T\in \Hom_{\R}\left(\C,\C\right)$ or $T\in \Hom_{\C}\left(\C,\C\right)$.
  \begin{align*}
    T\left(z_1 + cz_1\right) &= \overline{z_1 + cz_2}\\
                             &= \overline{z_1} + \left(\overline{c} \right)\left(\overline{z_2}\right).
  \end{align*}
  We can see that $T\left(z_1 + cz_2\right) = T\left(z_1\right) cT\left(z_2\right)$ if and only if $c = \overline{c}$, meaning $c$ must be real. This means $T\in \Hom_{\R}\left(\C,\C\right)$, but $T\notin \Hom_{\C}\left(\C,\C\right)$.
\end{example}
\begin{example}[Matrices]
  Let $A \in \text{Mat}_{m,n}\left(\F\right)$. We define
  \begin{align*}
    T_{A}: \F^n \rightarrow \F^m\\
    x \mapsto Ax.
  \end{align*}
  Then, $T_A \in \Hom_{\F}\left(\F^n,\F^m\right)$.
\end{example}
\begin{example}[Linear Maps on Smooth Functions]
  Let $V = C^{\infty}\left(\R\right)$, which denotes the set of continuous functions with continuous derivatives at all orders. This is a vector space under pointwise addition and scalar multiplication.
  \begin{align*}
    (f+g)\left(x\right) &= f(x) + g(x)\\
    (cf)(x) &= (c)\left(f(x)\right).
  \end{align*}
  Let $a\in \R$.
  \begin{enumerate}[(1)]
    \item 
\begin{align*}
  E_a: V\rightarrow \R\\
  f \mapsto f(a).
\end{align*}
Then, $E_a \in \Hom_{\R}\left(V,\R\right)$.
\item
  \begin{align*}
    D: V\rightarrow V\\
    f\mapsto f'.
  \end{align*}
  Then, $D\in \Hom_{\R}\left(V,V\right)$.
\item 
  \begin{align*}
    I_a: V\rightarrow V\\
    f\mapsto \int_{a}^{x} f(t)\:dt.
  \end{align*}
  Then, $I_a\in \Hom_{\R}\left(V,V\right)$.
\item Treating $f(a)$ as a (constant) function,
  \begin{align*}
  \tilde{E}_a: V\rightarrow V\\
    f\mapsto f(a).
  \end{align*}
  Then, $\tilde{E}_{a}\in \Hom_{\R}\left(V,V\right)$.
  \end{enumerate}
  Additionally,
  \begin{itemize}
    \item $D\circ I_a = \text{id}_V$;
    \item $I_a\circ D = \text{id}_V - \tilde{E}_a$ for some $a\in \R$.
  \end{itemize}
\end{example}
\begin{exercise}
  Show $\Hom_{\mathbb{F}}\left(V,W\right)$ is an $F$-vector space.
\end{exercise}
\begin{exercise}
  Let $U,V,W$ be vector spaces. Let $S\in \Hom_{\mathbb{F}}\left(U,V\right)$ and $T\in \Hom_{\mathbb{F}}\left(V,W\right)$. Show $T\circ S \in \Hom_{\mathbb{F}}\left(U,W\right)$
\end{exercise}
\begin{lemma}[Image of Identity]
  Let $T\in \Hom_{V,W}$. Then, $T\left(0_V\right) = 0_W$.
\end{lemma}
\begin{definition}[Isomorphism]
  Let $T\in \Hom_{\mathbb{F}}\left(V,W\right)$ be invertible, meaning there exists $T^{-1}W\rightarrow V$ such that $T\circ T^{-1} = \text{id}_{W}$ and $T^{-1}\circ T = \text{id}_{V}$.\newline

  We say $T$ is an isomorphism, and $V,W$ are isomorphic.
\end{definition}
\begin{exercise}
  Show $T^{-1}\in \Hom_{\mathbb{F}}\left(W,V\right)$.
\end{exercise}
\begin{example}[$\R^2$ and $\C$]
  Let $V = \R^2$, $W = \C$. Define $T: \R^2\rightarrow \C$, $(x,y)\mapsto x + iy$.\newline

  We can verify that $T\in \Hom_{\R}\left(\R^2,\C\right)$. Let $\left(x_1,y_1\right),\left(x_2,y_2\right)\in \R^2$ and $r\in \R$. Then,
  \begin{align*}
    T\left(\left(x_1,y_1\right) + r\left(x_2,y_2\right)\right) &= T\left(\left(x_1 + rx_2,y_1 + ry_2\right)\right)\\
                                                               &= \left(x_1 + rx_2\right) + i\left(y_1 + ry_2\right)\\
                                                               &= x_1 + iy_1 + rx_2 + i\left(ry_2\right)\\
                                                               &= x_1 + iy_1 + r\left(x_2 + iy_2\right)\\
                                                               &= T\left(\left(x_1,y_1\right)\right) + rT\left(\left(x_2,y_2\right)\right).
  \end{align*}
  Define $T^{-1}\C \rightarrow \R^2$ by $x + iy \mapsto (x,y)$. We have $T\circ T^{-1}\left(x + iy\right) = x+iy$ is an inverse map and $T^{-1}\circ T\left(\left(x,y\right)\right) = \left(x,y\right)$. Thus, $\R^2\cong \C$ as $\R$-vector spaces.
\end{example}
\begin{example}[$\mathcal{P}_{n}\left(\mathbb{F}\right)$ and $\F^{n+1}$]
  Set $V = \mathcal{P}_{n}\left(\mathbb{F}\right)$ and $W = \mathbb{F}^{n+1}$.\newline

  Define $T: \mathcal{P}_{n}\left(\F\right) \mapsto \F^{n+1}$,
  \begin{align*}
    a_0 + a_1x + \cdots + a_nx^n &\mapsto \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can verify that $T$ is linear, with inverse map $T^{-1}: \F^{n+1}\rightarrow \mathcal{P}_{n}\left(\F\right)$
  \begin{align*}
    \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix} \mapsto a_0 + a_1x + \cdots + a_nx^n.
  \end{align*}
  Thus, $\mathcal{P}_n(\F) \cong \F^{n+1}$.
\end{example}
\begin{definition}[Kernel]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \ker T &= \set{v\in V\mid T(v) = 0_W}.
  \end{align*}
  We call this the kernel of $T$.
\end{definition}
\begin{definition}[Image]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \img\left(T\right) &= T(V)\\
                      &= \set{w\in W\mid \exists v\in V\text{ such that }T(v) = w}
  \end{align*}
\end{definition}
\begin{lemma}[Kernel and Image are Subspaces]
  The kernel, $\ker T$, is a subspace of $V$, and the image, $\img\left(T\right)$, is a subspace of $W$.
\end{lemma}
\begin{proof}
  Since $T\left(0_V\right) = 0_W$, we know that both $\ker T$ and $\img\left(T\right)$ are nonempty.\newline

  Let $c\in \F$ and $v_1,v_2\in \ker T$. Then,
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right)\\
                             &= 0.
  \end{align*}
  Thus, $v_1 + cv_2 \in \ker T$.\newline

  Let $w_1,w_2\in \img\left(T\right)$. Then, there exist $u_1,u_2\in V$ such that $T\left(u_1\right) = w_1$ and $T\left(u_2\right) = w_2$. We have
  \begin{align*}
    T\left(u_1 + cu_2\right) &= T\left(u_1\right) + cT\left(u_2\right)\\
                             &= w_1 + cw_2,
  \end{align*}
  meaning $w_1 + cw_2\in \img\left(T\right)$, meaning $\img\left(T\right)$ is a subspace of $W$.
\end{proof}
\begin{lemma}[Injectivity of a Linear Transformation]
  $T$ is injective and only if $\ker T = \set{0_V}$.
\end{lemma}
\begin{proof}
  Suppose $T$ is injective. Let $v\in V$ be such that $T\left(v\right) = 0_W$. We also know that $T\left(0_V\right) = 0_W$. Since $T$ is injective, this means $v = 0_V$.\newline

  Let $\ker T = \set{0_V}$. Suppose $T\left(v_1\right) = T\left(v_2\right)$. Then,
  \begin{align*}
    T\left(v_1\right) - T\left(v_2\right) &= 0_W\\
    T\left(v_1 - v_2\right) &= 0_W,
  \end{align*}
  meaning $v_1 - v_2 \in \ker T$, meaning $v_1 - v_2 = 0_V$. Thus, $v_1 = v_2$.
\end{proof}
\begin{example}[Projection Map]
  Let $m > n$. Define $T: \F^{m}\rightarrow \F^n$ by
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix} \mapsto \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can see that $\img\left(T\right) = \F^n$.\newline

  To examine the kernel, let
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\in \ker(T).
  \end{align*}
  Then,
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\mapsto \begin{pmatrix}0\\\vdots\\0\end{pmatrix},
  \end{align*}
  with $n$ entries. Thus,
  \begin{align*}
    \ker(T) &= \set{\left. \begin{pmatrix}0\\0\\\vdots\\0\\a_{n+1}\\\vdots\\a_{m}\end{pmatrix}\right|a_i\in \F^m}\\
            &\cong \F^{m-n}.
  \end{align*}
\end{example}
\subsection{Bases and Dimension}%
For this section, we let $V $ be a $ \F$-vector space.
\begin{definition}[Linear Combination]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $v\in V$ is an $\F$-linear combination of $\mathcal{B}$ if there is a set $\set{a_i}_{i\in I}$ with $a_i = 0$ for all but finitely many $i$ such that
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  We write $v\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{definition}
\begin{example}
  Let $V = \mathcal{P}_2\left(\F\right)$. Set $\mathcal{B} = \set{1,x,x^2}$. We have $\Span_{\F}\left(\mathcal{B}\right) = \mathcal{P}_2\left(\F\right)$.
\end{example}
\begin{definition}[Linear Independence]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is $\F$-linearly independent if whenever
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V,
  \end{align*}
  we have $a_i = 0 $ for all $i\in I$. Note that these are finite sums.
\end{definition}
\begin{definition}[Hamel Basis]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is a $\F$-basis for $V$ if
  \begin{enumerate}[(1)]
    \item $\Span\left(\mathcal{B}\right) = V$
    \item $\mathcal{B}$ is linearly independent.
  \end{enumerate}
\end{definition}
\begin{example}[Standard Basis for $\F^n$]
Let $V = \F^n$. We let
\begin{align*}
  \mathcal{E}_n = \set{e_1,\dots,e_n},
\end{align*}
where
\begin{align*}
  e_1 &= \begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}\\
  e_2 &= \begin{pmatrix}0\\1\\\vdots\\0\end{pmatrix}\\
      &\vdots\\
  e_n &= \begin{pmatrix}0\\0\\\vdots\\1\end{pmatrix}.
\end{align*}
We have $\mathcal{E}_n$ is a basis of $\F^n$ referred to as the standard basis.
\end{example}
We wish to show that every vector space has a basis. In order to do so, we require Zorn's lemma.
\begin{theorem}[Zorn's Lemma]
  Let $X$ be a nonempty partially ordered set. If every totally ordered subset of $X$ has an upper bound, then there exists at least one maximal element in $X$.
\end{theorem}
\begin{theorem}
  Let $\mathcal{A}$ and $\mathcal{C}$ be subsets of $V$ with $\mathcal{A}\subseteq \mathcal{C}$. Assume $\mathcal{A}$ is linearly independent and $\Span_{\F}\left(\mathcal{C}\right) = V$. Then, there exists a basis $\mathcal{B}$ of $V$ with $\mathcal{A}\subseteq \mathcal{B}\subseteq \mathcal{C}$.
\end{theorem}
\begin{proof}
  Take
  \begin{align*}
    X &= \set{\mathcal{B}'\subseteq V\mid \mathcal{A}\subseteq \mathcal{B}'\subseteq \mathcal{C},\mathcal{B}\text{ linearly independent}}.
  \end{align*}
  We have $\mathcal{A}\in X$, meaning $X$ is nonempty. We know that $X$ is partially ordered with respect to inclusion, and has an upper bound of $\mathcal{C}$.\newline

  Thus, by Zorn's lemma, we have a maximal element in $X$. We call this maximal element $\mathcal{B}$. By the definition of $X$, $\mathcal{B}$ is linearly independent.\newline

  We claim that $\Span_{\F}\left(\mathcal{B}\right) = V$. If not, there exists some $v\in \mathcal{C}$ such that $v\notin \Span_{\F}\left(\mathcal{B}\right)$. However, if $v\notin \Span_{\F}\left(\mathcal{B}\right)$, then $\mathcal{B}\cup \set{v}\subseteq \mathcal{C}$ is linearly independent. However, since $\mathcal{B}\subsetneq \mathcal{B}\cup \set{v}$, this implies that $\mathcal{B}$ is not maximal, which is a contradiction. Thus, $\Span_{\F}\left(\mathcal{B}\right) = V$.
\end{proof}
\begin{remark}
This proof applies to all vector spaces, not just those with finite dimensions.
\end{remark}
\begin{lemma}
  A homogeneous system of $m$ linear equations in $n$ unknowns with $m < n$ has a nonzero solution.
\end{lemma}
\begin{corollary}
  Let $\mathcal{B}\subseteq V$ with $\Span_{\F}\left(\mathcal{B}\right) = V$ and $\left\vert \mathcal{B} \right\vert = m$.\newline

  Then, any set with more than $m$ elements cannot be linearly independent.
\end{corollary}
\begin{proof}
  Let $\mathcal{C} = \set{w_1,\dots,w_n}$ with $n > m$. We wish to show that $\mathcal{C}$ cannot be linearly independent.\newline

  Write $\mathcal{B} = \set{v_1,\dots,v_m}$ with $\Span_{\F}\left(\mathcal{B}\right) = V$. For each $i$, write $w_i = \sum_{j=1}^{m}a_{ji}v_j$ for some $a_{ji}\in \F$.\newline

  Consider the equations
  \begin{align*}
    \sum_{i=1}^{n}a_{ji}x_i = 0.
  \end{align*}
  We have a solution to this $\left(c_1,\dots,c_n\right) \neq \left(0,\dots,0\right)$.\newline

  We have
  \begin{align*}
    0 &= \sum_{j=1}^{m} \left(\sum_{i=1}^{n}a_{ji}c_i\right)v_j\\
      &= \sum_{i=1}^{n}c_i\left(\sum_{j=1}^{m}a_{ji}v_j\right)\\
      &= \sum_{i=1}^{n}c_iw_i.
  \end{align*}
  Thus, $\mathcal{C}$ is not linearly independent.
\end{proof}
\begin{corollary}
  If $\mathcal{B}$ and $\mathcal{C}$ are bases over $V$, with $\mathcal{B}$ and $\mathcal{C}$ finite, then $\Card \mathcal{B} = \Card \mathcal{C}$.
\end{corollary}
\begin{proof}
  Let $|\mathcal{B}| = m$, $|\mathcal{C}| = n$. Since $\mathcal{C}$ is linearly independent, we know that $n\leq m$. We reverse the roles to see that $m\leq n$.
\end{proof}
\begin{definition}[Dimension]
  Let $V$ be a $\F$-vector space with Hamel basis $\mathcal{B}$. Then, we define $\Dim_{\F} V = \Card \mathcal{B}$.
\end{definition}
\begin{theorem}
  Let $V$ be finite-dimensional with $\Dim_{\F} V = n$. Let $\mathcal{C} \subseteq V$ with $\Card \mathcal{C} = m$.
  \begin{enumerate}[(1)]
    \item If $m > n$, then $\mathcal{C}$ is not linearly independent.
    \item If $m < n$, then $\Span_{\F}\left(\mathcal{C}\right) \neq V$.
    \item If $m = n$, then the following are equal:
      \begin{itemize}
        \item $\mathcal{C}$ is a basis;
        \item $\mathcal{C}$ is linearly independent;
        \item $\Span_{\F}\left(\mathcal{C}\right) = V$.
      \end{itemize}
  \end{enumerate}
\end{theorem}
\begin{corollary}
  Let $W\subseteq V$ be a subspace. We have $\Dim_{\F}W \leq \Dim_{\F} V$.\newline

  If $\Dim_{\F} V < \infty$, then $V = W$ if and only if $\Dim_{\F} W = \Dim_{\F} V$.
\end{corollary}
\begin{example}
  Let $V = \C$.\newline

  If $\F = \C$, then $\mathcal{B} = \set{1}$, and $\Dim_{\C}\C = 1$.\newline

  If $\F = \R$, then $\mathcal{B} = \set{1,i}$, and $\Dim_{\R}\C = 2$.

  %If $\F = \Q$, then $\mathcal{B}$ is uncountable.
\end{example}
\begin{example}
  Let $V = \F[x]$, and let $f(x) \in \F[x]$ be fixed.\newline

  Define an equivalence relation $g(x) \equiv h(x) $ if $f(x)|\left(g(x) - h(x)\right)$.\newline

  Given $g(x) \in \F[x]$, write $\left[g(x)\right]$ for the equivalence class containing $g(x)$.\newline

  Define $W = \F[x] / \left(f(x)\right) = \set{\left[g(x)\right]\mid g(x)\in \F[x]}$.\newline

  Define
  \begin{align*}
    [g(x)] + [h(x)] &= [g(x) + h(x)]\\
    c[g(x)] &= [cg(x)].
  \end{align*}
  This makes $W$ into a vector space. Set $n = \deg f(x)$.\newline

  Then, we claim
  \begin{align*}
    \mathcal{B} = \set{[1],[x],\dots,\left[x^{n-1}\right]}.
  \end{align*}
  Suppose there exist $a_0,\dots,a_{n-1} \in \F$ with
  \begin{align*}
    a_0 [1] + a_1[x] + \cdots + a_{n-1}\left[x^{n-1}\right] = [0].
  \end{align*}
  Then,
  \begin{align*}
    \left[a_0 + a_1x + \cdots + a_{n-1}x^{n-1}\right] = [0].
  \end{align*}
  Therefore,
  \begin{align*}
    f(x) | \left(a_0 + a_1x + \cdots + a_{n-1}x^{n-1} - 0\right),
  \end{align*}
  which means we must have $a_0 = a_1 = \cdots = a_{n-1}$.\newline

  Let $\left[g(x)\right]\in W$. By the Euclidean algorithm,
  \begin{align*}
    g(x) &= f(x)q(x) + r(x)
  \end{align*}
  for some $q(x),r(x) \in \F[x]$ with $r(x) = 0$ or $\deg r(x) < n$. Thus, we have
  \begin{align*}
    \left[g(x)\right] &= \left[f(x)q(x)\right] + \left[r(x)\right]\\
                      &= \left[r(x)\right].
  \end{align*}
  Since $r(x) = 0$ or $\deg r(x) < n$, we must have $\left[g(x)\right] = \left[r(x)\right]\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{example}
\begin{lemma}
  Let $V$ be an $\mathbb{F}$-vector space, with $\mathcal{C} = \set{v_i}_{i\in I}$ be a subset of $V$.\newline

  Then, $\mathcal{C}$ is a basis if and only if each $v\in V$ can be uniquely written as a linear combination of elements of $\mathcal{C}$.
\end{lemma}
\begin{proof}
  Suppose $\mathcal{C}$ is a basis. Let $v\in V$, and suppose
  \begin{align*}
    v &= \sum_{i\in I}a_iv_i\\
      &= \sum_{i\in I}b_iv_i
  \end{align*}
  for some $a_i,b_i\in \mathbb{F}$. Then,
  \begin{align*}
    0_V &= \sum_{i\in I}\left(a_i - b_i\right)v_i.
  \end{align*}
  Since $\mathcal{C}$ is a basis, $a_i - b_i = 0$ for all $i$, meaning $a_i = b_i$, so the expression is unique.\newline

  Suppose every $v$ can be written as a unique linear combination of $\mathcal{C}$. Certainly, this means $\Span_{\mathbb{F}}\left(\mathcal{C}\right) = V$. Suppose
  \begin{align*}
    0_V &= \sum_{i\in I}a_iv_i
  \end{align*}
  for some $a_i\in \mathbb{F}$. It is also true that $0_V = \sum_{i\in I}0v_i$, meaning $a_i = 0$ for all $i$ by uniqueness; thus, $\mathcal{C}$ is linearly independent.
\end{proof}
\begin{proposition}
  Let $V,W$ be $\mathbb{F}$-vector spaces.
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,W\right)$. We have $T$ is uniquely determined by the image of the basis of $V$.
    \item Let $\mathcal{B}=\set{v_i}_{i\in I}$ be a basis of $V$, and let $\mathcal{C} = \set{w_i}$ be a subset of $W$. If $\Card(\mathcal{B}) = \Card\left(\mathcal{C}\right)$, there is a $T\in \Hom_{\F}\left(V,W\right)$ such that $T\left(v_i\right) = w_i$ for every $i$
  \end{enumerate}
\end{proposition}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Let $v\in V$, let $\mathcal{B} = \set{v_i}$ be a basis of $V$, and write $v = \sum_{i\in I}a_iv_i$. We have
  \begin{align*}
    T\left(v\right) &= T\left(\sum_{i\in I}a_iv_i\right)\\
                    &= \sum_{i\in I}a_iT\left(v_i\right).
  \end{align*}
    \item  Define $T$ by setting
      \begin{align*}
        T\left(v\right) &= \sum_{i\in I}a_iw_i,
      \end{align*}
      for $v = \sum_{i\in I}a_iv_i$. We can verify that $T$ is linear.
  \end{enumerate}
\end{proof}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$, with $\mathcal{B} = \set{v_i}$ a basis of $V$ and $\mathcal{C} = \set{w_i}\subseteq W$, with $w_i = T\left(v_i\right)$. Then, we have $\mathcal{C}$ is a basis of $W$ if and only if $T$ is an isomorphism.
\end{corollary}
\begin{proof}
  Let $\mathcal{C}$ be a basis for $W$. Since $\mathcal{C}$ is a basis of $W$, we use the proposition to define $S\in \Hom_{\F}\left(W,V\right)$ with $S\left(w_i\right) = v_i$. We can verify that $T\circ S = \text{id}_{W}$ and $S\circ T = \text{id}_V$, meaning $S = T^{-1}$ and $T$ is an isomorphism.\newline

  Suppose $T$ is an isomorphism. Let $w\in W$. Since $T$ is an isomorphism, $T$ is surjective, meaning there exists $v\in V$ such that $T(v) = w$. Since $\mathcal{B}$ is a basis of $V$, we expand $v$ to have
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  Combining these two facts, we have
  \begin{align*}
    w &= T(v)\\
      &= T\left(\sum_{i\in I}a_iv_i\right)\\
      &= \sum_{i\in I}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right).
  \end{align*}
  Thus, $W = \Span_{\F}\left(\mathcal{C}\right)$.\newline

  Suppose there exists $a_i\in \F$ with $\sum_{i\in I}a_iT\left(v_i\right) = 0_W$. Since $T$ is linear, we have
  \begin{align*}
    \sum_{i\in I}a_iT\left(v_i\right) &= T\left(\sum_{i\in I}a_iv_i\right).
  \end{align*}
  Since $T$ is injective, we have
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V.
  \end{align*}
  Since $\mathcal{B}$ is a basis, we have $a_i = 0$.
\end{proof}
\begin{theorem}[Rank--Nullity]
  Let $V$ be finite-dimensional vector space over $\mathbb{F}$. Let $T\in \Hom_{\F}\left(V,W\right)$. Then,
  \begin{align*}
    \Dim_{\F}(V) &= \Dim_{\F}\left(\ker (T)\right) + \Dim_{\F}\left(\img (T)\right)
  \end{align*}
\end{theorem}
\begin{proof}
  Let $\Dim_{\F}\left(\ker(T)\right) = k$ and $\Dim_{\F}\left(V\right) = n$. Let $\mathcal{A} = \set{v_1,\dots,v_k}$ be a basis of $\ker(T)$. We extend $\mathcal{A}$ to a basis $\mathcal{B} = \set{v_1,\dots,v_n}$ of $V$.\newline

  We want to show that $\mathcal{C} = \set{T\left(v_{k+1}\right),\dots,T\left(v_n\right)}$ is a basis of $\img(T)$.\newline

  Let $w\in \img(T)$. Then, there is $v\in V$ such that $T(v) = w$. We write
  \begin{align*}
    v &= \sum_{i=1}^{n}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    w &= T\left(v\right)\\
      &= T\left(\sum_{i=1}^{n}a_iv_i\right)\\
      &= \sum_{i=1}^{n}a_iT\left(v_i\right)\\
      &= \sum_{i=k+1}^{n}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right),
  \end{align*}
  since $\set{v_1,\dots,v_k}\subseteq \ker(T)$, meaning $\Span_{\F}\left(\mathcal{C}\right) = \im(T)$.\newline

  Suppose we have
  \begin{align*}
    \sum_{i=k+1}^{n}a_iT\left(v_i\right) = 0_W.
  \end{align*}
  Then, we have
  \begin{align*}
    T\left(\sum_{i=k+1}^{n}a_iv_i\right) &= 0_W,
  \end{align*}
  meaning $\sum_{i=k+1}^{n}a_iv_i\in \ker(T)$. This means there exist $a_1,\dots,a_k$ such that
  \begin{align*}
    \sum_{i=k+1}^{n}a_iv_i &= \sum_{i=1}^{k}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    \sum_{i=1}^{k}a_iv_i + \sum_{i=k+1}^{n}\left(-a_i\right)v_i = 0_V.
  \end{align*}
  Since $\set{v_i}$ are a basis, this means $a_i = 0$ for all $i$.
\end{proof}
\begin{corollary}
  Let $V,W$ be $\mathbb{F}$-vector spaces with $\Dim_{\F}\left(V\right) = n$. Let $V_1\subseteq V$ be a subspace with $\Dim_{\F}\left(V_1\right) = k$, and $W_1\subseteq W$ a subspace with $\Dim_{\F}\left(W_1\right) = n-k$. Then, there exists $T\in \Hom_{\F}\left(V,W\right)$ such that $\ker(T) = V_1$ and $\img(T) = W_1$.
\end{corollary}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$ with $\Dim_{\F}\left(V\right) = \Dim_{\F}\left(W\right) < \infty$. Then, the following are equivalent:
  \begin{enumerate}[(1)]
    \item $T$ is an isomorphism;
    \item $T$ is injective;
    \item $T$ is surjective.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $A\in \Mat_{n}\left(\F\right)$. The following are equivalent:
  \begin{enumerate}[(1)]
    \item $A$ is invertible;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $BA = I_{n}$;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $AB = I_n$.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $\Dim_{\F}(V) = m$ and $\Dim_{\F}(W) = n$.
  \begin{enumerate}[(1)]
    \item If $m < n$ and $T\in \Hom_{\F}\left(V,W\right)$, then $T$ is not surjective.
    \item If $m > n$ and $T\in \Hom_\F\left(V,W\right)$, then $T$ is not injective.
    \item We have $m = n$ if and only if $V\cong W$.
  \end{enumerate}
\end{corollary}
\subsection{Direct Sums and Quotient Spaces}%
\begin{definition}[Sum of Subspaces]
  Let $V$ be a vector space, and $V_1,\dots,V_k$ be subspaces. Then, the sum of $V_1,\dots,V_k$ is
  \begin{align*}
    V_1 + \cdots + V_k &= \set{\sum_{i=1}^{k}v_i\mid v_i\in V_i}.
  \end{align*}
  This is a subspace of $V$.
\end{definition}
\begin{definition}[Independence of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V_1,\dots,V_k$ are independent if whenever $v_1 + \cdots v_k = 0_V$, we have $v_i = 0_V$.
\end{definition}
\begin{definition}[Direct Sum of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V$ is the direct sum of $V_1,\dots,V_k$, and write
  \begin{align*}
    V = V_1 \oplus \cdots \oplus V_k,
  \end{align*}
  if the following conditions hold.
  \begin{enumerate}[(1)]
    \item $V = V_1 + \cdots V_k$;
    \item $V_1,\dots,V_k$ are independent.
  \end{enumerate}
\end{definition}
\begin{example}[A Very Simple Direct Sum]
  Let $V = \F^2$, with $V_1 = \set{\left(x,0\right)\mid x\in \F}$ and $V_2 = \set{\left(0,y\right)\mid y\in \F}$, we can see that
  \begin{align*}
    V_1 + V_2 &= \set{\left(x,0\right) + \left(0,y\right)\mid x,y\in \F}\\
              &= \set{\left(x,y\right)\mid x,y\in \F}\\
              &= \F^2.
  \end{align*}
  If $\left(x,0\right) + \left(0,y\right) = 0$, then $x = 0$ and $y = 0$, meaning $\F^2 = V_1\oplus V_2$.
\end{example}
\begin{example}[Direct Sum Constructions]
  Let $V = \F[x]$.\newline

  Define $V_1 = \F$, $V_2 = \F x = \set{\alpha x\mid \alpha \in \F}$, $V_3 = \mathcal{P}_1\left(\F\right)$.\newline

  We can see that
  \begin{align*}
    \mathcal{P}_1 &= V_1\oplus V_2.
  \end{align*}
  However, $V_1$ and $V_3$ are not independent, since $1_{\F}\in V_1$ and $-1_{\F}\in V_3$ with $1_{\F} + \left(-1_{\F}\right) = 0_\F$.
\end{example}
\begin{example}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$, with $V_i = \Span(v_i)$. Then,
  \begin{align*}
    V = V_1\oplus \cdots \oplus V_n.
  \end{align*}
\end{example}
\begin{lemma}
  Let $V$ be a vector space, $V_1,\dots,V_k$ subspaces. We have $V = V_1\oplus \cdots \oplus V_k$ if and only if every $v\in V$ can be written uniquely in the form
  \begin{align*}
    v = v_1 +\cdots + v_k
  \end{align*}
  for $v_i\in V_i$.
\end{lemma}
\begin{proof}
  Suppose $V = V_1\oplus\cdots\oplus V_k$. Let $v\in V$. Then, $v = v_1 + \cdots + v_k$ for some $v_i\in V_i$ since $V = V_1+\cdots+V_k$. Suppose
  \begin{align*}
    v &= v_1 + \cdots v_k\\
      &= \tilde{v}_1 + \cdots + \tilde{v}_k
  \end{align*}
  for $v_i,\tilde{v}_i\in V_i$. Then,
  \begin{align*}
    0_V &= \left(v_1 - \tilde{v}_1\right) + \cdots + \left(v_k - \tilde{v}_k\right).
  \end{align*}
  Since $V_1,\dots,V_k$ are linearly independent, $v_i - \tilde{v}_i\in V_i$, we have $v_i - \tilde{v}_i = 0_V$, meaning the expression for $v$ is unique.\newline

  Suppose that every $v\in V$ can be written uniquely in the form $v = v_1 + \cdots + v_k$ with $v_i\in V_i$. Then,
  \begin{align*}
    V = V_1 + \cdots V_k
  \end{align*}
  by the definition of $V_1 + \cdots + V_k$. If
  \begin{align*}
    0_V = v_1 + \cdots v_k
  \end{align*}
  for $v_i\in V_i$, and it is also the case that
  \begin{align*}
    0_V = 0_V + \cdots + 0_V,
  \end{align*}
  with $0_V \in V_i$, then it must be the case that $v_i = 0_V$ for all $i$ by uniqueness. Thus, the $V_i$ are independent, so
  \begin{align*}
    V = V_1\oplus\cdots\oplus V_k.
  \end{align*}
\end{proof}
\begin{exercise}
  Let $V_1,\dots,V_k$ be subspaces of $V$. For each $i$, let $\mathcal{B}_i$ be a basis for $V_i$. Let $\mathcal{B} = \bigcup_{i =1}^{k}\mathcal{B}_i$. Show
  \begin{enumerate}[(1)]
    \item $\mathcal{B}$ spans $V$ if and only if $V = V_1 + \cdots + V_k$;
    \item $\mathcal{B}$ is linearly independent if and only if $V_1,\dots,V_k$ are independent;
    \item $\mathcal{B}$ is a basis if and only if $V = V_1 \oplus \cdots \oplus V_k$.
  \end{enumerate}
\end{exercise}
\begin{lemma}[Existence of Complement]
Let $V$ be a vector space, and $U\subseteq V$ be a subspace. Then, $U$ has a complement $W$ such that $U\oplus W = V$.
\end{lemma}
\begin{proof}
  Let $\mathcal{A} $ be a basis for $U$. Extend $\mathcal{A}$ to a basis $\mathcal{B}$ of $V$. Let $\mathcal{C} = \mathcal{B}\setminus \mathcal{A}$, and $W = \Span\left(\mathcal{C}\right)$.
\end{proof}
\begin{example}[Constructing a Quotient Group]
To introduce quotient spaces, consider the construction of the quotient group.\newline

Let $n\in \Z_{>1}$. We say $a \equiv b$ modulo $n$ if and only if $n|(a-b)$. This is an equivalence relation; we form $\Z/n\Z = \set{\left[a\right]_n\mid a\in\Z} = \set{\left[0\right]_n,\dots,\left[n-1\right]_n}$.\newline

However, we also do this by defining $n\Z = \set{nk\mid k\in\Z}$, and taking $a\equiv b$ mod $n$ if and only if $a-b \in n\Z$. Our equivalence classes are now
\begin{align*}
  \left[a\right]_n &= \set{a + nk\mid k\in\Z}\\
                   &= a + n\Z.
\end{align*}
\end{example}
\begin{definition}[Quotient Space]
  Let $W\subseteq V$ be a subspace. We say $v_1\sim_{W}v_w$ if $v_1 - v_2 \in W$. This is an equivalence relation, with
  \begin{align*}
    V/W &= \set{v + W\mid v\in V}.
  \end{align*}
\end{definition}
\end{document}
