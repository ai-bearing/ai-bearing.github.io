\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright}
%\usepackage{sfmath}
%\usepackage{bbold} %better blackboard bold

%serif font + different blackboard bold for serif font
\usepackage{newpxtext,eulerpx}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\usepackage{epigraph}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushleft}
%\DeclareMathOperator{\Hom}{Hom}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Advanced Linear Algebra: Class Notes}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\section{Introduction}%
\epigraph{It is my experience that proofs involving matrices can be shortened by 50\% if one throws the matrices out.}{Emil Artin}
The goal of this course is to prove a lot of the essential results of linear algebra without basis dependence (as in, using the properties of the linear transformations themselves rather than matrices).
\tableofcontents
\section{Vector Spaces}%
\subsection{Vector Spaces and Linear Transformations}
\begin{remark}
We let $\F$ be either $\R,\Q,\C,\F_{p}$ (where $p$ is a prime). Primarily, we let $\F = \Q,\R,\C$.\newline
\end{remark}
\begin{example}[Our First Vector Space]
  The primary vector space we study in lower-division linear algebra is
  \begin{align*}
    V &= \R^n\\
      &= \set{ \left. \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_1,\dots,a_n\in \R }
  \end{align*}
  We know that for
  \begin{align*}
    v &= \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\\
    w &= \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix},
  \end{align*}
  that
  \begin{align*}
    v+w &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    cv &= \begin{pmatrix}ca_1 \\\vdots\\ca_n\end{pmatrix},
  \end{align*}
  where $c\in\R$ is some constant.
\end{example}
\begin{definition}[Vector Space]
  Let $V$ be a nonempty set with the following operations:
  \begin{itemize}
    \item $a: V\times V \rightarrow V$, $a(v,w)\mapsto v+w$ (vector addition);
    \item $m: F\times V \rightarrow V$, $m(c,v) \mapsto cv$ (scalar multiplication);
  \end{itemize}
  satisfying the following:
  \begin{enumerate}[(1)]
    \item there exists $0_v\in V$ such that $0_v + v = v = v + 0_v$ for all $v\in V$;
    \item for every $v\in V$, there exists $-v$ such that $v + (-v) = 0_v = (-v) + v$;
    \item for every $u,v,w\in V$, $(u+v) + w = u + (v+w)$;
    \item for every $v,w\in V$, $v+w = w+v$;
    \item for every $v,w\in V$ and $c\in \F$, $c(v+w) = cv + cw$;
    \item for every $c,d\in \F$, $v\in V$, $(c+d)v = cv + dv$;
    \item for every $c,d\in \F$, $v\in V$, $(cd)v = c(dv)$;
    \item for every $v\in V$, $\left(1_{\F}\right)v = v$.
  \end{enumerate}
  We say $V$ is a $\F$-vector space.
\end{definition}
\begin{example}[$\F^{n}$]
  Let $\F$ be a field, $V = \F^n$.
  \begin{align*}
    V &= \set{ \left.\begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_i\in \F }.
    \intertext{Define:}
    \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} + \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix} &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    c \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} &= \begin{pmatrix}ca_1 \\\vdots \\ ca_n\end{pmatrix}.
  \end{align*}
  We set
  \begin{align*}
    0_{\F^n} &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}.
  \end{align*}
  Let
  \begin{align*}
    v &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
    w &= \begin{pmatrix}w_1\\\vdots\\w_n\end{pmatrix}\\
    u &= \begin{pmatrix}u_1\\\vdots\\u_n\end{pmatrix},
  \end{align*}
  $c,d\in \F$. We observe that
  \begin{align*}
    0_{\F^n} + v &= \begin{pmatrix}0 + v_1\\\vdots\\0 + v_n\end{pmatrix}\\
                 &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}.
  \end{align*}
  Define
  \begin{align*}
    -v &= \begin{pmatrix}-v_1\\\vdots\\-v_n\end{pmatrix}.
  \end{align*}
  Then,
  \begin{align*}
    v + (-v) &= \begin{pmatrix}v_1 + \left(-v_1\right)\\\vdots\\v_n + \left(-v_n\right)\end{pmatrix}\\
             &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}\\
             &= 0_{\F^n}.
  \end{align*}
  Note that
  \begin{align*}
    (u + v) + w &= \begin{pmatrix}\left(u_1 + v_1\right) + w_1 \\\vdots\\\left(u_n + v_n\right) + w_n\end{pmatrix}\\
                &= \begin{pmatrix}u_1 + \left(v_1 + w_1\right) \\\vdots\\u_n + \left(v_n + w_n\right)\end{pmatrix}\\
                &= u + (v+w).
  \end{align*}
  We have
  \begin{align*}
    v +w &= \begin{pmatrix}v_1 + w_1 \\\vdots\\v_n + w_n\end{pmatrix}\\
         &= \begin{pmatrix}w_1 + v_1\\\vdots\\w_n + v_n\end{pmatrix}\\
         &= w + v.
  \end{align*}
  Observe
  \begin{align*}
    c\left(v+w\right) &= c \begin{pmatrix}v_1 + w_1\\\vdots\\v_n + w_n\end{pmatrix}\\
                      &= \begin{pmatrix}c\left(v_1 + w_1\right)\\\vdots\\c\left(v_n + w_n\right)\end{pmatrix}\\
                      &= \begin{pmatrix}cv_1 + cw_1 \\\vdots \\cv_n + cw_n\end{pmatrix}\\
                      &= cv + cw,\\
    (c+d)v &= (c+d) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(c+d)v_1\\\vdots\\(c+d)v_n\end{pmatrix}\\
          &= \begin{pmatrix}cv_1 + dv_1 \\\vdots\\cv_n + dv_n\end{pmatrix}\\
          &= cv + dv,
          \intertext{and}
    (cd)v &= (cd) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(cd)v_1 \\\vdots\\(cd)v_n\end{pmatrix}\\
          &= \begin{pmatrix}c\left(dv_1\right) \\\vdots\\c\left(dv_n\right)\end{pmatrix}\\
          &= c\left(dv\right).
  \end{align*}
  Finally,
  \begin{align*}
    1_{\mathbb{F}} &= 1_{\mathbb{F}} \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}1_{\mathbb{F}}v_1\\\vdots 1_{\F}\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= v.
  \end{align*}
\end{example}
\begin{example}[Polynomials]
  Let $n\in \Z_{\geq 0}$. We define
  \begin{align*}
    P_{n}\left(\mathbb{F}\right) &= \set{a_0 + a_1x + \cdots + a_nx^n\left|a_i\in \F\right.}.
  \end{align*}
  For $f(x) = \sum_{j=0}^{n}a_jx^j$ and $g(x) = \sum_{j=0}^{n}b_jx^j$ in $P_n(\mathbb{F})$, we have
  \begin{align*}
    f(x) + g(x) &= \sum_{j=0}^{n}\left(a_j + b_j\right)x^j\\
    cf(x) &= \sum_{j=0}^{n}\left(ca_j\right)x^j.
  \end{align*}
  Note that these are not functions \textit{per se}, we are only $f(x)$ and $g(x)$ to represent elements of $P_n\left(\mathbb{F}\right)$. We can verify that $P_n\left(\mathbb{F}\right)$ is a $\mathbb{F}$-vector space.\newline

  We define
  \begin{align*}
    \mathbb{F}[x] &= \bigcup_{n\geq 0}P_n\left(\F\right),
  \end{align*}
  which is also a $\F$-vector space.
\end{example}
\begin{example}[Matrices]
  Let $m,n\in \Z_{> 0}$. We set
  \begin{align*}
    V &= \text{Mat}_{m,n}\left(\F\right),
  \end{align*}
  which is the set of $m\times n$ matrices with entries in $\F$. This is an $\F$-vector space with matrix addition and scalar multiplication.\newline

  In the case where $m = n$, we write $\text{Mat}_{n}\left(\F\right)$ to denote $\text{Mat}_{n,n}\left(\F\right)$.
\end{example}
\begin{example}[Complex Numbers]
  Let $V = \C$. Then, $V$ is a $\C$-vector space, an $\R$-vector space, and a $\Q$-vector space.\newline

  Note that the properties of a vector space change with the underlying scalar field.
\end{example}
\begin{lemma}[Basic Properties of Vector Spaces]
  Let $V$ be a $\F$-vector space.
  \begin{enumerate}[(1)]
    \item $0_V$ is unique.
    \item $0_{\mathbb{F}}v = 0_V$.
    \item $\left(-1_{\mathbb{F}}\right)v = -v$.
  \end{enumerate}
\end{lemma}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Suppose toward contradiction that there exist $0,0'$ both satisfy 
      \begin{align*}
        0 + v &= v\tag*{(\textasteriskcentered)}\\
        0' + v &= v.\tag*{(\textasteriskcentered\textasteriskcentered)}
      \end{align*}
      Then,
      \begin{align*}
        0 + v &= v\\
        0 + 0' &= 0'\tag*{by (\textasteriskcentered) with $v = 0'$}\\
               &= 0' + 0\\
               &= 0. \tag*{by (\textasteriskcentered\textasteriskcentered) with $v = 0$}
      \end{align*}
    \item Note
      \begin{align*}
        0_{\mathbb{F}}v &= \left(0_{\mathbb{F}} + 0_{\F}\right) v\\
                        &= 0_{\F}v + 0_{\F}v.
      \end{align*}
      We subtract $0_{\F}v$ from both sides.
    \item
      \begin{align*}
        \left(-1_{\mathbb{F}}\right)v + v &= \left(-1_{\mathbb{F}} \right)v + 1_{\F}v\\
                                          &= \left(-1_{\F} + 1_{\F}\right)v\\
                                          &= 0_{\F}v.
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{definition}[Subspaces]
  Let $V$ be an $\F$-vector space. We say $W\subseteq V$ is an $\F$-subspace (henceforth subspace) if $W$ is an $\F$-vector space under the same addition and scalar multiplication.
\end{definition}
\begin{example}[Subspaces of $\R^2$]
  Let $V = \R^2$. 
  \begin{center}
    \begin{tikzpicture}[scale = 0.5]
      \draw (0,5) -- (0,-5);
      \draw (5,0) -- (-5,0);
      \draw[thick, color=orange,<->] (-5,-5) -- (5,5);
      \node[anchor = south west] at (5,5){$W_1$};
      \draw[thick, color=yellow!40!black,<->] (-5,-1) -- (1,5);
      \node[anchor = south west] at (1,5) {$W_2$};
    \end{tikzpicture}
  \end{center}
  Here, we see that $W_1$ is a subspace, and $W_2$ is not a subspace (as $W_2$ does not contain $0_{V}$).
\end{example}
\begin{example}[Subspaces of $\C$]
  Let $V = \C$, $W = \set{a + 0i\mid a\in \R}$.
  \begin{itemize}
    \item If $\F = \R$, then $W$ is a subspace of $V$.
    \item If $\F = \C$, then $W$ is not a subspace; we can see that $2\in W$, $i\in \C$, but $2i\notin W$.
  \end{itemize}
\end{example}
\begin{example}[Matrices]
  It is not the case that $\text{Mat}_2(\R)$ is a subspace of $\text{Mat}_4(\R)$, since $\text{Mat}_2(\R)$ is not a subset of $\text{Mat}_4(\R)$.
\end{example}
\begin{example}[Polynomials]
  For the spaces $P_{m}(\F)$ and $P_{n}\left(\F\right)$, if $m \leq n$, then $P_{m}\left(\F\right)$ is a subspace of $P_{n}\left(\F\right)$.
\end{example}
\begin{lemma}[Proving Subspace Relation]
  Let $V$ be a $\F$-vector space, $W\subseteq V$. Then, $W$ is a subspace of $V$ if
  \begin{enumerate}[(1)]
    \item $W$ is nonempty;
    \item $W$ is closed under addition;
    \item $W$ is closed under scalar multiplication.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof is an exercise.
\end{proof}
\begin{definition}[Linear Transformation]
  Let $V,W$ be $\F$-vector spaces. Let $T: V\rightarrow W$. We say $T$ is a linear transformation (or linear map) if for every $v_1,v_2\in V$, $c\in \F$, we have
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right).
  \end{align*}
  Note that on the left side, addition is in $V$, and on the right side, addition is in $W$.\newline

  The collection of all linear maps from $V$ to $W$ is denoted $\Hom_{\F}\left(V,W\right)$, or $\mathcal{L}\left(V,W\right)$.
\end{definition}
\begin{example}[Identity Transformation]
  Define
  \begin{align*}
    \id_{V}: V\rightarrow V,
  \end{align*}
  where $\id_V(v) = v$. We can see that $\id_V \in \Hom_{\F}\left(V,V\right)$, since
  \begin{align*}
    \id_V\left(v_1 + cv_2\right) &= v_1 + cv_2\\
                            &= \id_V\left(v_1\right) + (c)\left(\id_{V}\left(v_2\right)\right)
  \end{align*}
\end{example}
\begin{example}[Complex Conjugation]
  Let $V = \C$. Define $T: V\rightarrow V$ by $z\mapsto \overline{z}$.\newline

  We may ask whether $T\in \Hom_{\R}\left(\C,\C\right)$ or $T\in \Hom_{\C}\left(\C,\C\right)$.
  \begin{align*}
    T\left(z_1 + cz_1\right) &= \overline{z_1 + cz_2}\\
                             &= \overline{z_1} + \left(\overline{c} \right)\left(\overline{z_2}\right).
  \end{align*}
  We can see that $T\left(z_1 + cz_2\right) = T\left(z_1\right) cT\left(z_2\right)$ if and only if $c = \overline{c}$, meaning $c$ must be real. This means $T\in \Hom_{\R}\left(\C,\C\right)$, but $T\notin \Hom_{\C}\left(\C,\C\right)$.
\end{example}
\begin{example}[Matrices]
  Let $A \in \text{Mat}_{m,n}\left(\F\right)$. We define
  \begin{align*}
    T_{A}: \F^n \rightarrow \F^m\\
    x \mapsto Ax.
  \end{align*}
  Then, $T_A \in \Hom_{\F}\left(\F^n,\F^m\right)$.
\end{example}
\begin{example}[Linear Maps on Smooth Functions]
  Let $V = C^{\infty}\left(\R\right)$, which denotes the set of continuous functions with continuous derivatives at all orders. This is a vector space under pointwise addition and scalar multiplication.
  \begin{align*}
    (f+g)\left(x\right) &= f(x) + g(x)\\
    (cf)(x) &= (c)\left(f(x)\right).
  \end{align*}
  Let $a\in \R$.
  \begin{enumerate}[(1)]
    \item 
\begin{align*}
  E_a: V\rightarrow \R\\
  f \mapsto f(a).
\end{align*}
Then, $E_a \in \Hom_{\R}\left(V,\R\right)$.
\item
  \begin{align*}
    D: V\rightarrow V\\
    f\mapsto f'.
  \end{align*}
  Then, $D\in \Hom_{\R}\left(V,V\right)$.
\item 
  \begin{align*}
    I_a: V\rightarrow V\\
    f\mapsto \int_{a}^{x} f(t)\:dt.
  \end{align*}
  Then, $I_a\in \Hom_{\R}\left(V,V\right)$.
\item Treating $f(a)$ as a (constant) function,
  \begin{align*}
  \tilde{E}_a: V\rightarrow V\\
    f\mapsto f(a).
  \end{align*}
  Then, $\tilde{E}_{a}\in \Hom_{\R}\left(V,V\right)$.
  \end{enumerate}
  Additionally,
  \begin{itemize}
    \item $D\circ I_a = \text{id}_V$;
    \item $I_a\circ D = \text{id}_V - \tilde{E}_a$ for some $a\in \R$.
  \end{itemize}
\end{example}
\begin{exercise}
  Show $\Hom_{\mathbb{F}}\left(V,W\right)$ is an $F$-vector space.
\end{exercise}
\begin{exercise}
  Let $U,V,W$ be vector spaces. Let $S\in \Hom_{\mathbb{F}}\left(U,V\right)$ and $T\in \Hom_{\mathbb{F}}\left(V,W\right)$. Show $T\circ S \in \Hom_{\mathbb{F}}\left(U,W\right)$
\end{exercise}
\begin{lemma}[Image of Identity]
  Let $T\in \Hom_{V,W}$. Then, $T\left(0_V\right) = 0_W$.
\end{lemma}
\begin{definition}[Isomorphism]
  Let $T\in \Hom_{\mathbb{F}}\left(V,W\right)$ be invertible, meaning there exists $T^{-1}W\rightarrow V$ such that $T\circ T^{-1} = \text{id}_{W}$ and $T^{-1}\circ T = \text{id}_{V}$.\newline

  We say $T$ is an isomorphism, and $V,W$ are isomorphic.
\end{definition}
\begin{exercise}
  Show $T^{-1}\in \Hom_{\mathbb{F}}\left(W,V\right)$.
\end{exercise}
\begin{example}[$\R^2$ and $\C$]
  Let $V = \R^2$, $W = \C$. Define $T: \R^2\rightarrow \C$, $(x,y)\mapsto x + iy$.\newline

  We can verify that $T\in \Hom_{\R}\left(\R^2,\C\right)$. Let $\left(x_1,y_1\right),\left(x_2,y_2\right)\in \R^2$ and $r\in \R$. Then,
  \begin{align*}
    T\left(\left(x_1,y_1\right) + r\left(x_2,y_2\right)\right) &= T\left(\left(x_1 + rx_2,y_1 + ry_2\right)\right)\\
                                                               &= \left(x_1 + rx_2\right) + i\left(y_1 + ry_2\right)\\
                                                               &= x_1 + iy_1 + rx_2 + i\left(ry_2\right)\\
                                                               &= x_1 + iy_1 + r\left(x_2 + iy_2\right)\\
                                                               &= T\left(\left(x_1,y_1\right)\right) + rT\left(\left(x_2,y_2\right)\right).
  \end{align*}
  Define $T^{-1}\C \rightarrow \R^2$ by $x + iy \mapsto (x,y)$. We have $T\circ T^{-1}\left(x + iy\right) = x+iy$ is an inverse map and $T^{-1}\circ T\left(\left(x,y\right)\right) = \left(x,y\right)$. Thus, $\R^2\cong \C$ as $\R$-vector spaces.
\end{example}
\begin{example}[$P_{n}\left(\mathbb{F}\right)$ and $\F^{n+1}$]
  Set $V = P_{n}\left(\mathbb{F}\right)$ and $W = \mathbb{F}^{n+1}$.\newline

  Define $T: P_{n}\left(\F\right) \mapsto \F^{n+1}$,
  \begin{align*}
    a_0 + a_1x + \cdots + a_nx^n &\mapsto \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can verify that $T$ is linear, with inverse map $T^{-1}: \F^{n+1}\rightarrow P_{n}\left(\F\right)$
  \begin{align*}
    \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix} \mapsto a_0 + a_1x + \cdots + a_nx^n.
  \end{align*}
  Thus, $P_n(\F) \cong \F^{n+1}$.
\end{example}
\begin{definition}[Kernel]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \ker (T) &= \set{v\in V\mid T(v) = 0_W}.
  \end{align*}
  We call this the kernel of $T$.
\end{definition}
\begin{definition}[Image]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \img\left(T\right) &= T(V)\\
                      &= \set{w\in W\mid \exists v\in V\text{ such that }T(v) = w}
  \end{align*}
\end{definition}
\begin{lemma}[Kernel and Image are Subspaces]
  The kernel, $\ker(T)$, is a subspace of $V$, and the image, $\img\left(T\right)$, is a subspace of $W$.
\end{lemma}
\begin{proof}
  Since $T\left(0_V\right) = 0_W$, we know that both $\ker(T)$ and $\img\left(T\right)$ are nonempty.\newline

  Let $c\in \F$ and $v_1,v_2\in \ker(T)$. Then,
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right)\\
                             &= 0.
  \end{align*}
  Thus, $v_1 + cv_2 \in \ker(T)$.\newline

  Let $w_1,w_2\in \img\left(T\right)$. Then, there exist $u_1,u_2\in V$ such that $T\left(u_1\right) = w_1$ and $T\left(u_2\right) = w_2$. We have
  \begin{align*}
    T\left(u_1 + cu_2\right) &= T\left(u_1\right) + cT\left(u_2\right)\\
                             &= w_1 + cw_2,
  \end{align*}
  meaning $w_1 + cw_2\in \img\left(T\right)$, meaning $\img\left(T\right)$ is a subspace of $W$.
\end{proof}
\begin{lemma}[Injectivity of a Linear Transformation]
  $T$ is injective and only if $\ker(T) = \set{0_V}$.
\end{lemma}
\begin{proof}
  Suppose $T$ is injective. Let $v\in V$ be such that $T\left(v\right) = 0_W$. We also know that $T\left(0_V\right) = 0_W$. Since $T$ is injective, this means $v = 0_V$.\newline

  Let $\ker(T) = \set{0_V}$. Suppose $T\left(v_1\right) = T\left(v_2\right)$. Then,
  \begin{align*}
    T\left(v_1\right) - T\left(v_2\right) &= 0_W\\
    T\left(v_1 - v_2\right) &= 0_W,
  \end{align*}
  meaning $v_1 - v_2 \in \ker(T)$, meaning $v_1 - v_2 = 0_V$. Thus, $v_1 = v_2$.
\end{proof}
\begin{example}[Projection Map]
  Let $m > n$. Define $T: \F^{m}\rightarrow \F^n$ by
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix} \mapsto \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can see that $\img\left(T\right) = \F^n$.\newline

  To examine the kernel, let
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\in \ker(T).
  \end{align*}
  Then,
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\mapsto \begin{pmatrix}0\\\vdots\\0\end{pmatrix},
  \end{align*}
  with $n$ entries. Thus,
  \begin{align*}
    \ker(T) &= \set{\left. \begin{pmatrix}0\\0\\\vdots\\0\\a_{n+1}\\\vdots\\a_{m}\end{pmatrix}\right|a_i\in \F^m}\\
            &\cong \F^{m-n}.
  \end{align*}
\end{example}
\subsection{Bases and Dimension}%
For this section, we let $V $ be a $ \F$-vector space.
\begin{definition}[Linear Combination]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $v\in V$ is an $\F$-linear combination of $\mathcal{B}$ if there is a set $\set{a_i}_{i\in I}$ with $a_i = 0$ for all but finitely many $i$ such that
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  We write $v\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{definition}
\begin{example}
  Let $V = P_2\left(\F\right)$. Set $\mathcal{B} = \set{1,x,x^2}$. We have $\Span_{\F}\left(\mathcal{B}\right) = P_2\left(\F\right)$.
\end{example}
\begin{definition}[Linear Independence]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is $\F$-linearly independent if whenever
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V,
  \end{align*}
  we have $a_i = 0 $ for all $i\in I$. Note that these are finite sums.
\end{definition}
\begin{definition}[Hamel Basis]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is a $\F$-basis for $V$ if
  \begin{enumerate}[(1)]
    \item $\Span\left(\mathcal{B}\right) = V$
    \item $\mathcal{B}$ is linearly independent.
  \end{enumerate}
\end{definition}
\begin{example}[Standard Basis for $\F^n$]
Let $V = \F^n$. We let
\begin{align*}
  \mathcal{E}_n = \set{e_1,\dots,e_n},
\end{align*}
where
\begin{align*}
  e_1 &= \begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}\\
  e_2 &= \begin{pmatrix}0\\1\\\vdots\\0\end{pmatrix}\\
      &\vdots\\
  e_n &= \begin{pmatrix}0\\0\\\vdots\\1\end{pmatrix}.
\end{align*}
We have $\mathcal{E}_n$ is a basis of $\F^n$ referred to as the standard basis.
\end{example}
We wish to show that every vector space has a basis. In order to do so, we require Zorn's lemma.
\begin{theorem}[Zorn's Lemma]
  Let $X$ be a nonempty partially ordered set. If every totally ordered subset of $X$ has an upper bound, then there exists at least one maximal element in $X$.
\end{theorem}
\begin{theorem}
  Let $\mathcal{A}$ and $\mathcal{C}$ be subsets of $V$ with $\mathcal{A}\subseteq \mathcal{C}$. Assume $\mathcal{A}$ is linearly independent and $\Span_{\F}\left(\mathcal{C}\right) = V$. Then, there exists a basis $\mathcal{B}$ of $V$ with $\mathcal{A}\subseteq \mathcal{B}\subseteq \mathcal{C}$.
\end{theorem}
\begin{proof}
  Take
  \begin{align*}
    X &= \set{\mathcal{B}'\subseteq V\mid \mathcal{A}\subseteq \mathcal{B}'\subseteq \mathcal{C},\mathcal{B}\text{ linearly independent}}.
  \end{align*}
  We have $\mathcal{A}\in X$, meaning $X$ is nonempty. We know that $X$ is partially ordered with respect to inclusion, and has an upper bound of $\mathcal{C}$.\newline

  Thus, by Zorn's lemma, we have a maximal element in $X$. We call this maximal element $\mathcal{B}$. By the definition of $X$, $\mathcal{B}$ is linearly independent.\newline

  We claim that $\Span_{\F}\left(\mathcal{B}\right) = V$. If not, there exists some $v\in \mathcal{C}$ such that $v\notin \Span_{\F}\left(\mathcal{B}\right)$. However, if $v\notin \Span_{\F}\left(\mathcal{B}\right)$, then $\mathcal{B}\cup \set{v}\subseteq \mathcal{C}$ is linearly independent. However, since $\mathcal{B}\subsetneq \mathcal{B}\cup \set{v}$, this implies that $\mathcal{B}$ is not maximal, which is a contradiction. Thus, $\Span_{\F}\left(\mathcal{B}\right) = V$.
\end{proof}
\begin{remark}
This proof applies to all vector spaces, not just those with finite dimensions.
\end{remark}
\begin{lemma}
  A homogeneous system of $m$ linear equations in $n$ unknowns with $m < n$ has a nonzero solution.
\end{lemma}
\begin{corollary}
  Let $\mathcal{B}\subseteq V$ with $\Span_{\F}\left(\mathcal{B}\right) = V$ and $\left\vert \mathcal{B} \right\vert = m$.\newline

  Then, any set with more than $m$ elements cannot be linearly independent.
\end{corollary}
\begin{proof}
  Let $\mathcal{C} = \set{w_1,\dots,w_n}$ with $n > m$. We wish to show that $\mathcal{C}$ cannot be linearly independent.\newline

  Write $\mathcal{B} = \set{v_1,\dots,v_m}$ with $\Span_{\F}\left(\mathcal{B}\right) = V$. For each $i$, write $w_i = \sum_{j=1}^{m}a_{ji}v_j$ for some $a_{ji}\in \F$.\newline

  Consider the equations
  \begin{align*}
    \sum_{i=1}^{n}a_{ji}x_i = 0.
  \end{align*}
  We have a solution to this $\left(c_1,\dots,c_n\right) \neq \left(0,\dots,0\right)$.\newline

  We have
  \begin{align*}
    0 &= \sum_{j=1}^{m} \left(\sum_{i=1}^{n}a_{ji}c_i\right)v_j\\
      &= \sum_{i=1}^{n}c_i\left(\sum_{j=1}^{m}a_{ji}v_j\right)\\
      &= \sum_{i=1}^{n}c_iw_i.
  \end{align*}
  Thus, $\mathcal{C}$ is not linearly independent.
\end{proof}
\begin{corollary}
  If $\mathcal{B}$ and $\mathcal{C}$ are bases over $V$, with $\mathcal{B}$ and $\mathcal{C}$ finite, then $\Card \mathcal{B} = \Card \mathcal{C}$.
\end{corollary}
\begin{proof}
  Let $|\mathcal{B}| = m$, $|\mathcal{C}| = n$. Since $\mathcal{C}$ is linearly independent, we know that $n\leq m$. We reverse the roles to see that $m\leq n$.
\end{proof}
\begin{definition}[Dimension]
  Let $V$ be a $\F$-vector space with Hamel basis $\mathcal{B}$. Then, we define $\Dim_{\F} V = \Card \mathcal{B}$.
\end{definition}
\begin{theorem}
  Let $V$ be finite-dimensional with $\Dim_{\F} V = n$. Let $\mathcal{C} \subseteq V$ with $\Card \mathcal{C} = m$.
  \begin{enumerate}[(1)]
    \item If $m > n$, then $\mathcal{C}$ is not linearly independent.
    \item If $m < n$, then $\Span_{\F}\left(\mathcal{C}\right) \neq V$.
    \item If $m = n$, then the following are equal:
      \begin{itemize}
        \item $\mathcal{C}$ is a basis;
        \item $\mathcal{C}$ is linearly independent;
        \item $\Span_{\F}\left(\mathcal{C}\right) = V$.
      \end{itemize}
  \end{enumerate}
\end{theorem}
\begin{corollary}
  Let $W\subseteq V$ be a subspace. We have $\Dim_{\F}W \leq \Dim_{\F} V$.\newline

  If $\Dim_{\F} V < \infty$, then $V = W$ if and only if $\Dim_{\F} W = \Dim_{\F} V$.
\end{corollary}
\begin{example}
  Let $V = \C$.\newline

  If $\F = \C$, then $\mathcal{B} = \set{1}$, and $\Dim_{\C}\C = 1$.\newline

  If $\F = \R$, then $\mathcal{B} = \set{1,i}$, and $\Dim_{\R}\C = 2$.

  %If $\F = \Q$, then $\mathcal{B}$ is uncountable.
\end{example}
\begin{example}
  Let $V = \F[x]$, and let $f(x) \in \F[x]$ be fixed.\newline

  Define an equivalence relation $g(x) \equiv h(x) $ if $f(x)|\left(g(x) - h(x)\right)$.\newline

  Given $g(x) \in \F[x]$, write $\left[g(x)\right]$ for the equivalence class containing $g(x)$.\newline

  Define $W = \F[x] / \left(f(x)\right) = \set{\left[g(x)\right]\mid g(x)\in \F[x]}$.\newline

  Define
  \begin{align*}
    [g(x)] + [h(x)] &= [g(x) + h(x)]\\
    c[g(x)] &= [cg(x)].
  \end{align*}
  This makes $W$ into a vector space. Set $n = \deg f(x)$.\newline

  Then, we claim
  \begin{align*}
    \mathcal{B} = \set{[1],[x],\dots,\left[x^{n-1}\right]}.
  \end{align*}
  Suppose there exist $a_0,\dots,a_{n-1} \in \F$ with
  \begin{align*}
    a_0 [1] + a_1[x] + \cdots + a_{n-1}\left[x^{n-1}\right] = [0].
  \end{align*}
  Then,
  \begin{align*}
    \left[a_0 + a_1x + \cdots + a_{n-1}x^{n-1}\right] = [0].
  \end{align*}
  Therefore,
  \begin{align*}
    f(x) | \left(a_0 + a_1x + \cdots + a_{n-1}x^{n-1} - 0\right),
  \end{align*}
  which means we must have $a_0 = a_1 = \cdots = a_{n-1}$.\newline

  Let $\left[g(x)\right]\in W$. By the Euclidean algorithm,
  \begin{align*}
    g(x) &= f(x)q(x) + r(x)
  \end{align*}
  for some $q(x),r(x) \in \F[x]$ with $r(x) = 0$ or $\deg r(x) < n$. Thus, we have
  \begin{align*}
    \left[g(x)\right] &= \left[f(x)q(x)\right] + \left[r(x)\right]\\
                      &= \left[r(x)\right].
  \end{align*}
  Since $r(x) = 0$ or $\deg r(x) < n$, we must have $\left[g(x)\right] = \left[r(x)\right]\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{example}
\begin{lemma}
  Let $V$ be an $\mathbb{F}$-vector space, with $\mathcal{C} = \set{v_i}_{i\in I}$ be a subset of $V$.\newline

  Then, $\mathcal{C}$ is a basis if and only if each $v\in V$ can be uniquely written as a linear combination of elements of $\mathcal{C}$.
\end{lemma}
\begin{proof}
  Suppose $\mathcal{C}$ is a basis. Let $v\in V$, and suppose
  \begin{align*}
    v &= \sum_{i\in I}a_iv_i\\
      &= \sum_{i\in I}b_iv_i
  \end{align*}
  for some $a_i,b_i\in \mathbb{F}$. Then,
  \begin{align*}
    0_V &= \sum_{i\in I}\left(a_i - b_i\right)v_i.
  \end{align*}
  Since $\mathcal{C}$ is a basis, $a_i - b_i = 0$ for all $i$, meaning $a_i = b_i$, so the expression is unique.\newline

  Suppose every $v$ can be written as a unique linear combination of $\mathcal{C}$. Certainly, this means $\Span_{\mathbb{F}}\left(\mathcal{C}\right) = V$. Suppose
  \begin{align*}
    0_V &= \sum_{i\in I}a_iv_i
  \end{align*}
  for some $a_i\in \mathbb{F}$. It is also true that $0_V = \sum_{i\in I}0v_i$, meaning $a_i = 0$ for all $i$ by uniqueness; thus, $\mathcal{C}$ is linearly independent.
\end{proof}
\begin{proposition}
  Let $V,W$ be $\mathbb{F}$-vector spaces.
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,W\right)$. We have $T$ is uniquely determined by the image of the basis of $V$.
    \item Let $\mathcal{B}=\set{v_i}_{i\in I}$ be a basis of $V$, and let $\mathcal{C} = \set{w_i}$ be a subset of $W$. If $\Card(\mathcal{B}) = \Card\left(\mathcal{C}\right)$, there is a $T\in \Hom_{\F}\left(V,W\right)$ such that $T\left(v_i\right) = w_i$ for every $i$
  \end{enumerate}
\end{proposition}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Let $v\in V$, let $\mathcal{B} = \set{v_i}$ be a basis of $V$, and write $v = \sum_{i\in I}a_iv_i$. We have
  \begin{align*}
    T\left(v\right) &= T\left(\sum_{i\in I}a_iv_i\right)\\
                    &= \sum_{i\in I}a_iT\left(v_i\right).
  \end{align*}
    \item  Define $T$ by setting
      \begin{align*}
        T\left(v\right) &= \sum_{i\in I}a_iw_i,
      \end{align*}
      for $v = \sum_{i\in I}a_iv_i$. We can verify that $T$ is linear.
  \end{enumerate}
\end{proof}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$, with $\mathcal{B} = \set{v_i}$ a basis of $V$ and $\mathcal{C} = \set{w_i}\subseteq W$, with $w_i = T\left(v_i\right)$. Then, we have $\mathcal{C}$ is a basis of $W$ if and only if $T$ is an isomorphism.
\end{corollary}
\begin{proof}
  Let $\mathcal{C}$ be a basis for $W$. Since $\mathcal{C}$ is a basis of $W$, we use the proposition to define $S\in \Hom_{\F}\left(W,V\right)$ with $S\left(w_i\right) = v_i$. We can verify that $T\circ S = \text{id}_{W}$ and $S\circ T = \text{id}_V$, meaning $S = T^{-1}$ and $T$ is an isomorphism.\newline

  Suppose $T$ is an isomorphism. Let $w\in W$. Since $T$ is an isomorphism, $T$ is surjective, meaning there exists $v\in V$ such that $T(v) = w$. Since $\mathcal{B}$ is a basis of $V$, we expand $v$ to have
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  Combining these two facts, we have
  \begin{align*}
    w &= T(v)\\
      &= T\left(\sum_{i\in I}a_iv_i\right)\\
      &= \sum_{i\in I}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right).
  \end{align*}
  Thus, $W = \Span_{\F}\left(\mathcal{C}\right)$.\newline

  Suppose there exists $a_i\in \F$ with $\sum_{i\in I}a_iT\left(v_i\right) = 0_W$. Since $T$ is linear, we have
  \begin{align*}
    \sum_{i\in I}a_iT\left(v_i\right) &= T\left(\sum_{i\in I}a_iv_i\right).
  \end{align*}
  Since $T$ is injective, we have
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V.
  \end{align*}
  Since $\mathcal{B}$ is a basis, we have $a_i = 0$.
\end{proof}
\begin{theorem}[Rank--Nullity]
  Let $V$ be finite-dimensional vector space over $\mathbb{F}$. Let $T\in \Hom_{\F}\left(V,W\right)$. Then,
  \begin{align*}
    \Dim_{\F}(V) &= \Dim_{\F}\left(\ker (T)\right) + \Dim_{\F}\left(\img (T)\right)
  \end{align*}
\end{theorem}
\begin{proof}
  Let $\Dim_{\F}\left(\ker(T)\right) = k$ and $\Dim_{\F}\left(V\right) = n$. Let $\mathcal{A} = \set{v_1,\dots,v_k}$ be a basis of $\ker(T)$. We extend $\mathcal{A}$ to a basis $\mathcal{B} = \set{v_1,\dots,v_n}$ of $V$.\newline

  We want to show that $\mathcal{C} = \set{T\left(v_{k+1}\right),\dots,T\left(v_n\right)}$ is a basis of $\img(T)$.\newline

  Let $w\in \img(T)$. Then, there is $v\in V$ such that $T(v) = w$. We write
  \begin{align*}
    v &= \sum_{i=1}^{n}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    w &= T\left(v\right)\\
      &= T\left(\sum_{i=1}^{n}a_iv_i\right)\\
      &= \sum_{i=1}^{n}a_iT\left(v_i\right)\\
      &= \sum_{i=k+1}^{n}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right),
  \end{align*}
  since $\set{v_1,\dots,v_k}\subseteq \ker(T)$, meaning $\Span_{\F}\left(\mathcal{C}\right) = \im(T)$.\newline

  Suppose we have
  \begin{align*}
    \sum_{i=k+1}^{n}a_iT\left(v_i\right) = 0_W.
  \end{align*}
  Then, we have
  \begin{align*}
    T\left(\sum_{i=k+1}^{n}a_iv_i\right) &= 0_W,
  \end{align*}
  meaning $\sum_{i=k+1}^{n}a_iv_i\in \ker(T)$. This means there exist $a_1,\dots,a_k$ such that
  \begin{align*}
    \sum_{i=k+1}^{n}a_iv_i &= \sum_{i=1}^{k}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    \sum_{i=1}^{k}a_iv_i + \sum_{i=k+1}^{n}\left(-a_i\right)v_i = 0_V.
  \end{align*}
  Since $\set{v_i}$ are a basis, this means $a_i = 0$ for all $i$.
\end{proof}
\begin{corollary}
  Let $V,W$ be $\mathbb{F}$-vector spaces with $\Dim_{\F}\left(V\right) = n$. Let $V_1\subseteq V$ be a subspace with $\Dim_{\F}\left(V_1\right) = k$, and $W_1\subseteq W$ a subspace with $\Dim_{\F}\left(W_1\right) = n-k$. Then, there exists $T\in \Hom_{\F}\left(V,W\right)$ such that $\ker(T) = V_1$ and $\img(T) = W_1$.
\end{corollary}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$ with $\Dim_{\F}\left(V\right) = \Dim_{\F}\left(W\right) < \infty$. Then, the following are equivalent:
  \begin{enumerate}[(1)]
    \item $T$ is an isomorphism;
    \item $T$ is injective;
    \item $T$ is surjective.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $A\in \Mat_{n}\left(\F\right)$. The following are equivalent:
  \begin{enumerate}[(1)]
    \item $A$ is invertible;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $BA = I_{n}$;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $AB = I_n$.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $\Dim_{\F}(V) = m$ and $\Dim_{\F}(W) = n$.
  \begin{enumerate}[(1)]
    \item If $m < n$ and $T\in \Hom_{\F}\left(V,W\right)$, then $T$ is not surjective.
    \item If $m > n$ and $T\in \Hom_\F\left(V,W\right)$, then $T$ is not injective.
    \item We have $m = n$ if and only if $V\cong W$.
  \end{enumerate}
\end{corollary}
\subsection{Direct Sums and Quotient Spaces}%
\begin{definition}[Sum of Subspaces]
  Let $V$ be a vector space, and $V_1,\dots,V_k$ be subspaces. Then, the sum of $V_1,\dots,V_k$ is
  \begin{align*}
    V_1 + \cdots + V_k &= \set{\sum_{i=1}^{k}v_i\mid v_i\in V_i}.
  \end{align*}
  This is a subspace of $V$.
\end{definition}
\begin{definition}[Independence of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V_1,\dots,V_k$ are independent if whenever $v_1 + \cdots v_k = 0_V$, we have $v_i = 0_V$.
\end{definition}
\begin{definition}[Direct Sum of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V$ is the direct sum of $V_1,\dots,V_k$, and write
  \begin{align*}
    V = V_1 \oplus \cdots \oplus V_k,
  \end{align*}
  if the following conditions hold.
  \begin{enumerate}[(1)]
    \item $V = V_1 + \cdots V_k$;
    \item $V_1,\dots,V_k$ are independent.
  \end{enumerate}
\end{definition}
\begin{example}[A Very Simple Direct Sum]
  Let $V = \F^2$, with $V_1 = \set{\left(x,0\right)\mid x\in \F}$ and $V_2 = \set{\left(0,y\right)\mid y\in \F}$, we can see that
  \begin{align*}
    V_1 + V_2 &= \set{\left(x,0\right) + \left(0,y\right)\mid x,y\in \F}\\
              &= \set{\left(x,y\right)\mid x,y\in \F}\\
              &= \F^2.
  \end{align*}
  If $\left(x,0\right) + \left(0,y\right) = 0$, then $x = 0$ and $y = 0$, meaning $\F^2 = V_1\oplus V_2$.
\end{example}
\begin{example}[Direct Sum Constructions]
  Let $V = \F[x]$.\newline

  Define $V_1 = \F$, $V_2 = \F x = \set{\alpha x\mid \alpha \in \F}$, $V_3 = P_1\left(\F\right)$.\newline

  We can see that
  \begin{align*}
    P_1 &= V_1\oplus V_2.
  \end{align*}
  However, $V_1$ and $V_3$ are not independent, since $1_{\F}\in V_1$ and $-1_{\F}\in V_3$ with $1_{\F} + \left(-1_{\F}\right) = 0_\F$.
\end{example}
\begin{example}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$, with $V_i = \Span(v_i)$. Then,
  \begin{align*}
    V = V_1\oplus \cdots \oplus V_n.
  \end{align*}
\end{example}
\begin{lemma}
  Let $V$ be a vector space, $V_1,\dots,V_k$ subspaces. We have $V = V_1\oplus \cdots \oplus V_k$ if and only if every $v\in V$ can be written uniquely in the form
  \begin{align*}
    v = v_1 +\cdots + v_k
  \end{align*}
  for $v_i\in V_i$.
\end{lemma}
\begin{proof}
  Suppose $V = V_1\oplus\cdots\oplus V_k$. Let $v\in V$. Then, $v = v_1 + \cdots + v_k$ for some $v_i\in V_i$ since $V = V_1+\cdots+V_k$. Suppose
  \begin{align*}
    v &= v_1 + \cdots v_k\\
      &= \tilde{v}_1 + \cdots + \tilde{v}_k
  \end{align*}
  for $v_i,\tilde{v}_i\in V_i$. Then,
  \begin{align*}
    0_V &= \left(v_1 - \tilde{v}_1\right) + \cdots + \left(v_k - \tilde{v}_k\right).
  \end{align*}
  Since $V_1,\dots,V_k$ are linearly independent, $v_i - \tilde{v}_i\in V_i$, we have $v_i - \tilde{v}_i = 0_V$, meaning the expression for $v$ is unique.\newline

  Suppose that every $v\in V$ can be written uniquely in the form $v = v_1 + \cdots + v_k$ with $v_i\in V_i$. Then,
  \begin{align*}
    V = V_1 + \cdots V_k
  \end{align*}
  by the definition of $V_1 + \cdots + V_k$. If
  \begin{align*}
    0_V = v_1 + \cdots v_k
  \end{align*}
  for $v_i\in V_i$, and it is also the case that
  \begin{align*}
    0_V = 0_V + \cdots + 0_V,
  \end{align*}
  with $0_V \in V_i$, then it must be the case that $v_i = 0_V$ for all $i$ by uniqueness. Thus, the $V_i$ are independent, so
  \begin{align*}
    V = V_1\oplus\cdots\oplus V_k.
  \end{align*}
\end{proof}
\begin{exercise}
  Let $V_1,\dots,V_k$ be subspaces of $V$. For each $i$, let $\mathcal{B}_i$ be a basis for $V_i$. Let $\mathcal{B} = \bigcup_{i =1}^{k}\mathcal{B}_i$. Show
  \begin{enumerate}[(1)]
    \item $\mathcal{B}$ spans $V$ if and only if $V = V_1 + \cdots + V_k$;
    \item $\mathcal{B}$ is linearly independent if and only if $V_1,\dots,V_k$ are independent;
    \item $\mathcal{B}$ is a basis if and only if $V = V_1 \oplus \cdots \oplus V_k$.
  \end{enumerate}
\end{exercise}
\begin{lemma}[Existence of Complement]
Let $V$ be a vector space, and $U\subseteq V$ be a subspace. Then, $U$ has a complement $W$ such that $U\oplus W = V$.
\end{lemma}
\begin{proof}
  Let $\mathcal{A} $ be a basis for $U$. Extend $\mathcal{A}$ to a basis $\mathcal{B}$ of $V$. Let $\mathcal{C} = \mathcal{B}\setminus \mathcal{A}$, and $W = \Span\left(\mathcal{C}\right)$.
\end{proof}
\begin{example}[Constructing a Quotient Group]
To introduce quotient spaces, consider the construction of the quotient group.\newline

Let $n\in \Z_{>1}$. We say $a \equiv b$ modulo $n$ if and only if $n|(a-b)$. This is an equivalence relation; we form $\Z/n\Z = \set{\left[a\right]_n\mid a\in\Z} = \set{\left[0\right]_n,\dots,\left[n-1\right]_n}$.\newline

However, we also do this by defining $n\Z = \set{nk\mid k\in\Z}$, and taking $a\equiv b$ mod $n$ if and only if $a-b \in n\Z$. Our equivalence classes are now
\begin{align*}
  \left[a\right]_n &= \set{a + nk\mid k\in\Z}\\
                   &= a + n\Z.
\end{align*}
\end{example}
\begin{definition}[Quotient Space]
  Let $W\subseteq V$ be a subspace. We say $v_1\sim v_2$ if $v_1 - v_2 \in W$. Note that if $w\in W$, then $w\sim 0_V$ since $w-0_V\in W$.\newline

  This is an equivalence relation.
  \begin{itemize}
    \item Reflexivity: since $W$ is a subspace, $0_V\in W$, meaning $v-v\in W$ for all $v\in V$.
    \item Symmetry: if $v_1\sim v_2$, then $v_1 - v_2 \in W$, meaning $-\left(v_1 - v_2\right)\in W$, so $v_2 - v_1\in W$, or $v_2 \sim v_1$.
    \item Transitivity: Let $v_1\sim v_2$ and $v_2\sim v_3$. Then, $v_1 - v_2\in W$ and $v_2 - v_3\in W$. Since $W$ is a subspace, $\left(v_1 - v_2\right) + \left(v_2 - v_3\right)\in W$, meaning $v_1 - v_3 \in W$, so $v_1 \sim v_3$.
  \end{itemize}
  We denote the equivalence classes by
  \begin{align*}
    \left[v\right] &= \left[v\right]_W\\
                   &= v+W\\
                   &= \set{\tilde{v}\in V\mid v\sim \tilde{v}}\\
                   &= \set{v+w\mid w\in W}.
  \end{align*}
  We set
  \begin{align*}
    V/W &:= \set{v + W\mid v\in V}.
  \end{align*}
  We need to define vector addition and scalar multiplication on $V/W$. Let $v_1 + W,v_2 + W\in V/W$ and $c\in\F$. Define
  \begin{align*}
    \left(v_1 + W\right) + \left(v_2 + W\right) &= \left(v_1 + v_2\right) + W\\
    c\left(v_1 + W\right) &= cv_1 + W.
  \end{align*}
  We will show that addition and scalar-multiplication are well-defined.
  \begin{description}
    \item[Addition:] Let $v_1 + W = \tilde{v}_1 + W$, $v_2 + W = \tilde{v}_2 + W$, meaning $v_1 = \tilde{v}_1 + w_1$ and $v_2 = \tilde{v}_2 + w_2$ for some $w_1,w_2 \in W$. We have
      \begin{align*}
        \left(v_1 + W\right) + \left(v_2 + W\right) &= \left(v_1 + v_2\right) + W\\
                                                    &= \left(\tilde{v}_1 + w_1 + \tilde{v}_2 + w_2\right) + W\\
                                                    &= \left(\tilde{v}_1 + \tilde{v}_2\right) + W
      \end{align*}
    \item[Scalar Multiplication:] Let $v + W = \tilde{v} + W$. Then, we have $v = \tilde{v} + w$ for some $w\in W$. For $c\in\F$, we have
      \begin{align*}
        c\left(v + W\right) &= cv + W\\
                            &= c\left(\tilde{v} + w\right) + W\\
                            &= c\tilde{v} + W\\
                            &= c\left(\tilde{v} + W\right).
      \end{align*}
  \end{description}
  We say $V/W$ is the quotient space of $V$ by $W$.
\end{definition}
\begin{example}[Quotient Space of $\R^2$]
  Let $V = \R^2$, and $W = \set{\left(x,0\right)\mid x\in \R}$.\newline

  Let $\left(x_0,y_0\right)\in V$. We have
  \begin{align*}
  \left(x_0,y_0\right) \sim \left(x,y\right)
  \end{align*}
  if
  \begin{align*}
    \left(x_0 - x,y_0 - y\right)\in W.
  \end{align*}
  The only condition is thus that the $y$-coordinates in $\R^2$ must be equal. Therefore,
  \begin{align*}
    \left(x_0,y_0\right) + W &= \set{(x,y_0)\mid x\in\R}.
  \end{align*}
  Define $\tau: \R\rightarrow V/W$, $y\mapsto \left(0,y\right) + W$. We claim that $\tau$ is an isomorphism.\newline

  Let $y_1,y_2,c\in\R$. We have
  \begin{align*}
    \tau\left(y_1 + cy_2\right) &= \left(0,y_1 + cy_2\right) + W\\
                                &= \left(\left(0,y_1\right) + W\right) + c\left(\left(0,y_2\right) + W\right)\\
                                &= \tau\left(y_1\right) + c\tau\left(y_2\right).
  \end{align*}
  Thus, we see that $\tau$ is a linear map.\newline

  To show surjectivity, let $\left(x,y\right) + W\in V/W$. We have $\left(x,y\right) + W = \left(0,y\right) + W$. Thus, $\tau$ is surjective, since
  \begin{align*}
    \tau\left(y\right) &= \left(0,y\right) + W\\
                       &= \left(x,y\right) + W.
  \end{align*}
  Finally, to show injectivity, we let $y\in\ker\left(\tau\right)$. We have
  \begin{align*}
    \tau\left(y\right) &= \left(0,y\right) + W\\
                       &= \left(0,0\right) + W,
  \end{align*}
  implying that $y = 0$. Thus, $\tau$ is injective.
\end{example}
\begin{example}[Quotient Space of Polynomials]
  Let $V = \F[x]$, $f(x) \in V$, and
  \begin{align*}
    W &= \set{g(x)\in \F[x]\mid f(x)|g(x)}.
  \end{align*}
  We can see that $W$ is a subspace, which we refer to as $\left\langle f(x) \right\rangle$.\newline

  We defined an equivalence class $g(x) \sim h(x)$ if $f(x) | \left(g(x) - h(x)\right)$, where we then constructed a vector space from this set.\newline

  In particular, this construction is realized as $V/W$.\footnote{The ramifications of this construction are covered in depth in Algebra II.}
\end{example}
\begin{definition}[Canonical Projection]
  Let $W\subseteq V$ be a subspace. The canonical projection map $\pi_W$ is defined by
  \begin{align*}
    \pi_W: V\rightarrow V/W\\
    v\mapsto v + W.
  \end{align*}
  Note that $\pi_W\in \Hom_{\F}\left(V,V/W\right)$.
\end{definition}
\begin{remark}
  To define a map $T: V/W\rightarrow U$, one must always verify that $T$ is well-defined.
\end{remark}
\begin{theorem}[First Isomorphism Theorem for Vector Spaces]
  Let $T\in\Hom_{\F}\left(V,W\right)$. Define $\overline{T}: V/\ker(T)\rightarrow W$ by taking $v + \ker(T) \mapsto T(v)$. Then, $\overline{T}\in \Hom_{\F}\left(V/\ker(T),W\right)$. Moreover, $V/\ker(T)\cong \img(T)$.
\end{theorem}
\begin{proof}
  We will first show that $\overline{T}$ is well-defined. Let $v_1 + \ker(T) = v_2 + \ker(T)$. Then, for some $\tilde{v}\in \ker(T)$, we have $v_1 = v_2 + \tilde{v}$. Then,
  \begin{align*}
    \overline{T}\left(v_1 + \ker(T)\right) &= T\left(v_1\right)\\
                                           &= T\left(v_2 + \tilde{v}\right)\\
                                           &= T\left(v_2\right) + T\left(\tilde{v}\right)\\
                                           &= T\left(v_2\right)\\
                                           &= \overline{T}\left(v_2 + \ker(T)\right).
  \end{align*}
  Let $v_1 + \ker(T)$, $v_2 + \ker(T)\in V/\ker(T)$, and $c\in \F$. Then, we have
  \begin{align*}
    \overline{T}\left(\left(v_1 + \ker(T)\right) + c\left(v_2 + \ker(T)\right)\right) &= \overline{T}\left(\left(v_1 + cv_2\right) + \ker(T)\right)\\
                                                                                      &= T\left(v_1 + cv_2\right)\\
                                                                                      &= T\left(v_1\right) + cT\left(v_2\right)\\
                                                                                      &= \overline{T}\left(v_1 + \ker(T)\right) + c\overline{T}\left(v_2 + \ker(T)\right).
  \end{align*}
  Let $w\in \img(T)$. Then, $w = T(v)$ for some $v\in V$, meaning
  \begin{align*}
    w &= T\left(v\right)\\
    &= \overline{T} \left(v + \ker(T)\right).
  \end{align*}
  Thus, $\overline{T}$ is surjective onto $\img(T)$.\newline

  Let $v + \ker(T)\in \ker\left(\overline{T}\right)$. Then,
  \begin{align*}
    \overline{T}\left(v + \ker(T)\right) &= 0_W.
  \end{align*}
  This gives
  \begin{align*}
    T\left(v\right) = 0_W,
  \end{align*}
  meaning $v\in \ker(T)$, meaning $v + \ker(T) = 0_V + \ker(T)$. Thus, $\overline{T}$ is injective.
\end{proof}
\subsection{Dual Spaces}%
\begin{definition}[Dual Space]
  Let $V$ be an $\F$-vector space. The dual space, $V'$,\footnote{My professor denotes this as $V^{\vee}$, but it's too hard to type that out in real time, so I will use the $'$ to denote the algebraic dual, just as $V^{\ast}$ denotes the continuous dual of $V$.} is defined to be
  \begin{align*}
    V':= \Hom_{\F}\left(V,\F\right).
  \end{align*}
\end{definition}
\begin{theorem}
  We have $V$ is isomorphic to a subspace of $V'$. If $\Dim_{\F}\left(V\right) < \infty$, then $V\cong V'$.
\end{theorem}
\begin{remark}
  The isomorphism between $V$ and $V'$ in the finite-dimensional case is not canonical --- that is, it depends on a basis.
\end{remark}
\begin{proof}
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a basis for $V$.\newline

  For each $i\in I$, let $v_i'(v_j) = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta. We get $\set{v_i'}_{i\in I}$ are elements of $V'$. We obtain
  \begin{align*}
    T\in \Hom_{\F}\left(V,V'\right)
  \end{align*}
  by $T\left(v_i\right) = v_i'$.\newline

  To show $V$ is isomorphic to a subspace of $V'$, it suffices to show that $T$ is injective, since $V\cong \img(T)$, which is a subspace of $V'$.\newline

  Let $v\in V$ with $T(v) = 0_{V'}$. We write
  \begin{align*}
    v &= \sum_{i\in I}a_iv_i\\
    0_{V'}&= T(v)\\
                    &= \sum_{i\in I}a_iT\left(v_i\right)\\
                    &= \sum_{i\in I}a_iv_i'.
  \end{align*}
  Pick $j$ with $a_j\neq 0$. Note that
  \begin{align*}
    \sum_{i\in I}a_iv_i'(v_j) &= 0\\
                              &= a_j,
  \end{align*}
  which contradicts $a_j\neq 0$. Thus, $v = 0_V$, and $T$ is injective.\newline

  Suppose $\Dim_{\F}\left(V\right) = n$, with $\mathcal{B} = \set{v_1,\dots,v_n}$. Let $v'\in V'$. Define $a_i$ by
  \begin{align*}
    a_i &= v'\left(v_i\right).
  \end{align*}
  Set
  \begin{align*}
    v &= \sum_{i=1}^{n}a_iv_i.
  \end{align*}
  Define the map $S: V'\rightarrow V$ by taking
  \begin{align*}
    S\left(v'\right) &= \sum_{i=1}^{n}\left(v'\left(v_i\right)\right)v_i.
  \end{align*}
  We want to show that $S\in \Hom_{\F}\left(V',V\right)$, and $S$ is the inverse to $T$.\newline

  Let $v',w'\in V'$, $c\in \F$. Set $a_i = v'\left(v_i\right)$ and $b_i = w'\left(v_i\right)$. Then,
  \begin{align*}
    S\left(v' + cw'\right) &= \sum_{i=1}^{n}\left(v' cw'\right)\left(v_i\right)v_i\\
                           &= \sum_{i=1}^{n}\left(v'\left(v_i\right) + cw'\left(v_i\right)\right)v_i\\
                           &= \sum_{i=1}^{n}\left(v'\left(v_i\right)\right)v_i + c\sum_{i=1}^{n}w'\left(v_i\right)\\
                           &= S\left(v'\right) + cS\left(w'\right).
  \end{align*}
  We compute $S\circ T\left(v_i\right)$.
  \begin{align*}
    S\circ T\left(v_j\right) &= S\left(T\left(v_j\right)\right)\\
                             &= S\left(v_j'\right)\\
                             &= \sum_{i=1}^{n}v_j'\left(v_i\right)v_i\\
                             &= \sum_{i=1}^{n}\delta_{ij}v_i\\
                             &= v_j.
  \end{align*}
  Note that for $T\circ S$, we have $T\circ S$ maps $V'$ to $V'$, meaning we need to check that $T\circ S$ is the identity map on $V'$. Let $v'\in V'$. Then,
  \begin{align*}
    \left(T\circ S\right)\left(v'\right)\left(v_j\right) &= T\left(S\left(v'\right)\right)\left(v_j\right)\\
                                                         &= T\left(\sum_{i=1}^{n}v'\left(v_i\right)v_i\right)\left(v_j\right)\\
                                                         &= \left(\sum_{i=1}^{n}v'\left(v_i\right)T\left(v_i\right)\right)\left(v_j\right)\\
                                                         &= \sum_{i=1}^{n}v'\left(v_i\right)\left(v_i'\left(v_j\right)\right)\\
                                                         &= \sum_{i=1}^{n}v'\left(v_i\right)\delta_{ij}\\
                                                         &= v'\left(v_j\right).
  \end{align*}
\end{proof}
\begin{definition}[Dual Basis]
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$. The dual basis for $V'$ is
  \begin{align*}
    \mathcal{B}' &= \set{v_i',\dots,v_n'}.
  \end{align*}
\end{definition}
\begin{remark}
  It is possible to continue taking duals; in the case of finite-dimensional $V$, we have
  \begin{align*}
    V &\cong V'\\
    V' &\cong V''.
  \end{align*}
  Despite the isomorphism between $V$ and $V'$ not being canonical, it is the case that the isomorphism between $V$ and $V''$ \textit{is} canonical (i.e., not dependent on a basis).
\end{remark}
\begin{proposition}
  There is a canonical injective linear map from $V$ to $V''$. If $\Dim_{\F}\left(V\right) < \infty$, this is an isomorphism.
\end{proposition}
\begin{proof}
  Let $v\in V$. Define $\hat{v}: V' \rightarrow \F$, $\varphi \mapsto \varphi(v)$.\footnote{This can be notated as $\text{eval}_v$, but $\hat{v}$ is faster to type (and it's used in functional analysis).} We can easily verify that $\hat{v}$ is a linear map.\newline

  Therefore, we have $\hat{v}\in \Hom_{\F}\left(V',\F\right) = V''$. We have a map
  \begin{align*}
    \Phi: V\rightarrow V''\\
    v\mapsto \hat{v}.
  \end{align*}
  We want to verify that $\Phi$ is a linear and injective map. Let $v_1,v_2\in V$, $c\in \F$. Let $\varphi\in V'$.
  \begin{align*}
    \Phi\left(v_1 + cv_2\right)\left(\varphi\right) &= \left(\hat{v}_1 + c\hat{v}_2\right)\left(\varphi\right)\\
                                                    &= \varphi\left(v_1 + cv_2\right)\\
                                                    &= \varphi\left(v_1\right) + c\varphi\left(v_2\right)\\
                                                    &= \hat{v}_1\left(\varphi\right) + c\hat{v}_2\left(\varphi\right)\\
                                                    &= \Phi\left(v_1\right)(\varphi) + c\Phi\left(v_2\right)\left(\varphi\right).
  \end{align*}
  We will show that $\Phi$ is injective. Let $v\in V$; suppose $v\neq 0_V$. We form a basis $\mathcal{B}$ of $V$ that contains $v$. Note that $v'\in V'$, with $v'(v) = 1$ and $v'(w) = 0$ for $w\in \mathcal{B}$ and $w\neq v$.\newline
  
  Assume $v\in \ker\left(\Phi\right)$. Then, for any $\varphi\in V'$,
  \begin{align*}
    \Phi\left(v\right)(\varphi) &= 0\\
    \varphi(v) &= 0.
  \end{align*}
  However, this is a contradiction, as we can take $\varphi = v'$, where $\varphi(v) = 1$. Thus, it must be the case that $\Phi$ is injective.
\end{proof}
\begin{definition}[Dual Operator]
  Let $T\in \Hom_{\F}\left(V,W\right)$. We get an induced map $T': W'\rightarrow V'$. We define $T'\left(\varphi\right) = \varphi\circ T$.
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEio4ePrNWiEAB1dAMW7iYUAObwioAGYAnCAFskZEDghIATNU1SdAFQByAAp9els0AAssAEoQagY6ACMYBgAFfmUhEFssMwicORt7J0QXNyRRCS02P0KQO0cK6nLELyrfPV0wyKxjLiA
\begin{tikzcd}
V \arrow[rd, "T'(\varphi)"'] \arrow[r, "T"] & W \arrow[d, "\varphi"] \\
                                            & \F                    
\end{tikzcd}
  \end{center}
\end{definition}
\section{Choosing Coordinates}%
\subsection{Linear Transformations and Matrices}%
Let $V$ be a finite-dimensional $\mathbf{F}$-vector space. Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis. This vector space fixes an isomorphism $V \cong \mathbf{F}^n$.\newline

Let $v\in V$. We can write $v = \sum_{i=1}^{n}a_iv_i$ for some $a_i\in \F$. We take the map
\begin{align*}
  T_{\mathcal{B}}\left(v\right) &= \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\in \F^n.
\end{align*}
It is easy to see that $T$ is an isomorphism. Given $v\in V$, we write $\left[v\right]_{\mathcal{B}} = T_{\mathcal{B}}\left(v\right)$. We refer to this process as choosing coordinates.
\begin{example}
  Let $V = \Q^2$, and $\mathcal{B} = \set{ \begin{pmatrix}1\\1\end{pmatrix}, \begin{pmatrix}1\\-1\end{pmatrix} }$. We can check that $\mathcal{B}$ is a basis of $V$.\newline

  Let $v\in V$, $v = \begin{pmatrix}a\\b\end{pmatrix}$. We have
  \begin{align*}
    v &= \frac{a+b}{2} \begin{pmatrix}1\\1\end{pmatrix} + \frac{a-b}{2} \begin{pmatrix}1\\-1\end{pmatrix}.
  \end{align*}
  To represent $v$ in terms of this basis, we have
  \begin{align*}
    \left[v\right]_{\mathcal{B}} &= \begin{pmatrix}\frac{a+b}{2}\\\frac{a-b}{2}\end{pmatrix}.
  \end{align*}
  If we chose a different basis, such as the standard basis $\mathcal{E}_2 = \set{ \begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix} }$. In that case, we have
  \begin{align*}
    \left[v\right]_{\mathcal{E}_2} &= \begin{pmatrix}a\\b\end{pmatrix}.
  \end{align*}
\end{example}
\begin{example}
  Let $V = P_2\left(\R\right)$. Let $\mathcal{C} = \set{1,\left(x-1\right),\left(x-1\right)^2}$. We  know that $\mathcal{C}$ is a basis of $V$.\newline

  Let $f(x) = a + bx + cx^2\in P_2\left(\R\right)$. We can write $f$ in terms of this basis by taking
  \begin{align*}
    f(x) &= \left(a+b+c\right) + \left(b+2c\right)\left(x-1\right) + c\left(x-1\right)^2.
  \end{align*}
  In this case, we then have
  \begin{align*}
    \left[f(x)\right]_{\mathcal{C}} &= \begin{pmatrix}a+b+c\\b+2c\\c\end{pmatrix}.
  \end{align*}
\end{example}
Recall that given $A\in \Mat_{m,n}\left(\F\right)$, we obtain a linear map $T_{A}\in \Hom_{\F}\left(\F^{n},\F^{m}\right)$ by $T_A\left(v\right) = Av$. The converse is true as well. Given any map $T\in \Hom_{\F}\left(\F^{n},\F^{m}\right)$, there is a matrix $A$ such that $T = T_A$.\newline

Let $\mathcal{E}_n = \set{e_1,\dots,e_n}$ be the standard basis of $\F^n$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be the standard basis of $\F^{m}$.\newline

We have $T\left(e_j\right)\in \F^m$ for each $j$, meaning we have $a_{ij}\in \F$ with $T\left(e_j\right) = \sum_{i=1}^{m}a_{ij}f_j$.\newline

Define $A = \left(a_{ij}\right)_{ij}\in \Mat_{m,n}\left(\F\right)$. We want to show that $T_A\left(e_j\right) = T\left(e_j\right)$ for every $j$.\newline

Then, we have
\begin{align*}
  T_A\left(e_j\right) &= Ae_j\\
                      &= \sum_{a_{ij}}f_i\\
                      &= T\left(e_j\right).
\end{align*}
Let $T\in \Hom_{\F}\left(V,W\right)$. Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$ and $\mathcal{C} = \set{w_1,\dots,w_m}$ be a basis for $W$.\newline

Define $P = T_{\mathcal{B}}: V\rightarrow \F^n$, $v\mapsto \left[v\right]_{\mathcal{B}}$, $Q =  T_{\mathcal{C}}: W\rightarrow \F^{m}$, $w \mapsto \left[w\right]_{\mathcal{C}}$. This yields the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEiZYePrNWiEAB1dAMQB6wMFzl8lgoqPXVNUnfuPAAtua7iYUAObwioABmAE4QrkhkIDgQSKISWmwAKhYgIWER1NFIAEz2ktogAAog1Ax0AEYwDIX8ykIgwVg+ABY4KWnhiHFZiADMeQk6AIolIGWV1bXWOgwwgW08QaGduVExfQOOIEP6AMZYwbsABIl7B8eFJgC0wualFVU1Vio6jS0LFFxAA
\begin{tikzcd}
  V \arrow[r, "T"] \arrow[d, "T_{\mathcal{B}}"']          & W \arrow[d, "T_{\mathcal{C}}"] \\
  \F^{n} \arrow[r, "T_{\mathcal{C}}\circ T\circ T_{\mathcal{B}}^{-1}"'] & \F^{m}          
\end{tikzcd}
\end{center}
In particular, this means $T$ is given by a matrix $A\in \Mat_{m,n}\left(\F\right)$, which we write as $\left[T\right]_{\mathcal{B}}^{\mathcal{C}} = A$.\newline

In particular, $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$ is the unique matrix that satisfies
\begin{align*}
  \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left(\left[v\right]_{\mathcal{B}}\right) &= \left[T(v)\right]_{\mathcal{C}}.
\end{align*}

To compute $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$, we have
  \begin{align*}
    T\left(v_j\right) &= \sum_{i=1}^{m}a_{ij}w_i \tag*{$a_{ij}\in\F$}\\
    \left[T\left(v_j\right)\right]_{\mathcal{C}} &= \left[\sum_{i=1}^{m}a_{ij}w_j\right]_{\mathcal{C}}\\
                                                 &= \begin{pmatrix}a_{1j}\\\vdots\\a_{mj}\end{pmatrix}.
  \end{align*}
  Similarly, since $\left[v\right]_{\mathcal{B}} = e_j$, we have
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left(e_j\right) &= \left[T\left(v_j\right)\right]_{\mathcal{C}}\\
                                                               &= \begin{pmatrix}a_{1j}\\\vdots\\a_{mj}\end{pmatrix},
  \end{align*}
  which is exactly the $j$th column of $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$.\newline

  We thus get a matrix of the form
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}} &= \begin{pmatrix}\left[T\left(v_1\right)\right]_{\mathcal{C}} & \cdots & \left[T\left(v_n\right)\right]_{\mathcal{C}}\end{pmatrix},
  \end{align*}
  where $\left[T\left(v_j\right)\right]_{\mathcal{C}}$ are column vectors.
\begin{example}
  Let $V = P_{3}\left(\R\right)$. Define $T\in \Hom_{\R}\left(V,V\right)$ by $T\left(f(x)\right) = f'(x)$.\newline

  We take $\mathcal{B} = \set{1,x,x^2,x^3}$ as our basis. Then, we have
  \begin{align*}
    T\left(1\right) &= 0\\
    T\left(x\right) &= 1\\
    T\left(x^2\right) &= 2x\\
    T\left(x^3\right) &= 3x^2.
  \end{align*}
  As we fill in our matrix, we have
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{B}} &= \begin{pmatrix}0 & 1 & 0 & 0\\  0 & 0 & 2 & 0\\ 0 & 0 & 0 & 3 \\ 0 & 0 & 0 & 0 \end{pmatrix}.
  \end{align*}
  We can view each column as a basis vector of $\mathcal{B}$ and each row as the corresponding representation in $\mathcal{C}$ (where, in this case, $\mathcal{C} = \mathcal{B}$).
\end{example}
\begin{example}
  Let $V = P_{3}\left(\R\right)$, $T\left(f(x)\right) = f'(x)$. Let $\mathcal{B} = \set{1,x,x^2,x^3}$ and $\mathcal{C} = \set{1,\left(x-1\right),\left(x-1\right)^2,\left(x-1\right)^3}$.
  \begin{align*}
    T\left(1\right) &= 0\\
    T\left(x\right) &= 1\\
    T\left(x^2\right) &= 2x = 2 + 2\left(x-1\right)\\
    T\left(x^3\right) &= 3x^2 = -9 - 6\left(x-1\right) + 3\left(x-1\right)^2.
  \end{align*}
  Thus, our matrix $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$ is
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}} &= \begin{pmatrix}0 & 1 & 2 & -9 \\ 0 & 0 & 2 & -6\\ 0&0&0&3\\ 0&0&0&0\end{pmatrix}
  \end{align*}
\end{example}
\begin{exercise}\hfill
  \begin{enumerate}[(1)]
    \item Let $\mathcal{A}$ be a basis of $U$, $\mathcal{B}$ a basis of $V$, and $\mathcal{C}$ a basis of $W$. Let $S\in\Hom_{\F}\left(U,V\right)$ and $T\in\Hom_{\F}\left(V,W\right)$.\newline

  Show that
  \begin{align*}
    \left[T\circ S\right]_{\mathcal{A}}^{\mathcal{C}} &= \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left[\mathcal{S}\right]_{\mathcal{A}}^{\mathcal{B}}.
  \end{align*}
  \item We know that given $A\in \Mat_{m,k}\left(\F\right)$ and $B\in \Mat_{n,m}\left(\F\right)$, we have corresponding $T_A$ and $T_B$ linear maps.\newline

  Show that you recover the definition of matrix multiplication by using Part 1 to define matrix multiplication.
  \end{enumerate}
  \end{exercise}
  \begin{note}
    To refer to $\left[T\right]_{\mathcal{B}}^{\mathcal{B}}$, we will write $\left[T\right]_{\mathcal{B}}$.
  \end{note}
  Let $V$ be a vector space, with $\mathcal{B}$ and $\mathcal{B}'$ bases of $V$. We want to be able to transfer information about $V$ in terms of $\mathcal{B}$ to information about $V$ in terms of $\mathcal{B}'$ (i.e., change the basis).\footnote{Note that $\mathcal{B}'$ does not refer to the algebraic dual.}\newline

Let $\mathcal{B} = \set{v_1,\dots,v_n}$ and $\mathcal{B}' = \set{v_1',\dots,v_n'}$. Define
\begin{align*}
  T: V\rightarrow \F^{n}\\
  v\mapsto \left[v\right]_{\mathcal{B}}\\
  S: V\rightarrow \F^{n}\\
  v\mapsto \left[v\right]_{\mathcal{B}'}.
\end{align*}
In terms of a diagram, we have
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbTjz7Y8BImWHj6zVohAAdLQDEAeoVkgM8wUVErqaqZp0Gj4mFADm8IqABmAJwgBbJDIQHAgkACZqBjoAIxgGAAV+BSEQbywXAAscEGtJDRAAFW5eEB9-JFFg0MQAZlz1NgBlYq9fAMQgkIr6220tLCgAfWB2LhbStvDqLtrImLjEs0VNNMzsnvzGnQBjLG9tgAIdAeHRnb3Dgv1gAFphMa4KLiA
\begin{tikzcd}
V \arrow[d, "T"'] \arrow[r, "\id_{V}"]        & V \arrow[d, "S"] \\
\F^n \arrow[r, "S\circ \id_{V}\circ T^{-1}"'] & \F^n            
\end{tikzcd}
\end{center}
In particular, the change of basis matrix is
\begin{align*}
  \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'}.
\end{align*}
\begin{exercise}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$. Show that
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}\left[v_1\right]_{\mathcal{B'}} & \cdots & \left[v_n\right]_{\mathcal{B}'}\end{pmatrix}.
  \end{align*}
\end{exercise}
\begin{example}
  Let $V = \Q^2$, $\mathcal{B} = \mathcal{E}_2 = \set{ \begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix} }$. Let
  \begin{align*}
    \mathcal{B}' &= \set{v_1 = \begin{pmatrix}1\\-1\end{pmatrix},v_2 = \begin{pmatrix}1\\1\end{pmatrix}}.
  \end{align*}
  Notice that
  \begin{align*}
    e_1 &= \frac{1}{2}v_1 + \frac{1}{2}v_2\\
    e_2 &= -\frac{1}{2}v_1 + \frac{1}{2}v_2.
  \end{align*}
  In particular, we have
  \begin{align*}
    \left[e_1\right]_{\mathcal{B}'} &= \begin{pmatrix}\frac{1}{2}\\\frac{1}{2}\end{pmatrix}\\
    \left[e_2\right]_{\mathcal{B}'} &= \begin{pmatrix}-\frac{1}{2}\\\frac{1}{2}\end{pmatrix}.
  \end{align*}
  Thus,
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1/2 & -1/2 \\ 1/2 & 1/2\end{pmatrix}.
  \end{align*}
  Let
  \begin{align*}
    v &= \begin{pmatrix}\frac{2}{3}\end{pmatrix}.
  \end{align*}
  We have
  \begin{align*}
    \left[v\right]_{\mathcal{E}_2} &= \begin{pmatrix}2\\3\end{pmatrix}\\
    \left[v\right]_{\mathcal{E}_2}^{\mathcal{B}} &= \begin{pmatrix}1/2 & -1/2 \\ 1/2 & 1/2\end{pmatrix} \begin{pmatrix}2\\3\end{pmatrix}\\
                                                 &= \begin{pmatrix}-1/2\\5/2\end{pmatrix}\\
                                                 &= -\frac{1}{2} \begin{pmatrix}1\\-1\end{pmatrix} + \frac{5}{2} \begin{pmatrix}1\\1\end{pmatrix}\\
                                                 &= \left[v\right]_{\mathcal{B}'}.
  \end{align*}
\end{example}
\begin{example}
  Let $V = P_2\left(\R\right)$, $\mathcal{B} = \set{1,x,x^2}$, $\mathcal{B}' = \set{1,\left(x-2\right),\left(x-2\right)^2}$.
\end{example}
We have
\begin{align*}
  1 &= (1)(1) + (0)\left(x-2\right) + (0)\left(x-2\right)^2\\
  x &= (2)(1) + (1)\left(x-2\right) + (0)\left(x-2\right)^2\\
  x^2 &= (4)(1) + (4)\left(x-2\right) + (1)\left(x-2\right)^2.
\end{align*}
Thus, we have
\begin{align*}
  \left[1\right]_{\mathcal{B}'} &= \begin{pmatrix}1\\0\\0\end{pmatrix}\\
  \left[x\right]_{\mathcal{B}'} &= \begin{pmatrix}2\\1\\0\end{pmatrix}\\
  \left[x^2\right]_{\mathcal{B}'} &= \begin{pmatrix}4\\4\\1\end{pmatrix}.
\end{align*}
Therefore,
\begin{align*}
  \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1 & 2 & 4\\0 & 1 & 4\\0 & 0 & 1\end{pmatrix}.
\end{align*}
For example, if we let $f(x) = -7 + 3x + 4x^2$, we have
\begin{align*}
  \left[f(x)\right]_{\mathcal{B}} &= \begin{pmatrix}-7\\3\\4\end{pmatrix}\\
  \left[f(x)\right]_{\mathcal{B}'} &= \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'}\left[f(x)\right]_{\mathcal{B}}\\
                                   &= \begin{pmatrix}1 & 2 & 4 \\ 0 & 1 & 4 \\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}-7\\3\\4\end{pmatrix}\\
                                   &= \begin{pmatrix}15\\19\\4\end{pmatrix}
                                   \intertext{meaning}
                                   f(x) &= 15 + 19\left(x-2\right) + 4\left(x-2\right)^2.
\end{align*}
\begin{exercise}[Group Work]
  Let $V = P_2\left(\R\right)$, $\mathcal{B} = \set{1,\left(x-1\right),\left(x-1\right)^{2}}$ and $\mathcal{B}' = \set{1,\left(x+1\right),\left(x+1\right)^2}$. Find the change of basis matrix, and find $\left[2 - 6\left(x-1\right) + 2\left(x-1\right)^2\right]_{\mathcal{B}'}$.
\end{exercise}
\begin{solution}
  We have
  \begin{align*}
    1 &= (1)(1) + (0)\left(x+1\right) + (0)\left(x+1\right)^2\\
    \left(x-1\right) &= -2\left(1\right) + (1)\left(x+1\right) + \left(0\right)\left(x+1\right)^2\\
    \left(x-1\right)^2 &=  4(1) -(4)\left(x+1\right) + (1)\left(x+1\right)^2
  \end{align*}
  Thus, the change of basis matrix is
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1 & -2 & 4 \\ 0 & 1 & -4 \\ 0 & 0 & 1\end{pmatrix}.
  \end{align*}
  Thus, we have
  \begin{align*}
    \left[2 - 6\left(x-1\right) + 2\left(x-1\right)^2\right]_{\mathcal{B}'} &= \begin{pmatrix}1 & -2 & 4\\ 0 & 1 & -4 \\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}2\\-6\\2\end{pmatrix}\\
                                                                            &= \begin{pmatrix}22\\-14\\2\end{pmatrix}
  \end{align*}
\end{solution}
%Recall that if $\Dim_{\F}\left(V\right) = n$, with $\mathcal{B} = \set{v_1,\dots,v_n}$, there is an isomorphism
%\begin{align*}
%  T_{\mathcal{B}}:V\rightarrow \F^n\\
%  v\mapsto \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix},
%\end{align*}
%where $v = \sum_{i=1}^{n}a_iv_i$.\newline
\begin{definition}[Similar Matrices]
  Given $A,B\in \Mat_{n}\left(\F\right)$, we say $A$ and $B$ are similar if there exists $P\in \text{GL}_{n}\left(\F\right)$\footnote{$\text{GL}_{n}\left(\F\right) = \set{C\in \Mat_{n}\left(\F\right)\mid C^{-1}\text{ exists}}$} such that $A = PBP^{-1}$.
\end{definition}
We wish to rephrase this definition in terms of matrices. Given $A\in \Mat_{n}\left(\F\right)$, there exists $T_A\in \Hom_{\F}\left(F^n,H^n\right)$ with $T_A(v) = Av$. Given a basis $\mathcal{B}$, we have the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbTjz7Y8BImWHj6zVohAAdLQDEAeoVkgM8wUVErqaqZp0Gj4mFADm8IqABmAJwgBbJDIQHAgkUQl1NgAVAH1gAEEubl4QH39A6hCkACZqBjoAIxgGAAV+BSEQbywXAAscEGtJDRBY4B0-OhxagGNGYAAhLiTjNIDEcKzEAGYmyM02jq7e-qGRlLGczNCZudttLQYYTxxkNsSdarqcCjil7r6GQeHGkHyi0vLzTSv67gouEA
\begin{tikzcd}
  \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{B}}"']    & \F^n \arrow[d, "T_{\mathcal{B}}"] \\
\F^n \arrow[r, "{\left[T_{A}\right]_{\mathcal{B}}}"'] & \F^n                          
\end{tikzcd}
\end{center}
If $\mathcal{E}_n$ is the standard basis, then $A = \left[T_{A}\right]_{\mathcal{E}_n}$, meaning we have the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDgMQD1CAvqXSZc+QigBMFanSat2XPoOEgM2PASJlishizaJOPfiCEiN4otN019Co0tPm1ozROQBmGXfmHjymaq6mJaKN62cgaKJioWoR7kPlEOAc7BblYoSZH2-k6CsjBQAObwRKAAZgBOEAC2SEkgOBBIZCn+ACoA+sAAggJBVbUNiE0tSNIgjPQARjCMAAqZYSDVWCUAFjggvtFGPcBcdfQ4mwDGTMAAogLdYIMuNfVtNBOI3h3sh8enF1e3e6PVTPUZTd6fPLsPq7aZzBbLSyrdZbHZPEavZqtRAAFj2qS4WCgvQKwOGLw+b2xAFZ8f5FrxgABaYgCAAEAF42VxGDBKjhkITiUdYgIuCjtpQSRwTmdLowbncHozfnKrgAhASDGgzeZLFYSNYbbZDECgpB4rFIWlfA7S2X-RhszWm82IABsVMadJiROlyjJZoxiAA7F7EFMoUYGczWZzuRxefzBRw-SKA+LjTgpSKHfLgJqBCqZX984CHrDdQiDewJWiQcHPVbQzr4fqkYa67CoyAfiW1QrC2ZKAIgA
  \begin{tikzcd}
\F^n \arrow[r, "\id_{\F^n}"] \arrow[d, "T_{\mathcal{B}}"']                          & \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{E}_n}"'] & \F^n \arrow[d, "T_{\mathcal{E}_n}"] \arrow[r, "\id_{\F^n}"]                         & \F^n \arrow[d, "T_{\mathcal B}"] \\
\F^n \arrow[r, "{P^{-1} = \left[\id_{\F^n}\right]_{\mathcal{B}}^{\mathcal{E}_n}}"'] & \F^n \arrow[r, "A"']                                    & \F^n \arrow[r, "{P^{-1} = \left[\id_{\F^n}\right]_{\mathcal{E}_n}^{\mathcal{B}}}"'] & \F^n                            
\end{tikzcd}
\end{center}
%\begin{center}
%  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDgMQD1CAvqXSZc+QigBMFanSat2XPoOEgM2PASJlishizaJOPfiCEiN4otN019Co0tPm1ozROQBmGXfmHjymaq6mJaKN62cgaKJioWoR7kPlEOAc7BblYoSZH2-k6CsjBQAObwRKAAZgBOEAC2SEkgOBBIZCn+ACoA+sAAggJBVbUNiE0tSNIgjPQARjCMAAqZYSDVWCUAFjggvtFGPcBcdfQ4mwDGTMAAogLdYIMuNfVtNBOI3h3sh8enF1e3e6PVTPUZTd6fPLsPq7aZzBbLSyrdZbHZPEavZqtRAAFj2qS4WCgvQKwOGLw+b2xAFZ8f5FrCZvMlisJGsNtshiBQUg8VikLSvgcSRwTmdLowAAQAITJ3IxiAAbFTGnSYkSRco5TzEAB2FWIKZQoyLXjAAC0xEGNCZCNZ7BRnPRFOV-L1NvhLKRbMdOzVwqOor+EuAsselAEQA
%\begin{tikzcd}
%\F^n \arrow[r, "\id_{\F^n}"] \arrow[d, "T_{\mathcal{B}}"'] & \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{E}_n}"'] & \F^n \arrow[d, "T_{\mathcal{E}_n}"] \arrow[r, "\id_{\F^n}"] & \F^n \arrow[d, "T_{\mathcal B}"] \\\F^n \arrow[r, "P^{-1} = \id_{\F^n}\right]"']                                  & \F^n \arrow[r, "A"']                                    & \F^n \arrow[r, "P = \id_{\F^n}"']                                        & \F^n                            
%\end{tikzcd}
%\end{center}
Thus, $A = P \left[T_{A}\right]_{\mathcal{B}} P^{-1}$. In other words, $A\sim B$ if and only if $A = \left[T_{A}\right]_{\mathcal{B}}$ for some basis $\mathcal{B}$ and $B = \left[T_{A}\right]_{\mathcal{C}}$.
\subsection{Row Operations, Column Space, and Null Space}%
\begin{definition}[Pivot]
  Let $A = \left(a_{ij}\right)\in \Mat_{m,n}\left(\F\right)$. We say $a_{k\ell}$ is a pivot of $A$ if and only if $a_{k\ell}\neq 0$ and $a_{ij} = 0$ if $i\geq k$ or $j\leq \ell$, with $\left(i,j\right)\neq \left(k,\ell\right)$.
\end{definition}
\begin{example}
  For the matrix
  \begin{align*}
    A &= \begin{pmatrix}\boxed{2} & 1 & 4 & 5 \\ 0 & 0 & \boxed{1} & 7 \\ 0 & 0 & 0 & \boxed{5}\end{pmatrix},
  \end{align*}
  the boxed entries are pivots.
\end{example}
\begin{definition}
  Let $A\in \Mat_{m,n}\left(\F\right)$. We say $A$ is in row echelon form if all its nonzero rows have a pivot and all its zero rows are located below the nonzero rows. We say the matrix is in reduced row echelon form if it is in row echelon form and the pivots are the nonzero elements in the columns containing the pivots.
\end{definition}
\begin{example}
  We have
  \begin{align*}
    A &= \begin{pmatrix}2 & 1 & 4 & 5 \\ 0 & 0 & 1 & 7 \\ 0 & 0 & 0 & 5 \\ 0 & 0 & 0 & 0\end{pmatrix}
  \end{align*}
  is in row echelon form, and
  \begin{align*}
    B &= \begin{pmatrix}2 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\\0 & 0 & 0 & 0\end{pmatrix}
  \end{align*}
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}3 & 4 & 5 & 6 \\ 1 & 2 & 3 & 4 \\ 1 & 1 & 2 & 3\end{pmatrix}.
  \end{align*}
  We are going to put this matrix into reduced row echelon form. We have $T_A: \F^4 \rightarrow \F^3$. Let $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$ and $\mathcal{F}_3 = \set{f_1,f_2,f_3}$. Then, $A = \left[T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3}$. We have
  \begin{align*}
    T_A\left(e_1\right) &= 3f_1 + f_2 + f_3\\
    T_A\left(e_2\right) &= 4f_1 + 2f_2 + f_3\\
    T_A\left(e_3\right) &= 5f_1 + 3f_2 + 2f_3\\
    T_A\left(e_4\right) &= 6f_1 + 4f_2 + 3f_3
  \end{align*}
  \begin{description}
    \item[Step 1:] We switch $R_1\leftrightarrow R_3$, yielding
      \begin{align*}
        \mathcal{F}_3^{(2)} &= \set{f_1^{(2)} = f_3,f_{2}^{(2)},f_{3}^{(2)} = f_1},
      \end{align*}
      yielding
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(2)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 1 & 2 & 3 & 4 \\ 3 & 4 & 5 & 6\end{pmatrix}\\
        \\
        T_A\left(e_1\right) &= f_1^{(2)} + f_{2}^{(3)} + 3f_{3}^{(2)}\\
        T_A\left(e_2\right) &= f_{1}^{(2)} + 2f_{2}^{(3)} + 4f_3^{(2)}\\
        T_A\left(e_3\right) &= 2f_1^{(2)} + 3f_2^{(2)} + 5f_3^{(2)}\\
        T_A\left(e_4\right) &= 3f_1^{(2)} + f_2^{(2)} + 6f_3^{(2)}.
      \end{align*}
    \item[Step 2:] Our next step is $-R_1 + R_2 \rightarrow R_2$, yielding
      \begin{align*}
        \mathcal{F}_3^{(3)} &= \set{f_1^{(3)} = f_1^{(2)} + f_{2}^{(2)},f_{3}^{(2)} = f_{2}^{(2)},f_{3}^{(3)} = f_{2}^{(3)}}.
      \end{align*}
      Our new matrix is
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(3)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 3 & 4 & 5 & 6\end{pmatrix}\\
        \\
        T_A\left(e_1\right) &= \left(f_1^{(2)} + f_{2}^{(2)}\right) + 3f_{3}^{(2)}\\
                            &= f_{1}^{(3)} + 3f_3^{(3)}\\
        T_{A}\left(e_2\right) &= \left(f_1^{(2)} + f_2^{(2)}\right) + f_{2}^{(2)} + 4f_3^{(2)}\\
                              &= f_1^{(3)} + f_2^{(2)} + 4f_3^{(3)}\\
                              &\vdots
      \end{align*}
    \item[Step 3:] Next, we have $-3R_1 + R_3\rightarrow R_3$, which yields
      \begin{align*}
        \mathcal{F}_3^{(4)} &= \set{f_1^{(4)} = f_{1}^{(3)} + 3f_{3}^{(3)},f_{2}^{(4)} = f_{2}^{(3)},f_{3}^{(4)} = f_{3}^{(3)}}.
      \end{align*}
      Our matrix is now
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_{4}}^{\mathcal{F}_{3}^{(4)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 1 & -1 & -3\end{pmatrix}
      \end{align*}
    \item[Step 4:] Next, we have $-R_2 + R_3 \rightarrow R_3$, which yields
      \begin{align*}
        \mathcal{F}_{3}^{(5)} &= \set{f_{1}^{(5)} = f_{1}^{(4)},f_{2}^{(5)} = f_{2}^{(4)} + f_{3}^{(4)},f_{3}^{(5)} = f_{3}^{(4)}},
      \end{align*}
      and a matrix of
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(5)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & -4\end{pmatrix}.
      \end{align*}
  \end{description}
\end{example}
\begin{theorem}
  Let $A\in \Mat_{m,n}\left(\F\right)$. The matrix $A$ can be put in row echelon form through a series of row operations of the form:
  \begin{itemize}
    \item switching two rows: $R_i\leftrightarrow R_j$;
    \item multiplying a row by a scalar: $\R_i\rightarrow cR_i$;
    \item replacing a row by adding a scalar multiple of another row: $aR_i + R_j\rightarrow R_j$.
  \end{itemize}
\end{theorem}
\begin{proof}[Sketch of a Proof]
  For any matrix, we switch rows such that the value of $a_{11}$ is nonzero. Then, we take
  \begin{align*}
    f_{1}^{(2)} &= \sum_{j=1}^{m}a_{ji}f_j\\
    f_k^{(2)} = f_{k}.
  \end{align*}
\end{proof}
Instead of directly changing the bases, we can use linear maps to change the bases.\newline

We define $T_{i,j}: W\rightarrow W$ to be
\begin{align*}
  T_{i,j}\left(w_{k}\right) &= w_k\tag*{$k\neq i,j$}\\
  T_{i,j}\left(w_i\right) &= w_j\\
  T_{i,j}\left(w_j\right) &= w_i.
\end{align*}
Thus,
\begin{align*}
  E_{i,j} &= \left[T_{i,j}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
is the identity matrix except for switching the $i$ and $j$ rows.\newline

Let $c\in \F$, define $T_i^{(c)}:W\rightarrow W$ by
\begin{align*}
  T_i^{(c)}\left(w_k\right) &= w_k\tag*{$k\neq i$}\\
  T_{i}^{(c)}\left(w_i\right) &= cw_i,
\end{align*}
with
\begin{align*}
  E_i^{(c)} &= \left[T_{i}^{(c)}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
being the identity matrix except for row $i$ multiplied by $c$.\newline

Finally, we define $T_{i,j}^{(c)}:W\rightarrow W$ by
\begin{align*}
  T_{i,j}^{(c)}\left(w_k\right) &= w_k\tag*{$k\neq j$}\\
  T_{i,j}^{(c)}\left(w_j\right) &= cw_i + w_j,
\end{align*}
with
\begin{align*}
  E_{i,j}^{(c)} &= \left[T_{i,j}^{(c)}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
as the identity map with $c$ in the $ij$th entry.
\begin{example}
  Let 
  \begin{align*}
    A &= \begin{pmatrix}3 & 4 & 5 & 5\\ 1 & 2 & 3 & 4 \\ 1 & 1 & 2 & 3\end{pmatrix}.
  \end{align*}
  Define $T_A: \F^4\rightarrow \F^3$, $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$, and $\mathcal{F}_3 = \set{f_1,f_2,f_3}$. We have
  \begin{align*}
    T_A\left(e_1\right) &= 3f_1 + f_2 + f_3\\
    T_A\left(e_2\right) &= 4f_1 + 2f_2 + f_3\\
    T_A\left(e_3\right) &= 5f_1 + 3f_2 + 2f_3\\
    T_A\left(e_4\right) &= 6f_1 + 4f_2 + 3f_3.
  \end{align*}
  First, we interchange the rows by $T_{1,3}:\F^3\rightarrow \F^3$, Then,
  \begin{align*}
    \left(T_{1,3}\circ T_A\right)\left(e_1\right) &= T_{1,3}\left(3f_1 + f_2 + f_3\right)\\
                                                  &= 3T_{1,3}\left(f_1\right) + T_{1,3}\left(f_1\right) + T_{1,3}\left(f_3\right).
  \end{align*}
  If we look at the matrix, we then have
  \begin{align*}
    \left[T_{1,3}\circ T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 1 & 2 & 3 & 4 \\ 3 & 4 & 5 & 6\end{pmatrix}.
  \end{align*}
  For the full reduced row echelon form, we would have the following series of transformations:
  \begin{align*}
    \left[T_{1,3}^{(-1)}\circ T_{2,3}^{(-1)}\circ T_{3}^{(-2)}\circ T_{3,1}^{(-3)}\circ T_{1,2}^{-1}\circ T_{1,3}\circ T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3} &= \begin{pmatrix}1 & 0 & 0 & 0 \\ 0 & 1 & 0  & -1\\ 0 & 0 & 1 & 2\end{pmatrix}.
  \end{align*}
\end{example}
\begin{definition}[Column Space, Null Space, and Rank]
  Let $A\in \Mat_{m,n}\left(\F\right)$. The column space of $A$ is the $\F$-span of the column vectors. This is denoted $\Col(A)$.\newline

  The null space, $\Null(A)$, is the $\F$-span of the vectors $v\in \F^n$ such that $Av = 0_{\F^m}$.\newline

  The rank of $A$, denoted $\Rank(A)$, is $\Rank(A) = \Dim_{\F}\left(\Col(A)\right)$.
\end{definition}
  Let $\mathcal{E}_n = \set{e_1,\dots,e_n}$ be the standard basis for $\F^n$, with $T_A \in \Hom_{\F}\left(\F^n,\F^n\right)$, and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ the standard basis of $\F^m$.\newline

  We have $\left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m} = A$. We know that
  \begin{align*}
    A &= \begin{pmatrix}T_A\left(e_1\right) & \cdots & T_A\left(e_n\right)\end{pmatrix}.
  \end{align*}
  Thus, $\Col(A) = \img\left(T_A\right)$, meaning $\Rank(A) = \Dim_{\F}\left(\img\left(T_A\right)\right)$.\newline

In order to calculate $\col(A)$, we put the matrix $A$ into row echelon form, look at the columns that have pivots, and those columns form the basis for $\col(A)$.\newline

We have an isomorphism $E: \F^m\rightarrow \F^m$ such that
\begin{align*}
  \left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m} &= \left[E\right]_{\mathcal{F}_m}^{\mathcal{F}_m}
\end{align*}
is in row echelon form. In particular, the column space of $\left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$ has as its basis the columns containing pivots:
\begin{align*}
  \underbrace{\overbrace{\left[E\circ T_A\left(e_{i_1}\right)\right]}^{w_1}_{\mathcal{F}_m},\dots,\overbrace{\left[E\circ T_A\left(e_{i_k}\right)\right]}^{w_k}_{\mathcal{F}_m}}_{\text{basis of }\col\left(\left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}\right)}
\end{align*}
We have an inverse $E^{-1}: \F^{m}\rightarrow \F^m$. In particular,
\begin{align*}
  \underbrace{E^{-1}\left(w_1\right),\dots,E^{-1}\left(w_k\right)}_{=\left[T_{A}\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots,\left[T_{A}\left(e_{i_k}\right)\right]_{\mathcal{F}_m}}
\end{align*}
are linearly independent since $E^{-1}$ is an isomorphism.\newline

If there is a vector $v\in \col(A)$ that is not in the span of $\left[T_{A}\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots,\left[T_{A}\left(e_{i_k}\right)\right]_{\mathcal{F}_m}$, then $E(v)$ cannot be in the span of $w_1,\dots,w_k$.\newline

Thus, the columns $\left[T_A\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots \left[T_A\left(e_{i_k}\right)\right]_{\mathcal{F}_m}$ give a basis for $\col(A)$.
\begin{example}
  Consider the matrix
  \begin{align*}
    A &= \begin{pmatrix}3&4&5&6\\1&2&3&4\\1&1&2&3\end{pmatrix}.
  \end{align*}
  We put $A$ into row echelon form as
  \begin{align*}
    B &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & -4\end{pmatrix}.
  \end{align*}
  Examining the pivots, we have the column space as
  \begin{align*}
    \col(B) &= \Span_{\F}\left( \begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}1\\1\\0\end{pmatrix}, \begin{pmatrix}2\\1\\-2\end{pmatrix}\right),
  \end{align*}
  implying the basis of the column space for $A$ is
  \begin{align*}
    \col(A) &= \Span_{\F}\left( \begin{pmatrix}3\\1\\1\end{pmatrix}, \begin{pmatrix}4\\2\\1\end{pmatrix}, \begin{pmatrix}5\\3\\2\end{pmatrix}\right).
  \end{align*}
\end{example}
We have $v\in \Null(A)$ if and only if $Av = 0_{\F^m}$. Since $Av = T_A(v)$, we have $\Null(A) = \ker\left(T_A\right)$.
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}4 & -4 & 2 \\ -4 & 4 & -2 \\ 2 & -1 & 1\end{pmatrix}.
  \end{align*}
  The reduced row echelon form of $A$ is
  \begin{align*}
    B &= \begin{pmatrix}1 & 0 & 1/2 \\ 0 & 1 & 0 \\ 0 & 0 & 0\end{pmatrix}.
  \end{align*}
  Thus,
  \begin{align*}
    \col(A) &= \Span_{\F}\left( \begin{pmatrix}4\\-4\\2\end{pmatrix}, \begin{pmatrix}-4\\4\\-1\end{pmatrix}\right).
  \end{align*}
  We know that $\null(A) = \ker\left(T_A\right) \subseteq \F^{3}$-domain of $T_A$. When we put a matrix into reduced row echelon form, we do not impact the basis vectors of the domain of $T_A$, implying that $\Null(A) = \Null(B)$.\newline

  In particular, we want
  \begin{align*}
    \begin{pmatrix}1 & 0 & 1/2\\0 & 1 & 0 \\ 0 & 0 & 0\end{pmatrix} \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} &= \begin{pmatrix}x_1 + \left(1/2\right) x_3\\x_2\\0\end{pmatrix}\\
                     &= \begin{pmatrix}0\\0\\0\end{pmatrix}.
  \end{align*}
  Therefore, we have $x_2 = 0$, $x_1 = -1/2 x_3$, meaning
  \begin{align*}
    \Null(A) &= \Span_{\F}\left( \begin{pmatrix}-1/2\\0\\1\end{pmatrix}\right).
  \end{align*}
\end{example}
\subsection{Transpose of a Matrix}%
Recall that, given a linear map $T\in \Hom_{\F}\left(V,W\right)$, there is an induced map $T'\in \Hom_{\F}\left(W',V'\right)$ on the dual space given by $T'\left(\varphi\right) = \varphi\circ T$.\newline

Let $A\in \Mat_{m,n}\left(\F\right)$, $\mathcal{E}_n = \set{e_1,\dots,e_n}$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be standard bases for $\F^n$ and $\F^m$ respectively. Let $T_A \in \Hom_\F\left(\F^n,\F^m\right)$, meaning $A = \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$.\newline

We have $\mathcal{E}_n' = \set{e_1',\dots,e_n'}$ and $\mathcal{F}_{m}' = \set{f_1',\dots,f_m'}$. The dual map $T_A'\in \Hom_{\F}\left(\F^m,\F^n\right)$, and the transpose of $A$ is defined by
\begin{align*}
  A^{T} &= \left[T_{A}'\right]_{\mathcal{F}_m'}^{\mathcal{E}_n'}.
\end{align*}
\begin{lemma}
  Let $A = \left(a_{ij}\right) \in \Mat_{m,n}\left(\F\right)$. Then,
  \begin{align*}
    A^{T} &= \left(b_{ij}\right)\in \Mat_{n,m}\left(\F\right)
  \end{align*}
  with $b_{ij} = a_{ji}$.
\end{lemma}
\begin{proof}
  Let $A\in \Mat_{m,n}\left(\F\right)$, $\mathcal{E}_n = \set{e_1,\dots,e_n}$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be standard bases for $\F^n$ and $\F^m$ respectively. Let $\mathcal{E}_n'$ and $\mathcal{F}_m'$ denote the dual bases.\newline

  Let $T_A \in \Hom_\F\left(\F^n,\F^m\right)$, meaning $A = \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$. In particular, we have
  \begin{align*}
    T_A\left(e_i\right) &= \sum_{k=1}^{m}a_{ki}f_k. \tag*{(\textasteriskcentered)}
  \end{align*}
  We have
  \begin{align*}
    A^{t} &= \left[T_{A}'\right]_{\mathcal{F}_m'}^{\mathcal{E}_n'} \tag*{(\textasteriskcentered\textasteriskcentered)}\\
          &= \left(b_{ij}\right)
  \end{align*}
  Now, we have
  \begin{align*}
    T_{A}' \left(f_j'\right) &= \sum_{j=1}^{n}b_{kj}e_{k}'.
  \end{align*}
  Apply $f_j'$ to (\textasteriskcentered). Then,
  \begin{align*}
    \left(f_j'\circ T_A\right)\left(e_i\right) &= f_j'\left(\sum_{k=1}^{m}a_{ki}f_k\right)\\
                                               &= \sum_{k=1}^{m}a_{ki}f_j'\left(f_k\right)\\
                                               &= a_{ji}.
  \end{align*}
  Apply (\textasteriskcentered\textasteriskcentered) to $e_i$. Then,
  \begin{align*}
    T_A'\left(f_j'\right)\left(e_i\right) &= \sum_{k=1}^{n}b_{kj}e_k'\left(e_i\right)\\
                                          &= b_{ij}.
  \end{align*}
  We have
  \begin{align*}
    \left(f_j'\circ T_A\right)\left(e_i\right) &= \left(T_A'\left(f_j'\right)\right)\left(e_i\right)
  \end{align*}
  by the definition of $T_A'$, meaning $b_{ij} = a_{ji}$.
\end{proof}
\begin{exercise}
  Let $A_1,A_2\in \Mat_{m,n}\left(\F\right)$, $c\in \F$. Use the definition of the transpose to show
  \begin{align*}
    \left(A_1 + A_2\right)^{T} &= A_1^T + A_2^T\\
    \left(cA_1\right)^T &= cA_1^T.
  \end{align*}
\end{exercise}
\begin{lemma}
  Let $A\in \Mat_{m,n}\left(\F\right)$, $B\in \Mat_{p,m}\left(\F\right)$. Then,
  \begin{align*}
    \left(BA\right)^T &= A^TB^T.
  \end{align*}
\end{lemma}
\begin{proof}
  Let $\mathcal{E}_m$, $\mathcal{E}_n$, and $\mathcal{E}_p$ be standard bases.\newline

  We have
  \begin{align*}
    \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_m} &= A\\
    \left[T_B\right]_{\mathcal{E}_m}^{\mathcal{E}_p} &= B.
  \end{align*}
  So,
  \begin{align*}
    BA &= \left[T_B\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_p}.
  \end{align*}
  Thus,
  \begin{align*}
    \left(BA\right)^{T} &= \left[\left(T_B\circ T_A\right)'\right]_{\mathcal{E}_p'}^{\mathcal{E}_n'}\\
                        &= \left[T_A'\circ T_B'\right]_{\mathcal{E}_p'}^{\mathcal{E}_n'}\\
                        &= \left[T_A'\right]_{\mathcal{E}_m'}^{\mathcal{E}_n'}\left[T_{B}'\right]_{\mathcal{E}_p'}^{\mathcal{E}_m'}\\
                        &= A^TB^T.
  \end{align*}
\end{proof}
\begin{lemma}
  Let $A\in \text{GL}_{n}\left(\F\right)$. Then,
  \begin{align*}
    \left(A^{-1}\right)^{T} &= \left(A^{T}\right)^{-1}.
  \end{align*}
\end{lemma}
\begin{proof}
  We will show that $A^{T} \left(A^{-1}\right)^{T} = I_n = \left(A^{-1}\right)^{T} A^T$, and use the fact that inverses are unique.\newline

  We have
  \begin{align*}
    A &= \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_n}\\
    A^{-1} &= \left[T_A^{-1}\right]_{\mathcal{E}_n}^{\mathcal{E}_n}
  \end{align*}
  We have
  \begin{align*}
    I_n &= \left[\id_{\F^n}'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\circ T_A\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[T_A'\circ \left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\left[\left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= A^T\left(A^{-1}\right)^T.\\
    I_n &= \left[\left(T_A\circ T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\right)'\circ T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\left[T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left(A^{-1}\right)^{T}A^{T}.
  \end{align*}
\end{proof}
\section{Generalized Eigenvectors and Jordan Canonical Form}%
\subsection{Eigenvalues and Eigenvectors}%
Recall that we say $A \sim B$ if $A = PBP^{-1}$ for some $P\in \text{GL}_{n}\left(\F\right)$. In particular, this means that $A = \left[T\right]_{\mathcal{A}}$ and $B = \left[T\right]_{\mathcal{B}}$ for some bases $\mathcal{A}$ and $\mathcal{B}$.
\begin{definition}[Diagonalizable]
  We say $A$ is diagonalizable if $A \sim D$ for some $D$ a diagonal matrix.\newline

  If $A = \left[T\right]_{\mathcal{A}}$, $A$ is diagonalizable if there is a basis $\mathcal{B}$ if $\left[T\right]_{\mathcal{B}} = D$ for $D$ a diagonal matrix.\newline

  If $A\sim B$, $A$ is diagonalizable if and only if $B$ is diagonalizable. If $A$ and $B$ are diagonalizable, they must be similar to the same diagonal matrix up to reordering the diagonals.
\end{definition}
\begin{example}
  Let $V = \F^2$, $T \in \Hom_{\F}\left(V,V\right)$. We take $T\left(e_1\right) = 3e_1$ and $T\left(e_2\right) = -2e_2$.\newline

  In particular, we can see that
  \begin{align*}
    \left[T\right]_{\mathcal{E}_2} &= \begin{pmatrix}3 & 0 \\ 0 & -2\end{pmatrix}.
  \end{align*}
  When we look at $V = V_1\oplus V_2$, with $V_1 = \Span_{\F}\left(e_1\right)$ and $V_2 = \Span_{\F}\left(e_2\right)$.\newline

  In this case, we have $T\left(V_1\right)\subseteq V_1$ and $T\left(V_2\right) \subseteq V_2$, which allows us to write $T$ as a diagonal matrix.
\end{example}
\begin{example}
  Let $V = \F^2$, $T \in \Hom_{\F}\left(V,V\right)$. We take $T\left(e_1\right) = 3e_1$ and $T\left(e_2\right) = e_1 + 3e_2$.\newline

  In particular, we can see that
  \begin{align*}
    \left[T\right]_{\mathcal{E}_2} &= \begin{pmatrix}3 & 1 \\ 0 & 3\end{pmatrix}.
  \end{align*}
  We still have $V = V_1\oplus V_2$ with $V_1 = \Span_{\F}\left(e_1\right)$ and $V_2 = \Span_{\F}\left(e_2\right)$.\newline

  While we have $T\left(V_1\right)\subseteq V_1$, we do not have $T\left(V_2\right)\subseteq V_2$. We will find a diagonalization (or lack thereof) of $T$.\newline

  Suppose we have $W_1,W_2\neq \set{0}$ with $V = W_1 \oplus W_2$ with $T\left(W_1\right)\subseteq W_1$ and $T\left(W_2\right)\subseteq W_2$.\newline

  Write $W_i = \Span_{\F}\left(w_i\right)$. In particular, this means we can write $T\left(w_1\right) = \alpha w_1$ and $T\left(w_2\right) = \beta w_2$. For $\mathcal{B} = \set{w_1,w_2}$, we would be able to write
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\alpha & 0 \\ 0 & \beta\end{pmatrix}.
  \end{align*}
  Write $w_1 = ae_1 + be_2$ and $w_2 = ce_1 + de_2$.
  \begin{align*}
    \alpha w_1 &= T\left(w_1\right)\\
               &= aT\left(e_1\right) + bT\left(e_2\right)\\
               &= a\left(3e_1\right) + b\left(e_1 + 3e_2\right)\\
               &= \left(3a+b\right)e_1 + 3be_2
  \end{align*}
  Thus, $\alpha\left(ae_1 + be_2\right) = \left(3a+b\right)e_1 + 3be_2$, meaning $\alpha a = 3a + b$, $\alpha b = 3b$. Either $b = 0$ or $\alpha = 3$, but we still end with $\alpha = 3$. Thus, $T\left(w_1\right) = 3w_1$.\newline

  Applying to $w_2$, we have
  \begin{align*}
    \beta w_2 &= \left(3c + d\right)e_1 + \left(3d\right)e_2,
  \end{align*}
  implying $\beta c = ec + d$ and $\beta d = 3d$, meaning either $\beta = 3$ (which contradicts the first equation)or $w_2 = ce_1$, which contradicts $w_1,w_2$ being a basis.
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1& 2\\ 3&4\end{pmatrix}.
  \end{align*}
  Let $\F = \Q$. Can we find $P \in \text{GL}_2\left(\Q\right)$ such that $P^{-1}AP = \begin{pmatrix}\alpha & 0\\ 0 & \beta\end{pmatrix}$.\newline

  If we write $P = \begin{pmatrix}a & b \\c & d\end{pmatrix}$, we have
  \begin{align*}
    P^{-1}AP &= \frac{1}{ad-bc} \begin{pmatrix}ad - 3ab + 2cd - 4bc & -3bd - 3b^2 + 2d^2\\ 3ac + 3a^2 - 2c^2 & -bc + 3ab - 2cd + 4ad\end{pmatrix}.
  \end{align*}
  By the definition of diagonal matrix, we must have
  \begin{align*}
    3a^2 + 3ac - 2c^2 &= 0.
  \end{align*}
  If $c = 0$, then $a = 0$, which is a contradiction since $P$ is invertible. We have $c\neq 0$, meaning we can divide by $c^2$ and set $x = a/c$
  \begin{align*}
    3x^2 + 3x - 2 &= 0\\
    x &= \frac{-3 \pm \sqrt{33}}{6}\\
    a &= \frac{-3 \pm \sqrt{33}}{6}c.
  \end{align*}
  Since $c\neq 0$, $\frac{-3 \pm \sqrt{33}}{6}c \notin \Q$. Thus, we cannot diagonalize $A$ over $\Q$.\newline

  If we take $\F = \Q\left(\sqrt{33}\right)$, then we take
  \begin{align*}
    \mathcal{B} &= \set{v_1 = \begin{pmatrix}1\\ \frac{3 + \sqrt{33}}{4}\end{pmatrix}, v_2 = \begin{pmatrix}1\\\frac{3-\sqrt{33}}{4}\end{pmatrix}},\\
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\frac{5 + \sqrt{33}}{2} & 0 \\ 0 & \frac{5-\sqrt{33}}{2}\end{pmatrix}.
  \end{align*}
\end{example}
\begin{recall}
  The fundamental question we are investigating is whether given a $A\in \Mat_{n}\left(\F\right)$, can we choose $P\in \text{GL}_{n}\left(\F\right)$ such that $PAP^{-1}$ is diagonal.\newline

  We saw that if $\F^2 = V_1\oplus V_2$ with $A\left(V_1\right) \subseteq V_1$, $A\left(V_2\right)\subseteq V_2$, then it is possible to diagonalize $A$.
\end{recall}
\begin{definition}
  Let $V$ be an $\F$-vector space with $T\in \Hom_{\F}\left(V,V\right)$. We say a subspace $W\subseteq V$ is $T$-invariant or $T$-stable if $T\left(W\right)\subseteq W$.
\end{definition}
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$, $W\subseteq V$ a $k$-dimensional subspace.\newline

Let $\mathcal{B}_{W} = \set{v_1,\dots,v_k}$ be a basis for $W$, and extend to a basis $\mathcal{B} = \set{v_1,\dots,v_n}$ of $V$.\newline

Let $T\in \Hom_{\F}\left(V,V\right)$.\newline

Then, $W$ is $T$-stable if and only if $\left[T\right]_{\mathcal{B}}$ is block-upper triangular of the form
\begin{align*}
  \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}A & B \\ 0 & D\end{pmatrix},
\end{align*}
where $A = \left[T\vert_{W}\right]_{\mathcal{B}_W}$.
\end{theorem}
\begin{example}
  Let $V = \Q^4$, $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$ the standard basis. Define $T$ by
  \begin{align*}
    T\left(e_1\right) &= 2e_1 + 3e_3\\
    T\left(e_2\right) &= e_1 + e_4\\
    T\left(e_3\right) &= e_1 - e_3\\
    T\left(e_4\right) &= 2e_1 - 2e_2 + 5e_3 - 4e_4.
  \end{align*}
  Notice that if we set $W = \Span_{\Q}\left(e_1,e_3\right)$, then $W$ is $T$-stable. We set $\mathcal{B}_W = \set{e_1,e_3}$, $\mathcal{B} = \set{e_1,e_2,e_3,e_4}$.
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}2 & 1 & 1 & 2 \\ 3 & -1 & 0 & 5 \\ 0 & 0 & 0 & -2 \\ 0 & 0 & 1 & -4\end{pmatrix}
  \end{align*}
\end{example}
A special case is when $\Dim_{\F}\left(W\right) = 1$. If $W = \Span_{\F}\left(w_1\right)$, and $W$ is $T$-stable, then $T\left(w_1\right) \in W$, meaning $T\left(w_1\right) = \lambda w_1$ for some $\lambda \in \F$.\newline

We can rewrite this as $T\left(w_1\right) - \lambda\left(w_1\right) = 0_V$, meaning $\left(T - \lambda \id_V\right)\left(w_1\right) = 0_V$, meaning $w_1\in \ker\left(T - \lambda\id_V\right)$.
\begin{definition}
  Let $T\in \Hom_{\F}\left(V,V\right)$, and $\lambda \in F$. If $\ker\left(T - \lambda\id_V\right) \neq \set{0_V}$, we say $\lambda$ is an eigenvalue of $T$.\newline

  Any nonzero vector in $\ker\left(T - \lambda \id_V\right)$ is called an eigenvector.\newline

  The set $E_{\lambda} = \ker\left(T - \lambda \id_V\right)$ is called the eigenspace associated with $\lambda$.
\end{definition}
\begin{exercise}
  Show $E_{\lambda}$ is a subspace of $V$.
\end{exercise}
\begin{exercise}
  Let $T\in \Hom_{\F}\left(V,V\right)$. If $\lambda_1,\lambda_2 \in \F$ with $\lambda_1\neq \lambda_2$, then $E_{\lambda_1} \cap E_{\lambda_2} = \set{0_V}$.
\end{exercise}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix}\in \Mat_{2}\left(\Q\right),
  \end{align*}
  with $T_A \in \Hom_{\Q}\left(\Q^2,\Q^2\right)$ the associated linear map.\newline

  We have
  \begin{align*}
    \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix} \begin{pmatrix}1\\2/5\end{pmatrix} &= 2 \begin{pmatrix}1\\2/5\end{pmatrix}\\
    \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix} \begin{pmatrix}1\\3/7\end{pmatrix} &= 3 \begin{pmatrix}1\\3/7\end{pmatrix}.
  \end{align*}
  Therefore, $T_A$ has eigenvalues of $2$ and $3$, with
  \begin{align*}
    E_2 &= \Span_{\Q} \left(\begin{pmatrix}1\\2/5\end{pmatrix}\right) = \Span_{\Q}\left(v_1\right)\\
    E_3 &= \Span_{\Q}\left( \begin{pmatrix}1\\3/7\end{pmatrix}\right) = \Span_{\Q}\left(v_2\right),
  \end{align*}
  meaning
  \begin{align*}
    \left[T_{A}\right]_{\set{v_1,v_2}} &= \begin{pmatrix}2 & 0 \\ 0 & 3\end{pmatrix}.
  \end{align*}
\end{example}
\begin{notation}
  Let $T\in \Hom_{\F}\left(V,V\right)$. We write $T^m = \underbrace{T\circ \cdots \circ T}_{m\text{ times}}$.\newline

  If $f(x) \in \F[x]$, $f(x) = a_mx^m + \cdots + a_1 x + a_0$, then
  \begin{align*}
    f\left(T\right) &= a_mT^m + \cdots + a_1T + a_0 \id_V\\
                    &\in \Hom_{\F}\left(V,V\right).
  \end{align*}
  If $f(x) = g(x)h(x)$, then
  \begin{align*}
    f(T) &= g(T)\circ h(T)
  \end{align*}
\end{notation}
\begin{example}
  If $g(x) = 2x^2 + 3$, then
  \begin{align*}
    g\left(T\right) &= 2T^2 + 3\id_V\\
    g\left(T\right)\left(v\right) &= 2T\left(T\left(v\right)\right) + 3v.
  \end{align*}
\end{example}
Let $\Dim_{\F}\left(V\right) = n$. Recall that $\Hom_{\F}\left(V,V\right)$ is an $\F$-vector space, meaning $\Hom_{\F}\left(V,V\right) \cong \Mat_{n}\left(\F\right)$. Thus, $\Dim_{\F}\left(\Hom_{\F}\left(V,V\right)\right) = n^2$.\newline

Given $T\in \Hom_{\F}\left(V,V\right)$, consider
\begin{align*}
  \set{\id_V,T,T^2,\dots,T^{n^2}}\subseteq \Hom_{\F}\left(V,V\right).
\end{align*}
Since this set contains $n^2 + 1$ elements, it must be linearly dependent. Let $m$ be the smallest integer such that $a_m T^{m} + \cdots + a_1 T + a_0\id_V = 0_{\Hom_{\F}\left(V,V\right)}$. Since $m$ is minimal, $a_m \neq 0$.\newline

Define $f(x) = x^m + b_{m-1}x^{m-1} + \cdots + b_1 x + b_0\in \F[x]$, where $b_i = \frac{a_i}{a_m}$.\newline

Observe that $f(T) = 0_{\Hom_{\F}\left(V,V\right)}$. In other words, $f\left(T\right)\left(v\right) = 0_V$ for all $v\in V$.
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$. There is a unique monic polynomial $m_T(x) \in \F[x]$ of lowest degree such that
  \begin{align*}
    m_T\left(T\right)\left(v\right) &= 0_V
  \end{align*}
  for every $v\in V$. Moreover, $\deg\left( m_T\left(x\right)\right)\leq n^2$
\end{theorem}
\begin{proof}[Proof of Uniqueness]
  Suppose $f(x) \in \F[x]$ satisfies $f(T)(v) = 0$ for all $v\in V$.\newline

  We write
  \begin{align*}
    f(x) &= m_T\left(x\right)q(x) + r(x),
  \end{align*}
  for some $q(x),r(x) \in \F[x]$, with $r(x) = 0$ or $\deg r(x) < \deg m_T(x)$.\newline

  Plugging in $T$, we have for all $v\in V$,
  \begin{align*}
    0_V &= f(T)(v)\\
        &= q(T)m_T(T)(v) + r(T)(v)\\
        &= q(T)\left(0_V\right) + r(T)(v)\\
        &= r(T)(v)
  \end{align*}
  Thus, $r(T) (v) = 0$ for all $v\in V$; thus, it must be the case that $r(T) = 0$.\newline

  Thus, $m_T(x)|f(x)$. However, if $m_T(x)$ and $f(x)$ are monic and of minimal degree, with $m_T(x)|f(x)$, then $m_T(x) = f(x)$.
\end{proof}
\begin{definition}
  The unique monic polynomial $m_T(x)$ is called the minimal polynomial.
\end{definition}
\begin{corollary}
  If $f(x)\in \F[x]$ satisfies $f(T)(v) = 0$ for all $v\in V$, then $m_T(x)|f(x)$.
\end{corollary}
\begin{example}
  Let $F = \Q$,
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 \\ 3 & 4\end{pmatrix}.
  \end{align*}
  We can see that for any $a_0\in \Q$,
  \begin{align*}
    A - a_0I_{2} \neq 0_{\Mat_{2}\left(\Q\right)}.
  \end{align*}
  However, for
  \begin{align*}
    A^2 &= \begin{pmatrix}7 & 10 \\ 15 & 22\end{pmatrix},
  \end{align*}
  we have
  \begin{align*}
    A^2 - 5A - 2I_{2} &= 0_{\Mat_{2}\left(\Q\right)},
  \end{align*}
  yielding $m_{A}\left(x\right) = x^2 - 5x - 2$.\newline

  The roots of $m_A(x)$ are $\frac{5\pm \sqrt{33}}{2}$.
\end{example}
\begin{example}
  Let $V = \Q^3$, $\mathcal{E}_3 = \set{e_1,e_2,e_3}$, with $T_A$ given by
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  We can find
  \begin{align*}
    A^2 &= \begin{pmatrix}1 & 4 & 8 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{pmatrix}\\
    A^3 &= \begin{pmatrix}1 & 6 & 11 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  Thus, we find
  \begin{align*}
    A^3 - A^2 - A + I &= 0,\\
    \left(x-1\right)^2 \left(x+1\right) &= m_{T_A}\left(x\right)
  \end{align*}
\end{example}
\begin{theorem}
  Let $V$ be an $\F$-vector space, and let $T\in \Hom_{\F}\left(V,V\right)$. We have $\lambda$ is an eigenvalue if and only if $\lambda$ is a root of $m_{T}\left(x\right)$.\newline

  In particular, if $\left(x-\lambda\right)\vert m_T(x)$, then $E_{\lambda}\neq \set{0_V}$.
\end{theorem}
\begin{proof}
  Let $\lambda$ be an eigenvalue with eigenvector $v$, and write $m_{T}\left(x\right) = x^m + \cdots + a_1x + a_0$. Notice that $T^{k}\left(v\right) = \lambda^{k}\left(v\right)$.\newline

  We have 
  \begin{align*}
    0_V &= m_T(T)(v)\\
        &= \left(T^{m} + a_{m-1}T^{m-1} + \cdots + a_1T + a_0\id_V\right)\left(v\right)\\
        &= T^{m}\left(v\right) + a_{m-1}T^{m-1}\left(v\right) + \cdots + a_1T\left(v\right) + a_0 v\\
        &= \lambda^{m}v + a_{m-1}\lambda^{m-1}v + \cdots + a_1\lambda v + a_0 v\\
        &= \left(\lambda^{m} + a_{m-1}\lambda^{m-1} + \cdots + a_1\lambda + a_0\right)v\\
        &= m_{T}\left(\lambda\right)v,
  \end{align*}
  meaning $m_T\left(\lambda\right)v = 0_V$. Since $m_T\left(\lambda\right)\in \F$ and $v \neq 0_V$, it is the case that $m_T\left(\lambda\right) = 0$, meaning $\lambda$ is a root of $m_T(x)$.\newline

  Suppose $m_T\left(\lambda\right) = 0$. This gives
  \begin{align*}
    m_T\left(x\right) &= \left(x-\lambda\right)f(x)
  \end{align*}
  for some $f(x)\in \F[x]$. Therefore, $\deg\left(f(x)\right) < \deg \left(m_{T}\left(x\right)\right)$. There must exist a nonzero vector $v\in V$ such that $f(T)(v) \neq 0_V$. Set $w = f(T)(v)$. Observe that $m_T(T)(v) = 0_V$, so $\left(T-\lambda\id_V\right)f(T)(v) = 0_V$, meaning $\left(T-\lambda\id_V\right)\left(w\right) = 0_V$, so $T(w) = \lambda w$. Thus, $\lambda$ is an eigenvalue.
\end{proof}
\begin{corollary}
  Let $\lambda_1,\dots,\lambda_m\in \F$ be distinct eigenvalues of $T$. For each $i$, let $v_i$ be an eigenvector with eigenvalue $\lambda_i$. Then, $\set{v_1,\dots,v_m}$ is linearly independent 
\end{corollary}
\begin{proof}
  We can write
  \begin{align*}
    m_T(x) &= \left(x-\lambda_1\right)\cdots\left(x-\lambda_m\right)f(x).
  \end{align*}
  Suppose $a_1v_1 + \cdots + a_mv_m = 0_V$ for some $a_i\in \F$.\newline

  Define $g_1(x) = \left(x-\lambda_2\right)\cdots\left(x-\lambda_m\right)f(x)$. Note that $g_1(T)(v_i) = 0_V$ for all $2\leq i \leq m$. Then,
  \begin{align*}
    0_V &= g_1(T)\left(0_V\right)\\
        &= \sum_{j=1}^{m}a_jg_1(T)\left(v_j\right)\\
        &= a_1g_1(T)\left(v_1\right)\\
        &= a_1g_1\left(\lambda_1\right)v_1.
  \end{align*}
  Since $g_1\left(\lambda_1\right)\neq 0$, and $v_1\neq 0$, it must be the case that $a_1 = 0$. Symmetry provides the case for $2,\dots,m$.
\end{proof}
\end{document}
