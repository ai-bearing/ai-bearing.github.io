\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright}
%\usepackage{sfmath}
%\usepackage{bbold} %better blackboard bold

%serif font + different blackboard bold for serif font
\DeclareMathOperator*{\lcm}{lcm}
\usepackage{newpxtext,eulerpx}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\usepackage{epigraph}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\sourceflush}{flushleft}
%\DeclareMathOperator{\Hom}{Hom}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Advanced Linear Algebra: Class Notes}
\cfoot{\thepage}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\section{Introduction}%
\epigraph{It is my experience that proofs involving matrices can be shortened by 50\% if one throws the matrices out.}{Emil Artin}
The goal of this course is to prove a lot of the essential results of linear algebra without basis dependence (as in, using the properties of the linear transformations themselves rather than matrices).
\tableofcontents
\section{Vector Spaces}%
\subsection{Vector Spaces and Linear Transformations}
\begin{remark}
We let $\F$ be either $\R,\Q,\C,\F_{p}$ (where $p$ is a prime). Primarily, we let $\F = \Q,\R,\C$.\newline
\end{remark}
\begin{example}[Our First Vector Space]
  The primary vector space we study in lower-division linear algebra is
  \begin{align*}
    V &= \R^n\\
      &= \set{ \left. \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_1,\dots,a_n\in \R }
  \end{align*}
  We know that for
  \begin{align*}
    v &= \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\\
    w &= \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix},
  \end{align*}
  that
  \begin{align*}
    v+w &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    cv &= \begin{pmatrix}ca_1 \\\vdots\\ca_n\end{pmatrix},
  \end{align*}
  where $c\in\R$ is some constant.
\end{example}
\begin{definition}[Vector Space]
  Let $V$ be a nonempty set with the following operations:
  \begin{itemize}
    \item $a: V\times V \rightarrow V$, $a(v,w)\mapsto v+w$ (vector addition);
    \item $m: F\times V \rightarrow V$, $m(c,v) \mapsto cv$ (scalar multiplication);
  \end{itemize}
  satisfying the following:
  \begin{enumerate}[(1)]
    \item there exists $0_v\in V$ such that $0_v + v = v = v + 0_v$ for all $v\in V$;
    \item for every $v\in V$, there exists $-v$ such that $v + (-v) = 0_v = (-v) + v$;
    \item for every $u,v,w\in V$, $(u+v) + w = u + (v+w)$;
    \item for every $v,w\in V$, $v+w = w+v$;
    \item for every $v,w\in V$ and $c\in \F$, $c(v+w) = cv + cw$;
    \item for every $c,d\in \F$, $v\in V$, $(c+d)v = cv + dv$;
    \item for every $c,d\in \F$, $v\in V$, $(cd)v = c(dv)$;
    \item for every $v\in V$, $\left(1_{\F}\right)v = v$.
  \end{enumerate}
  We say $V$ is a $\F$-vector space.
\end{definition}
\begin{example}[$\F^{n}$]
  Let $\F$ be a field, $V = \F^n$.
  \begin{align*}
    V &= \set{ \left.\begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\right|a_i\in \F }.
    \intertext{Define:}
    \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} + \begin{pmatrix}b_1\\\vdots\\b_n\end{pmatrix} &= \begin{pmatrix}a_1 + b_1\\\vdots\\a_n + b_n\end{pmatrix}\\
    c \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix} &= \begin{pmatrix}ca_1 \\\vdots \\ ca_n\end{pmatrix}.
  \end{align*}
  We set
  \begin{align*}
    0_{\F^n} &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}.
  \end{align*}
  Let
  \begin{align*}
    v &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
    w &= \begin{pmatrix}w_1\\\vdots\\w_n\end{pmatrix}\\
    u &= \begin{pmatrix}u_1\\\vdots\\u_n\end{pmatrix},
  \end{align*}
  $c,d\in \F$. We observe that
  \begin{align*}
    0_{\F^n} + v &= \begin{pmatrix}0 + v_1\\\vdots\\0 + v_n\end{pmatrix}\\
                 &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}.
  \end{align*}
  Define
  \begin{align*}
    -v &= \begin{pmatrix}-v_1\\\vdots\\-v_n\end{pmatrix}.
  \end{align*}
  Then,
  \begin{align*}
    v + (-v) &= \begin{pmatrix}v_1 + \left(-v_1\right)\\\vdots\\v_n + \left(-v_n\right)\end{pmatrix}\\
             &= \begin{pmatrix}0\\\vdots\\0\end{pmatrix}\\
             &= 0_{\F^n}.
  \end{align*}
  Note that
  \begin{align*}
    (u + v) + w &= \begin{pmatrix}\left(u_1 + v_1\right) + w_1 \\\vdots\\\left(u_n + v_n\right) + w_n\end{pmatrix}\\
                &= \begin{pmatrix}u_1 + \left(v_1 + w_1\right) \\\vdots\\u_n + \left(v_n + w_n\right)\end{pmatrix}\\
                &= u + (v+w).
  \end{align*}
  We have
  \begin{align*}
    v +w &= \begin{pmatrix}v_1 + w_1 \\\vdots\\v_n + w_n\end{pmatrix}\\
         &= \begin{pmatrix}w_1 + v_1\\\vdots\\w_n + v_n\end{pmatrix}\\
         &= w + v.
  \end{align*}
  Observe
  \begin{align*}
    c\left(v+w\right) &= c \begin{pmatrix}v_1 + w_1\\\vdots\\v_n + w_n\end{pmatrix}\\
                      &= \begin{pmatrix}c\left(v_1 + w_1\right)\\\vdots\\c\left(v_n + w_n\right)\end{pmatrix}\\
                      &= \begin{pmatrix}cv_1 + cw_1 \\\vdots \\cv_n + cw_n\end{pmatrix}\\
                      &= cv + cw,\\
    (c+d)v &= (c+d) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(c+d)v_1\\\vdots\\(c+d)v_n\end{pmatrix}\\
          &= \begin{pmatrix}cv_1 + dv_1 \\\vdots\\cv_n + dv_n\end{pmatrix}\\
          &= cv + dv,
          \intertext{and}
    (cd)v &= (cd) \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
          &= \begin{pmatrix}(cd)v_1 \\\vdots\\(cd)v_n\end{pmatrix}\\
          &= \begin{pmatrix}c\left(dv_1\right) \\\vdots\\c\left(dv_n\right)\end{pmatrix}\\
          &= c\left(dv\right).
  \end{align*}
  Finally,
  \begin{align*}
    1_{\mathbb{F}} &= 1_{\mathbb{F}} \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}1_{\mathbb{F}}v_1\\\vdots 1_{\F}\\v_n\end{pmatrix}\\
                   &= \begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}\\
                   &= v.
  \end{align*}
\end{example}
\begin{example}[Polynomials]
  Let $n\in \Z_{\geq 0}$. We define
  \begin{align*}
    P_{n}\left(\mathbb{F}\right) &= \set{a_0 + a_1x + \cdots + a_nx^n\left|a_i\in \F\right.}.
  \end{align*}
  For $f(x) = \sum_{j=0}^{n}a_jx^j$ and $g(x) = \sum_{j=0}^{n}b_jx^j$ in $P_n(\mathbb{F})$, we have
  \begin{align*}
    f(x) + g(x) &= \sum_{j=0}^{n}\left(a_j + b_j\right)x^j\\
    cf(x) &= \sum_{j=0}^{n}\left(ca_j\right)x^j.
  \end{align*}
  Note that these are not functions \textit{per se}, we are only $f(x)$ and $g(x)$ to represent elements of $P_n\left(\mathbb{F}\right)$. We can verify that $P_n\left(\mathbb{F}\right)$ is a $\mathbb{F}$-vector space.\newline

  We define
  \begin{align*}
    \mathbb{F}[x] &= \bigcup_{n\geq 0}P_n\left(\F\right),
  \end{align*}
  which is also a $\F$-vector space.
\end{example}
\begin{example}[Matrices]
  Let $m,n\in \Z_{> 0}$. We set
  \begin{align*}
    V &= \text{Mat}_{m,n}\left(\F\right),
  \end{align*}
  which is the set of $m\times n$ matrices with entries in $\F$. This is an $\F$-vector space with matrix addition and scalar multiplication.\newline

  In the case where $m = n$, we write $\text{Mat}_{n}\left(\F\right)$ to denote $\text{Mat}_{n,n}\left(\F\right)$.
\end{example}
\begin{example}[Complex Numbers]
  Let $V = \C$. Then, $V$ is a $\C$-vector space, an $\R$-vector space, and a $\Q$-vector space.\newline

  Note that the properties of a vector space change with the underlying scalar field.
\end{example}
\begin{lemma}[Basic Properties of Vector Spaces]
  Let $V$ be a $\F$-vector space.
  \begin{enumerate}[(1)]
    \item $0_V$ is unique.
    \item $0_{\mathbb{F}}v = 0_V$.
    \item $\left(-1_{\mathbb{F}}\right)v = -v$.
  \end{enumerate}
\end{lemma}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Suppose toward contradiction that there exist $0,0'$ both satisfy 
      \begin{align*}
        0 + v &= v\tag*{(\textasteriskcentered)}\\
        0' + v &= v.\tag*{(\textasteriskcentered\textasteriskcentered)}
      \end{align*}
      Then,
      \begin{align*}
        0 + v &= v\\
        0 + 0' &= 0'\tag*{by (\textasteriskcentered) with $v = 0'$}\\
               &= 0' + 0\\
               &= 0. \tag*{by (\textasteriskcentered\textasteriskcentered) with $v = 0$}
      \end{align*}
    \item Note
      \begin{align*}
        0_{\mathbb{F}}v &= \left(0_{\mathbb{F}} + 0_{\F}\right) v\\
                        &= 0_{\F}v + 0_{\F}v.
      \end{align*}
      We subtract $0_{\F}v$ from both sides.
    \item
      \begin{align*}
        \left(-1_{\mathbb{F}}\right)v + v &= \left(-1_{\mathbb{F}} \right)v + 1_{\F}v\\
                                          &= \left(-1_{\F} + 1_{\F}\right)v\\
                                          &= 0_{\F}v.
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{definition}[Subspaces]
  Let $V$ be an $\F$-vector space. We say $W\subseteq V$ is an $\F$-subspace (henceforth subspace) if $W$ is an $\F$-vector space under the same addition and scalar multiplication.
\end{definition}
\begin{example}[Subspaces of $\R^2$]
  Let $V = \R^2$. 
  \begin{center}
    \begin{tikzpicture}[scale = 0.5]
      \draw (0,5) -- (0,-5);
      \draw (5,0) -- (-5,0);
      \draw[thick, color=orange,<->] (-5,-5) -- (5,5);
      \node[anchor = south west] at (5,5){$W_1$};
      \draw[thick, color=yellow!40!black,<->] (-5,-1) -- (1,5);
      \node[anchor = south west] at (1,5) {$W_2$};
    \end{tikzpicture}
  \end{center}
  Here, we see that $W_1$ is a subspace, and $W_2$ is not a subspace (as $W_2$ does not contain $0_{V}$).
\end{example}
\begin{example}[Subspaces of $\C$]
  Let $V = \C$, $W = \set{a + 0i\mid a\in \R}$.
  \begin{itemize}
    \item If $\F = \R$, then $W$ is a subspace of $V$.
    \item If $\F = \C$, then $W$ is not a subspace; we can see that $2\in W$, $i\in \C$, but $2i\notin W$.
  \end{itemize}
\end{example}
\begin{example}[Matrices]
  It is not the case that $\text{Mat}_2(\R)$ is a subspace of $\text{Mat}_4(\R)$, since $\text{Mat}_2(\R)$ is not a subset of $\text{Mat}_4(\R)$.
\end{example}
\begin{example}[Polynomials]
  For the spaces $P_{m}(\F)$ and $P_{n}\left(\F\right)$, if $m \leq n$, then $P_{m}\left(\F\right)$ is a subspace of $P_{n}\left(\F\right)$.
\end{example}
\begin{lemma}[Proving Subspace Relation]
  Let $V$ be a $\F$-vector space, $W\subseteq V$. Then, $W$ is a subspace of $V$ if
  \begin{enumerate}[(1)]
    \item $W$ is nonempty;
    \item $W$ is closed under addition;
    \item $W$ is closed under scalar multiplication.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof is an exercise.
\end{proof}
\begin{definition}[Linear Transformation]
  Let $V,W$ be $\F$-vector spaces. Let $T: V\rightarrow W$. We say $T$ is a linear transformation (or linear map) if for every $v_1,v_2\in V$, $c\in \F$, we have
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right).
  \end{align*}
  Note that on the left side, addition is in $V$, and on the right side, addition is in $W$.\newline

  The collection of all linear maps from $V$ to $W$ is denoted $\Hom_{\F}\left(V,W\right)$, or $\mathcal{L}\left(V,W\right)$.
\end{definition}
\begin{example}[Identity Transformation]
  Define
  \begin{align*}
    \id_{V}: V\rightarrow V,
  \end{align*}
  where $\id_V(v) = v$. We can see that $\id_V \in \Hom_{\F}\left(V,V\right)$, since
  \begin{align*}
    \id_V\left(v_1 + cv_2\right) &= v_1 + cv_2\\
                            &= \id_V\left(v_1\right) + (c)\left(\id_{V}\left(v_2\right)\right)
  \end{align*}
\end{example}
\begin{example}[Complex Conjugation]
  Let $V = \C$. Define $T: V\rightarrow V$ by $z\mapsto \overline{z}$.\newline

  We may ask whether $T\in \Hom_{\R}\left(\C,\C\right)$ or $T\in \Hom_{\C}\left(\C,\C\right)$.
  \begin{align*}
    T\left(z_1 + cz_1\right) &= \overline{z_1 + cz_2}\\
                             &= \overline{z_1} + \left(\overline{c} \right)\left(\overline{z_2}\right).
  \end{align*}
  We can see that $T\left(z_1 + cz_2\right) = T\left(z_1\right) cT\left(z_2\right)$ if and only if $c = \overline{c}$, meaning $c$ must be real. This means $T\in \Hom_{\R}\left(\C,\C\right)$, but $T\notin \Hom_{\C}\left(\C,\C\right)$.
\end{example}
\begin{example}[Matrices]
  Let $A \in \text{Mat}_{m,n}\left(\F\right)$. We define
  \begin{align*}
    T_{A}: \F^n \rightarrow \F^m\\
    x \mapsto Ax.
  \end{align*}
  Then, $T_A \in \Hom_{\F}\left(\F^n,\F^m\right)$.
\end{example}
\begin{example}[Linear Maps on Smooth Functions]
  Let $V = C^{\infty}\left(\R\right)$, which denotes the set of continuous functions with continuous derivatives at all orders. This is a vector space under pointwise addition and scalar multiplication.
  \begin{align*}
    (f+g)\left(x\right) &= f(x) + g(x)\\
    (cf)(x) &= (c)\left(f(x)\right).
  \end{align*}
  Let $a\in \R$.
  \begin{enumerate}[(1)]
    \item 
\begin{align*}
  E_a: V\rightarrow \R\\
  f \mapsto f(a).
\end{align*}
Then, $E_a \in \Hom_{\R}\left(V,\R\right)$.
\item
  \begin{align*}
    D: V\rightarrow V\\
    f\mapsto f'.
  \end{align*}
  Then, $D\in \Hom_{\R}\left(V,V\right)$.
\item 
  \begin{align*}
    I_a: V\rightarrow V\\
    f\mapsto \int_{a}^{x} f(t)\:dt.
  \end{align*}
  Then, $I_a\in \Hom_{\R}\left(V,V\right)$.
\item Treating $f(a)$ as a (constant) function,
  \begin{align*}
  \tilde{E}_a: V\rightarrow V\\
    f\mapsto f(a).
  \end{align*}
  Then, $\tilde{E}_{a}\in \Hom_{\R}\left(V,V\right)$.
  \end{enumerate}
  Additionally,
  \begin{itemize}
    \item $D\circ I_a = \text{id}_V$;
    \item $I_a\circ D = \text{id}_V - \tilde{E}_a$ for some $a\in \R$.
  \end{itemize}
\end{example}
\begin{exercise}
  Show $\Hom_{\mathbb{F}}\left(V,W\right)$ is an $F$-vector space.
\end{exercise}
\begin{exercise}
  Let $U,V,W$ be vector spaces. Let $S\in \Hom_{\mathbb{F}}\left(U,V\right)$ and $T\in \Hom_{\mathbb{F}}\left(V,W\right)$. Show $T\circ S \in \Hom_{\mathbb{F}}\left(U,W\right)$
\end{exercise}
\begin{lemma}[Image of Identity]
  Let $T\in \Hom_{V,W}$. Then, $T\left(0_V\right) = 0_W$.
\end{lemma}
\begin{definition}[Isomorphism]
  Let $T\in \Hom_{\mathbb{F}}\left(V,W\right)$ be invertible, meaning there exists $T^{-1}W\rightarrow V$ such that $T\circ T^{-1} = \text{id}_{W}$ and $T^{-1}\circ T = \text{id}_{V}$.\newline

  We say $T$ is an isomorphism, and $V,W$ are isomorphic.
\end{definition}
\begin{exercise}
  Show $T^{-1}\in \Hom_{\mathbb{F}}\left(W,V\right)$.
\end{exercise}
\begin{example}[$\R^2$ and $\C$]
  Let $V = \R^2$, $W = \C$. Define $T: \R^2\rightarrow \C$, $(x,y)\mapsto x + iy$.\newline

  We can verify that $T\in \Hom_{\R}\left(\R^2,\C\right)$. Let $\left(x_1,y_1\right),\left(x_2,y_2\right)\in \R^2$ and $r\in \R$. Then,
  \begin{align*}
    T\left(\left(x_1,y_1\right) + r\left(x_2,y_2\right)\right) &= T\left(\left(x_1 + rx_2,y_1 + ry_2\right)\right)\\
                                                               &= \left(x_1 + rx_2\right) + i\left(y_1 + ry_2\right)\\
                                                               &= x_1 + iy_1 + rx_2 + i\left(ry_2\right)\\
                                                               &= x_1 + iy_1 + r\left(x_2 + iy_2\right)\\
                                                               &= T\left(\left(x_1,y_1\right)\right) + rT\left(\left(x_2,y_2\right)\right).
  \end{align*}
  Define $T^{-1}\C \rightarrow \R^2$ by $x + iy \mapsto (x,y)$. We have $T\circ T^{-1}\left(x + iy\right) = x+iy$ is an inverse map and $T^{-1}\circ T\left(\left(x,y\right)\right) = \left(x,y\right)$. Thus, $\R^2\cong \C$ as $\R$-vector spaces.
\end{example}
\begin{example}[$P_{n}\left(\mathbb{F}\right)$ and $\F^{n+1}$]
  Set $V = P_{n}\left(\mathbb{F}\right)$ and $W = \mathbb{F}^{n+1}$.\newline

  Define $T: P_{n}\left(\F\right) \mapsto \F^{n+1}$,
  \begin{align*}
    a_0 + a_1x + \cdots + a_nx^n &\mapsto \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can verify that $T$ is linear, with inverse map $T^{-1}: \F^{n+1}\rightarrow P_{n}\left(\F\right)$
  \begin{align*}
    \begin{pmatrix}a_0\\a_1\\\vdots\\a_n\end{pmatrix} \mapsto a_0 + a_1x + \cdots + a_nx^n.
  \end{align*}
  Thus, $P_n(\F) \cong \F^{n+1}$.
\end{example}
\begin{definition}[Kernel]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \ker (T) &= \set{v\in V\mid T(v) = 0_W}.
  \end{align*}
  We call this the kernel of $T$.
\end{definition}
\begin{definition}[Image]
  Let $T\in \Hom_{\F}\left(V,W\right)$. Define
  \begin{align*}
    \img\left(T\right) &= T(V)\\
                      &= \set{w\in W\mid \exists v\in V\text{ such that }T(v) = w}
  \end{align*}
\end{definition}
\begin{lemma}[Kernel and Image are Subspaces]
  The kernel, $\ker(T)$, is a subspace of $V$, and the image, $\img\left(T\right)$, is a subspace of $W$.
\end{lemma}
\begin{proof}
  Since $T\left(0_V\right) = 0_W$, we know that both $\ker(T)$ and $\img\left(T\right)$ are nonempty.\newline

  Let $c\in \F$ and $v_1,v_2\in \ker(T)$. Then,
  \begin{align*}
    T\left(v_1 + cv_2\right) &= T\left(v_1\right) + cT\left(v_2\right)\\
                             &= 0.
  \end{align*}
  Thus, $v_1 + cv_2 \in \ker(T)$.\newline

  Let $w_1,w_2\in \img\left(T\right)$. Then, there exist $u_1,u_2\in V$ such that $T\left(u_1\right) = w_1$ and $T\left(u_2\right) = w_2$. We have
  \begin{align*}
    T\left(u_1 + cu_2\right) &= T\left(u_1\right) + cT\left(u_2\right)\\
                             &= w_1 + cw_2,
  \end{align*}
  meaning $w_1 + cw_2\in \img\left(T\right)$, meaning $\img\left(T\right)$ is a subspace of $W$.
\end{proof}
\begin{lemma}[Injectivity of a Linear Transformation]
  $T$ is injective and only if $\ker(T) = \set{0_V}$.
\end{lemma}
\begin{proof}
  Suppose $T$ is injective. Let $v\in V$ be such that $T\left(v\right) = 0_W$. We also know that $T\left(0_V\right) = 0_W$. Since $T$ is injective, this means $v = 0_V$.\newline

  Let $\ker(T) = \set{0_V}$. Suppose $T\left(v_1\right) = T\left(v_2\right)$. Then,
  \begin{align*}
    T\left(v_1\right) - T\left(v_2\right) &= 0_W\\
    T\left(v_1 - v_2\right) &= 0_W,
  \end{align*}
  meaning $v_1 - v_2 \in \ker(T)$, meaning $v_1 - v_2 = 0_V$. Thus, $v_1 = v_2$.
\end{proof}
\begin{example}[Projection Map]
  Let $m > n$. Define $T: \F^{m}\rightarrow \F^n$ by
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix} \mapsto \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}.
  \end{align*}
  We can see that $\img\left(T\right) = \F^n$.\newline

  To examine the kernel, let
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\in \ker(T).
  \end{align*}
  Then,
  \begin{align*}
    \begin{pmatrix}a_1\\\vdots\\a_m\end{pmatrix}\mapsto \begin{pmatrix}0\\\vdots\\0\end{pmatrix},
  \end{align*}
  with $n$ entries. Thus,
  \begin{align*}
    \ker(T) &= \set{\left. \begin{pmatrix}0\\0\\\vdots\\0\\a_{n+1}\\\vdots\\a_{m}\end{pmatrix}\right|a_i\in \F^m}\\
            &\cong \F^{m-n}.
  \end{align*}
\end{example}
\subsection{Bases and Dimension}%
For this section, we let $V $ be a $ \F$-vector space.
\begin{definition}[Linear Combination]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $v\in V$ is an $\F$-linear combination of $\mathcal{B}$ if there is a set $\set{a_i}_{i\in I}$ with $a_i = 0$ for all but finitely many $i$ such that
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  We write $v\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{definition}
\begin{example}
  Let $V = P_2\left(\F\right)$. Set $\mathcal{B} = \set{1,x,x^2}$. We have $\Span_{\F}\left(\mathcal{B}\right) = P_2\left(\F\right)$.
\end{example}
\begin{definition}[Linear Independence]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is $\F$-linearly independent if whenever
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V,
  \end{align*}
  we have $a_i = 0 $ for all $i\in I$. Note that these are finite sums.
\end{definition}
\begin{definition}[Hamel Basis]
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a subset of $V$. We say $\mathcal{B}$ is a $\F$-basis for $V$ if
  \begin{enumerate}[(1)]
    \item $\Span\left(\mathcal{B}\right) = V$
    \item $\mathcal{B}$ is linearly independent.
  \end{enumerate}
\end{definition}
\begin{example}[Standard Basis for $\F^n$]
Let $V = \F^n$. We let
\begin{align*}
  \mathcal{E}_n = \set{e_1,\dots,e_n},
\end{align*}
where
\begin{align*}
  e_1 &= \begin{pmatrix}1\\0\\\vdots\\0\end{pmatrix}\\
  e_2 &= \begin{pmatrix}0\\1\\\vdots\\0\end{pmatrix}\\
      &\vdots\\
  e_n &= \begin{pmatrix}0\\0\\\vdots\\1\end{pmatrix}.
\end{align*}
We have $\mathcal{E}_n$ is a basis of $\F^n$ referred to as the standard basis.
\end{example}
We wish to show that every vector space has a basis. In order to do so, we require Zorn's lemma.
\begin{theorem}[Zorn's Lemma]
  Let $X$ be a nonempty partially ordered set. If every totally ordered subset of $X$ has an upper bound, then there exists at least one maximal element in $X$.
\end{theorem}
\begin{theorem}
  Let $\mathcal{A}$ and $\mathcal{C}$ be subsets of $V$ with $\mathcal{A}\subseteq \mathcal{C}$. Assume $\mathcal{A}$ is linearly independent and $\Span_{\F}\left(\mathcal{C}\right) = V$. Then, there exists a basis $\mathcal{B}$ of $V$ with $\mathcal{A}\subseteq \mathcal{B}\subseteq \mathcal{C}$.
\end{theorem}
\begin{proof}
  Take
  \begin{align*}
    X &= \set{\mathcal{B}'\subseteq V\mid \mathcal{A}\subseteq \mathcal{B}'\subseteq \mathcal{C},\mathcal{B}\text{ linearly independent}}.
  \end{align*}
  We have $\mathcal{A}\in X$, meaning $X$ is nonempty. We know that $X$ is partially ordered with respect to inclusion, and has an upper bound of $\mathcal{C}$.\newline

  Thus, by Zorn's lemma, we have a maximal element in $X$. We call this maximal element $\mathcal{B}$. By the definition of $X$, $\mathcal{B}$ is linearly independent.\newline

  We claim that $\Span_{\F}\left(\mathcal{B}\right) = V$. If not, there exists some $v\in \mathcal{C}$ such that $v\notin \Span_{\F}\left(\mathcal{B}\right)$. However, if $v\notin \Span_{\F}\left(\mathcal{B}\right)$, then $\mathcal{B}\cup \set{v}\subseteq \mathcal{C}$ is linearly independent. However, since $\mathcal{B}\subsetneq \mathcal{B}\cup \set{v}$, this implies that $\mathcal{B}$ is not maximal, which is a contradiction. Thus, $\Span_{\F}\left(\mathcal{B}\right) = V$.
\end{proof}
\begin{remark}
This proof applies to all vector spaces, not just those with finite dimensions.
\end{remark}
\begin{lemma}
  A homogeneous system of $m$ linear equations in $n$ unknowns with $m < n$ has a nonzero solution.
\end{lemma}
\begin{corollary}
  Let $\mathcal{B}\subseteq V$ with $\Span_{\F}\left(\mathcal{B}\right) = V$ and $\left\vert \mathcal{B} \right\vert = m$.\newline

  Then, any set with more than $m$ elements cannot be linearly independent.
\end{corollary}
\begin{proof}
  Let $\mathcal{C} = \set{w_1,\dots,w_n}$ with $n > m$. We wish to show that $\mathcal{C}$ cannot be linearly independent.\newline

  Write $\mathcal{B} = \set{v_1,\dots,v_m}$ with $\Span_{\F}\left(\mathcal{B}\right) = V$. For each $i$, write $w_i = \sum_{j=1}^{m}a_{ji}v_j$ for some $a_{ji}\in \F$.\newline

  Consider the equations
  \begin{align*}
    \sum_{i=1}^{n}a_{ji}x_i = 0.
  \end{align*}
  We have a solution to this $\left(c_1,\dots,c_n\right) \neq \left(0,\dots,0\right)$.\newline

  We have
  \begin{align*}
    0 &= \sum_{j=1}^{m} \left(\sum_{i=1}^{n}a_{ji}c_i\right)v_j\\
      &= \sum_{i=1}^{n}c_i\left(\sum_{j=1}^{m}a_{ji}v_j\right)\\
      &= \sum_{i=1}^{n}c_iw_i.
  \end{align*}
  Thus, $\mathcal{C}$ is not linearly independent.
\end{proof}
\begin{corollary}
  If $\mathcal{B}$ and $\mathcal{C}$ are bases over $V$, with $\mathcal{B}$ and $\mathcal{C}$ finite, then $\Card \mathcal{B} = \Card \mathcal{C}$.
\end{corollary}
\begin{proof}
  Let $|\mathcal{B}| = m$, $|\mathcal{C}| = n$. Since $\mathcal{C}$ is linearly independent, we know that $n\leq m$. We reverse the roles to see that $m\leq n$.
\end{proof}
\begin{definition}[Dimension]
  Let $V$ be a $\F$-vector space with Hamel basis $\mathcal{B}$. Then, we define $\Dim_{\F} V = \Card \mathcal{B}$.
\end{definition}
\begin{theorem}
  Let $V$ be finite-dimensional with $\Dim_{\F} V = n$. Let $\mathcal{C} \subseteq V$ with $\Card \mathcal{C} = m$.
  \begin{enumerate}[(1)]
    \item If $m > n$, then $\mathcal{C}$ is not linearly independent.
    \item If $m < n$, then $\Span_{\F}\left(\mathcal{C}\right) \neq V$.
    \item If $m = n$, then the following are equal:
      \begin{itemize}
        \item $\mathcal{C}$ is a basis;
        \item $\mathcal{C}$ is linearly independent;
        \item $\Span_{\F}\left(\mathcal{C}\right) = V$.
      \end{itemize}
  \end{enumerate}
\end{theorem}
\begin{corollary}
  Let $W\subseteq V$ be a subspace. We have $\Dim_{\F}W \leq \Dim_{\F} V$.\newline

  If $\Dim_{\F} V < \infty$, then $V = W$ if and only if $\Dim_{\F} W = \Dim_{\F} V$.
\end{corollary}
\begin{example}
  Let $V = \C$.\newline

  If $\F = \C$, then $\mathcal{B} = \set{1}$, and $\Dim_{\C}\C = 1$.\newline

  If $\F = \R$, then $\mathcal{B} = \set{1,i}$, and $\Dim_{\R}\C = 2$.

  %If $\F = \Q$, then $\mathcal{B}$ is uncountable.
\end{example}
\begin{example}
  Let $V = \F[x]$, and let $f(x) \in \F[x]$ be fixed.\newline

  Define an equivalence relation $g(x) \equiv h(x) $ if $f(x)|\left(g(x) - h(x)\right)$.\newline

  Given $g(x) \in \F[x]$, write $\left[g(x)\right]$ for the equivalence class containing $g(x)$.\newline

  Define $W = \F[x] / \left(f(x)\right) = \set{\left[g(x)\right]\mid g(x)\in \F[x]}$.\newline

  Define
  \begin{align*}
    [g(x)] + [h(x)] &= [g(x) + h(x)]\\
    c[g(x)] &= [cg(x)].
  \end{align*}
  This makes $W$ into a vector space. Set $n = \deg f(x)$.\newline

  Then, we claim
  \begin{align*}
    \mathcal{B} = \set{[1],[x],\dots,\left[x^{n-1}\right]}.
  \end{align*}
  Suppose there exist $a_0,\dots,a_{n-1} \in \F$ with
  \begin{align*}
    a_0 [1] + a_1[x] + \cdots + a_{n-1}\left[x^{n-1}\right] = [0].
  \end{align*}
  Then,
  \begin{align*}
    \left[a_0 + a_1x + \cdots + a_{n-1}x^{n-1}\right] = [0].
  \end{align*}
  Therefore,
  \begin{align*}
    f(x) | \left(a_0 + a_1x + \cdots + a_{n-1}x^{n-1} - 0\right),
  \end{align*}
  which means we must have $a_0 = a_1 = \cdots = a_{n-1}$.\newline

  Let $\left[g(x)\right]\in W$. By the Euclidean algorithm,
  \begin{align*}
    g(x) &= f(x)q(x) + r(x)
  \end{align*}
  for some $q(x),r(x) \in \F[x]$ with $r(x) = 0$ or $\deg r(x) < n$. Thus, we have
  \begin{align*}
    \left[g(x)\right] &= \left[f(x)q(x)\right] + \left[r(x)\right]\\
                      &= \left[r(x)\right].
  \end{align*}
  Since $r(x) = 0$ or $\deg r(x) < n$, we must have $\left[g(x)\right] = \left[r(x)\right]\in \Span_{\F}\left(\mathcal{B}\right)$.
\end{example}
\begin{lemma}
  Let $V$ be an $\mathbb{F}$-vector space, with $\mathcal{C} = \set{v_i}_{i\in I}$ be a subset of $V$.\newline

  Then, $\mathcal{C}$ is a basis if and only if each $v\in V$ can be uniquely written as a linear combination of elements of $\mathcal{C}$.
\end{lemma}
\begin{proof}
  Suppose $\mathcal{C}$ is a basis. Let $v\in V$, and suppose
  \begin{align*}
    v &= \sum_{i\in I}a_iv_i\\
      &= \sum_{i\in I}b_iv_i
  \end{align*}
  for some $a_i,b_i\in \mathbb{F}$. Then,
  \begin{align*}
    0_V &= \sum_{i\in I}\left(a_i - b_i\right)v_i.
  \end{align*}
  Since $\mathcal{C}$ is a basis, $a_i - b_i = 0$ for all $i$, meaning $a_i = b_i$, so the expression is unique.\newline

  Suppose every $v$ can be written as a unique linear combination of $\mathcal{C}$. Certainly, this means $\Span_{\mathbb{F}}\left(\mathcal{C}\right) = V$. Suppose
  \begin{align*}
    0_V &= \sum_{i\in I}a_iv_i
  \end{align*}
  for some $a_i\in \mathbb{F}$. It is also true that $0_V = \sum_{i\in I}0v_i$, meaning $a_i = 0$ for all $i$ by uniqueness; thus, $\mathcal{C}$ is linearly independent.
\end{proof}
\begin{proposition}
  Let $V,W$ be $\mathbb{F}$-vector spaces.
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,W\right)$. We have $T$ is uniquely determined by the image of the basis of $V$.
    \item Let $\mathcal{B}=\set{v_i}_{i\in I}$ be a basis of $V$, and let $\mathcal{C} = \set{w_i}$ be a subset of $W$. If $\Card(\mathcal{B}) = \Card\left(\mathcal{C}\right)$, there is a $T\in \Hom_{\F}\left(V,W\right)$ such that $T\left(v_i\right) = w_i$ for every $i$
  \end{enumerate}
\end{proposition}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Let $v\in V$, let $\mathcal{B} = \set{v_i}$ be a basis of $V$, and write $v = \sum_{i\in I}a_iv_i$. We have
  \begin{align*}
    T\left(v\right) &= T\left(\sum_{i\in I}a_iv_i\right)\\
                    &= \sum_{i\in I}a_iT\left(v_i\right).
  \end{align*}
    \item  Define $T$ by setting
      \begin{align*}
        T\left(v\right) &= \sum_{i\in I}a_iw_i,
      \end{align*}
      for $v = \sum_{i\in I}a_iv_i$. We can verify that $T$ is linear.
  \end{enumerate}
\end{proof}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$, with $\mathcal{B} = \set{v_i}$ a basis of $V$ and $\mathcal{C} = \set{w_i}\subseteq W$, with $w_i = T\left(v_i\right)$. Then, we have $\mathcal{C}$ is a basis of $W$ if and only if $T$ is an isomorphism.
\end{corollary}
\begin{proof}
  Let $\mathcal{C}$ be a basis for $W$. Since $\mathcal{C}$ is a basis of $W$, we use the proposition to define $S\in \Hom_{\F}\left(W,V\right)$ with $S\left(w_i\right) = v_i$. We can verify that $T\circ S = \text{id}_{W}$ and $S\circ T = \text{id}_V$, meaning $S = T^{-1}$ and $T$ is an isomorphism.\newline

  Suppose $T$ is an isomorphism. Let $w\in W$. Since $T$ is an isomorphism, $T$ is surjective, meaning there exists $v\in V$ such that $T(v) = w$. Since $\mathcal{B}$ is a basis of $V$, we expand $v$ to have
  \begin{align*}
    v = \sum_{i\in I}a_iv_i.
  \end{align*}
  Combining these two facts, we have
  \begin{align*}
    w &= T(v)\\
      &= T\left(\sum_{i\in I}a_iv_i\right)\\
      &= \sum_{i\in I}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right).
  \end{align*}
  Thus, $W = \Span_{\F}\left(\mathcal{C}\right)$.\newline

  Suppose there exists $a_i\in \F$ with $\sum_{i\in I}a_iT\left(v_i\right) = 0_W$. Since $T$ is linear, we have
  \begin{align*}
    \sum_{i\in I}a_iT\left(v_i\right) &= T\left(\sum_{i\in I}a_iv_i\right).
  \end{align*}
  Since $T$ is injective, we have
  \begin{align*}
    \sum_{i\in I}a_iv_i = 0_V.
  \end{align*}
  Since $\mathcal{B}$ is a basis, we have $a_i = 0$.
\end{proof}
\begin{theorem}[Rank--Nullity]
  Let $V$ be finite-dimensional vector space over $\mathbb{F}$. Let $T\in \Hom_{\F}\left(V,W\right)$. Then,
  \begin{align*}
    \Dim_{\F}(V) &= \Dim_{\F}\left(\ker (T)\right) + \Dim_{\F}\left(\img (T)\right)
  \end{align*}
\end{theorem}
\begin{proof}
  Let $\Dim_{\F}\left(\ker(T)\right) = k$ and $\Dim_{\F}\left(V\right) = n$. Let $\mathcal{A} = \set{v_1,\dots,v_k}$ be a basis of $\ker(T)$. We extend $\mathcal{A}$ to a basis $\mathcal{B} = \set{v_1,\dots,v_n}$ of $V$.\newline

  We want to show that $\mathcal{C} = \set{T\left(v_{k+1}\right),\dots,T\left(v_n\right)}$ is a basis of $\img(T)$.\newline

  Let $w\in \img(T)$. Then, there is $v\in V$ such that $T(v) = w$. We write
  \begin{align*}
    v &= \sum_{i=1}^{n}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    w &= T\left(v\right)\\
      &= T\left(\sum_{i=1}^{n}a_iv_i\right)\\
      &= \sum_{i=1}^{n}a_iT\left(v_i\right)\\
      &= \sum_{i=k+1}^{n}a_iT\left(v_i\right)\\
      &\in \Span_{\F}\left(\mathcal{C}\right),
  \end{align*}
  since $\set{v_1,\dots,v_k}\subseteq \ker(T)$, meaning $\Span_{\F}\left(\mathcal{C}\right) = \im(T)$.\newline

  Suppose we have
  \begin{align*}
    \sum_{i=k+1}^{n}a_iT\left(v_i\right) = 0_W.
  \end{align*}
  Then, we have
  \begin{align*}
    T\left(\sum_{i=k+1}^{n}a_iv_i\right) &= 0_W,
  \end{align*}
  meaning $\sum_{i=k+1}^{n}a_iv_i\in \ker(T)$. This means there exist $a_1,\dots,a_k$ such that
  \begin{align*}
    \sum_{i=k+1}^{n}a_iv_i &= \sum_{i=1}^{k}a_iv_i,
  \end{align*}
  meaning
  \begin{align*}
    \sum_{i=1}^{k}a_iv_i + \sum_{i=k+1}^{n}\left(-a_i\right)v_i = 0_V.
  \end{align*}
  Since $\set{v_i}$ are a basis, this means $a_i = 0$ for all $i$.
\end{proof}
\begin{corollary}
  Let $V,W$ be $\mathbb{F}$-vector spaces with $\Dim_{\F}\left(V\right) = n$. Let $V_1\subseteq V$ be a subspace with $\Dim_{\F}\left(V_1\right) = k$, and $W_1\subseteq W$ a subspace with $\Dim_{\F}\left(W_1\right) = n-k$. Then, there exists $T\in \Hom_{\F}\left(V,W\right)$ such that $\ker(T) = V_1$ and $\img(T) = W_1$.
\end{corollary}
\begin{corollary}
  Let $T\in \Hom_{\F}\left(V,W\right)$ with $\Dim_{\F}\left(V\right) = \Dim_{\F}\left(W\right) < \infty$. Then, the following are equivalent:
  \begin{enumerate}[(1)]
    \item $T$ is an isomorphism;
    \item $T$ is injective;
    \item $T$ is surjective.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $A\in \Mat_{n}\left(\F\right)$. The following are equivalent:
  \begin{enumerate}[(1)]
    \item $A$ is invertible;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $BA = I_{n}$;
    \item There exists $B\in \Mat_{n}\left(\F\right)$ such that $AB = I_n$.
  \end{enumerate}
\end{corollary}
\begin{corollary}
  Let $\Dim_{\F}(V) = m$ and $\Dim_{\F}(W) = n$.
  \begin{enumerate}[(1)]
    \item If $m < n$ and $T\in \Hom_{\F}\left(V,W\right)$, then $T$ is not surjective.
    \item If $m > n$ and $T\in \Hom_\F\left(V,W\right)$, then $T$ is not injective.
    \item We have $m = n$ if and only if $V\cong W$.
  \end{enumerate}
\end{corollary}
\subsection{Direct Sums and Quotient Spaces}%
\begin{definition}[Sum of Subspaces]
  Let $V$ be a vector space, and $V_1,\dots,V_k$ be subspaces. Then, the sum of $V_1,\dots,V_k$ is
  \begin{align*}
    V_1 + \cdots + V_k &= \set{\sum_{i=1}^{k}v_i\mid v_i\in V_i}.
  \end{align*}
  This is a subspace of $V$.
\end{definition}
\begin{definition}[Independence of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V_1,\dots,V_k$ are independent if whenever $v_1 + \cdots v_k = 0_V$, we have $v_i = 0_V$.
\end{definition}
\begin{definition}[Direct Sum of Subspaces]
  Let $V_1,\dots,V_k$ be subspaces of $V$. We say $V$ is the direct sum of $V_1,\dots,V_k$, and write
  \begin{align*}
    V = V_1 \oplus \cdots \oplus V_k,
  \end{align*}
  if the following conditions hold.
  \begin{enumerate}[(1)]
    \item $V = V_1 + \cdots V_k$;
    \item $V_1,\dots,V_k$ are independent.
  \end{enumerate}
\end{definition}
\begin{example}[A Very Simple Direct Sum]
  Let $V = \F^2$, with $V_1 = \set{\left(x,0\right)\mid x\in \F}$ and $V_2 = \set{\left(0,y\right)\mid y\in \F}$, we can see that
  \begin{align*}
    V_1 + V_2 &= \set{\left(x,0\right) + \left(0,y\right)\mid x,y\in \F}\\
              &= \set{\left(x,y\right)\mid x,y\in \F}\\
              &= \F^2.
  \end{align*}
  If $\left(x,0\right) + \left(0,y\right) = 0$, then $x = 0$ and $y = 0$, meaning $\F^2 = V_1\oplus V_2$.
\end{example}
\begin{example}[Direct Sum Constructions]
  Let $V = \F[x]$.\newline

  Define $V_1 = \F$, $V_2 = \F x = \set{\alpha x\mid \alpha \in \F}$, $V_3 = P_1\left(\F\right)$.\newline

  We can see that
  \begin{align*}
    P_1 &= V_1\oplus V_2.
  \end{align*}
  However, $V_1$ and $V_3$ are not independent, since $1_{\F}\in V_1$ and $-1_{\F}\in V_3$ with $1_{\F} + \left(-1_{\F}\right) = 0_\F$.
\end{example}
\begin{example}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$, with $V_i = \Span(v_i)$. Then,
  \begin{align*}
    V = V_1\oplus \cdots \oplus V_n.
  \end{align*}
\end{example}
\begin{lemma}
  Let $V$ be a vector space, $V_1,\dots,V_k$ subspaces. We have $V = V_1\oplus \cdots \oplus V_k$ if and only if every $v\in V$ can be written uniquely in the form
  \begin{align*}
    v = v_1 +\cdots + v_k
  \end{align*}
  for $v_i\in V_i$.
\end{lemma}
\begin{proof}
  Suppose $V = V_1\oplus\cdots\oplus V_k$. Let $v\in V$. Then, $v = v_1 + \cdots + v_k$ for some $v_i\in V_i$ since $V = V_1+\cdots+V_k$. Suppose
  \begin{align*}
    v &= v_1 + \cdots v_k\\
      &= \tilde{v}_1 + \cdots + \tilde{v}_k
  \end{align*}
  for $v_i,\tilde{v}_i\in V_i$. Then,
  \begin{align*}
    0_V &= \left(v_1 - \tilde{v}_1\right) + \cdots + \left(v_k - \tilde{v}_k\right).
  \end{align*}
  Since $V_1,\dots,V_k$ are linearly independent, $v_i - \tilde{v}_i\in V_i$, we have $v_i - \tilde{v}_i = 0_V$, meaning the expression for $v$ is unique.\newline

  Suppose that every $v\in V$ can be written uniquely in the form $v = v_1 + \cdots + v_k$ with $v_i\in V_i$. Then,
  \begin{align*}
    V = V_1 + \cdots V_k
  \end{align*}
  by the definition of $V_1 + \cdots + V_k$. If
  \begin{align*}
    0_V = v_1 + \cdots v_k
  \end{align*}
  for $v_i\in V_i$, and it is also the case that
  \begin{align*}
    0_V = 0_V + \cdots + 0_V,
  \end{align*}
  with $0_V \in V_i$, then it must be the case that $v_i = 0_V$ for all $i$ by uniqueness. Thus, the $V_i$ are independent, so
  \begin{align*}
    V = V_1\oplus\cdots\oplus V_k.
  \end{align*}
\end{proof}
\begin{exercise}
  Let $V_1,\dots,V_k$ be subspaces of $V$. For each $i$, let $\mathcal{B}_i$ be a basis for $V_i$. Let $\mathcal{B} = \bigcup_{i =1}^{k}\mathcal{B}_i$. Show
  \begin{enumerate}[(1)]
    \item $\mathcal{B}$ spans $V$ if and only if $V = V_1 + \cdots + V_k$;
    \item $\mathcal{B}$ is linearly independent if and only if $V_1,\dots,V_k$ are independent;
    \item $\mathcal{B}$ is a basis if and only if $V = V_1 \oplus \cdots \oplus V_k$.
  \end{enumerate}
\end{exercise}
\begin{lemma}[Existence of Complement]
Let $V$ be a vector space, and $U\subseteq V$ be a subspace. Then, $U$ has a complement $W$ such that $U\oplus W = V$.
\end{lemma}
\begin{proof}
  Let $\mathcal{A} $ be a basis for $U$. Extend $\mathcal{A}$ to a basis $\mathcal{B}$ of $V$. Let $\mathcal{C} = \mathcal{B}\setminus \mathcal{A}$, and $W = \Span\left(\mathcal{C}\right)$.
\end{proof}
\begin{example}[Constructing a Quotient Group]
To introduce quotient spaces, consider the construction of the quotient group.\newline

Let $n\in \Z_{>1}$. We say $a \equiv b$ modulo $n$ if and only if $n|(a-b)$. This is an equivalence relation; we form $\Z/n\Z = \set{\left[a\right]_n\mid a\in\Z} = \set{\left[0\right]_n,\dots,\left[n-1\right]_n}$.\newline

However, we also do this by defining $n\Z = \set{nk\mid k\in\Z}$, and taking $a\equiv b$ mod $n$ if and only if $a-b \in n\Z$. Our equivalence classes are now
\begin{align*}
  \left[a\right]_n &= \set{a + nk\mid k\in\Z}\\
                   &= a + n\Z.
\end{align*}
\end{example}
\begin{definition}[Quotient Space]
  Let $W\subseteq V$ be a subspace. We say $v_1\sim v_2$ if $v_1 - v_2 \in W$. Note that if $w\in W$, then $w\sim 0_V$ since $w-0_V\in W$.\newline

  This is an equivalence relation.
  \begin{itemize}
    \item Reflexivity: since $W$ is a subspace, $0_V\in W$, meaning $v-v\in W$ for all $v\in V$.
    \item Symmetry: if $v_1\sim v_2$, then $v_1 - v_2 \in W$, meaning $-\left(v_1 - v_2\right)\in W$, so $v_2 - v_1\in W$, or $v_2 \sim v_1$.
    \item Transitivity: Let $v_1\sim v_2$ and $v_2\sim v_3$. Then, $v_1 - v_2\in W$ and $v_2 - v_3\in W$. Since $W$ is a subspace, $\left(v_1 - v_2\right) + \left(v_2 - v_3\right)\in W$, meaning $v_1 - v_3 \in W$, so $v_1 \sim v_3$.
  \end{itemize}
  We denote the equivalence classes by
  \begin{align*}
    \left[v\right] &= \left[v\right]_W\\
                   &= v+W\\
                   &= \set{\tilde{v}\in V\mid v\sim \tilde{v}}\\
                   &= \set{v+w\mid w\in W}.
  \end{align*}
  We set
  \begin{align*}
    V/W &:= \set{v + W\mid v\in V}.
  \end{align*}
  We need to define vector addition and scalar multiplication on $V/W$. Let $v_1 + W,v_2 + W\in V/W$ and $c\in\F$. Define
  \begin{align*}
    \left(v_1 + W\right) + \left(v_2 + W\right) &= \left(v_1 + v_2\right) + W\\
    c\left(v_1 + W\right) &= cv_1 + W.
  \end{align*}
  We will show that addition and scalar-multiplication are well-defined.
  \begin{description}
    \item[Addition:] Let $v_1 + W = \tilde{v}_1 + W$, $v_2 + W = \tilde{v}_2 + W$, meaning $v_1 = \tilde{v}_1 + w_1$ and $v_2 = \tilde{v}_2 + w_2$ for some $w_1,w_2 \in W$. We have
      \begin{align*}
        \left(v_1 + W\right) + \left(v_2 + W\right) &= \left(v_1 + v_2\right) + W\\
                                                    &= \left(\tilde{v}_1 + w_1 + \tilde{v}_2 + w_2\right) + W\\
                                                    &= \left(\tilde{v}_1 + \tilde{v}_2\right) + W
      \end{align*}
    \item[Scalar Multiplication:] Let $v + W = \tilde{v} + W$. Then, we have $v = \tilde{v} + w$ for some $w\in W$. For $c\in\F$, we have
      \begin{align*}
        c\left(v + W\right) &= cv + W\\
                            &= c\left(\tilde{v} + w\right) + W\\
                            &= c\tilde{v} + W\\
                            &= c\left(\tilde{v} + W\right).
      \end{align*}
  \end{description}
  We say $V/W$ is the quotient space of $V$ by $W$.
\end{definition}
\begin{example}[Quotient Space of $\R^2$]
  Let $V = \R^2$, and $W = \set{\left(x,0\right)\mid x\in \R}$.\newline

  Let $\left(x_0,y_0\right)\in V$. We have
  \begin{align*}
  \left(x_0,y_0\right) \sim \left(x,y\right)
  \end{align*}
  if
  \begin{align*}
    \left(x_0 - x,y_0 - y\right)\in W.
  \end{align*}
  The only condition is thus that the $y$-coordinates in $\R^2$ must be equal. Therefore,
  \begin{align*}
    \left(x_0,y_0\right) + W &= \set{(x,y_0)\mid x\in\R}.
  \end{align*}
  Define $\tau: \R\rightarrow V/W$, $y\mapsto \left(0,y\right) + W$. We claim that $\tau$ is an isomorphism.\newline

  Let $y_1,y_2,c\in\R$. We have
  \begin{align*}
    \tau\left(y_1 + cy_2\right) &= \left(0,y_1 + cy_2\right) + W\\
                                &= \left(\left(0,y_1\right) + W\right) + c\left(\left(0,y_2\right) + W\right)\\
                                &= \tau\left(y_1\right) + c\tau\left(y_2\right).
  \end{align*}
  Thus, we see that $\tau$ is a linear map.\newline

  To show surjectivity, let $\left(x,y\right) + W\in V/W$. We have $\left(x,y\right) + W = \left(0,y\right) + W$. Thus, $\tau$ is surjective, since
  \begin{align*}
    \tau\left(y\right) &= \left(0,y\right) + W\\
                       &= \left(x,y\right) + W.
  \end{align*}
  Finally, to show injectivity, we let $y\in\ker\left(\tau\right)$. We have
  \begin{align*}
    \tau\left(y\right) &= \left(0,y\right) + W\\
                       &= \left(0,0\right) + W,
  \end{align*}
  implying that $y = 0$. Thus, $\tau$ is injective.
\end{example}
\begin{example}[Quotient Space of Polynomials]
  Let $V = \F[x]$, $f(x) \in V$, and
  \begin{align*}
    W &= \set{g(x)\in \F[x]\mid f(x)|g(x)}.
  \end{align*}
  We can see that $W$ is a subspace, which we refer to as $\left\langle f(x) \right\rangle$.\newline

  We defined an equivalence class $g(x) \sim h(x)$ if $f(x) | \left(g(x) - h(x)\right)$, where we then constructed a vector space from this set.\newline

  In particular, this construction is realized as $V/W$.\footnote{The ramifications of this construction are covered in depth in Algebra II.}
\end{example}
\begin{definition}[Canonical Projection]
  Let $W\subseteq V$ be a subspace. The canonical projection map $\pi_W$ is defined by
  \begin{align*}
    \pi_W: V\rightarrow V/W\\
    v\mapsto v + W.
  \end{align*}
  Note that $\pi_W\in \Hom_{\F}\left(V,V/W\right)$.
\end{definition}
\begin{remark}
  To define a map $T: V/W\rightarrow U$, one must always verify that $T$ is well-defined.
\end{remark}
\begin{theorem}[First Isomorphism Theorem for Vector Spaces]
  Let $T\in\Hom_{\F}\left(V,W\right)$. Define $\overline{T}: V/\ker(T)\rightarrow W$ by taking $v + \ker(T) \mapsto T(v)$. Then, $\overline{T}\in \Hom_{\F}\left(V/\ker(T),W\right)$. Moreover, $V/\ker(T)\cong \img(T)$.
\end{theorem}
\begin{proof}
  We will first show that $\overline{T}$ is well-defined. Let $v_1 + \ker(T) = v_2 + \ker(T)$. Then, for some $\tilde{v}\in \ker(T)$, we have $v_1 = v_2 + \tilde{v}$. Then,
  \begin{align*}
    \overline{T}\left(v_1 + \ker(T)\right) &= T\left(v_1\right)\\
                                           &= T\left(v_2 + \tilde{v}\right)\\
                                           &= T\left(v_2\right) + T\left(\tilde{v}\right)\\
                                           &= T\left(v_2\right)\\
                                           &= \overline{T}\left(v_2 + \ker(T)\right).
  \end{align*}
  Let $v_1 + \ker(T)$, $v_2 + \ker(T)\in V/\ker(T)$, and $c\in \F$. Then, we have
  \begin{align*}
    \overline{T}\left(\left(v_1 + \ker(T)\right) + c\left(v_2 + \ker(T)\right)\right) &= \overline{T}\left(\left(v_1 + cv_2\right) + \ker(T)\right)\\
                                                                                      &= T\left(v_1 + cv_2\right)\\
                                                                                      &= T\left(v_1\right) + cT\left(v_2\right)\\
                                                                                      &= \overline{T}\left(v_1 + \ker(T)\right) + c\overline{T}\left(v_2 + \ker(T)\right).
  \end{align*}
  Let $w\in \img(T)$. Then, $w = T(v)$ for some $v\in V$, meaning
  \begin{align*}
    w &= T\left(v\right)\\
    &= \overline{T} \left(v + \ker(T)\right).
  \end{align*}
  Thus, $\overline{T}$ is surjective onto $\img(T)$.\newline

  Let $v + \ker(T)\in \ker\left(\overline{T}\right)$. Then,
  \begin{align*}
    \overline{T}\left(v + \ker(T)\right) &= 0_W.
  \end{align*}
  This gives
  \begin{align*}
    T\left(v\right) = 0_W,
  \end{align*}
  meaning $v\in \ker(T)$, meaning $v + \ker(T) = 0_V + \ker(T)$. Thus, $\overline{T}$ is injective.
\end{proof}
\subsection{Dual Spaces}%
\begin{definition}[Dual Space]
  Let $V$ be an $\F$-vector space. The dual space, $V'$,\footnote{My professor denotes this as $V^{\vee}$, but it's too hard to type that out in real time, so I will use the $'$ to denote the algebraic dual, just as $V^{\ast}$ denotes the continuous dual of $V$.} is defined to be
  \begin{align*}
    V':= \Hom_{\F}\left(V,\F\right).
  \end{align*}
\end{definition}
\begin{theorem}
  We have $V$ is isomorphic to a subspace of $V'$. If $\Dim_{\F}\left(V\right) < \infty$, then $V\cong V'$.
\end{theorem}
\begin{remark}
  The isomorphism between $V$ and $V'$ in the finite-dimensional case is not canonical --- that is, it depends on a basis.
\end{remark}
\begin{proof}
  Let $\mathcal{B} = \set{v_i}_{i\in I}$ be a basis for $V$.\newline

  For each $i\in I$, let $v_i'(v_j) = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta. We get $\set{v_i'}_{i\in I}$ are elements of $V'$. We obtain
  \begin{align*}
    T\in \Hom_{\F}\left(V,V'\right)
  \end{align*}
  by $T\left(v_i\right) = v_i'$.\newline

  To show $V$ is isomorphic to a subspace of $V'$, it suffices to show that $T$ is injective, since $V\cong \img(T)$, which is a subspace of $V'$.\newline

  Let $v\in V$ with $T(v) = 0_{V'}$. We write
  \begin{align*}
    v &= \sum_{i\in I}a_iv_i\\
    0_{V'}&= T(v)\\
                    &= \sum_{i\in I}a_iT\left(v_i\right)\\
                    &= \sum_{i\in I}a_iv_i'.
  \end{align*}
  Pick $j$ with $a_j\neq 0$. Note that
  \begin{align*}
    \sum_{i\in I}a_iv_i'(v_j) &= 0\\
                              &= a_j,
  \end{align*}
  which contradicts $a_j\neq 0$. Thus, $v = 0_V$, and $T$ is injective.\newline

  Suppose $\Dim_{\F}\left(V\right) = n$, with $\mathcal{B} = \set{v_1,\dots,v_n}$. Let $v'\in V'$. Define $a_i$ by
  \begin{align*}
    a_i &= v'\left(v_i\right).
  \end{align*}
  Set
  \begin{align*}
    v &= \sum_{i=1}^{n}a_iv_i.
  \end{align*}
  Define the map $S: V'\rightarrow V$ by taking
  \begin{align*}
    S\left(v'\right) &= \sum_{i=1}^{n}\left(v'\left(v_i\right)\right)v_i.
  \end{align*}
  We want to show that $S\in \Hom_{\F}\left(V',V\right)$, and $S$ is the inverse to $T$.\newline

  Let $v',w'\in V'$, $c\in \F$. Set $a_i = v'\left(v_i\right)$ and $b_i = w'\left(v_i\right)$. Then,
  \begin{align*}
    S\left(v' + cw'\right) &= \sum_{i=1}^{n}\left(v' cw'\right)\left(v_i\right)v_i\\
                           &= \sum_{i=1}^{n}\left(v'\left(v_i\right) + cw'\left(v_i\right)\right)v_i\\
                           &= \sum_{i=1}^{n}\left(v'\left(v_i\right)\right)v_i + c\sum_{i=1}^{n}w'\left(v_i\right)\\
                           &= S\left(v'\right) + cS\left(w'\right).
  \end{align*}
  We compute $S\circ T\left(v_i\right)$.
  \begin{align*}
    S\circ T\left(v_j\right) &= S\left(T\left(v_j\right)\right)\\
                             &= S\left(v_j'\right)\\
                             &= \sum_{i=1}^{n}v_j'\left(v_i\right)v_i\\
                             &= \sum_{i=1}^{n}\delta_{ij}v_i\\
                             &= v_j.
  \end{align*}
  Note that for $T\circ S$, we have $T\circ S$ maps $V'$ to $V'$, meaning we need to check that $T\circ S$ is the identity map on $V'$. Let $v'\in V'$. Then,
  \begin{align*}
    \left(T\circ S\right)\left(v'\right)\left(v_j\right) &= T\left(S\left(v'\right)\right)\left(v_j\right)\\
                                                         &= T\left(\sum_{i=1}^{n}v'\left(v_i\right)v_i\right)\left(v_j\right)\\
                                                         &= \left(\sum_{i=1}^{n}v'\left(v_i\right)T\left(v_i\right)\right)\left(v_j\right)\\
                                                         &= \sum_{i=1}^{n}v'\left(v_i\right)\left(v_i'\left(v_j\right)\right)\\
                                                         &= \sum_{i=1}^{n}v'\left(v_i\right)\delta_{ij}\\
                                                         &= v'\left(v_j\right).
  \end{align*}
\end{proof}
\begin{definition}[Dual Basis]
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$. The dual basis for $V'$ is
  \begin{align*}
    \mathcal{B}' &= \set{v_i',\dots,v_n'}.
  \end{align*}
\end{definition}
\begin{remark}
  It is possible to continue taking duals; in the case of finite-dimensional $V$, we have
  \begin{align*}
    V &\cong V'\\
    V' &\cong V''.
  \end{align*}
  Despite the isomorphism between $V$ and $V'$ not being canonical, it is the case that the isomorphism between $V$ and $V''$ \textit{is} canonical (i.e., not dependent on a basis).
\end{remark}
\begin{proposition}
  There is a canonical injective linear map from $V$ to $V''$. If $\Dim_{\F}\left(V\right) < \infty$, this is an isomorphism.
\end{proposition}
\begin{proof}
  Let $v\in V$. Define $\hat{v}: V' \rightarrow \F$, $\varphi \mapsto \varphi(v)$.\footnote{This can be notated as $\text{eval}_v$, but $\hat{v}$ is faster to type (and it's used in functional analysis).} We can easily verify that $\hat{v}$ is a linear map.\newline

  Therefore, we have $\hat{v}\in \Hom_{\F}\left(V',\F\right) = V''$. We have a map
  \begin{align*}
    \Phi: V\rightarrow V''\\
    v\mapsto \hat{v}.
  \end{align*}
  We want to verify that $\Phi$ is a linear and injective map. Let $v_1,v_2\in V$, $c\in \F$. Let $\varphi\in V'$.
  \begin{align*}
    \Phi\left(v_1 + cv_2\right)\left(\varphi\right) &= \left(\hat{v}_1 + c\hat{v}_2\right)\left(\varphi\right)\\
                                                    &= \varphi\left(v_1 + cv_2\right)\\
                                                    &= \varphi\left(v_1\right) + c\varphi\left(v_2\right)\\
                                                    &= \hat{v}_1\left(\varphi\right) + c\hat{v}_2\left(\varphi\right)\\
                                                    &= \Phi\left(v_1\right)(\varphi) + c\Phi\left(v_2\right)\left(\varphi\right).
  \end{align*}
  We will show that $\Phi$ is injective. Let $v\in V$; suppose $v\neq 0_V$. We form a basis $\mathcal{B}$ of $V$ that contains $v$. Note that $v'\in V'$, with $v'(v) = 1$ and $v'(w) = 0$ for $w\in \mathcal{B}$ and $w\neq v$.\newline
  
  Assume $v\in \ker\left(\Phi\right)$. Then, for any $\varphi\in V'$,
  \begin{align*}
    \Phi\left(v\right)(\varphi) &= 0\\
    \varphi(v) &= 0.
  \end{align*}
  However, this is a contradiction, as we can take $\varphi = v'$, where $\varphi(v) = 1$. Thus, it must be the case that $\Phi$ is injective.
\end{proof}
\begin{definition}[Dual Operator]
  Let $T\in \Hom_{\F}\left(V,W\right)$. We get an induced map $T': W'\rightarrow V'$. We define $T'\left(\varphi\right) = \varphi\circ T$.
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEio4ePrNWiEAB1dAMW7iYUAObwioAGYAnCAFskZEDghIATNU1SdAFQByAAp9els0AAssAEoQagY6ACMYBgAFfmUhEFssMwicORt7J0QXNyRRCS02P0KQO0cK6nLELyrfPV0wyKxjLiA
\begin{tikzcd}
V \arrow[rd, "T'(\varphi)"'] \arrow[r, "T"] & W \arrow[d, "\varphi"] \\
                                            & \F                    
\end{tikzcd}
  \end{center}
\end{definition}
\section{Choosing Coordinates}%
\subsection{Linear Transformations and Matrices}%
Let $V$ be a finite-dimensional $\mathbf{F}$-vector space. Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis. This vector space fixes an isomorphism $V \cong \mathbf{F}^n$.\newline

Let $v\in V$. We can write $v = \sum_{i=1}^{n}a_iv_i$ for some $a_i\in \F$. We take the map
\begin{align*}
  T_{\mathcal{B}}\left(v\right) &= \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix}\in \F^n.
\end{align*}
It is easy to see that $T$ is an isomorphism. Given $v\in V$, we write $\left[v\right]_{\mathcal{B}} = T_{\mathcal{B}}\left(v\right)$. We refer to this process as choosing coordinates.
\begin{example}
  Let $V = \Q^2$, and $\mathcal{B} = \set{ \begin{pmatrix}1\\1\end{pmatrix}, \begin{pmatrix}1\\-1\end{pmatrix} }$. We can check that $\mathcal{B}$ is a basis of $V$.\newline

  Let $v\in V$, $v = \begin{pmatrix}a\\b\end{pmatrix}$. We have
  \begin{align*}
    v &= \frac{a+b}{2} \begin{pmatrix}1\\1\end{pmatrix} + \frac{a-b}{2} \begin{pmatrix}1\\-1\end{pmatrix}.
  \end{align*}
  To represent $v$ in terms of this basis, we have
  \begin{align*}
    \left[v\right]_{\mathcal{B}} &= \begin{pmatrix}\frac{a+b}{2}\\\frac{a-b}{2}\end{pmatrix}.
  \end{align*}
  If we chose a different basis, such as the standard basis $\mathcal{E}_2 = \set{ \begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix} }$. In that case, we have
  \begin{align*}
    \left[v\right]_{\mathcal{E}_2} &= \begin{pmatrix}a\\b\end{pmatrix}.
  \end{align*}
\end{example}
\begin{example}
  Let $V = P_2\left(\R\right)$. Let $\mathcal{C} = \set{1,\left(x-1\right),\left(x-1\right)^2}$. We  know that $\mathcal{C}$ is a basis of $V$.\newline

  Let $f(x) = a + bx + cx^2\in P_2\left(\R\right)$. We can write $f$ in terms of this basis by taking
  \begin{align*}
    f(x) &= \left(a+b+c\right) + \left(b+2c\right)\left(x-1\right) + c\left(x-1\right)^2.
  \end{align*}
  In this case, we then have
  \begin{align*}
    \left[f(x)\right]_{\mathcal{C}} &= \begin{pmatrix}a+b+c\\b+2c\\c\end{pmatrix}.
  \end{align*}
\end{example}
Recall that given $A\in \Mat_{m,n}\left(\F\right)$, we obtain a linear map $T_{A}\in \Hom_{\F}\left(\F^{n},\F^{m}\right)$ by $T_A\left(v\right) = Av$. The converse is true as well. Given any map $T\in \Hom_{\F}\left(\F^{n},\F^{m}\right)$, there is a matrix $A$ such that $T = T_A$.\newline

Let $\mathcal{E}_n = \set{e_1,\dots,e_n}$ be the standard basis of $\F^n$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be the standard basis of $\F^{m}$.\newline

We have $T\left(e_j\right)\in \F^m$ for each $j$, meaning we have $a_{ij}\in \F$ with $T\left(e_j\right) = \sum_{i=1}^{m}a_{ij}f_j$.\newline

Define $A = \left(a_{ij}\right)_{ij}\in \Mat_{m,n}\left(\F\right)$. We want to show that $T_A\left(e_j\right) = T\left(e_j\right)$ for every $j$.\newline

Then, we have
\begin{align*}
  T_A\left(e_j\right) &= Ae_j\\
                      &= \sum_{a_{ij}}f_i\\
                      &= T\left(e_j\right).
\end{align*}
Let $T\in \Hom_{\F}\left(V,W\right)$. Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$ and $\mathcal{C} = \set{w_1,\dots,w_m}$ be a basis for $W$.\newline

Define $P = T_{\mathcal{B}}: V\rightarrow \F^n$, $v\mapsto \left[v\right]_{\mathcal{B}}$, $Q =  T_{\mathcal{C}}: W\rightarrow \F^{m}$, $w \mapsto \left[w\right]_{\mathcal{C}}$. This yields the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbAOrdeIDNjwEiZYePrNWiEAB1dAMQB6wMFzl8lgoqPXVNUnfuPAAtua7iYUAObwioABmAE4QrkhkIDgQSKISWmwAKhYgIWER1NFIAEz2ktogAAog1Ax0AEYwDIX8ykIgwVg+ABY4KWnhiHFZiADMeQk6AIolIGWV1bXWOgwwgW08QaGduVExfQOOIEP6AMZYwbsABIl7B8eFJgC0wualFVU1Vio6jS0LFFxAA
\begin{tikzcd}
  V \arrow[r, "T"] \arrow[d, "T_{\mathcal{B}}"']          & W \arrow[d, "T_{\mathcal{C}}"] \\
  \F^{n} \arrow[r, "T_{\mathcal{C}}\circ T\circ T_{\mathcal{B}}^{-1}"'] & \F^{m}          
\end{tikzcd}
\end{center}
In particular, this means $T$ is given by a matrix $A\in \Mat_{m,n}\left(\F\right)$, which we write as $\left[T\right]_{\mathcal{B}}^{\mathcal{C}} = A$.\newline

In particular, $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$ is the unique matrix that satisfies
\begin{align*}
  \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left(\left[v\right]_{\mathcal{B}}\right) &= \left[T(v)\right]_{\mathcal{C}}.
\end{align*}

To compute $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$, we have
  \begin{align*}
    T\left(v_j\right) &= \sum_{i=1}^{m}a_{ij}w_i \tag*{$a_{ij}\in\F$}\\
    \left[T\left(v_j\right)\right]_{\mathcal{C}} &= \left[\sum_{i=1}^{m}a_{ij}w_j\right]_{\mathcal{C}}\\
                                                 &= \begin{pmatrix}a_{1j}\\\vdots\\a_{mj}\end{pmatrix}.
  \end{align*}
  Similarly, since $\left[v\right]_{\mathcal{B}} = e_j$, we have
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left(e_j\right) &= \left[T\left(v_j\right)\right]_{\mathcal{C}}\\
                                                               &= \begin{pmatrix}a_{1j}\\\vdots\\a_{mj}\end{pmatrix},
  \end{align*}
  which is exactly the $j$th column of $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$.\newline

  We thus get a matrix of the form
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}} &= \begin{pmatrix}\left[T\left(v_1\right)\right]_{\mathcal{C}} & \cdots & \left[T\left(v_n\right)\right]_{\mathcal{C}}\end{pmatrix},
  \end{align*}
  where $\left[T\left(v_j\right)\right]_{\mathcal{C}}$ are column vectors.
\begin{example}
  Let $V = P_{3}\left(\R\right)$. Define $T\in \Hom_{\R}\left(V,V\right)$ by $T\left(f(x)\right) = f'(x)$.\newline

  We take $\mathcal{B} = \set{1,x,x^2,x^3}$ as our basis. Then, we have
  \begin{align*}
    T\left(1\right) &= 0\\
    T\left(x\right) &= 1\\
    T\left(x^2\right) &= 2x\\
    T\left(x^3\right) &= 3x^2.
  \end{align*}
  As we fill in our matrix, we have
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{B}} &= \begin{pmatrix}0 & 1 & 0 & 0\\  0 & 0 & 2 & 0\\ 0 & 0 & 0 & 3 \\ 0 & 0 & 0 & 0 \end{pmatrix}.
  \end{align*}
  We can view each column as a basis vector of $\mathcal{B}$ and each row as the corresponding representation in $\mathcal{C}$ (where, in this case, $\mathcal{C} = \mathcal{B}$).
\end{example}
\begin{example}
  Let $V = P_{3}\left(\R\right)$, $T\left(f(x)\right) = f'(x)$. Let $\mathcal{B} = \set{1,x,x^2,x^3}$ and $\mathcal{C} = \set{1,\left(x-1\right),\left(x-1\right)^2,\left(x-1\right)^3}$.
  \begin{align*}
    T\left(1\right) &= 0\\
    T\left(x\right) &= 1\\
    T\left(x^2\right) &= 2x = 2 + 2\left(x-1\right)\\
    T\left(x^3\right) &= 3x^2 = -9 - 6\left(x-1\right) + 3\left(x-1\right)^2.
  \end{align*}
  Thus, our matrix $\left[T\right]_{\mathcal{B}}^{\mathcal{C}}$ is
  \begin{align*}
    \left[T\right]_{\mathcal{B}}^{\mathcal{C}} &= \begin{pmatrix}0 & 1 & 2 & -9 \\ 0 & 0 & 2 & -6\\ 0&0&0&3\\ 0&0&0&0\end{pmatrix}
  \end{align*}
\end{example}
\begin{exercise}\hfill
  \begin{enumerate}[(1)]
    \item Let $\mathcal{A}$ be a basis of $U$, $\mathcal{B}$ a basis of $V$, and $\mathcal{C}$ a basis of $W$. Let $S\in\Hom_{\F}\left(U,V\right)$ and $T\in\Hom_{\F}\left(V,W\right)$.\newline

  Show that
  \begin{align*}
    \left[T\circ S\right]_{\mathcal{A}}^{\mathcal{C}} &= \left[T\right]_{\mathcal{B}}^{\mathcal{C}}\left[\mathcal{S}\right]_{\mathcal{A}}^{\mathcal{B}}.
  \end{align*}
  \item We know that given $A\in \Mat_{m,k}\left(\F\right)$ and $B\in \Mat_{n,m}\left(\F\right)$, we have corresponding $T_A$ and $T_B$ linear maps.\newline

  Show that you recover the definition of matrix multiplication by using Part 1 to define matrix multiplication.
  \end{enumerate}
  \end{exercise}
  \begin{note}
    To refer to $\left[T\right]_{\mathcal{B}}^{\mathcal{B}}$, we will write $\left[T\right]_{\mathcal{B}}$.
  \end{note}
  Let $V$ be a vector space, with $\mathcal{B}$ and $\mathcal{B}'$ bases of $V$. We want to be able to transfer information about $V$ in terms of $\mathcal{B}$ to information about $V$ in terms of $\mathcal{B}'$ (i.e., change the basis).\footnote{Note that $\mathcal{B}'$ does not refer to the algebraic dual.}\newline

Let $\mathcal{B} = \set{v_1,\dots,v_n}$ and $\mathcal{B}' = \set{v_1',\dots,v_n'}$. Define
\begin{align*}
  T: V\rightarrow \F^{n}\\
  v\mapsto \left[v\right]_{\mathcal{B}}\\
  S: V\rightarrow \F^{n}\\
  v\mapsto \left[v\right]_{\mathcal{B}'}.
\end{align*}
In terms of a diagram, we have
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbTjz7Y8BImWHj6zVohAAdLQDEAeoVkgM8wUVErqaqZp0Gj4mFADm8IqABmAJwgBbJDIQHAgkACZqBjoAIxgGAAV+BSEQbywXAAscEGtJDRAAFW5eEB9-JFFg0MQAZlz1NgBlYq9fAMQgkIr6220tLCgAfWB2LhbStvDqLtrImLjEs0VNNMzsnvzGnQBjLG9tgAIdAeHRnb3Dgv1gAFphMa4KLiA
\begin{tikzcd}
V \arrow[d, "T"'] \arrow[r, "\id_{V}"]        & V \arrow[d, "S"] \\
\F^n \arrow[r, "S\circ \id_{V}\circ T^{-1}"'] & \F^n            
\end{tikzcd}
\end{center}
In particular, the change of basis matrix is
\begin{align*}
  \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'}.
\end{align*}
\begin{exercise}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$. Show that
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}\left[v_1\right]_{\mathcal{B'}} & \cdots & \left[v_n\right]_{\mathcal{B}'}\end{pmatrix}.
  \end{align*}
\end{exercise}
\begin{example}
  Let $V = \Q^2$, $\mathcal{B} = \mathcal{E}_2 = \set{ \begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix} }$. Let
  \begin{align*}
    \mathcal{B}' &= \set{v_1 = \begin{pmatrix}1\\-1\end{pmatrix},v_2 = \begin{pmatrix}1\\1\end{pmatrix}}.
  \end{align*}
  Notice that
  \begin{align*}
    e_1 &= \frac{1}{2}v_1 + \frac{1}{2}v_2\\
    e_2 &= -\frac{1}{2}v_1 + \frac{1}{2}v_2.
  \end{align*}
  In particular, we have
  \begin{align*}
    \left[e_1\right]_{\mathcal{B}'} &= \begin{pmatrix}\frac{1}{2}\\\frac{1}{2}\end{pmatrix}\\
    \left[e_2\right]_{\mathcal{B}'} &= \begin{pmatrix}-\frac{1}{2}\\\frac{1}{2}\end{pmatrix}.
  \end{align*}
  Thus,
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1/2 & -1/2 \\ 1/2 & 1/2\end{pmatrix}.
  \end{align*}
  Let
  \begin{align*}
    v &= \begin{pmatrix}\frac{2}{3}\end{pmatrix}.
  \end{align*}
  We have
  \begin{align*}
    \left[v\right]_{\mathcal{E}_2} &= \begin{pmatrix}2\\3\end{pmatrix}\\
    \left[v\right]_{\mathcal{E}_2}^{\mathcal{B}} &= \begin{pmatrix}1/2 & -1/2 \\ 1/2 & 1/2\end{pmatrix} \begin{pmatrix}2\\3\end{pmatrix}\\
                                                 &= \begin{pmatrix}-1/2\\5/2\end{pmatrix}\\
                                                 &= -\frac{1}{2} \begin{pmatrix}1\\-1\end{pmatrix} + \frac{5}{2} \begin{pmatrix}1\\1\end{pmatrix}\\
                                                 &= \left[v\right]_{\mathcal{B}'}.
  \end{align*}
\end{example}
\begin{example}
  Let $V = P_2\left(\R\right)$, $\mathcal{B} = \set{1,x,x^2}$, $\mathcal{B}' = \set{1,\left(x-2\right),\left(x-2\right)^2}$.
\end{example}
We have
\begin{align*}
  1 &= (1)(1) + (0)\left(x-2\right) + (0)\left(x-2\right)^2\\
  x &= (2)(1) + (1)\left(x-2\right) + (0)\left(x-2\right)^2\\
  x^2 &= (4)(1) + (4)\left(x-2\right) + (1)\left(x-2\right)^2.
\end{align*}
Thus, we have
\begin{align*}
  \left[1\right]_{\mathcal{B}'} &= \begin{pmatrix}1\\0\\0\end{pmatrix}\\
  \left[x\right]_{\mathcal{B}'} &= \begin{pmatrix}2\\1\\0\end{pmatrix}\\
  \left[x^2\right]_{\mathcal{B}'} &= \begin{pmatrix}4\\4\\1\end{pmatrix}.
\end{align*}
Therefore,
\begin{align*}
  \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1 & 2 & 4\\0 & 1 & 4\\0 & 0 & 1\end{pmatrix}.
\end{align*}
For example, if we let $f(x) = -7 + 3x + 4x^2$, we have
\begin{align*}
  \left[f(x)\right]_{\mathcal{B}} &= \begin{pmatrix}-7\\3\\4\end{pmatrix}\\
  \left[f(x)\right]_{\mathcal{B}'} &= \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'}\left[f(x)\right]_{\mathcal{B}}\\
                                   &= \begin{pmatrix}1 & 2 & 4 \\ 0 & 1 & 4 \\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}-7\\3\\4\end{pmatrix}\\
                                   &= \begin{pmatrix}15\\19\\4\end{pmatrix}
                                   \intertext{meaning}
                                   f(x) &= 15 + 19\left(x-2\right) + 4\left(x-2\right)^2.
\end{align*}
\begin{exercise}[Group Work]
  Let $V = P_2\left(\R\right)$, $\mathcal{B} = \set{1,\left(x-1\right),\left(x-1\right)^{2}}$ and $\mathcal{B}' = \set{1,\left(x+1\right),\left(x+1\right)^2}$. Find the change of basis matrix, and find $\left[2 - 6\left(x-1\right) + 2\left(x-1\right)^2\right]_{\mathcal{B}'}$.
\end{exercise}
\begin{solution}
  We have
  \begin{align*}
    1 &= (1)(1) + (0)\left(x+1\right) + (0)\left(x+1\right)^2\\
    \left(x-1\right) &= -2\left(1\right) + (1)\left(x+1\right) + \left(0\right)\left(x+1\right)^2\\
    \left(x-1\right)^2 &=  4(1) -(4)\left(x+1\right) + (1)\left(x+1\right)^2
  \end{align*}
  Thus, the change of basis matrix is
  \begin{align*}
    \left[\id_{V}\right]_{\mathcal{B}}^{\mathcal{B}'} &= \begin{pmatrix}1 & -2 & 4 \\ 0 & 1 & -4 \\ 0 & 0 & 1\end{pmatrix}.
  \end{align*}
  Thus, we have
  \begin{align*}
    \left[2 - 6\left(x-1\right) + 2\left(x-1\right)^2\right]_{\mathcal{B}'} &= \begin{pmatrix}1 & -2 & 4\\ 0 & 1 & -4 \\ 0 & 0 & 1\end{pmatrix} \begin{pmatrix}2\\-6\\2\end{pmatrix}\\
                                                                            &= \begin{pmatrix}22\\-14\\2\end{pmatrix}
  \end{align*}
\end{solution}
%Recall that if $\Dim_{\F}\left(V\right) = n$, with $\mathcal{B} = \set{v_1,\dots,v_n}$, there is an isomorphism
%\begin{align*}
%  T_{\mathcal{B}}:V\rightarrow \F^n\\
%  v\mapsto \begin{pmatrix}a_1\\\vdots\\a_n\end{pmatrix},
%\end{align*}
%where $v = \sum_{i=1}^{n}a_iv_i$.\newline
\begin{definition}[Similar Matrices]
  Given $A,B\in \Mat_{n}\left(\F\right)$, we say $A$ and $B$ are similar if there exists $P\in \text{GL}_{n}\left(\F\right)$\footnote{$\text{GL}_{n}\left(\F\right) = \set{C\in \Mat_{n}\left(\F\right)\mid C^{-1}\text{ exists}}$} such that $A = PBP^{-1}$.
\end{definition}
We wish to rephrase this definition in terms of matrices. Given $A\in \Mat_{n}\left(\F\right)$, there exists $T_A\in \Hom_{\F}\left(F^n,H^n\right)$ with $T_A(v) = Av$. Given a basis $\mathcal{B}$, we have the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUQBfU9TXfIRQBGclVqMWbTjz7Y8BImWHj6zVohAAdLQDEAeoVkgM8wUVErqaqZp0Gj4mFADm8IqABmAJwgBbJDIQHAgkUQl1NgAVAH1gAEEubl4QH39A6hCkACZqBjoAIxgGAAV+BSEQbywXAAscEGtJDRBY4B0-OhxagGNGYAAhLiTjNIDEcKzEAGYmyM02jq7e-qGRlLGczNCZudttLQYYTxxkNsSdarqcCjil7r6GQeHGkHyi0vLzTSv67gouEA
\begin{tikzcd}
  \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{B}}"']    & \F^n \arrow[d, "T_{\mathcal{B}}"] \\
\F^n \arrow[r, "{\left[T_{A}\right]_{\mathcal{B}}}"'] & \F^n                          
\end{tikzcd}
\end{center}
If $\mathcal{E}_n$ is the standard basis, then $A = \left[T_{A}\right]_{\mathcal{E}_n}$, meaning we have the following diagram:
\begin{center}
  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDgMQD1CAvqXSZc+QigBMFanSat2XPoOEgM2PASJlishizaJOPfiCEiN4otN019Co0tPm1ozROQBmGXfmHjymaq6mJaKN62cgaKJioWoR7kPlEOAc7BblYoSZH2-k6CsjBQAObwRKAAZgBOEAC2SEkgOBBIZCn+ACoA+sAAggJBVbUNiE0tSNIgjPQARjCMAAqZYSDVWCUAFjggvtFGPcBcdfQ4mwDGTMAAogLdYIMuNfVtNBOI3h3sh8enF1e3e6PVTPUZTd6fPLsPq7aZzBbLSyrdZbHZPEavZqtRAAFj2qS4WCgvQKwOGLw+b2xAFZ8f5FrxgABaYgCAAEAF42VxGDBKjhkITiUdYgIuCjtpQSRwTmdLowbncHozfnKrgAhASDGgzeZLFYSNYbbZDECgpB4rFIWlfA7S2X-RhszWm82IABsVMadJiROlyjJZoxiAA7F7EFMoUYGczWZzuRxefzBRw-SKA+LjTgpSKHfLgJqBCqZX984CHrDdQiDewJWiQcHPVbQzr4fqkYa67CoyAfiW1QrC2ZKAIgA
  \begin{tikzcd}
\F^n \arrow[r, "\id_{\F^n}"] \arrow[d, "T_{\mathcal{B}}"']                          & \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{E}_n}"'] & \F^n \arrow[d, "T_{\mathcal{E}_n}"] \arrow[r, "\id_{\F^n}"]                         & \F^n \arrow[d, "T_{\mathcal B}"] \\
\F^n \arrow[r, "{P^{-1} = \left[\id_{\F^n}\right]_{\mathcal{B}}^{\mathcal{E}_n}}"'] & \F^n \arrow[r, "A"']                                    & \F^n \arrow[r, "{P^{-1} = \left[\id_{\F^n}\right]_{\mathcal{E}_n}^{\mathcal{B}}}"'] & \F^n                            
\end{tikzcd}
\end{center}
%\begin{center}
%  % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQAdDgMQD1CAvqXSZc+QigBMFanSat2XPoOEgM2PASJlishizaJOPfiCEiN4otN019Co0tPm1ozROQBmGXfmHjymaq6mJaKN62cgaKJioWoR7kPlEOAc7BblYoSZH2-k6CsjBQAObwRKAAZgBOEAC2SEkgOBBIZCn+ACoA+sAAggJBVbUNiE0tSNIgjPQARjCMAAqZYSDVWCUAFjggvtFGPcBcdfQ4mwDGTMAAogLdYIMuNfVtNBOI3h3sh8enF1e3e6PVTPUZTd6fPLsPq7aZzBbLSyrdZbHZPEavZqtRAAFj2qS4WCgvQKwOGLw+b2xAFZ8f5FrCZvMlisJGsNtshiBQUg8VikLSvgcSRwTmdLowAAQAITJ3IxiAAbFTGnSYkSRco5TzEAB2FWIKZQoyLXjAAC0xEGNCZCNZ7BRnPRFOV-L1NvhLKRbMdOzVwqOor+EuAsselAEQA
%\begin{tikzcd}
%\F^n \arrow[r, "\id_{\F^n}"] \arrow[d, "T_{\mathcal{B}}"'] & \F^n \arrow[r, "T_{A}"] \arrow[d, "T_{\mathcal{E}_n}"'] & \F^n \arrow[d, "T_{\mathcal{E}_n}"] \arrow[r, "\id_{\F^n}"] & \F^n \arrow[d, "T_{\mathcal B}"] \\\F^n \arrow[r, "P^{-1} = \id_{\F^n}\right]"']                                  & \F^n \arrow[r, "A"']                                    & \F^n \arrow[r, "P = \id_{\F^n}"']                                        & \F^n                            
%\end{tikzcd}
%\end{center}
Thus, $A = P \left[T_{A}\right]_{\mathcal{B}} P^{-1}$. In other words, $A\sim B$ if and only if $A = \left[T_{A}\right]_{\mathcal{B}}$ for some basis $\mathcal{B}$ and $B = \left[T_{A}\right]_{\mathcal{C}}$.
\subsection{Row Operations, Column Space, and Null Space}%
\begin{definition}[Pivot]
  Let $A = \left(a_{ij}\right)\in \Mat_{m,n}\left(\F\right)$. We say $a_{k\ell}$ is a pivot of $A$ if and only if $a_{k\ell}\neq 0$ and $a_{ij} = 0$ if $i\geq k$ or $j\leq \ell$, with $\left(i,j\right)\neq \left(k,\ell\right)$.
\end{definition}
\begin{example}
  For the matrix
  \begin{align*}
    A &= \begin{pmatrix}\boxed{2} & 1 & 4 & 5 \\ 0 & 0 & \boxed{1} & 7 \\ 0 & 0 & 0 & \boxed{5}\end{pmatrix},
  \end{align*}
  the boxed entries are pivots.
\end{example}
\begin{definition}
  Let $A\in \Mat_{m,n}\left(\F\right)$. We say $A$ is in row echelon form if all its nonzero rows have a pivot and all its zero rows are located below the nonzero rows. We say the matrix is in reduced row echelon form if it is in row echelon form and the pivots are the nonzero elements in the columns containing the pivots.
\end{definition}
\begin{example}
  We have
  \begin{align*}
    A &= \begin{pmatrix}2 & 1 & 4 & 5 \\ 0 & 0 & 1 & 7 \\ 0 & 0 & 0 & 5 \\ 0 & 0 & 0 & 0\end{pmatrix}
  \end{align*}
  is in row echelon form, and
  \begin{align*}
    B &= \begin{pmatrix}2 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\\0 & 0 & 0 & 0\end{pmatrix}
  \end{align*}
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}3 & 4 & 5 & 6 \\ 1 & 2 & 3 & 4 \\ 1 & 1 & 2 & 3\end{pmatrix}.
  \end{align*}
  We are going to put this matrix into reduced row echelon form. We have $T_A: \F^4 \rightarrow \F^3$. Let $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$ and $\mathcal{F}_3 = \set{f_1,f_2,f_3}$. Then, $A = \left[T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3}$. We have
  \begin{align*}
    T_A\left(e_1\right) &= 3f_1 + f_2 + f_3\\
    T_A\left(e_2\right) &= 4f_1 + 2f_2 + f_3\\
    T_A\left(e_3\right) &= 5f_1 + 3f_2 + 2f_3\\
    T_A\left(e_4\right) &= 6f_1 + 4f_2 + 3f_3
  \end{align*}
  \begin{description}
    \item[Step 1:] We switch $R_1\leftrightarrow R_3$, yielding
      \begin{align*}
        \mathcal{F}_3^{(2)} &= \set{f_1^{(2)} = f_3,f_{2}^{(2)},f_{3}^{(2)} = f_1},
      \end{align*}
      yielding
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(2)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 1 & 2 & 3 & 4 \\ 3 & 4 & 5 & 6\end{pmatrix}\\
        \\
        T_A\left(e_1\right) &= f_1^{(2)} + f_{2}^{(3)} + 3f_{3}^{(2)}\\
        T_A\left(e_2\right) &= f_{1}^{(2)} + 2f_{2}^{(3)} + 4f_3^{(2)}\\
        T_A\left(e_3\right) &= 2f_1^{(2)} + 3f_2^{(2)} + 5f_3^{(2)}\\
        T_A\left(e_4\right) &= 3f_1^{(2)} + f_2^{(2)} + 6f_3^{(2)}.
      \end{align*}
    \item[Step 2:] Our next step is $-R_1 + R_2 \rightarrow R_2$, yielding
      \begin{align*}
        \mathcal{F}_3^{(3)} &= \set{f_1^{(3)} = f_1^{(2)} + f_{2}^{(2)},f_{3}^{(2)} = f_{2}^{(2)},f_{3}^{(3)} = f_{2}^{(3)}}.
      \end{align*}
      Our new matrix is
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(3)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 3 & 4 & 5 & 6\end{pmatrix}\\
        \\
        T_A\left(e_1\right) &= \left(f_1^{(2)} + f_{2}^{(2)}\right) + 3f_{3}^{(2)}\\
                            &= f_{1}^{(3)} + 3f_3^{(3)}\\
        T_{A}\left(e_2\right) &= \left(f_1^{(2)} + f_2^{(2)}\right) + f_{2}^{(2)} + 4f_3^{(2)}\\
                              &= f_1^{(3)} + f_2^{(2)} + 4f_3^{(3)}\\
                              &\vdots
      \end{align*}
    \item[Step 3:] Next, we have $-3R_1 + R_3\rightarrow R_3$, which yields
      \begin{align*}
        \mathcal{F}_3^{(4)} &= \set{f_1^{(4)} = f_{1}^{(3)} + 3f_{3}^{(3)},f_{2}^{(4)} = f_{2}^{(3)},f_{3}^{(4)} = f_{3}^{(3)}}.
      \end{align*}
      Our matrix is now
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_{4}}^{\mathcal{F}_{3}^{(4)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 1 & -1 & -3\end{pmatrix}
      \end{align*}
    \item[Step 4:] Next, we have $-R_2 + R_3 \rightarrow R_3$, which yields
      \begin{align*}
        \mathcal{F}_{3}^{(5)} &= \set{f_{1}^{(5)} = f_{1}^{(4)},f_{2}^{(5)} = f_{2}^{(4)} + f_{3}^{(4)},f_{3}^{(5)} = f_{3}^{(4)}},
      \end{align*}
      and a matrix of
      \begin{align*}
        \left[T_{A}\right]_{\mathcal{E}_4}^{\mathcal{F}_3^{(5)}} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & -4\end{pmatrix}.
      \end{align*}
  \end{description}
\end{example}
\begin{theorem}
  Let $A\in \Mat_{m,n}\left(\F\right)$. The matrix $A$ can be put in row echelon form through a series of row operations of the form:
  \begin{itemize}
    \item switching two rows: $R_i\leftrightarrow R_j$;
    \item multiplying a row by a scalar: $\R_i\rightarrow cR_i$;
    \item replacing a row by adding a scalar multiple of another row: $aR_i + R_j\rightarrow R_j$.
  \end{itemize}
\end{theorem}
\begin{proof}[Sketch of a Proof]
  For any matrix, we switch rows such that the value of $a_{11}$ is nonzero. Then, we take
  \begin{align*}
    f_{1}^{(2)} &= \sum_{j=1}^{m}a_{ji}f_j\\
    f_k^{(2)} = f_{k}.
  \end{align*}
\end{proof}
Instead of directly changing the bases, we can use linear maps to change the bases.\newline

We define $T_{i,j}: W\rightarrow W$ to be
\begin{align*}
  T_{i,j}\left(w_{k}\right) &= w_k\tag*{$k\neq i,j$}\\
  T_{i,j}\left(w_i\right) &= w_j\\
  T_{i,j}\left(w_j\right) &= w_i.
\end{align*}
Thus,
\begin{align*}
  E_{i,j} &= \left[T_{i,j}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
is the identity matrix except for switching the $i$ and $j$ rows.\newline

Let $c\in \F$, define $T_i^{(c)}:W\rightarrow W$ by
\begin{align*}
  T_i^{(c)}\left(w_k\right) &= w_k\tag*{$k\neq i$}\\
  T_{i}^{(c)}\left(w_i\right) &= cw_i,
\end{align*}
with
\begin{align*}
  E_i^{(c)} &= \left[T_{i}^{(c)}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
being the identity matrix except for row $i$ multiplied by $c$.\newline

Finally, we define $T_{i,j}^{(c)}:W\rightarrow W$ by
\begin{align*}
  T_{i,j}^{(c)}\left(w_k\right) &= w_k\tag*{$k\neq j$}\\
  T_{i,j}^{(c)}\left(w_j\right) &= cw_i + w_j,
\end{align*}
with
\begin{align*}
  E_{i,j}^{(c)} &= \left[T_{i,j}^{(c)}\right]_{\mathcal{C}}^{\mathcal{C}}
\end{align*}
as the identity map with $c$ in the $ij$th entry.
\begin{example}
  Let 
  \begin{align*}
    A &= \begin{pmatrix}3 & 4 & 5 & 5\\ 1 & 2 & 3 & 4 \\ 1 & 1 & 2 & 3\end{pmatrix}.
  \end{align*}
  Define $T_A: \F^4\rightarrow \F^3$, $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$, and $\mathcal{F}_3 = \set{f_1,f_2,f_3}$. We have
  \begin{align*}
    T_A\left(e_1\right) &= 3f_1 + f_2 + f_3\\
    T_A\left(e_2\right) &= 4f_1 + 2f_2 + f_3\\
    T_A\left(e_3\right) &= 5f_1 + 3f_2 + 2f_3\\
    T_A\left(e_4\right) &= 6f_1 + 4f_2 + 3f_3.
  \end{align*}
  First, we interchange the rows by $T_{1,3}:\F^3\rightarrow \F^3$, Then,
  \begin{align*}
    \left(T_{1,3}\circ T_A\right)\left(e_1\right) &= T_{1,3}\left(3f_1 + f_2 + f_3\right)\\
                                                  &= 3T_{1,3}\left(f_1\right) + T_{1,3}\left(f_1\right) + T_{1,3}\left(f_3\right).
  \end{align*}
  If we look at the matrix, we then have
  \begin{align*}
    \left[T_{1,3}\circ T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3} &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 1 & 2 & 3 & 4 \\ 3 & 4 & 5 & 6\end{pmatrix}.
  \end{align*}
  For the full reduced row echelon form, we would have the following series of transformations:
  \begin{align*}
    \left[T_{1,3}^{(-1)}\circ T_{2,3}^{(-1)}\circ T_{3}^{(-2)}\circ T_{3,1}^{(-3)}\circ T_{1,2}^{-1}\circ T_{1,3}\circ T_A\right]_{\mathcal{E}_4}^{\mathcal{F}_3} &= \begin{pmatrix}1 & 0 & 0 & 0 \\ 0 & 1 & 0  & -1\\ 0 & 0 & 1 & 2\end{pmatrix}.
  \end{align*}
\end{example}
\begin{definition}[Column Space, Null Space, and Rank]
  Let $A\in \Mat_{m,n}\left(\F\right)$. The column space of $A$ is the $\F$-span of the column vectors. This is denoted $\Col(A)$.\newline

  The null space, $\Null(A)$, is the $\F$-span of the vectors $v\in \F^n$ such that $Av = 0_{\F^m}$.\newline

  The rank of $A$, denoted $\Rank(A)$, is $\Rank(A) = \Dim_{\F}\left(\Col(A)\right)$.
\end{definition}
  Let $\mathcal{E}_n = \set{e_1,\dots,e_n}$ be the standard basis for $\F^n$, with $T_A \in \Hom_{\F}\left(\F^n,\F^n\right)$, and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ the standard basis of $\F^m$.\newline

  We have $\left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m} = A$. We know that
  \begin{align*}
    A &= \begin{pmatrix}T_A\left(e_1\right) & \cdots & T_A\left(e_n\right)\end{pmatrix}.
  \end{align*}
  Thus, $\Col(A) = \img\left(T_A\right)$, meaning $\Rank(A) = \Dim_{\F}\left(\img\left(T_A\right)\right)$.\newline

In order to calculate $\col(A)$, we put the matrix $A$ into row echelon form, look at the columns that have pivots, and those columns form the basis for $\col(A)$.\newline

We have an isomorphism $E: \F^m\rightarrow \F^m$ such that
\begin{align*}
  \left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m} &= \left[E\right]_{\mathcal{F}_m}^{\mathcal{F}_m}
\end{align*}
is in row echelon form. In particular, the column space of $\left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$ has as its basis the columns containing pivots:
\begin{align*}
  \underbrace{\overbrace{\left[E\circ T_A\left(e_{i_1}\right)\right]}^{w_1}_{\mathcal{F}_m},\dots,\overbrace{\left[E\circ T_A\left(e_{i_k}\right)\right]}^{w_k}_{\mathcal{F}_m}}_{\text{basis of }\col\left(\left[E\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}\right)}
\end{align*}
We have an inverse $E^{-1}: \F^{m}\rightarrow \F^m$. In particular,
\begin{align*}
  \underbrace{E^{-1}\left(w_1\right),\dots,E^{-1}\left(w_k\right)}_{=\left[T_{A}\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots,\left[T_{A}\left(e_{i_k}\right)\right]_{\mathcal{F}_m}}
\end{align*}
are linearly independent since $E^{-1}$ is an isomorphism.\newline

If there is a vector $v\in \col(A)$ that is not in the span of $\left[T_{A}\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots,\left[T_{A}\left(e_{i_k}\right)\right]_{\mathcal{F}_m}$, then $E(v)$ cannot be in the span of $w_1,\dots,w_k$.\newline

Thus, the columns $\left[T_A\left(e_{i_1}\right)\right]_{\mathcal{F}_m},\dots \left[T_A\left(e_{i_k}\right)\right]_{\mathcal{F}_m}$ give a basis for $\col(A)$.
\begin{example}
  Consider the matrix
  \begin{align*}
    A &= \begin{pmatrix}3&4&5&6\\1&2&3&4\\1&1&2&3\end{pmatrix}.
  \end{align*}
  We put $A$ into row echelon form as
  \begin{align*}
    B &= \begin{pmatrix}1 & 1 & 2 & 3 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & -2 & -4\end{pmatrix}.
  \end{align*}
  Examining the pivots, we have the column space as
  \begin{align*}
    \col(B) &= \Span_{\F}\left( \begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}1\\1\\0\end{pmatrix}, \begin{pmatrix}2\\1\\-2\end{pmatrix}\right),
  \end{align*}
  implying the basis of the column space for $A$ is
  \begin{align*}
    \col(A) &= \Span_{\F}\left( \begin{pmatrix}3\\1\\1\end{pmatrix}, \begin{pmatrix}4\\2\\1\end{pmatrix}, \begin{pmatrix}5\\3\\2\end{pmatrix}\right).
  \end{align*}
\end{example}
We have $v\in \Null(A)$ if and only if $Av = 0_{\F^m}$. Since $Av = T_A(v)$, we have $\Null(A) = \ker\left(T_A\right)$.
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}4 & -4 & 2 \\ -4 & 4 & -2 \\ 2 & -1 & 1\end{pmatrix}.
  \end{align*}
  The reduced row echelon form of $A$ is
  \begin{align*}
    B &= \begin{pmatrix}1 & 0 & 1/2 \\ 0 & 1 & 0 \\ 0 & 0 & 0\end{pmatrix}.
  \end{align*}
  Thus,
  \begin{align*}
    \col(A) &= \Span_{\F}\left( \begin{pmatrix}4\\-4\\2\end{pmatrix}, \begin{pmatrix}-4\\4\\-1\end{pmatrix}\right).
  \end{align*}
  We know that $\null(A) = \ker\left(T_A\right) \subseteq \F^{3}$-domain of $T_A$. When we put a matrix into reduced row echelon form, we do not impact the basis vectors of the domain of $T_A$, implying that $\Null(A) = \Null(B)$.\newline

  In particular, we want
  \begin{align*}
    \begin{pmatrix}1 & 0 & 1/2\\0 & 1 & 0 \\ 0 & 0 & 0\end{pmatrix} \begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} &= \begin{pmatrix}x_1 + \left(1/2\right) x_3\\x_2\\0\end{pmatrix}\\
                     &= \begin{pmatrix}0\\0\\0\end{pmatrix}.
  \end{align*}
  Therefore, we have $x_2 = 0$, $x_1 = -1/2 x_3$, meaning
  \begin{align*}
    \Null(A) &= \Span_{\F}\left( \begin{pmatrix}-1/2\\0\\1\end{pmatrix}\right).
  \end{align*}
\end{example}
\subsection{Transpose of a Matrix}%
Recall that, given a linear map $T\in \Hom_{\F}\left(V,W\right)$, there is an induced map $T'\in \Hom_{\F}\left(W',V'\right)$ on the dual space given by $T'\left(\varphi\right) = \varphi\circ T$.\newline

Let $A\in \Mat_{m,n}\left(\F\right)$, $\mathcal{E}_n = \set{e_1,\dots,e_n}$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be standard bases for $\F^n$ and $\F^m$ respectively. Let $T_A \in \Hom_\F\left(\F^n,\F^m\right)$, meaning $A = \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$.\newline

We have $\mathcal{E}_n' = \set{e_1',\dots,e_n'}$ and $\mathcal{F}_{m}' = \set{f_1',\dots,f_m'}$. The dual map $T_A'\in \Hom_{\F}\left(\F^m,\F^n\right)$, and the transpose of $A$ is defined by
\begin{align*}
  A^{T} &= \left[T_{A}'\right]_{\mathcal{F}_m'}^{\mathcal{E}_n'}.
\end{align*}
\begin{lemma}
  Let $A = \left(a_{ij}\right) \in \Mat_{m,n}\left(\F\right)$. Then,
  \begin{align*}
    A^{T} &= \left(b_{ij}\right)\in \Mat_{n,m}\left(\F\right)
  \end{align*}
  with $b_{ij} = a_{ji}$.
\end{lemma}
\begin{proof}
  Let $A\in \Mat_{m,n}\left(\F\right)$, $\mathcal{E}_n = \set{e_1,\dots,e_n}$ and $\mathcal{F}_m = \set{f_1,\dots,f_m}$ be standard bases for $\F^n$ and $\F^m$ respectively. Let $\mathcal{E}_n'$ and $\mathcal{F}_m'$ denote the dual bases.\newline

  Let $T_A \in \Hom_\F\left(\F^n,\F^m\right)$, meaning $A = \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{F}_m}$. In particular, we have
  \begin{align*}
    T_A\left(e_i\right) &= \sum_{k=1}^{m}a_{ki}f_k. \tag*{(\textasteriskcentered)}
  \end{align*}
  We have
  \begin{align*}
    A^{t} &= \left[T_{A}'\right]_{\mathcal{F}_m'}^{\mathcal{E}_n'} \tag*{(\textasteriskcentered\textasteriskcentered)}\\
          &= \left(b_{ij}\right)
  \end{align*}
  Now, we have
  \begin{align*}
    T_{A}' \left(f_j'\right) &= \sum_{j=1}^{n}b_{kj}e_{k}'.
  \end{align*}
  Apply $f_j'$ to (\textasteriskcentered). Then,
  \begin{align*}
    \left(f_j'\circ T_A\right)\left(e_i\right) &= f_j'\left(\sum_{k=1}^{m}a_{ki}f_k\right)\\
                                               &= \sum_{k=1}^{m}a_{ki}f_j'\left(f_k\right)\\
                                               &= a_{ji}.
  \end{align*}
  Apply (\textasteriskcentered\textasteriskcentered) to $e_i$. Then,
  \begin{align*}
    T_A'\left(f_j'\right)\left(e_i\right) &= \sum_{k=1}^{n}b_{kj}e_k'\left(e_i\right)\\
                                          &= b_{ij}.
  \end{align*}
  We have
  \begin{align*}
    \left(f_j'\circ T_A\right)\left(e_i\right) &= \left(T_A'\left(f_j'\right)\right)\left(e_i\right)
  \end{align*}
  by the definition of $T_A'$, meaning $b_{ij} = a_{ji}$.
\end{proof}
\begin{exercise}
  Let $A_1,A_2\in \Mat_{m,n}\left(\F\right)$, $c\in \F$. Use the definition of the transpose to show
  \begin{align*}
    \left(A_1 + A_2\right)^{T} &= A_1^T + A_2^T\\
    \left(cA_1\right)^T &= cA_1^T.
  \end{align*}
\end{exercise}
\begin{lemma}
  Let $A\in \Mat_{m,n}\left(\F\right)$, $B\in \Mat_{p,m}\left(\F\right)$. Then,
  \begin{align*}
    \left(BA\right)^T &= A^TB^T.
  \end{align*}
\end{lemma}
\begin{proof}
  Let $\mathcal{E}_m$, $\mathcal{E}_n$, and $\mathcal{E}_p$ be standard bases.\newline

  We have
  \begin{align*}
    \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_m} &= A\\
    \left[T_B\right]_{\mathcal{E}_m}^{\mathcal{E}_p} &= B.
  \end{align*}
  So,
  \begin{align*}
    BA &= \left[T_B\circ T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_p}.
  \end{align*}
  Thus,
  \begin{align*}
    \left(BA\right)^{T} &= \left[\left(T_B\circ T_A\right)'\right]_{\mathcal{E}_p'}^{\mathcal{E}_n'}\\
                        &= \left[T_A'\circ T_B'\right]_{\mathcal{E}_p'}^{\mathcal{E}_n'}\\
                        &= \left[T_A'\right]_{\mathcal{E}_m'}^{\mathcal{E}_n'}\left[T_{B}'\right]_{\mathcal{E}_p'}^{\mathcal{E}_m'}\\
                        &= A^TB^T.
  \end{align*}
\end{proof}
\begin{lemma}
  Let $A\in \text{GL}_{n}\left(\F\right)$. Then,
  \begin{align*}
    \left(A^{-1}\right)^{T} &= \left(A^{T}\right)^{-1}.
  \end{align*}
\end{lemma}
\begin{proof}
  We will show that $A^{T} \left(A^{-1}\right)^{T} = I_n = \left(A^{-1}\right)^{T} A^T$, and use the fact that inverses are unique.\newline

  We have
  \begin{align*}
    A &= \left[T_A\right]_{\mathcal{E}_n}^{\mathcal{E}_n}\\
    A^{-1} &= \left[T_A^{-1}\right]_{\mathcal{E}_n}^{\mathcal{E}_n}
  \end{align*}
  We have
  \begin{align*}
    I_n &= \left[\id_{\F^n}'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\circ T_A\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[T_A'\circ \left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\left[\left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= A^T\left(A^{-1}\right)^T.\\
    I_n &= \left[\left(T_A\circ T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\right)'\circ T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left[\left(T_A^{-1}\right)'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\left[T_A'\right]_{\mathcal{E}_n'}^{\mathcal{E}_n'}\\
        &= \left(A^{-1}\right)^{T}A^{T}.
  \end{align*}
\end{proof}
\section{Generalized Eigenvectors and Jordan Canonical Form}%
\subsection{Eigenvalues and Eigenvectors}%
Recall that we say $A \sim B$ if $A = PBP^{-1}$ for some $P\in \text{GL}_{n}\left(\F\right)$. In particular, this means that $A = \left[T\right]_{\mathcal{A}}$ and $B = \left[T\right]_{\mathcal{B}}$ for some bases $\mathcal{A}$ and $\mathcal{B}$.
\begin{definition}[Diagonalizable]
  We say $A$ is diagonalizable if $A \sim D$ for some $D$ a diagonal matrix.\newline

  If $A = \left[T\right]_{\mathcal{A}}$, $A$ is diagonalizable if there is a basis $\mathcal{B}$ if $\left[T\right]_{\mathcal{B}} = D$ for $D$ a diagonal matrix.\newline

  If $A\sim B$, $A$ is diagonalizable if and only if $B$ is diagonalizable. If $A$ and $B$ are diagonalizable, they must be similar to the same diagonal matrix up to reordering the diagonals.
\end{definition}
\begin{example}
  Let $V = \F^2$, $T \in \Hom_{\F}\left(V,V\right)$. We take $T\left(e_1\right) = 3e_1$ and $T\left(e_2\right) = -2e_2$.\newline

  In particular, we can see that
  \begin{align*}
    \left[T\right]_{\mathcal{E}_2} &= \begin{pmatrix}3 & 0 \\ 0 & -2\end{pmatrix}.
  \end{align*}
  When we look at $V = V_1\oplus V_2$, with $V_1 = \Span_{\F}\left(e_1\right)$ and $V_2 = \Span_{\F}\left(e_2\right)$.\newline

  In this case, we have $T\left(V_1\right)\subseteq V_1$ and $T\left(V_2\right) \subseteq V_2$, which allows us to write $T$ as a diagonal matrix.
\end{example}
\begin{example}
  Let $V = \F^2$, $T \in \Hom_{\F}\left(V,V\right)$. We take $T\left(e_1\right) = 3e_1$ and $T\left(e_2\right) = e_1 + 3e_2$.\newline

  In particular, we can see that
  \begin{align*}
    \left[T\right]_{\mathcal{E}_2} &= \begin{pmatrix}3 & 1 \\ 0 & 3\end{pmatrix}.
  \end{align*}
  We still have $V = V_1\oplus V_2$ with $V_1 = \Span_{\F}\left(e_1\right)$ and $V_2 = \Span_{\F}\left(e_2\right)$.\newline

  While we have $T\left(V_1\right)\subseteq V_1$, we do not have $T\left(V_2\right)\subseteq V_2$. We will find a diagonalization (or lack thereof) of $T$.\newline

  Suppose we have $W_1,W_2\neq \set{0}$ with $V = W_1 \oplus W_2$ with $T\left(W_1\right)\subseteq W_1$ and $T\left(W_2\right)\subseteq W_2$.\newline

  Write $W_i = \Span_{\F}\left(w_i\right)$. In particular, this means we can write $T\left(w_1\right) = \alpha w_1$ and $T\left(w_2\right) = \beta w_2$. For $\mathcal{B} = \set{w_1,w_2}$, we would be able to write
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\alpha & 0 \\ 0 & \beta\end{pmatrix}.
  \end{align*}
  Write $w_1 = ae_1 + be_2$ and $w_2 = ce_1 + de_2$.
  \begin{align*}
    \alpha w_1 &= T\left(w_1\right)\\
               &= aT\left(e_1\right) + bT\left(e_2\right)\\
               &= a\left(3e_1\right) + b\left(e_1 + 3e_2\right)\\
               &= \left(3a+b\right)e_1 + 3be_2
  \end{align*}
  Thus, $\alpha\left(ae_1 + be_2\right) = \left(3a+b\right)e_1 + 3be_2$, meaning $\alpha a = 3a + b$, $\alpha b = 3b$. Either $b = 0$ or $\alpha = 3$, but we still end with $\alpha = 3$. Thus, $T\left(w_1\right) = 3w_1$.\newline

  Applying to $w_2$, we have
  \begin{align*}
    \beta w_2 &= \left(3c + d\right)e_1 + \left(3d\right)e_2,
  \end{align*}
  implying $\beta c = ec + d$ and $\beta d = 3d$, meaning either $\beta = 3$ (which contradicts the first equation)or $w_2 = ce_1$, which contradicts $w_1,w_2$ being a basis.
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1& 2\\ 3&4\end{pmatrix}.
  \end{align*}
  Let $\F = \Q$. Can we find $P \in \text{GL}_2\left(\Q\right)$ such that $P^{-1}AP = \begin{pmatrix}\alpha & 0\\ 0 & \beta\end{pmatrix}$.\newline

  If we write $P = \begin{pmatrix}a & b \\c & d\end{pmatrix}$, we have
  \begin{align*}
    P^{-1}AP &= \frac{1}{ad-bc} \begin{pmatrix}ad - 3ab + 2cd - 4bc & -3bd - 3b^2 + 2d^2\\ 3ac + 3a^2 - 2c^2 & -bc + 3ab - 2cd + 4ad\end{pmatrix}.
  \end{align*}
  By the definition of diagonal matrix, we must have
  \begin{align*}
    3a^2 + 3ac - 2c^2 &= 0.
  \end{align*}
  If $c = 0$, then $a = 0$, which is a contradiction since $P$ is invertible. We have $c\neq 0$, meaning we can divide by $c^2$ and set $x = a/c$
  \begin{align*}
    3x^2 + 3x - 2 &= 0\\
    x &= \frac{-3 \pm \sqrt{33}}{6}\\
    a &= \frac{-3 \pm \sqrt{33}}{6}c.
  \end{align*}
  Since $c\neq 0$, $\frac{-3 \pm \sqrt{33}}{6}c \notin \Q$. Thus, we cannot diagonalize $A$ over $\Q$.\newline

  If we take $\F = \Q\left(\sqrt{33}\right)$, then we take
  \begin{align*}
    \mathcal{B} &= \set{v_1 = \begin{pmatrix}1\\ \frac{3 + \sqrt{33}}{4}\end{pmatrix}, v_2 = \begin{pmatrix}1\\\frac{3-\sqrt{33}}{4}\end{pmatrix}},\\
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\frac{5 + \sqrt{33}}{2} & 0 \\ 0 & \frac{5-\sqrt{33}}{2}\end{pmatrix}.
  \end{align*}
\end{example}
\begin{recall}
  The fundamental question we are investigating is whether given a $A\in \Mat_{n}\left(\F\right)$, can we choose $P\in \text{GL}_{n}\left(\F\right)$ such that $PAP^{-1}$ is diagonal.\newline

  We saw that if $\F^2 = V_1\oplus V_2$ with $A\left(V_1\right) \subseteq V_1$, $A\left(V_2\right)\subseteq V_2$, then it is possible to diagonalize $A$.
\end{recall}
\begin{definition}
  Let $V$ be an $\F$-vector space with $T\in \Hom_{\F}\left(V,V\right)$. We say a subspace $W\subseteq V$ is $T$-invariant or $T$-stable if $T\left(W\right)\subseteq W$.
\end{definition}
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$, $W\subseteq V$ a $k$-dimensional subspace.\newline

Let $\mathcal{B}_{W} = \set{v_1,\dots,v_k}$ be a basis for $W$, and extend to a basis $\mathcal{B} = \set{v_1,\dots,v_n}$ of $V$.\newline

Let $T\in \Hom_{\F}\left(V,V\right)$.\newline

Then, $W$ is $T$-stable if and only if $\left[T\right]_{\mathcal{B}}$ is block-upper triangular of the form
\begin{align*}
  \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}A & B \\ 0 & D\end{pmatrix},
\end{align*}
where $A = \left[T\vert_{W}\right]_{\mathcal{B}_W}$.
\end{theorem}
\begin{example}
  Let $V = \Q^4$, $\mathcal{E}_4 = \set{e_1,e_2,e_3,e_4}$ the standard basis. Define $T$ by
  \begin{align*}
    T\left(e_1\right) &= 2e_1 + 3e_3\\
    T\left(e_2\right) &= e_1 + e_4\\
    T\left(e_3\right) &= e_1 - e_3\\
    T\left(e_4\right) &= 2e_1 - 2e_2 + 5e_3 - 4e_4.
  \end{align*}
  Notice that if we set $W = \Span_{\Q}\left(e_1,e_3\right)$, then $W$ is $T$-stable. We set $\mathcal{B}_W = \set{e_1,e_3}$, $\mathcal{B} = \set{e_1,e_2,e_3,e_4}$.
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}2 & 1 & 1 & 2 \\ 3 & -1 & 0 & 5 \\ 0 & 0 & 0 & -2 \\ 0 & 0 & 1 & -4\end{pmatrix}
  \end{align*}
\end{example}
A special case is when $\Dim_{\F}\left(W\right) = 1$. If $W = \Span_{\F}\left(w_1\right)$, and $W$ is $T$-stable, then $T\left(w_1\right) \in W$, meaning $T\left(w_1\right) = \lambda w_1$ for some $\lambda \in \F$.\newline

We can rewrite this as $T\left(w_1\right) - \lambda\left(w_1\right) = 0_V$, meaning $\left(T - \lambda \id_V\right)\left(w_1\right) = 0_V$, meaning $w_1\in \ker\left(T - \lambda\id_V\right)$.
\begin{definition}
  Let $T\in \Hom_{\F}\left(V,V\right)$, and $\lambda \in F$. If $\ker\left(T - \lambda\id_V\right) \neq \set{0_V}$, we say $\lambda$ is an eigenvalue of $T$.\newline

  Any nonzero vector in $\ker\left(T - \lambda \id_V\right)$ is called an eigenvector.\newline

  The set $E^{1}_{\lambda} = \ker\left(T - \lambda \id_V\right)$ is called the eigenspace associated with $\lambda$.
\end{definition}
\begin{exercise}
  Show $E^{1}_{\lambda}$ is a subspace of $V$.
\end{exercise}
\begin{exercise}
  Let $T\in \Hom_{\F}\left(V,V\right)$. If $\lambda_1,\lambda_2 \in \F$ with $\lambda_1\neq \lambda_2$, then $E^{1}_{\lambda_1} \cap E^{1}_{\lambda_2} = \set{0_V}$.
\end{exercise}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix}\in \Mat_{2}\left(\Q\right),
  \end{align*}
  with $T_A \in \Hom_{\Q}\left(\Q^2,\Q^2\right)$ the associated linear map.\newline

  We have
  \begin{align*}
    \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix} \begin{pmatrix}1\\2/5\end{pmatrix} &= 2 \begin{pmatrix}1\\2/5\end{pmatrix}\\
    \begin{pmatrix}-12 & 35 \\ -6 & 17\end{pmatrix} \begin{pmatrix}1\\3/7\end{pmatrix} &= 3 \begin{pmatrix}1\\3/7\end{pmatrix}.
  \end{align*}
  Therefore, $T_A$ has eigenvalues of $2$ and $3$, with
  \begin{align*}
    E_2 &= \Span_{\Q} \left(\begin{pmatrix}1\\2/5\end{pmatrix}\right) = \Span_{\Q}\left(v_1\right)\\
    E_3 &= \Span_{\Q}\left( \begin{pmatrix}1\\3/7\end{pmatrix}\right) = \Span_{\Q}\left(v_2\right),
  \end{align*}
  meaning
  \begin{align*}
    \left[T_{A}\right]_{\set{v_1,v_2}} &= \begin{pmatrix}2 & 0 \\ 0 & 3\end{pmatrix}.
  \end{align*}
\end{example}
\begin{notation}
  Let $T\in \Hom_{\F}\left(V,V\right)$. We write $T^m = \underbrace{T\circ \cdots \circ T}_{m\text{ times}}$.\newline

  If $f(x) \in \F[x]$, $f(x) = a_mx^m + \cdots + a_1 x + a_0$, then
  \begin{align*}
    f\left(T\right) &= a_mT^m + \cdots + a_1T + a_0 \id_V\\
                    &\in \Hom_{\F}\left(V,V\right).
  \end{align*}
  If $f(x) = g(x)h(x)$, then
  \begin{align*}
    f(T) &= g(T)\circ h(T)
  \end{align*}
\end{notation}
\begin{example}
  If $g(x) = 2x^2 + 3$, then
  \begin{align*}
    g\left(T\right) &= 2T^2 + 3\id_V\\
    g\left(T\right)\left(v\right) &= 2T\left(T\left(v\right)\right) + 3v.
  \end{align*}
\end{example}
Let $\Dim_{\F}\left(V\right) = n$. Recall that $\Hom_{\F}\left(V,V\right)$ is an $\F$-vector space, meaning $\Hom_{\F}\left(V,V\right) \cong \Mat_{n}\left(\F\right)$. Thus, $\Dim_{\F}\left(\Hom_{\F}\left(V,V\right)\right) = n^2$.\newline

Given $T\in \Hom_{\F}\left(V,V\right)$, consider
\begin{align*}
  \set{\id_V,T,T^2,\dots,T^{n^2}}\subseteq \Hom_{\F}\left(V,V\right).
\end{align*}
Since this set contains $n^2 + 1$ elements, it must be linearly dependent. Let $m$ be the smallest integer such that $a_m T^{m} + \cdots + a_1 T + a_0\id_V = 0_{\Hom_{\F}\left(V,V\right)}$. Since $m$ is minimal, $a_m \neq 0$.\newline

Define $f(x) = x^m + b_{m-1}x^{m-1} + \cdots + b_1 x + b_0\in \F[x]$, where $b_i = \frac{a_i}{a_m}$.\newline

Observe that $f(T) = 0_{\Hom_{\F}\left(V,V\right)}$. In other words, $f\left(T\right)\left(v\right) = 0_V$ for all $v\in V$.
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$. There is a unique monic polynomial $m_T(x) \in \F[x]$ of lowest degree such that
  \begin{align*}
    m_T\left(T\right)\left(v\right) &= 0_V
  \end{align*}
  for every $v\in V$. Moreover, $\deg\left( m_T\left(x\right)\right)\leq n^2$
\end{theorem}
\begin{proof}[Proof of Uniqueness]
  Suppose $f(x) \in \F[x]$ satisfies $f(T)(v) = 0$ for all $v\in V$.\newline

  We write
  \begin{align*}
    f(x) &= m_T\left(x\right)q(x) + r(x),
  \end{align*}
  for some $q(x),r(x) \in \F[x]$, with $r(x) = 0$ or $\deg r(x) < \deg m_T(x)$.\newline

  Plugging in $T$, we have for all $v\in V$,
  \begin{align*}
    0_V &= f(T)(v)\\
        &= q(T)m_T(T)(v) + r(T)(v)\\
        &= q(T)\left(0_V\right) + r(T)(v)\\
        &= r(T)(v)
  \end{align*}
  Thus, $r(T) (v) = 0$ for all $v\in V$; thus, it must be the case that $r(T) = 0$.\newline

  Thus, $m_T(x)|f(x)$. However, if $m_T(x)$ and $f(x)$ are monic and of minimal degree, with $m_T(x)|f(x)$, then $m_T(x) = f(x)$.
\end{proof}
\begin{definition}
  The unique monic polynomial $m_T(x)$ is called the minimal polynomial.
\end{definition}
\begin{corollary}
  If $f(x)\in \F[x]$ satisfies $f(T)(v) = 0$ for all $v\in V$, then $m_T(x)|f(x)$.
\end{corollary}
\begin{example}
  Let $F = \Q$,
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 \\ 3 & 4\end{pmatrix}.
  \end{align*}
  We can see that for any $a_0\in \Q$,
  \begin{align*}
    A - a_0I_{2} \neq 0_{\Mat_{2}\left(\Q\right)}.
  \end{align*}
  However, for
  \begin{align*}
    A^2 &= \begin{pmatrix}7 & 10 \\ 15 & 22\end{pmatrix},
  \end{align*}
  we have
  \begin{align*}
    A^2 - 5A - 2I_{2} &= 0_{\Mat_{2}\left(\Q\right)},
  \end{align*}
  yielding $m_{A}\left(x\right) = x^2 - 5x - 2$.\newline

  The roots of $m_A(x)$ are $\frac{5\pm \sqrt{33}}{2}$.
\end{example}
\begin{example}
  Let $V = \Q^3$, $\mathcal{E}_3 = \set{e_1,e_2,e_3}$, with $T_A$ given by
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  We can find
  \begin{align*}
    A^2 &= \begin{pmatrix}1 & 4 & 8 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{pmatrix}\\
    A^3 &= \begin{pmatrix}1 & 6 & 11 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  Thus, we find
  \begin{align*}
    A^3 - A^2 - A + I &= 0,\\
    \left(x-1\right)^2 \left(x+1\right) &= m_{T_A}\left(x\right)
  \end{align*}
\end{example}
\begin{theorem}
  Let $V$ be an $\F$-vector space, and let $T\in \Hom_{\F}\left(V,V\right)$. We have $\lambda$ is an eigenvalue if and only if $\lambda$ is a root of $m_{T}\left(x\right)$.\newline

  In particular, if $\left(x-\lambda\right)\vert m_T(x)$, then $E^{1}_{\lambda}\neq \set{0_V}$.
\end{theorem}
\begin{proof}
  Let $\lambda$ be an eigenvalue with eigenvector $v$, and write $m_{T}\left(x\right) = x^m + \cdots + a_1x + a_0$. Notice that $T^{k}\left(v\right) = \lambda^{k}\left(v\right)$.\newline

  We have 
  \begin{align*}
    0_V &= m_T(T)(v)\\
        &= \left(T^{m} + a_{m-1}T^{m-1} + \cdots + a_1T + a_0\id_V\right)\left(v\right)\\
        &= T^{m}\left(v\right) + a_{m-1}T^{m-1}\left(v\right) + \cdots + a_1T\left(v\right) + a_0 v\\
        &= \lambda^{m}v + a_{m-1}\lambda^{m-1}v + \cdots + a_1\lambda v + a_0 v\\
        &= \left(\lambda^{m} + a_{m-1}\lambda^{m-1} + \cdots + a_1\lambda + a_0\right)v\\
        &= m_{T}\left(\lambda\right)v,
  \end{align*}
  meaning $m_T\left(\lambda\right)v = 0_V$. Since $m_T\left(\lambda\right)\in \F$ and $v \neq 0_V$, it is the case that $m_T\left(\lambda\right) = 0$, meaning $\lambda$ is a root of $m_T(x)$.\newline

  Suppose $m_T\left(\lambda\right) = 0$. This gives
  \begin{align*}
    m_T\left(x\right) &= \left(x-\lambda\right)f(x)
  \end{align*}
  for some $f(x)\in \F[x]$. Therefore, $\deg\left(f(x)\right) < \deg \left(m_{T}\left(x\right)\right)$. There must exist a nonzero vector $v\in V$ such that $f(T)(v) \neq 0_V$. Set $w = f(T)(v)$. Observe that $m_T(T)(v) = 0_V$, so $\left(T-\lambda\id_V\right)f(T)(v) = 0_V$, meaning $\left(T-\lambda\id_V\right)\left(w\right) = 0_V$, so $T(w) = \lambda w$. Thus, $\lambda$ is an eigenvalue.
\end{proof}
\begin{corollary}
  Let $\lambda_1,\dots,\lambda_m\in \F$ be distinct eigenvalues of $T$. For each $i$, let $v_i$ be an eigenvector with eigenvalue $\lambda_i$. Then, $\set{v_1,\dots,v_m}$ is linearly independent 
\end{corollary}
\begin{proof}
  We can write
  \begin{align*}
    m_T(x) &= \left(x-\lambda_1\right)\cdots\left(x-\lambda_m\right)f(x).
  \end{align*}
  Suppose $a_1v_1 + \cdots + a_mv_m = 0_V$ for some $a_i\in \F$.\newline

  Define $g_1(x) = \left(x-\lambda_2\right)\cdots\left(x-\lambda_m\right)f(x)$. Note that $g_1(T)(v_i) = 0_V$ for all $2\leq i \leq m$. Then,
  \begin{align*}
    0_V &= g_1(T)\left(0_V\right)\\
        &= \sum_{j=1}^{m}a_jg_1(T)\left(v_j\right)\\
        &= a_1g_1(T)\left(v_1\right)\\
        &= a_1g_1\left(\lambda_1\right)v_1.
  \end{align*}
  Since $g_1\left(\lambda_1\right)\neq 0$, and $v_1\neq 0$, it must be the case that $a_1 = 0$. Symmetry provides the case for $2,\dots,m$.
\end{proof}
\begin{corollary}
  If $\deg m_{T}\left(x\right) = \dim_{\F}\left(V\right)$, and $m_{T}\left(x\right)$ has distinct roots, all of which are in $\F$, then we can find a basis $\mathcal{B}$ for $V$ such that $\left[T\right]_{\mathcal{B}}$ is diagonal.
\end{corollary}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2\end{pmatrix}\\
    B &= \begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2\end{pmatrix}.
  \end{align*}
  These matrices are not similar. However, $m_{A}(x) = m_{B}\left(x\right) = \left(x-1\right)\left(x-2\right)$.\newline

  Therefore, the minimal polynomial does not provide enough information about a matrix's similarity class.
\end{example}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & -1\end{pmatrix}.
  \end{align*}
  We found that the minimal polynomial for $A$ was $m_A(x) = \left(x-1\right)^2 \left(x+1\right)$.\newline

  We can see that $Ae_{1} = e_1$, meaning $\Span_{\F}\left(e_1\right) = E_{1}^{1}$. Note that
  \begin{align*}
    Ae_2 &= \begin{pmatrix}2\\1\\0\end{pmatrix},
  \end{align*}
  meaning $e_2 \notin E_{1}^{1}$.\newline

  We can see that
  \begin{align*}
    \left(A-I_{3}\right)^{2} &= \begin{pmatrix}0 & 0 & 2 \\ 0 & 0 & -8 \\ 0 & 0 & 4\end{pmatrix}.
  \end{align*}
  However,
  \begin{align*}
    \left(A-I_{3}\right)^2\left(e_2\right) &= \begin{pmatrix}0\\0\\0\end{pmatrix},
  \end{align*}
  meaning $e_1,e_2\in \ker\left(\left(T_A - \id_{\F^3}\right)^2\right)$.\newline

  Though we do not have distinct eigenvectors, we \textit{kinda} have them.
\end{example}
\begin{definition}[Generalized Eigenvector]
  Let $T\in \Hom_{\F}\left(V,V\right)$. For $k\geq 1$, the $k$th generalized eigenspace of $T$ with eigenvalue $\lambda$ is
  \begin{align*}
    E_{\lambda}^{k} &= \ker\left(\left(T_A - \lambda\id_{V}\right)^{k}\right)\\
                    &= \set{v\in V \mid \left(T-\lambda \id_{V}\right)^{k}v = 0_V}.
  \end{align*}
  Elements in $E_{\lambda}^{k}$ are called generalized $\lambda$-eigenvectors.\newline

  We set
  \begin{align*}
    E_{\lambda}^{\infty} &= \bigcup_{k\geq 1}E_{\lambda}^{k}.
  \end{align*}
\end{definition}
\begin{example}
  In the previous example, we saw that $\Span_{\F}\left(e_1,e_2\right)\subseteq E_{1}^{2}$, and we have $-1$ is an eigenvalue of $A$ with eigenvector
  \begin{align*}
    v_3 &= \begin{pmatrix}1/2\\-1/2\\1\end{pmatrix}.
  \end{align*}
  We can verify that $v_3 \notin E_{1}^{2}$.\newline

  Thus, $\dim_{F}E_{1}^{2} \leq 2$, meaning $E_{1}^{2} = \Span_{\F}\left(e_1,e_2\right)$.
\end{example}
\begin{example}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$, and $T\in \Hom_{\F}\left(V,V\right)$, $\lambda \in \F$ such that
  \begin{align*}
    A = \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}\lambda & 1 & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & \lambda\end{pmatrix},
  \end{align*}
  which is a matrix of $\lambda$ along the diagonal and $1$ along the superdiagonal. In particular, we can see that $A - \lambda I_{n}$ is the matrix with $1$ along the superdiagonal and $0$ everywhere else.\newline

  Notice that $\left(A - \lambda I_{n}\right)(v_1) = 0$, $\left(A - \lambda I_{n}\right)\left(v_2\right) = v_1$, etc.\newline

  Thus, we get that $E_{\lambda}^{1} = \Span_{\F}\left(v_1\right)$, $E_{\lambda}^{2} = \Span_{\F}\left(v_1,v_2\right)$, etc.\newline

  In general, $E_{\lambda}^{k} = \Span_{\F}\left(v_1,\dots,v_k\right)$ for $1 \leq k \leq n$.\newline

  Thus, $E_{\lambda}^{\infty} = E_{\lambda}^{n} = V$.
\end{example}
\begin{exercise}
  Describe the generalized eigenspaces of
  \begin{align*}
    \begin{pmatrix}\lambda_1 & 1 & 0 & 0 \\ 0 & \lambda_1 & 0 & 0 \\ 0 & 0 & \lambda_2 & 0 \\ 0 & 0 & 0 & \lambda_3\end{pmatrix}
  \end{align*}
\end{exercise}
We can see that we used $E_{\lambda}^{i} \subseteq E_{\lambda}^{i+1}$; this is true more generally.\newline

More generally, let $T\in \Hom_{\F}\left(V,V\right)$. We claim that if $i\geq j$, then $\ker\left(T^j\right)\subseteq \ker\left(T^i\right)$.\newline

Write $i = j + k$. Let $v\in \ker\left(T^j\right)$. Then, 
\begin{align*}
  T^i\left(v\right) &= T^{j+k}\left(v\right)\\
                    &= T^{k}\left(T^j\left(v\right)\right)\\
                    &= T^{k}\left(0_V\right)\\
                    &= 0_V.
\end{align*}
This gives $E_{\lambda}^{1}\subseteq E_{\lambda}^{2}\subseteq \cdots\subseteq E_{\lambda}^{\infty}$.
\begin{lemma}
  Let $V$ be a finite dimensional vector space with $\Dim_{\F}\left(V\right) = n$, and $T\in \Hom_{\F}\left(V,V\right)$. Then, there exists $m$ with $1 \leq m \leq n$ such that
  \begin{align*}
    \ker\left(T^{m}\right) = \ker\left(T^{m+1}\right).
  \end{align*}
  Moreover, for such an $m$, $\ker\left(T^{m}\right) = \ker\left(T^{m+j}\right)$ for all $j\geq 0$.
\end{lemma}
\begin{proof}
  We have
  \begin{align*}
    \ker\left(T^{1}\right)\subseteq \ker\left(T^{2}\right)\subseteq \cdots \subseteq \ker\left(T^{\infty}\right).
  \end{align*}
  If these containments are strict, then the dimension goes up indefinitely, contradicting $\Dim_{\F}\left(V\right) = n$.\newline

  Thus, we have $1 \leq m \leq n$ with
  \begin{align*}
    \ker\left(T^{m}\right) = \ker\left(T^{m+1}\right).
  \end{align*}
  Let $m$ be the smallest value such that $\ker\left(T^{m}\right) = \ker\left(T^{m+1}\right)$.\newline

  We use induction on $j$. The base case of $j = 1$ is what defines $m$. Assume $\ker\left(T^{m}\right) = \ker\left(T^{m+j}\right)$ for all $1 \leq j \leq N$.\newline

  Let $v\in \ker\left(T^{m+N+1}\right)$. This gives 
  \begin{align*}
    0_V &= T^{m+N+1}\left(v\right)\\
        &= T^{m+1}\left(T^{N}\left(v\right)\right),
  \end{align*}
  meaning $T^N(v)\in \ker\left(T^{m+1}\right)$. However, $\ker\left(T^{m+1}\right) = \ker\left(T^{m}\right)$, meaning $T^{N}\left(v\right) \in \ker\left(T^{m}\right)$, hence
  \begin{align*}
    0_V &= T^{m}\left(T^{N}(v)\right)\\
        &= T^{m+N}\left(v\right),
  \end{align*}
  meaning $v\in \ker\left(T^{m+N}\right)$. The inductive hypothesis gives $\ker\left(T^{m+N}\right) = \ker\left(T^{m}\right)$, meaning $v\in \ker\left(T^{m}\right)$. Thus, $\ker\left(T^{m+N+1}\right)\subseteq \ker\left(T^{m+N}\right)$, meaning $\ker\left(T^{m+N+1}\right) = \ker\left(T^{m+N}\right)$.
\end{proof}
\begin{corollary}
  If $\Dim_{\F}\left(V\right) = n$, and $T\in \Hom_{\F}\left(V,V\right)$, there exists $m$ with $1 \leq m \leq n$ such that for any $\lambda \in \F$,
  \begin{align*}
    E_{\lambda}^{\infty} &= E_{\lambda}^{m}.
  \end{align*}
\end{corollary}
\begin{theorem}
  Let $T\in \Hom_{\F}\left(V,V\right)$, $\lambda \in \F$, with $\left(x-\lambda\right)^{j}\mid m_{T}\left(x\right)$. We have
  \begin{align*}
    \Dim_{\F}\left(E_{\lambda}^{j}\right) \geq j.
  \end{align*}
\end{theorem}
\begin{proof}
  Write $m_T(x) = \left(x-\lambda\right)^{k}f(x)$, $f(x)\in \F[x]$, $f(x)\neq 0$.\newline

  Define $g_j\left(x\right) = \left(x-\lambda\right)^{j}$. We have $g_{k-1}f(x)$ is not the minimal polynomial, meaning there is $v\in V$ such that
  \begin{align*}
    g_{k-1}\left(T\right)f(T)(v) \neq 0_V.
  \end{align*}
  Set $v_k = f(T) v$. Note that $v_k \neq 0_V$.\newline

  Observe that 
  \begin{align*}
    \left(T - \lambda \id_{V}\right)^k\left(v_k\right) &= \left(T-\lambda \id_V\right)^{k}f(T) \left(v_k\right)\\
                                                       &= m_{T}\left(T\right)\left(v_k\right)\\
                                                       &= 0_V.
  \end{align*}
  Thus, $v\in E_{\lambda}^{k}$.\newline

  Moreover, by construction, 
  \begin{align*}
    \left(T-\lambda\id_{V}\right)^{k-1}\left(v_k\right) &= g_{k-1}(T)\left(v_k\right)\\
                                                        &= g_{k-1}(T)f(T)(v)\\
                                                        &\neq 0_{V}.
  \end{align*}
  Thus, $v_k\notin E_{\lambda}^{k-1}$.\newline

  Define
  \begin{align*}
    v_{k-1} &= \left(T - \lambda \id_V\right)\left(v_k\right)\\
            &= \left(T - \lambda \id_V\right)f(T)(v).
  \end{align*}
  Note that
  \begin{align*}
    \left(T - \lambda \id_V\right)^{k-1}\left(v_{k-1}\right) &= \left(T - \lambda\id_V\right)\left(v_k\right)\\
                                                             &= m_T\left(T\right)(v)\\
                                                             &= 0_V,
  \end{align*}
  meaning $v_{k-1}\in E_{\lambda}^{k-1}$.\newline

  Additionally,
  \begin{align*}
    \left(T - \lambda \id_{V}\right)^{k-1}\left(v_{k-1}\right) &= \left(T -\lambda \id_V\right)^{k-2}\left(v_{k}\right)\\
                                                               &\neq 0_{V},
  \end{align*}
  meaning $v_{k-1}\in E_{\lambda}^{k-1}\setminus E_{\lambda}^{k-2}$.\newline

  Continuing the process, we construct $\set{v_1,\dots,v_k}$ linearly independent.
\end{proof}
\begin{example}
  Let $T_A\in \Hom_{\F}\left(\F^3,\F^3\right)$ given by
  \begin{align*}
    A &= \begin{pmatrix}2 & 1 & 3 \\ 0 & 2 & 4 \\ 0 & 0 & 2\end{pmatrix}.
  \end{align*}
  We can verify that $m_{T}\left(x\right) = \left(x-2\right)^3$.\newline

  Observe that
  \begin{align*}
    \left(A - 2 I_{3}\right)^2 &= \begin{pmatrix}0 & 0 & 4 \\ 0 & 0 & 0 \\ 0 & 0 & 0\end{pmatrix}.
  \end{align*}
  Notice that $\left(A - 2I_3\right)^{3}\left(e_3\right) = 4e_3\neq 0$, meaning we set $v_3 = e_3$.\newline

  Note that $\left(T - 2\id_V\right)^{3}\left(e_3\right) = 0$, meaning $e_3\in E_{2}^{3}$.\newline

  We find $v_2 = \left(A - 2I_3\right)\left(v_3\right)$, meaning
  \begin{align*}
    v_2 &= \begin{pmatrix}0 & 1 & 3 \\ 0 & 0 & 4 \\ 0 & 0 & 0 \end{pmatrix} \begin{pmatrix}0 \\ 0 \\ 1\end{pmatrix}\\
        &= \begin{pmatrix}3\\4\\0\end{pmatrix}.
  \end{align*}
  Finally,
  \begin{align*}
    v_1 &= \left(A - 2I_{3}\right)\left(v_2\right)\\
        &= \begin{pmatrix}4\\0\\0\end{pmatrix}.
  \end{align*}
  Thus, our generalized eigenvectors are
  \begin{align*}
    E_{2}^{3} &= \Span\left( \begin{pmatrix}0\\0\\1\end{pmatrix}, \begin{pmatrix}3\\4\\0\end{pmatrix}, \begin{pmatrix}4\\0\\0\end{pmatrix}\right).
  \end{align*}
  If we say $\mathcal{B} = \set{v_1,v_2,v_3}$, then our matrix $\left[T_{A}\right]_{\mathcal{B}}$ is
  \begin{align*}
    \left[T_{A}\right]_{\mathcal{B}} &= \begin{pmatrix}2 & 1 & 0 \\ 0 & 2 & 1\\ 0 & 0 & 2\end{pmatrix}.
  \end{align*}
  \begin{remark}
    This matrix is in what is known as Jordan canonical form.
  \end{remark}
\end{example}
\subsection{Characteristic Polynomials and the Cayley--Hamilton Theorem}%
\begin{definition}
  Let $A \in \Mat_{n}\left(\F\right)$. The characteristic polynomial is $c_{A}\left(x\right) = \det\left(xI_n - A\right)$.
\end{definition}
\begin{remark}
  The Cayley--Hamilton theorem states that
  \begin{align*}
    c_A(A) &=0_{n}.
  \end{align*}
\end{remark}
\begin{definition}
  Let $f(x) = x^n + a_{n-1}x^{n-1} +  \cdots + a_1 x + a_0\in \F[x]$. The companion matrix of $f(x)$ is given by $C\left(f(x)\right)$, which consists of $-a_{n-1}$ through $-a_0$ along the first column, $0$ on the rest of the diagonal, and $1$ along the superdiagonal.
\end{definition}
\begin{lemma}
  If $A = C\left(f(x)\right)$, then $c_{A}\left(x\right) = f(x)$.
\end{lemma}
\begin{lemma}
  Let $A,B\in \Mat_{n}\left(\F\right)$ be similar matrices. Then, $c_{A}\left(x\right) = c_{B}\left(x\right)$.
\end{lemma}
\begin{proof}
  Let $A = PBP^{-1}$ for some $P\in \text{GL}_{n}\left(\F\right)$. Then, we have
  \begin{align*}
    c_{A}\left(x\right) &= \det\left(xI_{n} - A\right)\\
                        &= \det\left(xI_{n}- PBP^{-1}\right)\\
                        &= \det\left(P\left(xI_n\right)P^{-1}-PBP^{-1}\right)\\
                        &= \det\left(P\left(xI_n - B\right)P^{-1}\right)\\
                        &= \det\left(P\right)\det\left(xI_n-B\right)\det\left(P^{-1}\right)\\
                        &= \det\left(xI_n - B\right)\\
                        &= c_B\left(x\right).
  \end{align*}
\end{proof}
\begin{definition}[Characteristic Polynomial of Linear Transformation]
  For $T\in \Hom_{\F}\left(V,V\right)$, let $\mathcal{B}$ be a basis of $V$ and set
  \begin{align*}
    c_T(x) &= c_{\left[T\right]_{\mathcal{B}}}(x).
  \end{align*}
\end{definition}
\begin{theorem}
  Let $v\in V$, $v\neq 0$. Let $\Dim_{\F}\left(V\right) < \infty$. Then, there is a unique monic polynomial $m_{T,v}(x)\in \F[x]$ of minimal degree such that $m_{T,v}\left(T\right)\left(v\right) = 0_V$.\newline

  Moreover, if $f(x)\in \F[x]$ with $f(T)(v) = 0$, then $m_{T,v}(x)|f(x)$.
\end{theorem}
\begin{proof}
  Consider the set $\set{v,T(v),\dots,T^n(v)}$. This collection consists of $n+1$ elements of $V$, meaning it is linearly dependent. Let
  \begin{align*}
    a_mT^{m}\left(v\right) + \cdots + a_1T(v) + a_0 = 0_V
  \end{align*}
  for some $m \leq n$ of minimal degree with not all $a_i = 0$. Set
  \begin{align*}
    p(x) &= x^m + \frac{a_{m-1}}{a_m}x^{m-1} + \cdots + \frac{a_1}{a_m}x + \frac{a_0}{a_m}.
  \end{align*}
  Thus, $p(t)(v) = 0_V$ by construction.\newline

  Set
  \begin{align*}
    I_v &= \set{g(x)\in \F[x]\mid g(T)(v) = 0_V}.
  \end{align*}
  We know $p(x)\in I_v$, and $p(x)\neq 0$. We have $p(x)$ is a nonzero monic polynomial in $I_v$ of minimal degree.\newline

  Set $m_{T,v}(x) = p(x)$.\newline

  Let $f(x)\in I_v$. We want to show that $m_{T,v}(x)|f(x)$.\newline

  Write $f(x) = q(x)m_{T,v}(x) + r(x)$ for some $q(x),r(x)\in \F[x]$, with $r(x) = 0$ or $\deg \left(r(x)\right) < \deg m_{T,v}(x)$. We have $r(x) = f(x) - q(x)m_{T,v}(x)$, implying
  \begin{align*}
    r(T)(v) &= f(T)(v) - q(T)\left(m_{T,v}\left(T\right)(v)\right)\\
            &= 0_v- q(T)\left(0_v\right)\\
            &= 0_v,
  \end{align*}
  implying $r(x)\in I_v$. Since $m_{T,v}(x)$ was defined to have minimal degree, it has to be the case that $r(x) = 0$.\newline

  If $h(x) \in I_{v}$ with $\deg \left(h(x)\right) = \deg\left(m_{T,v}(x)\right)$ with $h(x)$ monic, then $m_{T,v}(x)|h(x)$ implies $h(x) = m_{T,v}(x)$.
\end{proof}
We will refer to $m_{T,v}(x)$ as the $T$-annihilator of $v$.
\begin{example}
  Let $V = \F^n$, $\mathcal{B} = \set{e_1,\dots,e_n}$. Define $T\in \Hom_{\F}\left(V,V\right)$ by
  \begin{align*}
    T\left(e_1\right) &= 0\\
    T\left(e_{j}\right) &= e_{j-1} \tag*{$2\leq j\leq n$}.
  \end{align*}
  Let $f(x) = x$. Then, $f(T)\left(e_1\right) = T\left(e_1\right) = 0_V$, implying that $m_{T,e_1}\left(x\right) | x$; thus, $m_{T,e_1}(x) = 1$ or $m_{T,e_1}(x) = x$, but $\id\left(e_1\right) = e_1 \neq 0_V$, meaning $m_{T,e_1}\left(x\right) = x$.\newline

  Let $g(x) = x^2$. Then,
  \begin{align*}
    g(T)\left(e_2\right) &= T^{2}\left(e_2\right)\\
                         &= T\left(T\left(e_1\right)\right)\\
                         &= T\left(0_V\right)\\
                         &= 0_V.
  \end{align*}
  This gives $m_{T,e_2}(x) | x^2$, so $m_{T,e_2}(x) = 1,x,x^2$. If $m_{T,e_2}(x) = 1$, then $\id_V\left(e_2\right) = e_2 = 0_V$, which is not the case. Similarly, if $m_{T,e_2}(x) = x$, then $T(e_2) = e_1 = 0_V$, so $m_{T,e_2}(x) = x^2$.\newline

  For each $1 \leq j\leq n$, $m_{T,e_j}(x) = x^j$.
\end{example}
\begin{example}
  Let $V = \Q^2$, $T\in \Hom_{\Q}\left(\Q^2,\Q^2\right)$, with
  \begin{align*}
    T\left(e_1\right) &= e_1 + 3e_2\\
    T\left(e_2\right) &= 2e_1 + 4e_2.
  \end{align*}
  We wish to find the annihilating polynomial for $e_1$.\newline

  We know that $m_{T,e_1}(x)$ has degree $1$ or $2$. Additionally, $m_{T,e_1}(x)$ cannot have degree $1$, as if $m_{T,e_1}\left(x\right) = x+a$, then
  \begin{align*}
    m_{T,e_1}\left(T\right)\left(e_1\right) &= T\left(e_1\right) + ae_1\\
                                            &= e_1 + 3e_2 + ae_1\\
                                            &\neq 0.
  \end{align*}
  Thus, $m_{T,e_1}$ is of degree $2$.
  \begin{align*}
    T^2\left(e_1\right) &= T\left(e_1 + 3e_2\right)\\
                        &= T\left(e_1\right) + 3T\left(e_2\right)\\
                        &= e_1 + 3e_2 + 3\left(2e_1 + 4e_2\right)\\
                        &= 7e_1 + 15 e_2.
  \end{align*}
  We want to find $b,c\in \Q$ such that
  \begin{align*}
    T^2\left(e_1\right) + bT\left(e_1\right) + ce_1 &= 0_V.
  \end{align*}
  Solving the resulting system of linear equation yields $b = -5$ and $c = -2$. The annihilating polynomial is, thus,
  \begin{align*}
    m_{T,e_1}(x) &= x^2 - 5x - 2.
  \end{align*}
\end{example}
\begin{exercise}\hfill
  \begin{enumerate}[(1)]
    \item Show that $m_{T,e_2}(x) = x^2 - 5x - 2$.
    \item Calculate $m_{T,e_1}(x)$ and $m_{T,e_2}(x)$ for $\F = \F_3 = \Z/3\Z$.
  \end{enumerate}
\end{exercise}
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$, and $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis of $V$. Let $T\in \Hom_{\F}\left(V,V\right)$. We have
  \begin{align*}
    m_{T}\left(x\right) &= \lcm_{1 \leq i \leq n}m_{T,v_i}\left(x\right).
  \end{align*}
\end{theorem}
\begin{proof}
  Let $f(x) = \lcm_{1\leq i\leq n} m_{T,v_i}\left(x\right)$. Then,
  \begin{align*}
    m_{T}\left(T\right)\left(v_i\right) &= 0
  \end{align*}
  meaning $m_{T,v_i}|m_{T}(x)$ for each $i$, so $f(x) | m_{T}(x)$.\newline

  Let $v\in V$; write $v = \sum_{i=1}^{n}a_iv_i$. Then,
  \begin{align*}
    f(T)(v) &= f(T)\left(\sum_{i=1}^{n}a_iv_i\right)\\
            &= \sum_{i=1}^{n}a_if(T)\left(v_i\right)\\
            &= 0,
  \end{align*}
  since $m_{T,v_i}(x)|f(x)$ for all $i$. Thus, $m_{T}(x) | f(x)$.
\end{proof}
\begin{lemma}
  Let $T \in \Hom_{\F}\left(V,V\right)$. Let $v_1,\dots,v_k\in V$, and set $p_i(x) = m_{T,v_i}(x)$. Suppose $p_i(x)$ are pairwise relatively prime. Set
  \begin{align*}
    v &= v_1 + \cdots v_k.
  \end{align*}
  Then,
  \begin{align*}
    m_{T,v}\left(x\right) &= \prod_{j=1}^{k}p_j(x).
  \end{align*}
\end{lemma}
\begin{proof}
  We will prove this for $k = 2$.\newline

  Since $p_1(x)$ and $p_2(x)$ are relatively prime, we can write
  \begin{align*}
    1 &= p_1(x)q_1(x) + p_2(x)q_2(x).
  \end{align*}
  Particularly,
  \begin{align*}
    \id_V &= p_1(T)q_1(T) + p_2(T)q_2(T).
  \end{align*}
  Set $v = v_1 + v_2$. Then,
  \begin{align*}
    v &= \id_V(v)\\
      &= \left(p_1(T)q_1(T) + p_2(T)q_2(T)\right)(v)\\
      &= p_1(T)q_2(T)(v) + p_2(T)q_2(T)(v)\\
      &= p_1(T)q_2(T)\left(v_1 + v_2\right) + p_2(T)q_2(T)\left(v_1 + v_2\right)\\
      &= \underbrace{p_1(T)q_2(T)\left(v_2\right)}_{w_2} + \underbrace{p_2(T)q_2(T)\left(v_2\right)}_{w_1}
      \intertext{meaning}
    v &= w_1 + w_2.
  \end{align*}
Note that
\begin{align*}
  p_1(T)(w_1) &= p_1(T)p_2(T)q_2(T)\left(v_1\right)\\
              &= q_2(T)p_2(T)p_1(T)\left(v_1\right)\\
              &= 0_V,
\end{align*}
meaning $w_1\in \ker \left(p_1(T)\right)$, and similarly, $w_2\in \ker \left(p_2(T)\right)$.\newline

Let $r(x)\in \F[x]$ with $r(T)(v) = 0$. We have $v = w_1 + w_2$ and $w_2\in \ker\left( p_2(T)\right)$, meaning
\begin{align*}
  p_2(T)(v) &= p_2(T)\left(w_1 + w_2\right)\\
            &= p_2(T)\left(w_1\right).
\end{align*}
Thus,
\begin{align*}
  0_V &= p_2(T)q_2(T)\left(0_V\right)\\
      &= p_2(T)q_2(T)r(T)(v)\\
      &= r(T)q_2(T)p_2(T)(v)\\
      &= r(T)q_2(T)p_2(T)\left(w_1\right).
\end{align*}
Similarly, $r(T)q_1(T)p_1(T)\left(w_1\right) = 0_V$ since $w_1\in \ker\left(p_1(T)\right)$. Hence,
\begin{align*}
  0_V &= r(T)p_2(T)q_2(T)\left(w_1\right) + r(t)p_1(T)q_1(T)\left(w_1\right)\\
      &= r(T)\underbrace{\left(p_2(T)q_2(T) + p_1(T)q_1(T)\right)}_{\id_V}\left(w_1\right)\\
      &= r(T)\left(w_1\right).
\end{align*}
This gives
\begin{align*}
  0_V &= r(T)\left(w_1\right)\\
      &= r(T)p_2(T)q_2(T)\left(v_1\right).
\end{align*}
Thus, $p_1(x)|r(x)p_2(x)q_2(x)$. Additionally,
\begin{align*}
  1 &= p_1(x)q_1(x) + p_2(x)q_2(x)\\
  \gcd\left(p_1(x),p_2(x)q_2(x)\right) &= 1,
\end{align*}
implying $p_1(x)|r(x)$, and similarly for $p_2(x) | r(x)$.\newline

Since $\gcd\left(p_1(x),p_2(x)\right) = 1$, we have
\begin{align*}
  \lcm\left(p_1(x),p_2(x)\right) &= p_1(x)p_2(x),
\end{align*}
so $p_1(x)p_2(x) | r(x)$. If we take $r(x) = m_{T,v}(x)$, implying $p_1(x)p_2(x)|m_{T,v}(x)$. Thus, $m_{T,v}(x) = p_1(x)p_2(x)$.
\end{proof}
\begin{exercise}
  Prove for $k > 2$.
\end{exercise}
\begin{theorem}
  Let $T\in \Hom_{\F}\left(V,V\right)$. There exists $v\in V$ such that $m_{T,v}\left(x\right) = m_{T}(x)$. In particular, $\deg m_{T}(x) \leq n$.
\end{theorem}
\begin{proof}
  Let $\mathcal{B} = \set{v_1,\dots,v_n}$ be a basis for $V$.\newline

  We know that
  \begin{align*}
    m_{T}\left(v\right) &= \lcm_{1 \leq i \leq n} m_{T,v_i}(x).
  \end{align*}
  Factor
  \begin{align*}
    m_T(x) &= p_1(x)^{e_1} \cdots p_{k}(x)e^{k},
  \end{align*}
  with $p_i$ relatively prime, $e_i \geq 1$.\newline

  For $1 \leq j \leq k$, there exists $i_j\in \set{1,\dots,n}$ and $q_{i_j}(x)\in \F[x]$ with
  \begin{align*}
    m_{T,v_{i_j}}(x) &= p_j(x)^{e_j}q_{i_j}(x).
  \end{align*}
  Define $w_j = q_{i_j}(T)\left(v_{i_j}\right)$. This gives
  \begin{align*}
    M_{T,w_j} &= p_j(x)^{e_j}.
  \end{align*}
  Set $w = w_1 + \cdots + w_k$. The previous result gives
  \begin{align*}
    m_{T,w}(x) &= \prod_{j=1}^{k}p_j(x)^{e_j}\\
               &= m_T(x).
  \end{align*}
\end{proof}
\begin{recall}
  We defined $m_{T,v}(x)$, and that $m_T(x)$ is $m_{T,v}(x)$ for some $v\in V$, meaning $\deg\left(m_T(x)\right) < n$.
\end{recall}
\begin{lemma}
  Let $W$ be a $T$-invariant subspace. We get a map $\overline{T}\in\Hom_{\F}\left(V/W,V/W\right)$ defined by
  \begin{align*}
    \overline{T}\left(v+W\right) = T(v) + W.
  \end{align*}
  Let $v\in V$. Then,
  \begin{align*}
    m_{\overline{T},v+W}(x)|m_{T,v}(x)
  \end{align*}
  and similarly,
  \begin{align*}
    m_{\overline{T}}(x) | m_T(x).
  \end{align*}
\end{lemma}
\begin{proof}
  We have
  \begin{align*}
    m_{T,v}\left(\overline{T}\right)\left(v+W\right) &= m_{T,v}\left(T\right)(v) + W\\
                                                     &= 0_V + W\\
                                                     &= 0_{V/W}.
  \end{align*}
  Thus, $m_{\overline{T},v+W}|m_{T,v}(x)$.
\end{proof}
\begin{definition}
  Let $T\in \Hom_{\F}\left(V,V\right)$, $\mathcal{A} = \set{v_1,\dots,v_k}$ a set of vectors. The $T$-span of $\mathcal{A}$ is
  \begin{align*}
    W &= \set{\sum_{i=1}^{k}p_i(T)\left(v_i\right)\mid p_i(x)\in \F[x]}.
  \end{align*}
\end{definition}
\begin{exercise}
  Show that $W$ is a $T$-invariant subspace of $V$. Moreover, show it is the smallest with respect to inclusion $T$-invariant subspace of $V$ that contains $\mathcal{A}$.
\end{exercise}
\begin{example}
  Let $V = \Q^4$. Take $T\in \Hom_{\F}\left(V,V\right)$ by
  \begin{align*}
    T\left(e_1\right) &= 2e_1 + 3e_3\\
    T\left(e_2\right) &= e_1 + e_4\\
    T\left(e_3\right) &= e_1 - e_3\\
    T\left(e_4\right) &= 2e_1 + 2e_2 + 5e_3 - 4e_4.
  \end{align*}
  Let $\mathcal{A} = \set{e_1}$. We want the $T$-span of $\mathcal{A}$. Set $p(x) = 1$, meaning $p(T)(e_1) = \id(e_1) = e_1$.\newline

  Set $q(x) = \frac{1}{3}\left(x-2\right)$. If we take $q(T)\left(e_1\right)$, we have
  \begin{align*}
    q(T)\left(e_1\right) &= \frac{1}{3}\left(T - 2\id_v\right)\left(e_1\right)\\
                         &= \frac{1}{3}\left(T\left(e_1\right) - 2e_1\right)\\
                         &= e_3.
  \end{align*}
  Thus, $\Span_{\F}\left(e_1,e_3\right)\subseteq T$-span of $\mathcal{A}$.\newline

  However, we also know that $\Span_{\F}\left(e_1,e_3\right)$ is $T$-invariant and contains $\mathcal{A}$.\newline

  Thus, the $T$-span of $\mathcal{A}$ is $\Span_{\F}\left(e_1,e_3\right)$.\newline

  If we set $f(x) = x^2 - 5x - 1$, then $f(T)\left(e_1\right) = 0_V$, meaning $m_{T,e_1}(x)|f(x)$. However, $f$ is irreducible over $\Q$, so $m_{T,e_1}(x) = x^2 - 5x - 1$. Note that $\deg\left(m_{T,e_1}(x)\right) = \Dim_{\F}\left(T\text{-span }\set{e_1}\right)$.
\end{example}
\begin{lemma}
  Let $T\in \Hom_{\F}\left(V,V\right)$, $w\in V$, and $W$ the subspace of $V$ that is generated by $T$ on $\set{w}$.\newline

  Then, $\Dim_{\F}\left(W\right) = \deg\left(m_{T,w}(x)\right)$.
\end{lemma}
\begin{proof}
  Let $\deg\left(m_{T,w}(x)\right) = k$. Consider the set $\set{w,T(w),\dots,T^{k-1}(w)}$. This has to be a basis for the $T$-span of $\set{w}$.
\end{proof}
\begin{theorem}
  Let $\Dim_{\F}\left(V\right) = n$.
  \begin{enumerate}[(1)]
    \item We have $m_{T}(x)|c_{T}(x)$.
    \item Every irreducible factor of $c_T(x)$ is a factor of $m_{T}(x)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Let $\deg\left(m_{T}(x)\right) = k \leq n$. Let $v\in V$ with $m_{T}(x) = m_{T,v}(x)$.\newline

  Let $W_1$ be the $T$-span of $\set{v}$, with $\Dim_{\F}\left(W_1\right) = K$\newline

  Set $v_k = v$, $v_{k-i} = T^{i}\left(v\right)$. We have
  \begin{align*}
    \mathcal{B} = \set{v_1,dots,v_k}
  \end{align*}
  is a basis of $W_1$, and
  \begin{align*}
    \left[T\bigr\vert_{W_1}\right]_{\mathcal{B}_1} &= c\left(m_{T}(x)\right).
  \end{align*}
  If $k = n$, then $W_1 = V$, so $\left[T\right]_{\mathcal{B}_1} = c\left(m_T(x)\right)$ which has characteristic polynomial $m_T(x)$,  meaning $m_T(x) = c_T(x)$.\newline

  Suppose $k < n$. Expand $\mathcal{B}_1$ to a full basis of $V$, $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$, with $\mathcal{B}_2 = \set{v_{k+1},\dots,v_{n}}$. In the upper triangular matrix
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}A & B \\ 0 & D\end{pmatrix},
  \end{align*}
  we have $A = c\left(m_{T}(x)\right)$, so
  \begin{align*}
    c_{T}(x) &= \det\left(xI_n - \left[T\right]_{\mathcal{B}}\right)\\
             &= \det\begin{pmatrix}xI_k - A & B \\ 0 & xI_{n-k}-D\end{pmatrix}\\
             &= \det\left(xI_{k} - A\right)\det\left(xI_{n-k}-D\right)\\
             &= c_{A}(x)\det\left(xI_{n-k}-D\right)\\
             &= m_{T}(x)\det\left(xI_{n-k}-D\right),
  \end{align*}
  meaning $m_{T}(x)|c_T(x)$.\newline

  To prove (2), we induct on $\Dim_{\F}\left(V\right) = n$. If $n=1$, then both characteristic polynomials are monic of degree $1$, so they are equal.\newline

  If $\deg\left(m_T(x)\right) = n$, then $ m_T(x)|c_T(x)$, and both have degree $n$ and are monic, so $c_T(x) = m_T(x)$.\newline

  Suppose $\deg\left(m_T(x)\right) = k < n$. Pick $v$ such that $m_T(x) = m_{T,v}(x)$. Define $W_1$ to be the $T$-span of $\set{v}$, with $\mathcal{B}_1 = \set{v_1,\dots,v_k}$ defined as above. Extend $\mathcal{B}_1$ to $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$ as above.\newline

  Consider $\overline{T}: V/W_1 \rightarrow V/W_1$, and $\overline{\mathcal{B}} = \set{v_{k_1}+W_1,\dots,v_{n}+W_1}=  \pi_{W_1}\left(\mathcal{B}\right)$.\newline

  In the upper triangular matrix
  \begin{align*}
    \left[T\right]_{\mathcal{B}} &= \begin{pmatrix}A & B \\ 0 & D\end{pmatrix},
  \end{align*}
  the matrix $\left[\overline{T}\right]_{\overline{\mathcal{B}}} = D$.\newline

  Since $\Dim_{\F}\left(V/W_1\right) < \Dim_{\F}\left(V\right)$, the induction hypothesis holds that $m_{\overline{T}}(x)$ and $c_{\overline{T}}(x)$ have the same irreducible factors.\newline

  Earlier, we had
  \begin{align*}
    c_{T}(x) &= m_{T}\det\left(xI_{n-k}-D\right),
  \end{align*}
  yielding
  \begin{align*}
    c_{T}(x) &= m_{T}(x)c_{\overline{T}}(x).
  \end{align*}
  Let $p(x)$ be an irreducible factor of $c_{T}(x)$. If $p(x)|m_{T}(x)$, we are done. Else, $p(x)|c_{\overline{T}}(x)$. However, $c_{\overline{T}}(x)$ and $m_{\overline{T}}(x)$ have the same irreducible factors, so $p|m_{\overline{T}}(x)$. However, $m_{\overline{T}}(x)|m_{T}(x)$, so $p(x)|m_{T}(x)$.
\end{proof}
\begin{example}
  Let
  \begin{align*}
    A &= \begin{pmatrix}1 & 2 & 0 & 0 & 0 & 0 \\ 3 & 4 & 0 & 0 & 0 & 0 \\ 0 & 0 & 3 & 7 & 0 & 0 \\ 0 & 0 & -1 & 2 & 0 & 0 \\ 0 & 0 & 0 & 0 & -5 & 6 \\ 0 & 0 & 0 & 0 & 2 & -3\end{pmatrix}\in \Mat_{6}\left(\Q\right).
  \end{align*}
  We can verify that
  \begin{align*}
    c_A(x) &- \left(x^{2}-5x-2\right)\left(x^2 - x - 1\right)\left(x^2 + 8x + 3\right),
  \end{align*}
  implying that
  \begin{align*}
    m_{A}\left(x\right) &= \left(x^{2}-5x-2\right)\left(x^2 - x - 1\right)\left(x^2 + 8x + 3\right).
  \end{align*}
\end{example}
\begin{theorem}[Cayley--Hamilton]\hfill
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,V\right)$, $\dim_{\F}\left(V\right) < \infty$. Then,
      \begin{align*}
        c_T(T) &= 0_{\Hom_{\F}\left(V,V\right)}
      \end{align*}
    \item Let $A\in \Mat_{n}\left(\F\right)$. Then,
      \begin{align*}
        c_A(A) &= 0_{n}.
      \end{align*}
  \end{enumerate}
\end{theorem}
\begin{proof}
  Write $c_T(x) = f(x)m_T(x)$. Then, for any $v\in V$, we have
  \begin{align*}
    c_T(T)(v) &= f(T)m_T(T)(v)\\
              &= f(T)\left(0_V\right)\\
              &= 0_V.
  \end{align*}
\end{proof}
\subsection{Jordan Canonical Form}%
For the purposes of this section, $V$ is always finite dimensional, and all polynomials split into linear factors over their respective fields.
\begin{definition}
  Let $T\in \Hom_{\F}\left(V,V\right)$. A Jordan basis for $V$ with regard to $T$ is a basis $\mathcal{B}$ such that $\left[T\right]_{\mathcal{B}}$ has some $\lambda \in \F$ along the diagonal and $1$ along the superdiagonal.\newline

  More generally, if $V = V_1\oplus \cdots \oplus V_k$ is a decomposition into $T$-invariant subspaces, then each $V_i$ has Jordan basis $\mathcal{B}_i$, and we say $\mathcal{B} = \bigcup_{i=1}^{k}\mathcal{B}_i$ is a Jordan basis for $V$.
\end{definition}
\begin{definition}
  A matrix with $\lambda$ along the diagonal and $1$ along the superdiagonal is called a Jordan block associated with eigenvalue $\lambda$.
  \begin{align*}
    J_i &= \begin{pmatrix}
\lambda_i & 1            & \;     & \;  \\
\;        & \lambda_i    & \ddots & \;  \\
\;        & \;           & \ddots & 1   \\
\;        & \;           & \;     & \lambda_i       
\end{pmatrix}
  \end{align*}
\end{definition}
\begin{definition}
  We say a matrix is in Jordan canonical form if it is block diagonal with Jordan blocks.
\end{definition}
\begin{theorem}\hfill
  \begin{enumerate}[(1)]
    \item Let $T\in \Hom_{\F}\left(V,V\right)$. Suppose $c_T(x) = \left(x-\lambda_1\right)^{e_1}\cdots \left(x-\lambda_k\right)^{e_k}$ over $\F$. Then, $V$ has a Jordan basis $\mathcal{B}$. Moreover, $J = \left[T\right]_{\mathcal{B}}$ is unique up to the order of the blocks.
    \item Let $A\in \Mat_{n}\left(\F\right)$ with $c_A = \left(x-\lambda_1\right)^{e_1}\cdots \left(x-\lambda_k\right)^{e_k}$ over $\F$. Then $A$ is similar to a matrix in Jordan canonical form that is unique up to the order of the blocks.
  \end{enumerate}
\end{theorem}
\begin{lemma}
  Let $T\in \Hom_{\F}\left(V,V\right)$. We have $\ker\left(\left(T-\lambda \id_V\right)^j\right)$ and $\img\left(\left(T-\lambda \id_V\right)^j\right)$ are $T$-invariant subspaces for all $j\geq 0$.
\end{lemma}
\begin{proof}
  Note that
  \begin{align*}
    T\circ \left(T - \lambda\id_{V}\right)^j &= \left(T - \lambda\id_{V}\right)^j\circ T.
  \end{align*}
  Let $v\in \ker\left(\left(T-\lambda \id_V\right)^j\right)$. We have
  \begin{align*}
    \left(T-\lambda\id_V\right)^j\left(T(v)\right) &= T\left(\left(T-\lambda\id_V\right)^j(v)\right)\\
                                                   &= T\left(0_V\right)\\
                                                   &= 0_V.
  \end{align*}
  Thus, $T(v)\in \ker\left(\left(T-\lambda\id_V\right)^j\right)$.\newline

  Let $w\in \img\left(\left(T-\lambda\id_V\right)^j\right)$. We can write
  \begin{align*}
    w &= \left(T-\lambda\id_V\right)^j(v)
  \end{align*}
  for some $v\in V$. Applying $T$ to both sides, we have
  \begin{align*}
    T(w) &= T\left(\left(T-\lambda\id_V\right)^j(v)\right)\\
         &= \left(T-\lambda\id_V\right)^j\left(T(v)\right),
  \end{align*}
  meaning $T(w)\in \img\left(\left(T-\lambda\id_V\right)^j\right)$.
\end{proof}
We know there exists $m$ such that $E_{\lambda}^{\infty} = E_{\lambda}^{m}$. We also know that if $\left(x-\lambda\right)^k|m_T(x)$, then $\dim_{\F}\left(E_{\lambda}^{k}\right)\geq k$.
\begin{lemma}
  Suppose $m_T(x) = \left(x-\lambda\right)^mp(x)$ with $p\left(\lambda\right)\neq 0$. Then,
  \begin{align*}
    E_{\lambda}^{\infty} &= E_{\lambda}^{m}.
  \end{align*}
\end{lemma}
\begin{proof}
  Let $v\in E_{\lambda}^{\infty}$ and $e$ be the least positive integer such that 
  \begin{align*}
    \left(T-\lambda\id_V\right)^{e}\left(v\right) = 0_V.
  \end{align*}
  Suppose toward contradiction that $e > m$. We have $m_{T,v}(x) | \left(x-\lambda\right)^e$, but $m_{T,v}\left(x\right) \nmid \left(x-\lambda\right)^{e-1}$. In particular, $m_{T,v}(x) = \left(x-\lambda\right)^{e}$. However, $m_{T,v}(x) | m_{T}(x)$.
\end{proof}
\begin{lemma}
  Let $\dim_{\F}\left(V\right) = n$. Let $m_T(x) = \left(x-\lambda\right)^mp(x)$ with $p\left(\lambda\right)\neq 0$. Then, we have
  \begin{align*}
    V &= E_{\lambda}^{m} \oplus \img\left(\left(T-\lambda\id_V\right)^m\right).
  \end{align*}
\end{lemma}
\begin{proof}
  Recall that
  \begin{align*}
    E_{\lambda}^{m} &= \ker\left(\left(T-\lambda\id_{V}\right)^m\right).
  \end{align*}
  Therefore, the dimensions line up. All we need show is that $E_{\lambda}^{m}\cap \img\left(\left(T-\lambda\id_V\right)^m\right) = \set{0_V}$.\newline

  Let $v\in E_{\lambda}^{m}\cap \img\left(\left(T-\lambda\id_V\right)^m\right)$. We have
  \begin{align*}
    v &= \left(T-\lambda\id_V\right)^{m}\left(w\right)
  \end{align*}
  for some $w\in V$. Applying $\left(T-\lambda\id_V\right)^{m}$ to both sides, we have
  \begin{align*}
    \left(T-\lambda\id_V\right)^{m}\left(v\right) &= \left(T-\lambda\id_V\right)^{2m}(w)\\
                                                  &=0_V,
  \end{align*}
  since $v\in \ker\left(\left(T-\lambda\id_V\right)^m\right)$. Thus,
  \begin{align*}
    \left(T-\lambda\right)^{2m}(w) &= 0_V.
  \end{align*}
  Thus, $w\in E_{\lambda}^{2m}$. However, since $E_{\lambda}^{\infty} = E_{\lambda}^{m}$, so too is $E_{\lambda}^{2m}$, so $w\in E_{\lambda}^{m}$, meaning
  \begin{align*}
    \left(T-\lambda\right)^{m}\left(w\right) &= 0_V,
  \end{align*}
  so $v = 0_V$.
\end{proof}
\begin{theorem}
  Assume $m_T(x) = \left(x-\lambda_1\right)^{m_1}\cdots\left(x-\lambda_k\right)^{m_k}$ with $\lambda_j\in \F$, $\lambda_j$ distinct, $m_j\geq 1$.\newline

  Then, 
  \begin{align*}
    V &= E_{\lambda_1}^{m_1} \oplus \cdots \oplus E_{\lambda_k}^{m_k}.
  \end{align*}
\end{theorem}
\begin{proof}
  We will use induction on $k$.\newline

  If $k = 1$, then $m_T(x) = \left(x-\lambda_1\right)^{m_1}$. Since $m_{T}\left(T\right)\left(v\right) = 0_V$ for all $v\in V$, we have
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}.
  \end{align*}
  Assume the result is true for any vector space $W$ and $S\in \Hom_{\F}\left(W,W\right)$, where $m_S(x)$ splits completely over $\F$ and has fewer than $k$ distinct roots.\newline

  We can break our vector space $V$ to be
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}\oplus \img\left(\left(T-\lambda_1\id_{V}\right)^{m_1}\right).
  \end{align*}
  Set $W = \img\left(\left(T-\lambda_1\right)^{m_1}\right)$. We have $W$ is $T$-invariant. Thus, $T_W := T\vert_{W}\in \Hom_{\F}\left(W,W\right)$.\newline

  We claim that $m_{T_W}(x) = \left(x-\lambda_2\right)^{m_2}\cdots \left(x-\lambda_k\right)^{m_k}$.\newline

  Set $p(x) = \left(x-\lambda_2\right)^{m_2}\cdots \left(x-\lambda_k\right)^{m_k}$. Suppose $w\in W$ satisfies $p\left(T_W\right)\left(w\right) \neq 0_V$. At the same time, we have $m_{T}(T)(w) = 0_V$. Thus,
  \begin{align*}
    \left(T-\lambda_1\id_V\right)^{m_1}\left(p(T)(w)\right) = 0_V,
  \end{align*}
  meaning $p(T)(w) \in E_{\lambda_1}^{m_1}$. This is a contradiction, since $p(T)(w) = p\left(T_W\right)(w)\in W$.\newline

  Thus, $m_{T_W}|p(x)$.\newline

  Suppose $m_{T_W}$ is a proper divisor of $p(x)$. If we set $f(x) = m_{T_W}(x)\left(x-\lambda_1\right)^{m_1}$. For $v\in V$, write
  \begin{align*}
    v &= v_1 + w
  \end{align*}
  with $v_1\in E_{\lambda_1}^{m_1}$ and $w\in W$. Notice that
  \begin{align*}
    f(T)(v) &= f(T)\left(v_1\right) + f(T)\left(w\right)\\
            &= m_{T_W}\left(\left(T-\lambda_1\id_V\right)^{m_1}\right)(v) + \left(T-\lambda_1\id_V\right)^{m_1}M_{T_W}(w)\\
            &= 0_V + 0_V\\
            &= 0_V.
  \end{align*}
  Thus, $m_{T}(x)|f(x)$, which is a contradiction if $m_{T_W}$ is a proper divisor of $p(x)$.\newline

  Thus, $m_{T_W}(x) = p(x)$ as claimed.\newline

  We have that
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}\oplus W,
  \end{align*}
  and we apply the induction hypothesis to $W$ to yield
  \begin{align*}
    V &= E_{\lambda_1}^{m_1}\oplus \left(E_{\lambda_2}^{m_2}\oplus \cdots \oplus E_{\lambda_k}^{m_k}\right).
  \end{align*}
\end{proof}
If $T$ has minimal polynomial of the form $m_T(x) = \left(x-\lambda\right)^mp(x)$ with $p\left(\lambda\right) \neq 0$, then we get at least one Jordan block with size $m$.
\begin{lemma}
  Let $m_T(x) = c_T(x) = \left(x-\lambda\right)^n$, with $\dim_{\F}\left(V\right) = n$. Then, a Jordan basis for $V$ exists.
\end{lemma}
\begin{proof}
  Let $w_1\in V$ with $m_{T,w_1}(x) = m_{T}(x) = c_T(x)$. Let $W_1$ be the space generated by $T$ on $\set{w_1}$. We claim $W_1 = V$.\newline

  Set $v_n = w_1$ and
  \begin{align*}
    v_i &= \left(T-\lambda\id_V\right)^{n-i}\left(v_n\right).
  \end{align*}
  Note that
  \begin{align*}
    v_i &= \left(T-\lambda\id_V\right)^{n-i}\left(v_n\right)\\
        &= \left(T-\lambda\id_V\right)\left(T-\lambda\id_V\right)^{n-i-1}\left(v_n\right)\\
        &= \left(T-\lambda\id_V\right)\left(v_{i+1}\right),
  \end{align*}
  meaning $T\left(v_{i+1}\right) = v_i + \lambda v_{i+1}$.\newline

  We claim that $\set{v_1,\dots,v_n}$ is a basis of $V$.\newline

  Suppose
  \begin{align*}
    c_1 v_1 + \cdots + c_nv_n = 0_V
  \end{align*}
  for some $c_i\in \F$> This gives
  \begin{align*}
    c_1\left(T-\lambda\id_V\right)^{n-1} + \cdots + c_{n}v_n = 0_V.
  \end{align*}
  Set $p(x) = c_1(x-\lambda)^{n-1} + \cdots + c_{n-1}\left(x-\lambda\right) + c_n$.\newline

  Then,
  \begin{align*}
    p(T)\left(v_n\right) &= 0_V\\
                         &= p(T)\left(w_1\right),
  \end{align*}
  meaning
  \begin{align*}
    m_{T,w_1}\left(x\right)|p(x),
  \end{align*}
  but $\deg\left(m_{T,w_1}(x)\right) = n$, meaning $p(x) = 0$, so $c_i = 0$ for all $i$.\newline

  Thus, $\set{v_1,\dots,v_n}$ is a Jordan basis.
\end{proof}
\begin{proposition}
  Let $\dim_{\F}\left(V\right) = n$ and $m_T(x) = \left(x-\lambda\right)^k$ for some $1\leq k \leq n$. Then, a Jordan basis for $V$ exists.
\end{proposition}
\begin{proof}
  We have $V = E_{\lambda}^{\infty} = E_{\lambda}^{k}$. We know the result if $k = n$. Assume $k < n$.\newline

  We claim that given any subspace $W_1$ of $V$ with $W_1\cap \ker\left(\left(T-\lambda\id_{V}\right)^{k-1}\right) = \set{0_V}$, there is a $T$-stable subspace $U$ of $V$ with 
  \begin{align*}
    V =\underbrace{\left( W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}(W_1)\right)}_{\text{$k\times k$ Jordan block}}\oplus U.
  \end{align*}
  We know there exists $v_k\in V$ with $\left(T-\lambda\id_V\right)^{k-1}\left(v_k\right) \neq 0_V$. Set $W_1 = \Span_{\F}\left(v_k\right)$. We have
  \begin{align*}
    W_1\cap \ker\left(\left(T-\lambda\id_V\right)^{k01}\right) &= \set{0_V}.
  \end{align*}
  Write
  \begin{align*}
    V &= W_1\oplus \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)\oplus W_2.
  \end{align*}
  Note that $W_2$ consists of other $k\times k$ Jordan block generators, though it can also be $0_V$.\newline

  Set $W = W_1\oplus W_2$. We have 
  \begin{align*}
    \left(T-\lambda\id_V\right)(W)\subseteq \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right).
  \end{align*}
  We also have
  \begin{align*}
    \left(T-\lambda\id_V\right)\left(W\right)\cap \ker\left(\left(T-\lambda\id_V\right)^{k-2}\right) &= \set{0_V}.
  \end{align*}
  If $w\in \left(T-\lambda\id_V\right)\left(W\right)\cap \ker\left(\left(T-\lambda\id_V\right)^{k-2}\right)$, then
  \begin{align*}
    w = \left(T-\lambda\id_V\right)\left(w_1 + w_2\right)
  \end{align*}
  for $w_i\in W_i$, and
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-2}\left(w\right) &= 0_V,
  \end{align*}
  meaning
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-2}\left(T-\lambda\id_V\right)\left(w_1 + w_2\right) &= 0_V\\
    \left(T-\lambda\id_V\right)^{k-1}\left(w_1\right) + \left(T-\lambda\id_V\right)^{k-1}w_2 &= 0_V,
  \end{align*}
  implying $w_1 = w_2 = 0_V$, since
  \begin{align*}
    V &= W_1 \oplus W_2 \oplus \underbrace{\ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)}_{\tilde{V}}.
  \end{align*}
  Note that $\dim_{\F}\left(\tilde{V}\right) < n$. We also know that $\tilde{V}$ is $T$-stable.\newline

  Let $\tilde{W} = \left(T-\lambda\id_V\right)(W)$. We have
  \begin{align*}
    \tilde{W} \cap \ker\left(\left(T-\lambda\id_V\right)^{k-2}\right) = \set{0_V}.
  \end{align*}
  We apply the induction hypothesis to $\tilde{V}$ and $\tilde{W}$ to get a $T$-stable subspace $\tilde{U}$ such that
  \begin{align*}
    \tilde{V} = \left(\tilde{W} + \left(T - \lambda\id_V\right)\left(\tilde{W}\right) + \cdots + \left(T-\lambda\id_V\right)^{k-2}\left(\tilde{W}\right)\right) \oplus \tilde{U}.
  \end{align*}
  Define
  \begin{align*}
    U &= \left(W_2 + \left(T-\lambda\id_V\right)\left(W_2\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_2\right)\right) + \tilde{U}.
  \end{align*}
  We have $U$ is $T$-stable. We need to show that
  \begin{align*}
    V &= \left(W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right)\right)\oplus U.
  \end{align*}
  We have
  \begin{align*}
    V &= W_1 + W_2 + \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)\\
      &= W_1 + W_2 + \tilde{V}\\
      &= W_1 + W_2 + \left(\tilde{W} + \left(T - \lambda\id_V\right)\left(\tilde{W}\right) + \cdots + \left(T-\lambda\id_V\right)^{k-2}\left(\tilde{W}\right)\right) + \tilde{U}\\
      &= W_1 + W_2 + \left(\left(W_1 + W_2\right) + \left(T - \lambda\id_V\right)\left(W_1 + W_2\right) + \cdots + \left(T-\lambda\id_V\right)^{k-2}\left(W_1 + W_2\right)\right) + \tilde{U}\\
      &= W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right) + U.
  \end{align*}
  Let $v\in \left(W_1 + \left(T-\lambda\id_V\right)\left(W_1\right) + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right)\right)\cap U$. Then,
  \begin{align*}
    v &= \sum_{j=0}^{k-1}\left(T-\lambda\id_V\right)^j\left(w_j\right)
  \end{align*}
  for $w_0,\dots,w_{k-1}\in W_1$. Additionally,
  \begin{align*}
    v &= \sum_{j=0}^{k-1}\left(T-\lambda\id_V\right)^{j}\left(w_j'\right) + \tilde{u}
  \end{align*}
  for $w_0',\dots,w_{k-1}'\in W_2$ and $\tilde{u}\in \tilde{U}$.\newline

  Applying $\left(T-\lambda\id_V\right)^{k-1}$ to both expressions for $v$, yielding
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-1}\left(w_0\right) &= \left(T-\lambda\id_V\right)\left(w_0'\right)
  \end{align*}
  since $\tilde{u}\in \ker\left(T-\lambda\id_V\right)^{k-1}$. Thus,
  \begin{align*}
    \left(T-\lambda\id_V\right)^{k-1}\left(w_0-w_0'\right) &= 0_V,
  \end{align*}
  meaning $w_0 - w_0'\in \ker\left(\left(T-\lambda\id_V\right)^{k-1}\right)$, and $w_0 - w_0'\in W$, so $w_0 = w_0'\in W_1\cap W_2 = \set{0_V}$.\newline

  To extract the basis, let $W_1 = \Span\left(v_k\right)$, $v_j = \left(T-\lambda\id_V\right)^{k-j}\left(v_k\right)$, and
  \begin{align*}
    \mathcal{B}_{\mathcal{W}} = \set{v_1,\dots,v_k}
  \end{align*}
  is a Jordan basis for $\mathcal{W} :=W_1 + \left(T-\lambda\id_V\right)W_1 + \cdots + \left(T-\lambda\id_V\right)^{k-1}\left(W_1\right)$. Thus, we have
  \begin{align*}
    V &= \mathcal{W}\oplus U,
  \end{align*}
  with $U$ having Jordan basis $\mathcal{B}_U$ by induction. Thus,
  \begin{align*}
    \mathcal{B} &= \mathcal{B}_{\mathcal{W}}\cup \mathcal{B}_U
  \end{align*}
  is a Jordan basis for $V$.
\end{proof}
\end{document}
