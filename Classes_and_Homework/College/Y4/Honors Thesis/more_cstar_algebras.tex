\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright,sfmath,bbold}
%\renewcommand{\mathcal}{\mathtt}

%Euler:
\usepackage{newpxtext,eulerpx,eucal,eufrak}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\renewcommand*{\hbar}{\hslash}
\DeclareMathOperator{\cb}{cb}

%kp fonts:
%\usepackage{kpfonts}
%\renewcommand{\mathbb}{\mathds}
%\usepackage{homework}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{More on $C^{\ast}$-Algebras}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\tableofcontents
\section{Introduction}%
Finally, the last part of my notes on $C^{\ast}$-algebras and amenability as part of my Honors Thesis independent study. Specifically, I am going to focus more on the theory of $C^{\ast}$-algebras, discussing ideas such as amenability and nuclearity in $C^{\ast}$-algebras. There are a few central results I'm going to be working on understanding and proving: almost-invariant vectors, Kesten's criterion, Hulanicki's criterion, nuclearity, and the equivalence of $C^{\ast}_{\lambda}\left(G\right)$ and $C^{\ast}\left(G\right)$.\newline

I will be using a variety of sources more focused on amenability, including but not limited to Volker Runde's \textit{Amenable Banach Algebras}, Kate Juschenko's \textit{Amenability of Discrete Groups by Examples}, and Brown and Ozawa's \textit{$C^{\ast}$-Algebras and Finite-Dimensional Approximations}.
\section{Review: Representations, the Reduced Group $C^{\ast}$-Algebra, and the Universal Group $C^{\ast}$-Algebra}%
\subsection{Left-Regular Representation}%
Let $\Gamma$ be a group. Consider the space $\ell_2\left(\Gamma\right)$. For every $s\in\Gamma$, we define the operator
\begin{align*}
  \lambda_s\left(\xi\right)\left(t\right) &= \xi\left(s^{-1}t\right).
\end{align*}
The map is linear, well-defined, and an isometry, as
\begin{align*}
  \norm{\lambda_s\left(\xi\right)}^2 &= \sum_{t\in\Gamma}\left\vert \lambda_s\left(\xi\right)\left(t\right) \right\vert^2\\
                                     &= \sum_{t\in\Gamma}\left\vert \xi\left(s^{-1}t\right) \right\vert^2\\
                                     &= \sum_{r\in\Gamma}\left\vert \xi\left(r\right) \right\vert^2\\
                                     &= \norm{\xi}^2.
\end{align*}
Additionally, each $\lambda_{s}$ admits an inverse, $\lambda_{s^{-1}} = \lambda_s^{\ast}$. Applying to the orthonormal basis $\set{\delta_t}_{t\in\Gamma}$, we get
\begin{align*}
  \lambda_s\left(\delta_t\right) &= \delta_{st}.
\end{align*}
Thus, $\lambda_{s}\circ \lambda_r = \lambda_{sr}$, and we have the unitary representation of $\Gamma$, $\lambda\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$, where $\lambda(s) = \lambda_s$, for $s\in \Gamma$. This is the left-regular representation of $\Gamma$.\newline
 
Note that the left regular representation is a faithful representation, hence injective.\newline

Because the $\lambda$ operator is linear, we may extend it to the case of any positive finitely supported function,
\begin{align*}
  \lambda_{f}\left(\xi\right)(t) &= \left(\sum_{s\in\Gamma}f(t)\lambda_{s}\left(\xi\right)\right)\left(t\right)\\
                                 &= \sum_{s\in\Gamma}f(s)\xi\left(s^{-1}t\right)
\end{align*}
Note that the space of finitely supported functions on $\Gamma$, $\C\left[\Gamma\right]$,\footnote{Also known as the free vector space over $\C$ with basis $\Gamma$.} is a $\ast$-algebra, where multiplication is given by convolution:
\begin{align*}
  f\ast g(t) &= \sum_{s\in\Gamma}f\left(s\right)g\left(s^{-1}t\right)\\
             &= \sum_{r\in\Gamma}f\left(tr^{-1}\right)g(r).
\end{align*}
Note that we are using $\ast$ both to refer to the involution (when as a superscript) as well as the group operation (when not a superscript). This is to maintain coherence with the traditional way that convolution is written. The involution on $\C\left[\Gamma\right]$ is given by
\begin{align*}
  f^{\ast}\left(t\right) &= \overline{f\left(t^{-1}\right)}.
\end{align*}
\subsection{A Bit on Representations and $C^{\ast}$-(Semi)norms}%
A $C^{\ast}$-seminorm on a $\ast$-algebra is a seminorm such that defined by
\begin{itemize}
  \item $\norm{ab}\leq \norm{a}\norm{b}$;
  \item $\norm{a^{\ast}} = \norm{a}$;
  \item $\norm{a^{\ast}a} = \norm{a}^2$.
\end{itemize}
If $A_0$ is a $\ast$-algebra, then a representation of $A_0$ is a pair $\left(\pi_0,\mathcal{H}\right)$, where $\mathcal{H}$ is a Hilbert space and $\pi\colon A_0\rightarrow \B\left(\mathcal{H}\right)$ is a $\ast$-homomorphism.\newline

Additionally, if $A_0$ is a $\ast$-algebra with representation $\pi_0$, then we have $C^{\ast}$-seminorm
\begin{align*}
  \norm{a}_{\pi_0} &= \norm{\pi_0\left(a\right)}_{\op}.
\end{align*}
If $\pi_0$ is injective, then $\norm{\cdot}_{\pi_0}$ is a $C^{\ast}$-norm. If $\pi_0$ is a $C^{\ast}$-norm, then the completion of $A_0$ with respect to $\norm{\cdot}_{\pi_0}$ is a $C^{\ast}$-algebra.\newline

The universal norm on $A_0$ is defined as
\begin{align*}
  \norm{a}_{u} &= \sup_{p\in \mathcal{P}}p(a),
\end{align*}
where $\mathcal{P}$ is the collection of all $C^{\ast}$-seminorms on $A_0$. If $\norm{a}_u < \infty$ for all $a\in A_0$, then $\norm{\cdot}_u$ is a $C^{\ast}$-seminorm on $A_0$. Note that if one of $p\in \mathcal{P}$ is a norm, then $\norm{\cdot}_{u}$ defines a $C^{\ast}$-norm on $A_0$.\newline

If we have the unitary representation $u\colon \C\left[\Gamma\right]\rightarrow \B\left(\mathcal{H}\right)$, then
\begin{align*}
  \pi_u(a) &= \sum_{s\in\Gamma}u_s
\end{align*}
is a representation of $\C\left[\Gamma\right]$. If $\lambda\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$ is the left-regular representation, then the left-regular group $C^{\ast}$-algebra is the group $\ast$-algebra with $C^{\ast}$-norm defined by $\norm{a} = \norm{\pi_{\lambda}(a)}$.\newline

The universal group $C^{\ast}$-algebra is defined as the norm completion of 
\begin{align*}
  \norm{a}_{\max} &= \sup\set{\norm{\pi\left(a\right)}_{\op} | \pi\colon \C\left[\Gamma\right]\rightarrow \B\left(\mathcal{H}_{\pi}\right) \text{ is a representation}}.
\end{align*}
Note that
\begin{align*}
  \norm{\pi\left(a\right)} &= \norm{\pi\left(\sum_{s\in\Gamma}a_s\delta_s\right)}\\
                           &= \norm{\sum_{s\in\Gamma}a_s\pi\left(\delta_s\right)}\\
                           &\leq \sum_{s\in\Gamma}\norm{a_s\pi\left(\delta_s\right)}\\
                           &= \sum_{s\in\Gamma}\left\vert a_s \right\vert.
\end{align*}
Note that since $\norm{\cdot}_{\lambda}$ is a norm, we must have $a=0$ if and only if $\norm{a}_{\max} = 0$. The full group $C^{\ast}$-algebra admits a universal property.
\begin{proposition}
  Let $\Gamma$ be a discrete group. If $u\colon \Gamma\rightarrow \B\left(\mathcal{H}\right)$, then there is a contractive $\ast$-homomorphism $\pi_u\colon C^{\ast}\left(\Gamma\right)\rightarrow \B\left(\mathcal{H}\right)$ that satisfies $\pi_u\left(\delta_s\right) = u(s)$.
\end{proposition}
\section{Using the Left-Regular Representation to Establish Amenability}%
If $\pi\colon \Gamma\rightarrow \mathcal{U}\left(\mathcal{H}\right)$ is a unitary representation of $\mathcal{H}$, then a vector $\xi\in \mathcal{H}$ is called invariant for $\pi$ if $\pi(g)\left(\xi\right) = \xi$ for all $g\in \Gamma$.
\begin{proposition}
  The left-regular representation for $\Gamma$ admits an invariant vector if and only if $\Gamma$ is finite.
\end{proposition}
\begin{proof}
  Let $\Gamma$ be finite. Since $\Gamma$ is finite, all functions $a\colon \Gamma\rightarrow \C$ are square-summable. Thus, $\xi = \1_{\Gamma}$ is square-summable, and since $s\Gamma = \Gamma$ for all $s\in\Gamma$, we have $\1_{\Gamma}$ is invariant for $\lambda$.\newline

  Now, let $\lambda\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$ be the left-regular representation, and suppose there is $\xi\in \ell_2\left(\Gamma\right)$ such that for all $s\in \Gamma$, we have
  \begin{align*}
    \lambda_s\left(\xi\right) &= \xi.
  \end{align*}
  In particular, this means that for any $t\in \Gamma$, we have
  \begin{align*}
    \lambda_s\left(\xi\right)\left(t\right) &= \xi\left(s^{-1}t\right)\\
                                            &= \xi\left(t\right).
  \end{align*}
  Since this holds for all $s\in \Gamma$, we have that $\xi = c\1_{\Gamma}$ for some $c\in \C$. However, since $\xi\in \ell_2\left(\Gamma\right)$, we must have that $\sum_{t\in\Gamma} \left\vert c \right\vert^2 < \infty$, which only holds if $\Gamma$ is finite.
\end{proof}
An almost-invariant vector for a representation $\pi\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$, as the name suggests,\footnote{I'm only mostly being facetious here.} a sequence (or net) of unit vectors $\left(\xi_i\right)_{i\in I}$ such that
\begin{align*}
  \lim_{i\in I}\norm{\pi(g)\left(\xi_i\right) - \xi_i} &= 0.
\end{align*}
\begin{theorem}
  A group $\Gamma$ is amenable if and only if the left-regular representation has an almost-invariant vector.
\end{theorem}
\begin{proof}
  Let $\Gamma$ be amenable, and let $F_i$ be a FÃ¸lner sequence, where $\frac{\left\vert sF_i\triangle F_i \right\vert}{\left\vert F_i \right\vert}\rightarrow 0$ for all $s\in\Gamma$.\newline

  Define $\xi_i = \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{F_i}$. Then,
  \begin{align*}
    \norm{\lambda_{s}\left(\xi_i\right) - \xi_i}^2 &= \sum_{t\in\Gamma} \left\vert \lambda_{s}\left(\xi_i\right)\left(t\right) - \xi_i\left(t\right) \right\vert^2\\
                                                   &= \sum_{t\in\Gamma} \left\vert \lambda_s\left(\frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{F_i}\right)\left(t\right) - \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{F_i} \right\vert^2\\
                                                   &= \sum_{t\in\Gamma}\left\vert \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{sF_i}(t) - \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{sF_i}(t) \right\vert^2\\
                                                   &= \frac{\left\vert sF_i\triangle F_i \right\vert}{\left\vert F_i \right\vert}.
  \end{align*}
  Thus, $\lambda$ has an almost-invariant vector.\newline

  Suppose there exists an almost-invariant vector $\left(\xi_i\right)_i\in \ell_2\left(\Gamma\right)$. It is sufficient to construct an approximate mean. Since $\xi_i\in \ell_2\left(\Gamma\right)$, we have that $\xi_i^2\in \ell_1\left(\Gamma\right)$. Setting $\mu_i = \xi_i^2$, we plug this into the expression for an approximate mean, and obtain
  \begin{align*}
    \norm{\lambda_s\left(u_i\right) - u_i}_{\ell_1} &= \sum_{t\in\Gamma}\left\vert \lambda_s\left(\xi_i^2\right)\left(t\right) - \xi_i^2\left(t\right) \right\vert\\
                                                    &= \sum_{t\in\Gamma}\left\vert \left(\lambda_s\left(\xi_i\right)\left(t\right) - \xi_i\left(t\right)\right)\left(\lambda_s\left(\xi_i\right)\left(t\right) + \xi_i\left(t\right)\right) \right\vert\\
                                                    &= \norm{\left(\lambda_s\left(\xi_i\right) - \xi_i\right)\left(\lambda_s\left(\xi_i\right) + \xi_i\right)}_{\ell_1}\\
                                                    &\leq \norm{\lambda_s\left(\xi_i\right) - \xi_i}_{\ell_2}\norm{\lambda_s\left(\xi_i\right) + \xi_{i}}\\
                                                    &\leq 2\norm{\lambda_s\left(\xi_i\right) - \xi_i}\\
                                                    &\rightarrow 0.
  \end{align*}
  Thus, $\mu_i$ is an approximate mean.
\end{proof}
Using the criterion of almost invariant vectors, we may show that a group is amenable if and only if the trivial representation --- defined by $1_{\Gamma}\colon \Gamma\rightarrow \C$, $1_{\Gamma}(g) = 1$ is what is known as weakly contained in the left-regular representation.\newline

A representation $\pi\colon \Gamma\rightarrow \mathcal{U}\left(\mathcal{H}\right)$ is weakly contained in another representation $\rho\colon \Gamma\rightarrow \mathcal{U}\left(\mathcal{H}\right)$, denoted $\pi\prec \rho$, if for every $\xi\in \mathcal{H}$, finite $E\subseteq \Gamma$, and $\ve > 0$, then there are $\eta_1,\dots,\eta_n\in \mathcal{K}$ such that
\begin{align*}
  \left\vert \iprod{\pi(g)\left(\xi\right)}{\xi} - \sum_{i=1}^{n} \iprod{\rho(g)\left(\eta_i\right)}{\eta_i} \right\vert < \ve.
\end{align*}
\begin{theorem}
  A discrete group $\Gamma$ is amenable if and only if $1_{\Gamma}\prec \lambda$, where $\lambda$ is the left-regular representation.
\end{theorem}
\begin{proof}
  We show that $1_{\Gamma}\prec \lambda$ is equivalent to the existence of an almost invariant vector for $\lambda$. We assume $\lambda$ admits an almost-invariant vector. It is sufficient to show that for every $\ve > 0$ and every finite set $E\subseteq \Gamma$, there are $\eta_1,\dots,\eta_n\in \ell_2\left(\Gamma\right)$ such that
  \begin{align*}
    \left\vert 1-\sum_{i=1}^{n} \iprod{\lambda_t\left(\eta_i\right)}{\eta_i} \right\vert < \ve
  \end{align*}
  for every $t\in E$. If we take $n = 1$ and $\eta_1 = \xi$, where $\xi$ is almost-invariant for all $g\in E$ --- i.e., $\norm{\lambda_g\left(\xi\right) - \xi}_{\ell_2} < \ve$ for all $g\in E$. Note that we have
  \begin{align*}
    \norm{\lambda_g\left(\xi\right) - \xi}^2 &= \iprod{\lambda_g\left(\xi\right) - \xi}{\lambda_g\left(\xi\right) - \xi}\\
                                             &= \iprod{\lambda_g\left(\xi\right)}{\lambda_g\left(\xi\right)} + \iprod{\xi}{\xi} - 2\re\left( \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                             &= 2 - 2\re\left( \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                             &= 2\re\left(1 -  \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                             &\leq 2\left\vert 1 - \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert.
  \end{align*}
  Additionally,
  \begin{align*}
    \left\vert 1- \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert^2 &= \left(1 - \iprod{\lambda_g\left(\xi\right)}{\xi}\right) \left( 1 - \overline{ \iprod{\lambda_g\left(\xi\right)}{\xi} } \right)\\
                                                                       &= 1 - \overline{ \iprod{\lambda_g\left(\xi\right)}{\xi} } - \iprod{\lambda_g\left(\xi\right)}{\xi} + \left\vert \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert^2\\
                                                                       &\leq 2 - 2\re\left( \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                                                       &= \norm{\lambda_g\left(\xi\right) - \xi}^2.
  \end{align*}
  Thus, we have that
  \begin{align*}
    \left\vert 1- \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert &\leq \norm{ \lambda_g\left(\xi\right) - \xi }\\
                                                                     &< \ve.
  \end{align*}
%  Now, we suppose that $1_{\Gamma}\prec \lambda$. For every $\ve > 0$ and finite subset $E\subseteq \Gamma$, there are $\eta_1,\dots,\eta_n\in \ell_2\left(\Gamma\right)$ such that 
%  \begin{align*}
%    \left\vert 1 - \sum_{i=1}^{n} \iprod{\lambda_t\left(\eta_i\right)}{\eta_i} \right\vert &< \ve.
%  \end{align*}
%  for all $t\in E$. We may assume that $e_G\in E$, yielding
%  \begin{align*}
%    \left\vert 1-\sum_{i=1}^{n} \norm{\eta_i}^2 \right\vert < \ve.
%  \end{align*}
%  Furthermore, we may assume that $\sum_{i=1}^{n} \norm{\eta_i}^2 = 1$.\newline
%
%  Suppose toward contradiction that $\lambda$ does not have an almost-invariant vector. Then, there exists $C > 0$ and $S\subseteq \Gamma$ such that
%  \begin{align*}
%    \norm{\xi}^2\left\vert S \right\vert - \sum_{\gamma\in S} \iprod{\lambda_{\Gamma}\left(\xi\right)}{\xi} &> C\norm{\xi}^2.
%  \end{align*}
  We start by showing that $1_{\Gamma}\prec \lambda$ if and only if for every finite $S\subseteq \Gamma$ and every $\ve > 0$, there exists a unit vector $\xi\in \mathcal{H}$ such that
  \begin{align*}
    \norm{\lambda_s\left(\xi\right) - \xi}_{\ell_2} < \ve.
  \end{align*}
  In the forward direction, we see that there exists a unit vector $\xi$ such that $\left\vert 1 - \iprod{\lambda_s\left(\xi\right)}{\xi} \right\vert < \ve^2/2$, meaning $\norm{\lambda_s\left(\xi\right) - \xi} < \ve$ by above. Similarly, if $\norm{\lambda_s\left(\xi\right)-\xi} < \ve$, then $1_{\Gamma}\prec \lambda$.\newline

  Now, we assume $1_{\Gamma} \prec \lambda$. Thus, for a finite $E\subseteq \Gamma$ and $\ve > 0$, then there exists $f\in \ell_2\left(\Gamma\right)$ with $\norm{f}_{\ell_2} = 1$ such that $\norm{\lambda_s\left(f\right) - f} < \ve$ for all $s\in E$.\newline

  Setting $g = \left\vert f \right\vert^2$, we have $g\in \ell_1\left(\Gamma\right)$. From HÃ¶lder's inequality, we have
  \begin{align*}
    \norm{\lambda_s\left(g\right) - g}_{\ell_1} &\leq \norm{\lambda_s\left(\overline{f}\right) + \overline{f}}_{\ell_2} \norm{\lambda_s\left(f\right) - f}\\
                                                &\leq 2\norm{\lambda_s\left(f\right) - f}_{\ell_2}\\
                                                &< 2\ve.
  \end{align*}
  Thus, $\Gamma$ admits an approximate mean, hence is amenable.
\end{proof}
Having obtained some more resources on Kesten's criterion, we can now prove that.
\begin{definition}
  Let $\lambda\colon \Gamma\rightarrow \B\left(\ell_2\left(\Gamma\right)\right)$ be the left-regular representation. Then, for a finite set $E\subseteq \Gamma$, we define the Markov operator $M\left(E\right)$ by
  \begin{align*}
    M\left(E\right) &= \sum_{t\in E}\lambda_t.
  \end{align*}
\end{definition}
Note that since $\lambda_t$ is an isometry for each $t$, we have
\begin{align*}
  \norm{M\left(E\right)}_{\op} &= \norm{\frac{1}{\left\vert E \right\vert}\sum_{t\in E}\lambda_t}_{\op}\\
                               &= \frac{1}{\left\vert E \right\vert} \norm{\sum_{t\in E}\lambda_t}_{\op}\\
                               &\leq \frac{1}{\left\vert E \right\vert}\sum_{t\in E}\norm{\lambda_t}_{\op}\\
                               &= 1,
\end{align*}
so the Markov operator is a bounded operator (indeed, a contraction).
\begin{theorem}[Kesten's Criterion]
  Let $\Gamma$ contain a finite symmetric generating set $S$. Then, $\Gamma$ is amenable if and only if
  \begin{align*}
    \norm{M(S)}_{\op} &= 1.
  \end{align*}
\end{theorem}
\begin{proof}
  Let $\Gamma$ be amenable. Then, $\lambda$ admits an almost-invariant vector, $\left(\xi_n\right)_n\subseteq S_{\ell_2\left(\Gamma\right)}$, such that
  \begin{align*}
    \norm{\lambda_s\left(\xi_n\right) -\xi_n}_{\ell_2} \rightarrow 0
  \end{align*}
  for all $s\in \Gamma$. In particular, we have
  \begin{align*}
    \left\vert \left(\norm{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi_n\right)}_{\ell_2}\right)  - \norm{\xi_n}_{\ell_2}\right\vert &\leq \norm{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi_n\right) - \xi_n}_{\ell_2}\\
                                                                                                                                                                                  &= \frac{1}{\left\vert S \right\vert}\norm{\left(\sum_{t\in S}\lambda_t\right)\left(\xi_n\right) - \left\vert S \right\vert\xi_n}_{\ell_2}\\
                                                                                                                        &\leq \frac{1}{\left\vert S \right\vert} \sum_{t\in S}\norm{\lambda_t\left(\xi_n\right) - \xi_n}_{\ell_2}\\
                                                                                                                        &\rightarrow 0,
  \end{align*}
  meaning that
  \begin{align*}
    \sup_{\xi\in S_{\ell_2\left(\Gamma\right)}} \norm{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi\right)} &= \norm{\xi},
  \end{align*}
  and so the norm of the Markov operator is $1$.\newline

  Suppose
  \begin{align*}
    \norm{\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t}_{\op} &= 1,
  \end{align*}
  or
  \begin{align*}
    \norm{\sum_{t\in S}\lambda_t}_{\op} &= \left\vert S \right\vert.
  \end{align*}
  \begin{proposition}
    If $T\in \B\left(\mathcal{H}\right)$ is a self-adjoint operator, then
    \begin{align*}
      \norm{T}_{\op} &= \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T\left(x\right)}{x} \right\vert.
    \end{align*}
  \end{proposition}
  \begin{proof}
    We have that
    \begin{align*}
      \left\vert \iprod{T\left(x\right)}{x} \right\vert &\leq \norm{T\left(x\right)}\norm{x}\\
                                                        &\leq \norm{T}_{\op}\norm{x}^2\\
                                                        &= \norm{T}_{\op}.
    \end{align*}
    Now, we seek to establish the opposite direction. Note that since $T$ is self-adjoint, we know that $ \iprod{T\left(x\right)}{x}\in \R $ for any $x\in \mathcal{H}$, so by the polarization identity, we have that
    \begin{align*}
      \iprod{T\left(x\right)}{y} &= \frac{1}{4}\left( \iprod{T\left(x+y\right)}{x+y} - \iprod{T\left(x-y\right)}{x-y}\right).
    \end{align*}
    Note that we know that
    \begin{align*}
      \norm{T}_{\op} &= \sup_{x,y\in S_{\mathcal{H}}}\left\vert \iprod{T\left(x\right)}{y} \right\vert.
    \end{align*}
    Now, we set $\alpha = \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T(x)}{x} \right\vert$. Note that for any nonzero $x\in \mathcal{H}$, we have
    \begin{align*}
      \left\vert \iprod{T\left(\frac{x}{\norm{x}}\right)}{\frac{x}{\norm{x}}} \right\vert &\leq \alpha\\
      \left\vert \iprod{T\left(x\right)}{x} \right\vert &\leq \alpha \norm{x}^2.
    \end{align*}
    Now, for any $x,y\in \mathcal{H}$, we may assume that $ \iprod{T\left(x\right)}{y} \in \R $, as we may multiply $ \iprod{T\left(x\right)}{y} $ by $\sgn \left( \iprod{T\left(x\right)}{y}\right)$. Thus, by the polarization identity and the fact that $T$ is self-adjoint, we have
    \begin{align*}
      \iprod{T\left(x\right)}{y} &= \frac{1}{4}\left( \iprod{T\left(x+y\right)}{x+y} - \iprod{T\left(x-y\right)}{x-y}\right)\\
      \left\vert \iprod{T\left(x\right)}{y} \right\vert &= \left\vert \frac{1}{4}\left( \iprod{T\left(x+y\right)}{x+y} - \iprod{T\left(x-y\right)}{x-y}\right) \right\vert\\
                                                        &\leq \frac{1}{4}\left( \left\vert \iprod{T\left(x+y\right)}{x+y} \right\vert + \left\vert \iprod{T\left(x-y\right)}{x-y} \right\vert\right)\\
                                                        &\leq \frac{\alpha}{4} \left(\norm{x+y}^2 + \norm{x-y}^2\right)\\
                                                        &= \frac{\alpha}{4}\left(2\norm{x}^2 + 2\norm{y}^2\right)\\
                                                        &= \alpha.
    \end{align*}
    Thus, we have $\norm{T}_{\op}\leq \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T\left(x\right)}{x} \right\vert$.
  \end{proof}
  Now, since $S$ is symmetric, we have that $M(S)$ is self-adjoint. Therefore, we know that there is some $\xi_n\in S_{\mathcal{H}}$ such that
  \begin{align*}
    1-\frac{1}{n} &< \iprod{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi_n\right)}{\xi_n}\\
                  &\leq \iprod{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\left\vert \xi_n \right\vert\right)}{\left\vert \xi_n \right\vert}.
  \end{align*}
  Thus, rearranging, we have
  \begin{align*}
    1 - \iprod{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\left\vert \xi_n \right\vert\right)}{\left\vert \xi_n \right\vert} &< \frac{1}{n}.
  \end{align*}
  Since $M(S)$ is a self-adjoint operator, we have that $\re\left( \iprod{\left( \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t \right)\left( \xi_n \right)}{\xi_n} \right) = \iprod{\left( \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t \right)\left( \xi_n \right)}{\xi_n}$. This gives
  \begin{align*}
    \norm{\left( \frac{1}{S}\sum_{t\in S}\lambda_t \right)\left( \xi \right) - \xi} &\leq \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\norm{\lambda_t\left( \xi \right) - \xi}\\
                                                                                    &\leq \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\sqrt{2}\left\vert 1 - \iprod{\lambda_t\left( \xi \right)}{\xi} \right\vert\\
                                                                                    &= \sqrt{2}\left\vert 1 - \frac{1}{\left\vert S \right\vert} \sum_{t\in S} \iprod{\lambda_t\left( \xi \right)}{\xi}\right\vert\\
                                                                                    &\rightarrow 0.
  \end{align*}
  Thus, $\lambda$ admits an almost-invariant vector.
\end{proof}
Next, we turn to Hulanicki's Criterion.
\begin{definition}
  Let $f\in \ell_1\left( \Gamma \right)$. Then, we define the bounded operator
  \begin{align*}
    \lambda_{f(t)} &= \sum_{t\in\Gamma}f(t)\lambda_t.
  \end{align*}
\end{definition}
\begin{theorem}
  If $\Gamma$ is a discrete group, then $\Gamma$ is amenable if and only if for every positive finitely-supported $f\colon \Gamma\rightarrow \C$, we have
  \begin{align*}
    \sum f(t) &\leq \norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
\end{theorem}
\begin{proof}
  Suppose $\Gamma$ is amenable. Let $f \geq 0$ be a finitely supported function, and let $\left( F_n \right)_n$ be a FÃ¸lner sequence such that for every $g\in \supp\left( f \right)$, we have
  \begin{align*}
    \frac{\left\vert gF_n\triangle F_n \right\vert}{\left\vert F_n \right\vert} &\leq \frac{1}{n}.
  \end{align*}
  Let $\xi_n = \frac{1}{\sqrt{\left\vert F_n \right\vert}}\1_{F_n}$. Note that $\norm{\xi_n}_{\ell_2} = 1$.\newline

  We will use the fact that
  \begin{align*}
    \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T\left( x \right)}{x} \right\vert &\leq \norm{T}_{\op}.
  \end{align*}
  We see that
  \begin{align*}
    \left\vert \iprod{\left( \sum_{t\in\Gamma}f(t)\lambda_t \right)\left( \xi_n \right)}{\xi_n} \right\vert &= \left\vert \sum_{t\in\Gamma}f(t) \iprod{\lambda_t\left( \xi_n \right)}{\xi_n} \right\vert\\
                                                                                                            &= \left\vert \sum_{t,s\in\Gamma}f(t) \xi_n\left( t^{-1}s \right)\xi_n\left( s \right) \right\vert\\
                                                                                                            &\leq \norm{\lambda_{f(t)}},
                                                                                                            \intertext{meaning}
    \lim_{n\rightarrow\infty} \left\vert \iprod{\left( \sum_{t\in\Gamma}f(t)\lambda_t \right)\left( \xi_n \right)}{\xi_n} \right\vert &\leq \norm{\lambda_{f(t)}}.
  \end{align*}
  Notice that $\xi_n$ is an almost-invariant vector for $\lambda$, meaning that $\xi_n\left( t^{-1}s \right)\rightarrow \xi_n\left( s \right)$. Therefore, this means
  \begin{align*}
    \lim_{n\rightarrow\infty}\left\vert \sum_{t,s\in\Gamma}f(t)\xi_n\left( t^{-1}s \right)\xi_n\left( s \right) \right\vert &= \lim_{n\rightarrow\infty} \left\vert \sum_{t,s\in\Gamma}f(t)\left\vert \xi_n\left( s \right) \right\vert^2 \right\vert\\
                                                                                                                            &= \sum_{t\in\Gamma}f(t)\left\vert \sum_{s\in\Gamma}\left\vert \xi_n\left( s \right) \right\vert^2 \right\vert\\
                                                                                                                            &= \sum_{t\in\Gamma} f(t)\\
                                                                                                                            &\leq \norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  This establishes that there is some $C > 0$ such that
  \begin{align*}
    \sum_{t\in\Gamma}f(t) &\leq C\norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  To show that $C = 1$, we note that, by the definition of convolution, we must have
  \begin{align*}
    \left( \sum_{t\in\Gamma}f(t) \right)^{n} &= \sum_{t\in\Gamma} \left( f\ast \cdots \ast f \right)(t),
  \end{align*}
  and
  \begin{align*}
    \left( \lambda_{f(t)} \right)^{n} &= \left( \sum_{t\in\Gamma}f(t)\lambda)_t \right)^{n}\\
                                      &= \sum_{t\in\Gamma}\left( f\ast\cdots\ast f \right)\left( t \right)\lambda_t\\
                                      &= \lambda_{\left( f\ast\cdots\ast f \right)\left( t \right)}.
  \end{align*}
  Thus, we have
  \begin{align*}
    \left( \sum_{t\in\Gamma}f(t) \right)^{n}  &= \sum_{t\in\Gamma}\left( f\ast\cdots\ast f \right)\left( t \right)\\
                                              &\leq C\norm{\lambda_{\left( f\ast\cdots\ast f \right)\left( t \right)}}\\
                                              &= C\left( \norm{\lambda_{f(t)}}_{\op} \right)^n.
  \end{align*}
  This means we have
  \begin{align*}
    \sum_{t\in\Gamma}f(t) &\leq C^{1/n}\norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  Since $n$ is arbitrary, this means $C = 1$.\newline

  Now, if for all finitely supported $f$, we have
  \begin{align*}
    \sum_{t\in\Gamma} f(t) &\leq \norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  If $f = \1_{E}$ for some finite $E\subseteq \Gamma$, we see that
  \begin{align*}
    \norm{\sum_{t\in E}\lambda_{t}}_{\op} &= \left\vert E \right\vert,
  \end{align*}
  so by Kesten's criterion, we have that $\Gamma$ is amenable.
\end{proof}
\section{Completely [Property] Maps}%
We begin this section with an overview of positive maps, completely positive maps, and extensions. These will be useful for understanding the theorem that a group is amenable if and only if the reduced group $C^{\ast}$-algebra is nuclear. The ultimate goal here is to prove Arveson's extension theorem (i.e., that $\B\left( \mathcal{H} \right)$ is injective with respect to completely positive maps). The primary text for this purpose will be Vern Paulsen's \textit{Completely Bounded Maps and Operator Algebras}.\newline

The idea behind completely positive maps is that they are positive when subjected to a certain amplification process related to the matrix algebras.
\begin{definition}
  An element of a $C^{\ast}$-algebra is positive if and only if it is self-adjoint and its spectrum is contained in the nonnegative reals. Alternatively, $b\in A$ is of the form $b = a^{\ast}a$ for some $a\in A$.
\end{definition}
To introduce a norm such that $\Mat_n\left( A \right)$ becomes a $C^{\ast}$-algebra, we begin with the most basic $C^{\ast}$-algebra, $\B\left( \mathcal{H} \right)$, and consider the $n$-fold amplification of $\mathcal{H}$, $\mathcal{H}^{(n)}$. This is a Hilbert space equipped with inner product
\begin{align*}
  \iprod{ \begin{pmatrix}h_1\\\vdots\\h_n\end{pmatrix} }{ \begin{pmatrix}k_1\\\vdots\\k_n\end{pmatrix} } &= \sum_{j=1}^{n} \iprod{h_j}{k_j}.
\end{align*}
Meanwhile, we may consider $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$ as a linear map on $\mathcal{H}^{(n)}$, by taking
\begin{align*}
  \left( T_{ij} \right)_{ij} &= \begin{pmatrix}\sum_{j=1}^{n}T_{1j}\left( h_j \right)\\\vdots\\\sum_{j=1}^{n}T_{nj}\left( h_j \right)\end{pmatrix}.
\end{align*}
This yields a $\ast$-isomorphism between $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$ and $\B\left( \mathcal{H}^{(n)} \right)$.\newline

Given any $C^{\ast}$-algebra $A$, we may theorize $\Mat_n\left( A \right)$ by first isometrically representing $\mathcal{A}$ on some Hilbert space $\mathcal{H}$, letting $A$ be a $C^{\ast}$-subalgebra of $\B\left( \mathcal{H} \right)$, and then identifying $\Mat_n\left( A \right)$ as a $\ast$-subalgebra of $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$.\newline

Using a faithful $\ast$-representation of $A$, we now have a way to turn $\Mat_n\left( A \right)$ into a $C^{\ast}$-algebra. However, since the norm is unique on a $C^{\ast}$-algebra, the norm on $\Mat_n\left( A \right)$ defined in this fashion is independent of the representation of $A$ that we choose. Furthermore, since $\ast$-isomorphisms are positive maps, the positive elements of $\Mat_n\left( A \right)$ are uniquely determined. This means that every $C^{\ast}$-algebra carries with it a set of canonically defined norms and orders on each $\Mat_n\left( A \right)$.\newline

For example, consider $\Mat_k\left( \C \right)$, which can be identified with $\mathcal{L}\left( \mathcal{C}^{k} \right)$. We identify $\Mat_n\left( \Mat_{k}\left( \C \right) \right) \cong \Mat_{nk}\left( \C \right)$. With this identification, the usual multiplication and involution on $\Mat_n\left( \Mat_k\left( \C \right) \right)$ become multiplication and involution on $\Mat_{nk}\left( \C \right)$.\newline

Now, let $X$ be a compact Hausdorff space, and let $C(X)$ be the $C^{\ast}$-algebra of continuous functions with $f^{\ast}\left( x \right) = \overline{f(x)}$, equipped with the norm $\norm{f} = \sup_{x\in X}\left\vert f(x) \right\vert$. Then, an element $F = \left( f_{ij} \right)_{ij}$ of $\Mat_n\left( C(X) \right)$ can be considered as a continuous $\Mat_n\left( \C \right)$-valued function. Addition, multiplication, and involution in $\Mat_n\left( C(X) \right)$ are pointwise. Recalling that the norm on $\Mat_n\left( C(X) \right)$ is unique, we may let $\norm{F} = \sup_{x\in X}\norm{F(x)}$, where the latter norm is the canonical matrix norm on $\Mat_n\left( C(X) \right)$. The positive elements of $\Mat_n\left( C(X) \right)$ are those $F$ for which $F(x)$ is a positive matrix for all $x$.\newline

Now, given two $C^{\ast}$-algebras $A$ and $B$ and a map $\phi\colon A\rightarrow B$, there are maps $\phi_n\colon \Mat_n\left( A \right)\rightarrow \Mat_n\left( B \right)$, given by
\begin{align*}
  \phi_n\left( \left( a_{ij} \right)_{ij} \right) &= \left( \phi\left( a_{ij} \right) \right)_{ij}.
\end{align*}
In general, when we say that $\phi$ is completely [property], then we say that all the $\phi_n$ have that property. For instance, if $\phi$ is positive, in that it maps positive elements of $A$ to positive elements of $B$, then we say $\phi$ is completely positive if $\phi_n$ is a positive map for each $n$, where the positive elements of $\Mat_n\left( A \right)$ and $\Mat_n\left( B \right)$ are defined canonically.\newline

Unfortunately, it's not always the case that (e.g.) positive maps are completely positive, or even that $\norm{\phi_n}_{\op} = \norm{\phi}_{\op}$ for each $n$.\newline

There is an isomorphism between $\Mat_n\left( A \right)$ and the tensor product $\Mat_n\left( \C \right)\otimes A$. We detail it here. The proof is from Timothy Rainone's \textit{Functional Analysis-En Route to Operator Algebras}.
\begin{theorem}
  Let $A$ be an algebra, and let $\Mat_n\left( A \right)$ denote the matrix algebra of $A$. Then, there is a $\ast$-isomorphism
  \begin{align*}
    \Mat_n\left( A \right) &\cong \Mat_n\left( \C \right)\otimes A.
  \end{align*}
\end{theorem}
\begin{proof}
  Define $\varphi\colon \Mat_n\left( A \right)\rightarrow \Mat_n\left( \C \right)\otimes A$ by
  \begin{align*}
    \varphi\left( \left( a_{ij} \right)_{ij} \right) &= \sum_{i,j=1}^{n}e_{ij}\otimes x_{ij}.
  \end{align*}
  Recall that if $A$ and $B$ are two algebras, multiplication in $A\otimes B$ is defined by
  \begin{align*}
    \left( a\otimes b \right)\left( c\otimes d \right) &= ac\otimes bd,
  \end{align*}
  and if $A$ and $B$ are $\ast$-algebras, then the involution is defined by
  \begin{align*}
    \left( a\otimes b \right)^{\ast} &= a^{\ast}\otimes b^{\ast}.
  \end{align*}
  We start by showing that $\Mat_n\left( A \right)\cong \Mat_n\left( \C \right)\otimes A$ as vector spaces. By the definition of the tensor product, the map $\varphi$ is linear.\newline

  Now, suppose
  \begin{align*}
    \varphi\left( \left( a_{ij} \right)_{ij} \right) &= \sum_{i,j=1}^{n}e_{ij}\otimes a_{ij}\\
                                                     &= 0.
  \end{align*}
  Then, since $\set{e_{ij}}_{ij}$ is linearly independent, we know that $x_{ij} = 0$ for all $i,j$, so $\left( a_{ij} \right)_{ij} = 0$, so $\varphi$ is injective.\newline

  Now, let $t\in \Mat_n\left( \C \right)\otimes A$ be given by
  \begin{align*}
    t &= \sum_{k} m_k\otimes a_k,
  \end{align*}
  where $m_k\in \Mat_n\left( \C \right)$ and $a_k\in A$. Then, using the matrix units, we write each $m_k$ as
  \begin{align*}
    m_k &= \sum_{i,j=1}^{n}m_k\left( i,j \right)e_{ij}.
  \end{align*}
  This gives
  \begin{align*}
    t &= \sum_{k}\left( \sum_{i,j=1}^{n}m_k(i,j)e_{ij} \right)\otimes a_k\\
      &= \sum_{i,j=1}^{n}e_{ij}\otimes \left( \sum_{k}m_k(i,j)a_k \right).
  \end{align*}
  Defining $a_{ij}\coloneq \sum_{k}m_{k}(i,j)a_k$, we get
  \begin{align*}
    t &= \sum_{i,j=1}^{n}e_{ij}\otimes a_{ij},
  \end{align*}
  meaning that
  \begin{align*}
    \varphi\left( \left( x_{ij} \right)_{ij} \right) &= t.
  \end{align*}
  Thus, $\varphi$ is surjective.\newline

  We will show now that $\varphi$ is multiplicative and $\ast$-preserving. If $\left( a_{ij} \right)_{ij}$ and $\left( b_{ij} \right)_{ij}$ belong to $\Mat_n\left( A \right)$.
  \begin{align*}
    \varphi\left( \left( a_{ik} \right)_{ik} \right)\varphi\left( \left( b_{lj} \right)_{lj} \right) &= \left( \sum_{i,k=1}^{n}e_{ik}\otimes a_{ik} \right)\left( \sum_{l,j=1}^{n}e_{lj}\otimes b_{lj} \right)\\
                                                                                                     &= \sum_{i,j,k,l=1}^{n}\left( e_{ik}\otimes a_{ik} \right)\left( e_{lj}\otimes b_{lj} \right)\\
                                                                                                     &= \sum_{i,j,k,l=1}^{n} e_{ik}e_{lj}\otimes a_{ik}b_{lj}\\
                                                                                                     &= \sum_{i,j,k=1}^{n}e_{ik}e_{kj}\otimes a_{ik}b_{kj}\\
                                                                                                     &= \sum_{ij,k=1}^{n}e_{ij}\otimes a_{ik}b_{kj}\\
                                                                                                     &= \sum_{i,j=1}^{n}e_{ij}\otimes \left( \sum_{k=1}^{n}a_{ik}b_{kj} \right)\\
                                                                                                     &= \varphi\left( \left( \sum_{k=1}^{n}a_{ik}b_{kj} \right)_{ij} \right)\\
                                                                                                     &= \varphi\left( \left( a_{ij} \right)_{ij}\left( b_{ij}\right)_{ij} \right).
  \end{align*}
  Similarly,
  \begin{align*}
    \varphi\left( \left( a_{ij} \right)_{ij} \right)^{\ast} &= \left( \sum_{i=1}^{n}e_{ij}\otimes a_{ij} \right)^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}\left( e_{ij}\otimes a_{ij} \right)^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}e_{ij}^{\ast}\otimes a_{ij}^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}e_{ji}\otimes a_{ij}^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}e_{ij}\otimes a_{ji}^{\ast}\\
                                                            &= \varphi\left( \left( a_{ji}^{\ast} \right)_{ij} \right)\\
                                                            &= \varphi\left( \left( a_{ij} \right)_{ij}^{\ast} \right).
  \end{align*}
\end{proof}
There are lots of useful results using amplification to the matrix algebras.
\begin{example}[Dilating an Isometry]
  Let $V$ be an isometry, and let $P = I_{\mathcal{H}} - VV^{\ast}$ be the projection onto $\Ran\left( V \right)^{\perp}$. Define $U$ on $\mathcal{K} = \mathcal{H} \oplus \mathcal{H}$ by
  \begin{align*}
    U &= \begin{pmatrix}V & P \\ 0 & V^{\ast}\end{pmatrix}.
  \end{align*}
  We find that
  \begin{align*}
    U^{\ast} &= \begin{pmatrix}V^{\ast} & 0 \\ P & V\end{pmatrix}\\
    UU^{\ast} &= \begin{pmatrix}V & P \\ 0 & V^{\ast}\end{pmatrix} \begin{pmatrix}V^{\ast} & 0 \\ P & V\end{pmatrix}\\
              &= \begin{pmatrix}VV^{\ast} + P  & PV \\ V^{\ast}P & V^{\ast}V\end{pmatrix}\\
              &= \begin{pmatrix}I_{\mathcal{H}} & 0 \\ 0 & I_{\mathcal{H}}\end{pmatrix}\\
              &= I_{\mathcal{K}}\\
    U^{\ast}U &= \begin{pmatrix}V^{\ast} & 0 \\ P & V\end{pmatrix} \begin{pmatrix}V & P \\ 0 & V^{\ast}\end{pmatrix}\\
              &= I_{\mathcal{K}}.
  \end{align*}
  Thus, $U$ is a unitary on $\mathcal{K}$. We may identify $\mathcal{H} \cong \mathcal{H}\oplus 0$, and take
  \begin{align*}
    V^n &= P_{\mathcal{H}}U^{n}|_{\mathcal{H}}
  \end{align*}
  for all $n\geq 0$. Thus, we are able to realize any isometry $V$ as the restriction of some unitary to a subspace that respects powers.
\end{example}
\begin{example}[Dilating a Contraction]
  Similarly, we may define the isometric dilation of a contraction. Let $T$ be an operator on $\mathcal{H}$ with $\norm{T}\leq 1$, and define $D_{T} = \left( I - T^{\ast}T \right)^{1/2}$. We see that
  \begin{align*}
    \norm{T\left( h \right)}^2 + \norm{D_T\left( h \right)}^2 &= \iprod{T^{\ast}T\left( h \right)}{h} + \iprod{D_T^2\left( h \right)}{h}\\
                                                              &= \norm{h}^2.
  \end{align*}
  We consider now the sequence space
  \begin{align*}
    \ell_2\left( \mathcal{H} \right) &= \set{\left( h_n \right)_{n\in\N} | h_n\in \mathcal{H},\sum_{n=1}^{\infty}\norm{h_n}^2 < \infty }.
  \end{align*}
  We have the norm
  \begin{align*}
    \norm{\left( h_n \right)_n}^2 &= \sum_{n=1}^{\infty}\norm{h_n}^2
  \end{align*}
  and the inner product
  \begin{align*}
    \iprod{\left( h_n \right)_n}{\left( k_n \right)_n} &= \sum_{n=1}^{\infty} \iprod{h_n}{k_n}.
  \end{align*}
  We define the operator $V\colon \ell_2\left( \mathcal{H} \right)\rightarrow \ell_2\left( \mathcal{H} \right)$ by
  \begin{align*}
    V\left( \left( h_n \right)_n \right) &= \left( T\left( h_1 \right),D_T\left( h_1 \right),h_2,\dots \right).
  \end{align*}
  It then follows that $V$ is an isometry on $\ell_2\left( \mathcal{H} \right)$, and that if we identify $\mathcal{H}\cong \mathcal{H}\oplus 0 \oplus \cdots$, then $T^n = P_{\mathcal{H}}V^n|_{\mathcal{H}}$.
\end{example}
\begin{theorem}[Sz.-Nagy's Dilation Theorem]
  Let $T$ be a contraction operator on $\mathcal{H}$. There is a Hilbert space $\mathcal{K}$ containing $\mathcal{H}$ as a subspace, and a unitary operator $U$ on $\mathcal{K}$ such that $T^n = P_{\mathcal{H}}U^{n}|_{\mathcal{H}}$.
\end{theorem}
\begin{proof}
  Take $\mathcal{K} = \ell_2\left( \mathcal{H} \right)\oplus \ell_2\left( \mathcal{H} \right)$, and identify $\mathcal{H}$ as $\left( \mathcal{H}\oplus 0 \oplus \cdots \right)\oplus 0$. Let $V$ be the isometric dilation of $T$ on $\ell_2\left( \mathcal{H} \right)$, and let $U$ be the unitary dilation of $V$ on $\ell_2\left( \mathcal{H} \right)\oplus \ell_2\left( \mathcal{H} \right)$. Then, since $\mathcal{H}\subseteq \ell_2\left( \mathcal{H} \right)\oplus 0$, we have that $P_{\mathcal{H}}U^{n}|_{\mathcal{H}} = P_{\mathcal{H}}V^{n}|_{\mathcal{H}} = T^n$ for all $n \geq 0$.
\end{proof}
Whenever $Y$ is an operator on $\mathcal{K}$, $\mathcal{H}$ a (closed) subspace of $\mathcal{K}$, and $X = P_{\mathcal{H}}Y|_{\mathcal{H}}$, then we say $X$ is a compression of $Y$.
\begin{corollary}[Von Neumann's Inequality]
Let $T$ be a contraction on a Hilbert space. Then, for any polynomial $p$,
\begin{align*}
  \norm{p(T)} &\leq \sup_{|z| \leq 1}\left\vert p(z) \right\vert.
\end{align*}
\end{corollary}
\begin{proof}
  Let $U$ be a unitary dilation of $T$. Since $T^n = P_{\mathcal{H}}U^n|_{\mathcal{H}}$, linearity means we have $p(T) = P_{\mathcal{H}}p(U)|_{\mathcal{H}}$. Since $U$ is defined on a larger space than $T$, then $\norm{p(T)}\leq \norm{p(U)}$. Furthermore, since unitaries are normal, we have
  \begin{align*}
    \norm{p(U)} &= \sup_{\lambda\in\sigma(U)} \left\vert p(\lambda) \right\vert,
  \end{align*}
  where $\sigma(U)$ is the spectrum of $U$. Since $U$ is unitary, $\sigma(U)\subseteq \mathbb{T}$, so von Neumann's inequality follows.
\end{proof}
\section{Positive and Completely Positive Maps}%
\subsection{Positive Maps}%
There are certain results on positive maps that are useful in the study of completely positive maps. We introduce them here.
\begin{definition}
  If $S$ is a subset of a $C^{\ast}$-algebra $A$, we say $S$ is an operator system if $A$ is unital and $S$ is a self-adjoint sub\textit{space} of $A$ with $1_A\in S$.
\end{definition}
Note that if $S$ is an operator system and $h\in S$ is self-adjoint, then though the values $h_{+}$ and $h_{-}$, defined by the continuous functional calculus with
\begin{align*}
  f^+(x) &= \max\set{0,x}\\
  f^{-}(x) &= \min_{0,-x}
\end{align*}
may not belong to $S$, we can write $h$ as the difference of two positive elements in $s$ by
\begin{align*}
  h &= \frac{1}{2}\left( \norm{h}1_A + h \right) - \frac{1}{2}\left( \norm{h}1_A - h \right).
\end{align*}
\begin{definition}
  If $S$ is an operator system, $B$ is a $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ is a linear map, then $\phi$ is called positive if it maps positive elements of $S$ to positive elements of $B$.
\end{definition}
\begin{theorem}
  If $\phi$ is a positive linear functional on an operator system $S$, then $\norm{\phi} = \phi\left( 1_A \right)$.
\end{theorem}
When the range of $\phi$ is not $\C$, but rather a $C^{\ast}$-algebra, then the situation is a bit different.
\begin{proposition}
  Let $S$ be an operator system, and let $B$ be a $C^{\ast}$-algebra. If $\phi\colon S\rightarrow B$ is a positive map, then $\phi$ is bounded, with
  \begin{align*}
    \norm{\phi} &\leq 2\norm{\phi\left( 1_A \right)}.
  \end{align*}
\end{proposition}
\begin{proof}
  Note that if $p$ is positive, then $0\leq p \leq \norm{p}1_A$, so $0\leq \phi(p)\leq \norm{p}\phi\left( 1_A \right)$ since positive functions are order-preserving. Thus, we get $\norm{\phi(p)}\leq \norm{p}\norm{\phi(1)}$ when $p\geq 0$.\newline

  Note that when $p_1$ and $p_2$ are positive, then $\norm{p_1 - p_2}\leq \max\set{\norm{p_1},\norm{p_2}}$. If $h$ is self-adjoint, then we have
  \begin{align*}
    \norm{\phi(h)} &= \frac{1}{2}\phi\left( \norm{h}1_A + h \right) - \frac{1}{2}\phi\left( \norm{h}1_A - h \right),
  \end{align*}
  which is the difference of two positive elements in $B$. Thus, we have
  \begin{align*}
    \norm{\phi(h)} &\leq \frac{1}{2}\max\set{\norm{\phi\left( \norm{h}1_A + h \right)},\norm{\phi\left( \norm{h}1_A - h \right)}}\\
                   &\leq \norm{h}\norm{\phi(1)}.
  \end{align*}
  Finally, if $a$ is arbitrary then write $a = h + ik$ via the Cartesian decomposition, where $\norm{h},\norm{k}\leq \norm{a}$, and $h,k$ are self-adjoint. Thus, we have
  \begin{align*}
    \norm{\phi(a)} &\leq \norm{\phi(h)} + \norm{\phi(k)}\\
                   &\leq 2\norm{a}\norm{\phi\left(1_A\right)}.
  \end{align*}
\end{proof}
As it turns out, $2$ is the best constant.
\begin{example}
  Let $\T$ be the unit circle in $\C$, and $C(\T)$ be the continuous functions on $z$. Let $z$ be the cordinate function, and let $S\subseteq C\left( \T \right)$ be the subspace spanned by $1,z,\overline{z}$. Defining
  \begin{align*}
    \phi\left( a + bz + c\overline{z} \right) &= \begin{pmatrix}a & 2b \\ 2c & a\end{pmatrix},
  \end{align*}
  An element of $S$ is positive if and only if $c = \overline{b}$ and $a \geq 2\left\vert b \right\vert$, and an element of $\Mat_2\left( \C \right)$ is positive if and only if its diagonal entries and determinant are nonnegative real numbers. Thus, it is the case that $\phi$ is a positive map, but also 
  \begin{align*}
    2\norm{\phi(1)} &= 2\\
                    &= \norm{\phi(z)}\\
                    &\leq \norm{\phi},
  \end{align*}
  meaning $\norm{\phi} = 2\norm{\phi(1)}$.
\end{example}
We are interested in seeing when unital, positive maps are contractive.
\begin{lemma}
  Let $A$ be a $C^{\ast}$-algebra, and let $p_i$ be positive elements of $A$ such that
  \begin{align*}
    \sum_{i=1}^{n}p_i \leq 1.
  \end{align*}
  If $\lambda_i$ are scalars with $\left\vert \lambda_i \right\vert \leq 1$, then
  \begin{align*}
    \norm{\sum_{i=1}^{n}\lambda_ip_i} &\leq 1.
  \end{align*}
\end{lemma}
\begin{proof}
  Note that
  \begin{align*}
    \begin{pmatrix}\sum_{i=1}^{n}\lambda_ip_i & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 \end{pmatrix} &= \begin{pmatrix}p_1^{1/2} &\cdots & p_n^{1/2} \\ 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0\end{pmatrix} \operatorname{diag}\left( \lambda_1,\dots,\lambda_n \right) \begin{pmatrix}p_1^{1/2} & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ p_n^{1/2} & 0 & \cdots & 0\end{pmatrix}.
  \end{align*}
  The norm on the matrix on the left is $\norm{\sum_{i=1}^{n}\lambda_ip_i}$, while the three matrices on the right have norm less than $1$, using the fact that $\norm{a^{\ast}a} = \norm{a}^2$.
\end{proof}
\begin{theorem}
  Let $B$ be a $C^{\ast}$-algebra, $X$ a compact Hausdorff space, and $C(X)$ the continuous functions on $X$. Let $\phi\colon C(X)\rightarrow B$ be a positive map. Then, $\norm{\phi} = \norm{\phi(1)}$.
\end{theorem}
\begin{proof}
  We may assume $\phi(1)\leq 1$. Let $f\in C(X)$ with $\norm{f}\leq 1$, and let $\ve > 0$. Now, we may choose a finite open cover $\set{U_i}_{i=1}^{n}$ of $X$ such that $\left\vert f(x) - f\left( x_i \right) \right\vert < \ve$ for all $x\in U_i$, and let $\set{p_i}_{i = 1}^{n}$ be a partition of unity subordinate to the cover. That is, $\set{p_i}_{i=1}^{n}$ are nonnegative continuous functions satisfying $\sum_{i=1}^{n}p_i = 1$ and $p_i(x) = 0$ for $x\notin U_i$.\newline

  Set $\lambda_i = f\left( x_i \right)$, and note that if $p_i(x)\neq 0$ for some $i$, then $x\in U_i$ and $\left\vert f(x) - \lambda_i \right\vert < \ve$. Hence, for any $x$, we have
  \begin{align*}
    \left\vert f(x) - \sum_{i=1}^{n}\lambda_ip_i(x) \right\vert &= \left\vert \sum_{i=1}^{n}\left( f(x) - \lambda_i \right)p_i(x) \right\vert\\
                                                                &\leq \sum_{i=1}^{n}\left\vert f(x) - \lambda_i \right\vert p_i(x)\\
                                                                &< \sum_{i=1}^{n}\ve p_i(x)\\
                                                                &= \ve.
  \end{align*}
  By above, we know that $\norm{\sum_{i=1}^{n}\lambda_ip_i} \leq 1$, we have
  \begin{align*}
    \norm{\phi(f)} &\leq \norm{\phi\left( f - \sum_{i=1}^{n}\lambda_ip_i \right)} + \norm{\sum_{i=1}^{n}\phi\left( p_i \right)}\\
                   &< 1 + \ve\norm{\phi}.
  \end{align*}
  Since $\ve$ was arbitrary, we have $\norm{\phi} \leq 1$.
\end{proof}
\begin{lemma}[Riesz--FejÃ©r Theorem]
  Let $\tau\left( e^{i\theta} \right) = \sum_{n=-N}^{N} a_ne^{in\theta}$ be a strictly positive function on $\T$. Then, there is a polynomial $p(z) = \sum_{n=0}^{n}p_nz^n$ such that
  \begin{align*}
    \tau\left( e^{i\theta} \right) &= \left\vert p\left( e^{i\theta} \right) \right\vert^{2}.
  \end{align*}
\end{lemma}
\begin{proof}
  Note that $\tau$ is real-valued, so $a_{-n} = \overline{a_n}$, and $a_0$ is real. Assuming $a_{-N} \neq 0$, we take $g(z) = \sum_{n=-N}^{N}a_nz^{n+N}$, so that $g$ is a polynomial of degree $2n$, $g(0)\neq 0$.\newline

  We have $g\left( e^{i\theta} \right) = \tau\left( e^{i\theta} \right)e^{iN\theta}\neq 0$, and that $\overline{g\left( 1/\overline{z} \right)} = z^{-2N}g(z)$.\newline

  We write the $2N$ zeros of $g$ as $z_1,\dots,z_N,1/\overline{z_1},\dots,1/\overline{z_N}$.\newline

  Set $q(z) = \left( z-z_1 \right)\cdots \left( z-z_N \right)$ and $h(z) = \left( z-1/\overline{z_1} \right)\cdots \left( z-1/\overline{z_N} \right)$. We have that
  \begin{align*}
    g(z) &= a_Nq(z)h(z),
  \end{align*}
  where
  \begin{align*}
    \overline{h(z)} &= \frac{\left( -1 \right)^N\overline{z}^Nq\left( 1/\overline{z} \right)}{z_1\cdots z_N}.
  \end{align*}
  Thus, we have
  \begin{align*}
    \tau\left( e^{i\theta} \right) &= e^{-iN\theta}g\left( e^{i\theta} \right)\\
                                   &= \left\vert g\left( e^{i\theta} \right) \right\vert\\
                                   &= \left\vert a_Nq\left( e^{i\theta} \right)\overline{h}\left( e^{i\theta} \right) \right\vert\\
                                   &= \frac{a_N}{z_1\cdots z_N}\left\vert q\left( e^{i\theta} \right) \right\vert^2.
  \end{align*}
\end{proof}
\begin{theorem}
  Let $T$ be an operator on $\mathcal{H}$ with $\norm{T}\leq 1$, and let $S\subseteq C\left( \T \right)$ be the operator system defined by
  \begin{align*}
    S &= \set{p\left( e^{i\theta} \right) + \overline{q\left( e^{i\theta} \right)} | p,q\text{ are polynomials}}.
  \end{align*}
  Then, $\phi\colon S\rightarrow \B\left( \mathcal{H} \right)$, given by $\phi\left( p + \overline{q} \right) = p(T) + q(T)^{\ast}$ is positive.
\end{theorem}
\begin{proof}
  It is enough to prove that $\phi(\tau)$ is positive for every \textit{strictly} positive $\tau$.\newline

  Let $\tau\left( e^{i\theta} \right)$ be strictly positive in $S$, meaning $\tau\left( e^{i\theta} \right) = \sum_{\ell,k=0}^{n}\alpha_{\ell}\overline{\alpha_{k}}e^{i\left( \ell - k \right)\theta}$. We must prove that
  \begin{align*}
    \phi\left( \tau \right) &= \sum_{\ell,k=0}^{n}\alpha_{\ell}\overline{\alpha_{k}} T\left( \ell - k \right),
  \end{align*}
  where
  \begin{align*}
    T\left( j \right) &= \begin{cases}
      T^j & j \geq 0\\
      \left( T^{\ast} \right)^{-j} & j < 0.
    \end{cases}
  \end{align*}
  Fix $x\in \mathcal{H}$. Note that
  \begin{align*}
    \iprod{\phi\left(\tau\right)(x)}{x} &= \iprod{ \begin{pmatrix}I & T^{\ast} & \cdots & \left( T^{\ast} \right)^{n}\\ T & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & T^{\ast} \\ T^{n} & \cdots & T & I\end{pmatrix} \begin{pmatrix}\overline{\alpha_1}x \\ \overline{\alpha_2}x \\ \vdots \\ \overline{\alpha_n} x\end{pmatrix} }{ \begin{pmatrix}\overline{\alpha_1}x \\ \overline{\alpha_2}x \\ \vdots \\ \overline{\alpha_n}x\end{pmatrix} },\tag*{(\textasteriskcentered)}
  \end{align*}
  where our matrix operator acts on $\mathcal{H}^{(n)}$. Thus, we only need to show that this matrix operator is positive.\newline

  To that end, define the $n\times n$ matrix
  \begin{align*}
    R &= \begin{pmatrix}0 & \cdots & \cdots & \cdots & 0 \\ T & \ddots & \ddots & \ddots & \vdots \\ 0 & \ddots & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots & \vdots \\ 0 & \cdots & 0 & T & 0\end{pmatrix},
  \end{align*}
  and note that $R^{n+1} = 0$, with $\norm{R}_{\op} \leq 1$ (as $T$ is a contraction).\newline

  We let $I$ denote the identity operator on $\mathcal{H}^{(n)}$. The matrix operator (\textasteriskcentered) can be written as
  \begin{align*}
    I + R + R^2 + \cdots + R^{n} + R^{\ast} + \cdots + \left( R^{\ast} \right)^{n} &= \left( I-R \right)^{-1} + \left( I-R^{\ast} \right)^{-1} - I,
  \end{align*}
  where we used the fact that $R^{n+1} = 0$ in the geometric series for $\left( I-R \right)^{-1}$ and $\left( I-R^{\ast} \right)^{-1}$. To see that this operator is positive, we let $h\in \mathcal{H}^{(n)}$, and let $h = \left( I-R \right)y$ for some $y\in \mathcal{H}^{(n)}$. Then,
  \begin{align*}
    \iprod{\left( \left( I-R \right)^{-1} + \left( I-R^{\ast} \right)^{-1} - I \right)\left( h \right)}{h} &= \iprod{y}{\left( I-R \right)y} + \iprod{\left( I-R \right)\left( y \right)}{y} - \iprod{\left( I-R \right)\left( y \right)}{\left( I-R \right)\left( y \right)}\\
                                                                                                           &= \norm{y}^2 - \norm{R\left( y \right)}^2\\
                                                                                                           &\geq 0,
  \end{align*}
  since $R$ is a contraction.
\end{proof}
Now, we may prove von Neumann's inequality in a different way.
\begin{theorem}[von Neumann's Inequality] 
  Let $T$ be an operator on a Hilbert space with $\norm{T}_{\op}\leq 1$. Then, for any polynomial $p$, we have
  \begin{align*}
    \norm{p(T)}_{\op} &\leq \norm{p},
  \end{align*}
  where $\norm{p} = \sup_{\theta} \left\vert p\left( e^{i\theta} \right) \right\vert$.
\end{theorem}
\begin{proof}
  The operator system defined by
  \begin{align*}
    S &= \set{p\left( e^{i\theta} \right) + \overline{q\left( e^{i\theta} \right)} | p,q\text{ polynomials}}
  \end{align*}
  is a $\ast$-algebra that separates points, so by the Stone--Weierstrass theorem, $S$ is dense in $C\left(\mathbb{T}\right)$. We know that $\phi$ is bounded, so it extends $C(\T)$. The extension to $\overline{S} = C(\T)$ also positive, so $\phi$ is contractive.
\end{proof}
Note that if $A\left( \D \right)$ denotes the functions analytic on $\D$ and continuous on $\overline{\D}$, we know that by the maximum modulus principle that the supremum of any function in $A(\D)$ occurs on $\T$. We may thus consider $A(\D)$ as a closed subalgebra of $C(\T)$.\newline

Furthermore, polynomials are dense in $A(\D)$. Thus, the homomorphism $p\mapsto p(T)$ extends to a homomorphism $f\mapsto f(T)$ that satisfies $\norm{f(T)}_{\op}\leq \norm{f}$ for all $f\in A(\D)$.\newline

Another consequence is that if $a$ is an element of some unital $C^{\ast}$-algebra $A$ with $\norm{a}\leq 1$, then there is a unital, positive map $\phi\colon C(\T)\rightarrow A$ such that $\phi(p) = p(a)$.
\begin{corollary}
  Let $B$ and $C$ be unital $C^{\ast}$-algebras. Let $A$ be a unital subalgebra of $B$, and let $S = A + A^{\ast}$ be an operator space. If $\phi\colon S\rightarrow C$ is positive, then $\norm{\phi(a)}\leq \norm{\phi(1)}\norm{a}$.
\end{corollary}
\begin{proof}
  Let $a\in A$ with $\norm{a}\leq 1$. We may extend $\phi$ to a positive map on $\overline{S}$. There is also a positive map $\psi\colon C(\T)\rightarrow B$ with $\psi\left( p \right) = p(a)$. Since $A$ is an algebra, we must have $\Ran\left( \psi \right)\subseteq \overline{S}$.\newline

  The composition of positive maps is positive, so we have
  \begin{align*}
    \norm{\phi(a)} &= \norm{\phi\circ\psi\left( e^{i\theta} \right)}\\
                   &\leq \norm{\phi\circ \psi(1)}\norm{e^{i\theta}}\\
                   &= \norm{\phi(1)}.
  \end{align*}
\end{proof}
If $\phi(1) = 1$, then $\phi$ is a contraction on $A$, though $\phi$ may not be a contraction on all of $S$.
\begin{corollary}
  Let $A$ and $B$ be unital $C^{\ast}$-algebras with $\phi\colon A\rightarrow B$ a positive map. Then, $\norm{\phi}_{\op}= \norm{\phi(1)}$.
\end{corollary}
\begin{lemma}
  Let $A$ be a $C^{\ast}$-algebra, $S\subseteq A$ an operator system, and $f\colon S\rightarrow \C$ a linear functional with $f(1) = 1 = \norm{f}$. If $a$ is a normal element of $A$, and $a\in S$, then $f(a)\in \overline{\operatorname{conv}}\left( \sigma\left( a \right) \right)$.
\end{lemma}
\begin{proof}
  Suppose not.\newline

  The convex hull of a compact set is the intersection of all closed disks containing the set. Then, there exists $\lambda$ and $r > 0$ such that $\left\vert f(a) - \lambda \right\vert > r$, where
  \begin{align*}
    \sigma\left( a \right) &\subseteq \set{z | \left\vert z - \lambda \right\vert \leq r}.
  \end{align*}
  Then, $\sigma\left( a - \lambda 1 \right)\subseteq \set{z | \left\vert z \right\vert \leq r}$. Since norm and spectral radius agree for normal elements, we have $\norm{a - \lambda 1} \leq r$, while $\left\vert f\left( a - \lambda 1 \right) \right\vert > r$. This contradicts the fact that $\norm{f}\leq 1$.
\end{proof}
\begin{proposition}
  Let $S$ be an operator system, $B$ a unital $C^{\ast}$-algebra, and let $\phi\colon S\rightarrow B$ be a unital contraction. Then, $\phi$ is positive.
\end{proposition}
\begin{proof}
  Since we can represent $B$ on $\B\left( \mathcal{H} \right)$, we assume $B = \B\left( \mathcal{H} \right)$ for some Hilbert space $\mathcal{H}$. Fix $x\in \mathcal{H}$ with $\norm{x} = 1$.\newline

  Setting $f(a) = \iprod{\phi(a)(x)}{x}$, we have $f(1) = 1$ and $\norm{f}\leq \norm{\phi}$. If $a$ is positive, then $f(a)$ is positive by the previous lemma, so since $x$ was arbitrary, $\phi(a)$ is also positive.
\end{proof}
\begin{proposition}
  Let $A$ be a unital $C^{\ast}$-algebra, and let $M$ be a unital subspace of $A$. If $B$ is a unital $C^{\ast}$-algebra, and $\phi\colon M\rightarrow B$ is a unital contraction, then the map $ \widetilde{\phi}\colon M + M^{\ast}\rightarrow B $, given by
  \begin{align*}
    \widetilde{\phi}\left( a + b^{\ast} \right) = \phi\left( a \right) + \phi\left( b \right)^{\ast}
  \end{align*}
  is well-defined and the unique positive extension of $\phi$ to $M + M^{\ast}$.
\end{proposition}
\begin{proof}
  To prove that $ \widetilde{\phi} $ is well-defined, it is enough to prove that if $a$ and $a^{\ast}$ belong to $M$, then $\phi\left( a \right)^{\ast} = \phi\left( a^{\ast} \right)$. Set
  \begin{align*}
    S_1 &= \set{a | a\in M\text{ and }a^{\ast}\in M}.
  \end{align*}
  Then, $S_1$ is an operator system, and $\phi$ is a unital, contractive map on $S_1$, hence positive by the previous proposition. Since $\phi$ is positive, $\phi$ is self-adjoint, so $\phi\left( a^{\ast} \right) = \phi\left( a \right)^{\ast}$, meaning $\widetilde{\phi}$ is well-defined.\newline

  To see that $\widetilde{\phi}$ is positive, we may assume $B = \B\left( \mathcal{H} \right)$. Fix $x\in S_{\mathcal{H}}$, and set $\widetilde{\rho}\left( a \right) = \iprod{\widetilde{\phi}\left( a \right)\left( x \right)}{x}$. We will show that $\widetilde{\rho}$ is positive.\newline

  Let $\rho\colon M\rightarrow C$ be defined by $\rho(a) = \iprod{\phi(a)(x)}{x}$. Then, $\norm{\rho} = 1$, and so by the Hahn--Banach theorem, $\rho$ extends to $\rho_1\colon M + M^{\ast}\rightarrow \C$ with $\norm{\rho_1} = 1$. Since $\rho_1$ is positive, $\rho_1\left( a + b^{\ast} \right) = \rho\left( a \right) + \overline{\rho\left( b \right)} = \widetilde{\rho}\left( a + b^{\ast} \right)$. Thus $\widetilde{\rho}$ is positive.
\end{proof}
\subsection{Completely Positive Maps}%
\begin{definition}
  If $A$ is a $C^{\ast}$ algebra and $M\subseteq A$ is a linear subspace, then we call $M$ an operator space. 
\end{definition}
We may regard $\Mat_n\left( M \right)$ as a subspace of $\Mat_n\left( A \right)$, with the norm structure inherited from the unique norm structure on $\Mat_n\left( A \right)$. The primary distinguishing feature of an operator space is the fact that $\Mat_n\left( M \right)$ has a unique norm for all $n\geq 1$.\newline

Similarly, if $S\subseteq A$ is an operator system, then we endow $\Mat_n\left( S \right)$ with the norm and order it inherits from $\Mat_n\left( A \right)$.
\begin{definition}
  If a matrix $S\in \Mat_n\left( \C \right)$ is positive definite and Hermitian, then $S$ is positive.
\end{definition}
\begin{proof}
  If $S$ is Hermitian, then we know that all the eigenvalues of $S$ are real and that $S$ is diagonalizable with orthonormal vectors $\set{v_1,\dots,v_n}$. Therefore, if 
  \begin{align*}
    \iprod{S\left( x \right)}{x} &\geq 0
  \end{align*}
  for all $x\in \C^n$, then so too does this hold for $v_j$ and corresponding $\lambda_j$. Thus, $\lambda_j\geq 0$ for all $j$, so $S$ is positive.
\end{proof}
\begin{lemma}[Ordering of $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$]
  We have that $\left( T_{ij} \right)_{ij}\in \Mat_{n}\left( \B\left( \mathcal{H} \right) \right)_{+}$ if and only if, for all $x_1,\dots,x_n\in \mathcal{H}$, we have $\left( \iprod{T_{ij}\left( x_j \right)}{x_i} \right)_{ij}\in \Mat_n\left( \C \right)_{+}$.
\end{lemma}
\begin{definition}
  If $B$ is a $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ is a linear map, then $\phi_n\colon \Mat_n\left( S \right)\rightarrow \Mat_n\left( B \right)$ is defined by $\phi_n\left( \left( a_{ij} \right)_{ij} \right) = \left( \phi\left( a_{ij} \right) \right)_{ij}$. We call $\phi$ $n$-positive if $\phi_n$ is positive, and $\phi$ is called completely positive if it is $n$-positive for all $n$.\newline

  We call $\phi$ completely bounded if $\sup_{n}\norm{\phi_n}$ is finite. We set
  \begin{align*}
    \norm{\phi}_{\cb} &= \sup_{n}\norm{\phi_n}.
  \end{align*}
  We say $\phi$ is completely isometric or completely contractive if each $\phi_n$ is isometric and that $\norm{\phi}_{\cb} \leq 1$ respectively.
\end{definition}
We investigate some of the properties of classes of completely positive maps such that we may prove when they are automatically completely positive.
\begin{lemma}
  Let $A$ be a $C^{\ast}$-algebra, and let $a,b\in A$. Then, the following hold.
  \begin{enumerate}[(i)]
    \item We have $\norm{a}\leq 1$ if and only if
      \begin{align*}
        \begin{pmatrix}1 & a \\ a^{\ast} & 1\end{pmatrix}
      \end{align*}
      is positive in $\Mat_2\left( A \right)$.
    \item We have
      \begin{align*}
        \begin{pmatrix}1 & a \\ a^{\ast} & b\end{pmatrix}
      \end{align*}
      is positive in $\Mat_2\left( A \right)$ if and only if $a^{\ast}a \leq b$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  Let $A$ be represented by $\pi\colon A\rightarrow \B\left( \mathcal{H} \right)$, and set $T = \pi(a)$. If $\norm{T}\leq 1$, then for any $x,y\in \mathcal{H}$, we have
  \begin{align*}
    \iprod{ \begin{pmatrix}I & T \\ T^{\ast} & I\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{T\left( y \right)}{x} + \iprod{x}{T\left( y \right)} + \iprod{y}{y}\\
                             &\geq \norm{x}^2 - 2\norm{T}_{\op}\norm{y}\norm{x} + \norm{y}^2\\
                             &\geq 0.
  \end{align*}
  Conversely, if $\norm{T}_{\op} > 1$, then there exist unit vectors $x$ and $y$ such that $ \iprod{T\left( y \right)}{x} < -1 $, and the above inner product is negative.\newline

  %Now, to see (ii), we begin by assuming that $b\geq a^{\ast}a$. Just as before, we represent $A$ on $\B\left( \mathcal{H} \right)$, and we use the abuse of notation as $b = \pi(b)$ and $a = \pi(a)$. Since $b\geq a^{\ast}a$, we have, for all $y\in \mathcal{H}$,
  %\begin{align*}
  %  \iprod{\left( b-a^{\ast}a \right)\left( y \right)}{y} &\geq 0,
  %\end{align*}
  %so that
  %\begin{align*}
  %  \iprod{b\left( y \right)}{y} \geq \norm{a\left( y \right)}^2.
  %\end{align*}
  %Thus, in the $2\times 2$ case, we have, for any $ \begin{pmatrix}x \\ y\end{pmatrix} \in \mathcal{H}^{(2)} $,
  %\begin{align*}
  %  \iprod{ \begin{pmatrix}1 & a \\ a^{\ast} & b\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{a(y)}{x} + \iprod{a^{\ast}\left( x \right)}{y} + \iprod{b\left( y \right)}{y}\\
  %                           &\geq \iprod{x}{x} + \iprod{a(y)}{x} + \iprod{a^{\ast}(x)}{y} + \iprod{a(y)}{a(y)}\\
  %                           &= \iprod{x}{x} + \iprod{a(y)}{a(y)} + 2\re\left( \iprod{a(y)}{x} \right)\\
  %                           &\geq \iprod{x}{x} + \iprod{a(y)}{a(y)} - 2 \norm{a(y)}\norm{x}\\
  %                           &= \norm{x}^2 + \norm{a(y)}^2 - 2\norm{a(y)}\norm{x}\\
  %                           &\geq 0.
  %\end{align*}
  %Thus, the matrix is positive.\newline

  %Now, we suppose that $b\ngeq a^{\ast}a$. Then, there exists some $y\in \mathcal{H}$ such that $ \iprod{b\left( y \right)}{y} < \norm{a(y)}^2 $.
\end{proof}
\begin{exercise}[Exercise 3.2]
  Let $P,Q,A$ be operators on a Hilbert space $\mathcal{H}$, with $P,Q$ positive.
  \begin{enumerate}[(i)]
    \item Show that
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix}\geq 0
      \end{align*}
      if and only if
      \begin{align*}
        \left\vert \iprod{Ax}{y} \right\vert^2 &\leq \iprod{Py}{y} \iprod{Qx}{x}.
      \end{align*}
    \item Show that
      \begin{align*}
        \begin{pmatrix}1 & A \\ A^{\ast} & B\end{pmatrix} &\geq 0
      \end{align*}
      if and only if $B\geq A^{\ast}A$.
    \item Show that if
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} &\geq 0,
      \end{align*}
      then for any $x\in \mathcal{H}$, we have
      \begin{align*}
        0 &\leq \iprod{\left( P + A + A^{\ast} + Q \right)x}{x}\\
          &\leq \left( \sqrt{ \iprod{Px}{x} } + \sqrt{ \iprod{Qx}{x} }\right) ^2,
      \end{align*}
      hence
      \begin{align*}
        \norm{P + AA^{\ast} + Q} &\leq \left( \norm{P}^{1/2} + \norm{Q}^{1/2} \right)^2.
      \end{align*}
    \item Show that if
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & P\end{pmatrix} &\geq 0,
      \end{align*}
      then $A^{\ast}A \leq \norm{P}P$, implying $\norm{A}\leq \norm{P}$.
  \end{enumerate}
\end{exercise}
\begin{solution}\hfill
  \begin{enumerate}[(i)]
    \item We see that
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} \geq 0
      \end{align*}
      if and only if, for any $x,y\in \mathcal{H}$, we have
      \begin{align*}
        \begin{pmatrix} \iprod{Px}{x} & \iprod{Ay}{x} \\ \iprod{A^{\ast}x}{y} & \iprod{Qy}{y}\end{pmatrix} &\geq 0.
      \end{align*}
      Thus, we have
      \begin{align*}
        \det \begin{pmatrix} \iprod{Px}{x} & \iprod{Ay}{x} \\ \iprod{A^{\ast}x}{y} & \iprod{Qy}{y}\end{pmatrix} &= \iprod{Px}{x} \iprod{Qy}{y} - \left\vert \iprod{Ay}{x} \right\vert^2\\
                                           &\geq 0,
      \end{align*}
      so that
      \begin{align*}
        \left\vert \iprod{Ay}{x} \right\vert^2 &\leq \iprod{Px}{x} \iprod{Qy}{y}.
      \end{align*}
      Suppose that
      \begin{align*}
        \left\vert \iprod{Ay}{x} \right\vert^2 &\leq \iprod{Px}{x} \iprod{Qy}{y}.
      \end{align*}
      Now, for any $x,y\in \mathcal{H}$, we have
      \begin{align*}
        \iprod{ \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{Px}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{Qy}{y}\\
                                 &= \iprod{Px}{x} + 2\re \left( \iprod{Ay}{x} \right) + \iprod{Qy}{y}\\
                                 &\geq \iprod{Px}{x} - 2\left\vert \iprod{Ay}{x} \right\vert + \iprod{Qy}{y}\\
                                 &\geq \iprod{Px}{x} - 2 \iprod{Px}{x}^{1/2} \iprod{Qy}{y}^{1/2} + \iprod{Qy}{y}\\
                                 &= \left( \iprod{Px}{x}^{1/2}  + \iprod{Qy}{y}^{1/2} \right)^{2}\\
                                 &\geq 0.
      \end{align*}
    \item We begin by assuming that $B\geq A^{\ast}A$. Since $B\geq A^{\ast}A$, we have
    \begin{align*}
      \iprod{\left( B-A^{\ast}A \right)\left( y \right)}{y} &\geq 0,
    \end{align*}
    so that
    \begin{align*}
      \iprod{By}{y} \geq \norm{Ay}^2.
    \end{align*}
    Thus, in the $2\times 2$ case, we have, for any $ \begin{pmatrix}x \\ y\end{pmatrix} \in \mathcal{H}^{(2)} $,
    \begin{align*}
      \iprod{ \begin{pmatrix}1 & A \\ A^{\ast} & B\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{By}{y}\\
                               &\geq \iprod{x}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{Ay}{Ay}\\
                               &= \iprod{x}{x} + \iprod{Ay}{Ay} + 2\re\left( \iprod{Ay}{x} \right)\\
                               &\geq \iprod{x}{x} + \iprod{Ay}{Ay} - 2 \norm{Ay}\norm{x}\\
                               &= \norm{x}^2 + \norm{Ay}^2 - 2\norm{Ay}\norm{x}\\
                               &\geq 0.
    \end{align*}
    Thus, the matrix is positive.\newline

    For the converse direction, we suppose $B\ngeq A^{\ast}A$. Then, there is some $y\in \mathcal{H}$ such that $ \iprod{\left( B-A^{\ast}A \right)(y)}{y} < 0$. This gives $ \iprod{By}{y} < \norm{Ay}^2 $. We may select $y$ such that $\norm{Ay}^2 = 1$. Setting $x = -Ay$, we have
    \begin{align*}
      \iprod{ \begin{pmatrix}1 & A \\ A^{\ast} & B\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{By}{y}\\
                               &= \iprod{x}{x} + \iprod{Ay}{x} + \iprod{x}{Ay} + \iprod{By}{y}\\
                               &= \iprod{-Ay}{-Ay} + \iprod{Ay}{-Ay} + \iprod{-Ay}{Ay} + \iprod{By}{y}\\
                               &= \norm{Ay}^2 - 2\norm{Ay}^2 + \iprod{By}{y}\\
                               &= -1 + \iprod{By}{y}\\
                               &< -1 + \norm{Ay}^2\\
                               &=0.
    \end{align*}
    Thus, the matrix is negative.
  \item We apply the result in (i) to the vector $ \begin{pmatrix}x\\x\end{pmatrix} $. This gives
    \begin{align*}
      \iprod{ \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} \begin{pmatrix}x\\x\end{pmatrix} }{ \begin{pmatrix}x\\x\end{pmatrix} } &= \iprod{Px}{x} + \iprod{Ax}{x} + \iprod{A^{\ast}x}{x} + \iprod{Qx}{x}\\
                               &= \iprod{Px}{x} + 2\re \left( \iprod{Ax}{x} \right) + \iprod{Qx}{x}\\
                               &\leq \iprod{Px}{x} + 2 \left\vert \iprod{Ax}{x} \right\vert + \iprod{Qx}{x}\\
                               &\leq \iprod{Px}{x} + 2 \iprod{Px}{x}^{1/2} \iprod{Qx}{x}^{1/2} + \iprod{Qx}{x}\\
                               &= \left( \iprod{Px}{x}^{1/2} + \iprod{Qx}{x}^{1/2} \right)^2.
    \end{align*}
  \item Setting $Q = P$ in the result from (i), we have
    \begin{align*}
      \left\vert \iprod{Ay}{x} \right\vert^2 &\leq \iprod{Px}{x} \iprod{Py}{y},
    \end{align*}
    which holds for all $x,y\in \mathcal{H}$. In particular, setting $x = Ay$, we have
    \begin{align*}
      \left\vert \iprod{Ay}{Ay} \right\vert &\leq \iprod{PAy}{Ay} \iprod{Py}{y}\\
                                            &\leq \norm{PAy}\norm{Ay} \iprod{Py}{y}\\
                                            &\leq \norm{P} \norm{Ay}^2 \iprod{Py}{y}.
    \end{align*}
    This gives
    \begin{align*}
      \norm{Ay}^4 &\leq \norm{P}\norm{Ay}^2 \iprod{Py}{y}\\
      \norm{Ay}^2 &\leq \norm{P} \iprod{Py}{y}\\
      \iprod{A^{\ast}Ay}{y} &\leq \norm{P} \iprod{Py}{y},
    \end{align*}
    or that $A^{\ast}A \leq \norm{P}P$.
  \end{enumerate}
\end{solution}
\begin{proposition}
  Let $S$ be an operator system, $B$ a unital $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ a unital $2$-positive map. Then, $\phi$ is contractive.
\end{proposition}
\begin{proof}
  Let $a\in S$ with $\norm{a}\leq 1$. Then,
  \begin{align*}
    \phi_2 \begin{pmatrix}1 & a \\ a^{\ast} & 1\end{pmatrix} &= \begin{pmatrix}1 & \phi(a) \\ \phi(a)^{\ast} & 1\end{pmatrix}
  \end{align*}
  is positive, hence $\norm{\phi(a)}\leq 1$.
\end{proof}
\begin{proposition}[Cauchy--Schwarz for 2-positive Maps]
  Let $A,B$ be unital $C^{\ast}$-algebras, and let $\phi\colon A\rightarrow B$ be a unital $2$-positive map. Then,
  \begin{align*}
    \phi(a)^{\ast}\phi(a) &\leq \phi\left( a^{\ast}a \right)
  \end{align*}
  for all $a\in A$.
\end{proposition}
\begin{proof}
  We have that
  \begin{align*}
    \begin{pmatrix}1 & a \\ 0 & 0\end{pmatrix}^{\ast} \begin{pmatrix}1 & a \\ 0 & 0\end{pmatrix} &= \begin{pmatrix}1 & \phi(a) \\ \phi(a)^{\ast} & \phi\left( a^{\ast}a \right)\end{pmatrix}\\
                     &\geq 0,
  \end{align*}
  meaning that $\phi(a)^{\ast}\phi(a)\leq \phi\left( a^{\ast}a \right)$ by above.
\end{proof}
\begin{proposition}
  Let $A$ and $B$ be unital $C^{\ast}$-algebras, and let $M$ be a unital subspace of $M$, with $S = M + M^{\ast}$. If $\phi\colon M\rightarrow B$ is unital and $2$-contractive, then $\widetilde{\phi}\colon S\rightarrow B$ given by $\widetilde{\phi}\left( a + b^{\ast} \right) = \phi(a) + \phi(b)^{\ast}$ is $2$-positive and contractive.
\end{proposition}
\begin{proof}
  Since $\phi$ is contractive, we know from above that $\widetilde{\phi}$ is well-defined. Furthermore, note that
  \begin{align*}
    \Mat_2\left( S \right) &= \Mat_2\left( M \right) + \Mat_2\left( M \right)^{\ast},
  \end{align*}
  and
  \begin{align*}
    \left( \widetilde{\phi} \right)_2 &= \left( \widetilde{\phi}_2 \right).
  \end{align*}
  Now, since $\phi_2$ is contractive, we have that $\widetilde{\phi}_2$ is positive, so $\widetilde{\phi}$ is contractive.
\end{proof}
\begin{proposition}
  Let $A$ and $B$ be unital $C^{\ast}$-algebras, let $M$ be a unital subspace, and let $S = M + M^{\ast}$. If $\phi\colon M\rightarrow B$ is unital and completely contractive, then $\widetilde{\phi}\colon S\rightarrow B$ is completely positive and completely contractive.
\end{proposition}
\begin{proof}
  Since $\phi_n$ is unital and contractive, $\widetilde{\phi}_n$ is positive. Additionally, since $\left( \widetilde{\phi}_n \right)_2$ is positive, $\widetilde{\phi}_n$ is contractive.
\end{proof}
Note that since $\Mat_{2}\left( \Mat_n\left( A \right) \right) \cong \Mat_{2n}\left( A \right)$ are $\ast$-isomorphic, the norm on $\Mat_2\left( \Mat_n\left( A \right) \right)$ is equal to the norm on $\Mat_{2n}\left( A \right)$.\newline

Now, we may see some examples that belong to these categories.
\begin{example}
  If $A$ and $B$ are $C^{\ast}$-algebras, and $\pi\colon A\rightarrow B$ is a $\ast$-homomorphism, then $\pi$ is completely positive and completely contractive, since each $\pi_n\colon \Mat_n\left( A \right)\rightarrow \Mat_n\left( B \right)$ is a $\ast$-homomorphism, and $\ast$-homomorphisms are both positive and contractive.
\end{example}
\begin{example}
  Fixing $x,y\in A$, we may define $\phi\colon A\rightarrow A$ by $\phi(a) = xay$. Note that if $\left( a_{ij} \right)_{ij}\in \Mat_n\left( A \right)$, then
  \begin{align*}
    \norm{\phi_n\left( \left( a_{ij} \right)_{ij} \right)} &= \norm{\left( xa_{ij}y \right)_{ij}}\\
                                                           &= \norm{\left( xI_{n} \right) \begin{pmatrix}a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn}\end{pmatrix} \left( yI_n \right)}\\
                                                           &\leq \norm{x}\norm{\left( a_{ij} \right)_{ij}}\norm{y}.
  \end{align*}
  This means $\phi$ is completely bounded with $\norm{\phi}_{\cb}\leq \norm{x}\norm{y}$. Similarly, if $x = y^{\ast}$, then $\phi_n$ is positive.\newline

  This gives us the archetype of a completely bounded map. If $\mathcal{H}_1$ and $\mathcal{H}_2$ are Hilbert spaces, and $v_i\colon \mathcal{H}_1\rightarrow \mathcal{H}_2$ are bounded operators for $i=1,2$, then if $\pi\colon A\rightarrow \B\left( \mathcal{H}_2 \right)$ is a $\ast$-homomorphism, we may define $\phi\colon A\rightarrow \B\left( \mathcal{H}_1 \right)$ by $\phi(a) = v_2^{\ast}\pi(a)v_1$. This function $\phi$ is completely bounded with $\norm{\phi}_{\cb}\leq \norm{v_1}\norm{v_2}$.\newline

  In fact, we will show that \textit{every} completely bounded map is of this form.
\end{example}
\begin{proposition}
  Let $S\subseteq A$ be an operator system, $B$ a $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ completely positive. Then, $\phi$ is completely bounded, and $\norm{\phi(1)} = \norm{\phi} = \norm{\phi}_{\cb}$.
\end{proposition}
\begin{proof}
  We have $\norm{\phi(1)}\leq \norm{\phi}\leq \norm{\phi}_{\cb}$, so it is sufficient to show that $\norm{\phi}_{\cb}\leq \norm{\phi(1)}$. Let $A = \left( a_{ij} \right)_{ij}$ be in $\Mat_n\left( S \right)$ with $\norm{A} \leq 1$, and let $I_n$ be the unit of $\Mat_n\left( A \right)$. Then, since
  \begin{align*}
    T = \begin{pmatrix}I_n & A \\ A^{\ast} & I_n\end{pmatrix}
  \end{align*}
  is positive, the map
  \begin{align*}
    \phi_{2n} \begin{pmatrix}\begin{pmatrix}I_n & A \\ A^{\ast} & I_n\end{pmatrix}\end{pmatrix}  &= \begin{pmatrix} \begin{pmatrix}\phi_n\left( I_n \right) & \phi_n\left( A \right) \\ \phi_n\left( A \right)^{\ast} & \phi_n\left( I_n \right)\end{pmatrix} \end{pmatrix}
  \end{align*}
  is positive, so $\norm{\phi_n\left( A \right)} \leq \norm{\phi_n\left( I_n \right)} = \norm{\phi(1)}$.
\end{proof}
\subsubsection{Schur Products and Tensor Products}%
We will apply the previous results on positive and completely positive maps on the Schur product.
\begin{definition}
  If $A = \left( a_{ij} \right)_{ij}$ and $B = \left( b_{ij} \right)_{ij}$, then the Schur product is defined by
  \begin{align*}
    A\ast B &= \left( a_{ij}b_{ij} \right)_{ij}.
  \end{align*}
\end{definition}
Note that for a fixed $A$, we get a linear map
\begin{align*}
  S_A\left( B \right) &= A\ast B.
\end{align*}
To study the Schur product, we review some results on tensor products.\newline

Let $A\in \Mat_n\left( \C \right)$ and $B\in \Mat_m\left( \C \right)$. Then, $A\otimes B$ is the linear transformation on $\C^n\otimes \C^m = \C^{nm}$, defined by $A\otimes B\left( x\otimes y \right) = Ax\otimes By$ with the unique linear extension provided by the tensor product.\newline

Note that we have $\norm{A\otimes B} = \norm{A}\norm{B}$, which is shown by writing $A\otimes B = \left( A\otimes I \right)\left( I\otimes B \right)$.\newline

Now, letting $\set{e_1,\dots,e_n}$ and $\set{f_1,\dots,f_m}$ be our canonical orthonormal bases for $\C^n$ and $\C^m$ respectively, we may order our basis as $e_1\otimes f_i$, then $e_2\otimes f_i$, etc., yielding the block matrices for $A\otimes B$ is
\begin{align*}
  \begin{pmatrix}a_{11}B & \cdots & a_{1n}B \\ \vdots & \ddots & \vdots \\ a_{n1}B &\cdots & a_{nn}B\end{pmatrix}.
\end{align*}
This matrix is known as the Kronecker product of $A$ and $B$. Now, similarly, we may order our basis by $e_i\otimes f_1$, then $e_i\otimes f_2$, etc., yielding a different block matrix of the form
\begin{align*}
  \begin{pmatrix}b_{11}A &\cdots & b_{1m}A \\ \vdots & \ddots & \vdots \\ b_{m1}A & \cdots & b_{mm} A\end{pmatrix},
\end{align*}
which is the Kronecker product of $B$ and $A$.\newline

Now, since both of these matrices represent the same linear transformation, they are unitarily equivalent, given by the permutation matrix that reorders the basis vectors. One obtains the $(k,\ell)$ entry of the $(i,j)$ block of $b_{ij}A$ by taking the $(i,j)$ entry of the $(k,\ell)$ block $a_{k,\ell}B$. We will call this the \textit{canonical shuffle}.\newline

Now, we let $A$ and $B$ be elements of $\Mat_n\left( \C \right)$, and define $V\colon \C^n\rightarrow \C^n\otimes \C^n$ to be the isometry given by $V\left( e_i \right) = e_i\otimes e_i$. We will show that $V^{\ast}\left( A\otimes B \right)V = A\ast B$. Note that
\begin{align*}
  \iprod{V^{\ast}\left( A\otimes B \right)Ve_j}{e_i} &= \iprod{\left( A\otimes B \right)\left( e_j\otimes e_j \right)}{e_i\otimes e_i}\\
                                                     &= \iprod{Ae_j}{e_i} \iprod{Be_j}{e_i}\\
                                                     &= a_{ij}b_{ij}\\
                                                     &= \iprod{A\ast B e_{j}}{e_i}.
\end{align*}
Thus,
\begin{align*}
  \norm{S_A(B)} &\leq \norm{V^{\ast}\left( A\otimes B \right)V}\\
                &\leq \norm{A}\norm{B},
\end{align*}
so that
\begin{align*}
  \norm{S_A}\leq \norm{A}.
\end{align*}
Now, if $\left( B_{ij} \right)_{ij}\in \Mat_k\left( \Mat_n\left( \C \right) \right)$, then
\begin{align*}
  \left( S_{A} \right)\left( \left( B_{ij} \right)_{ij} \right) &= \left( V^{\ast}\left( A\otimes B_{ij} \right)V \right)_{ij}\\
                                                                &= \begin{pmatrix}V^{\ast} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & V^{\ast}\end{pmatrix} A\otimes \begin{pmatrix}B_{11} & \cdots & B_{1n}\\\vdots & \ddots & \vdots \\ B_{n1} & \cdots & B_{nn}\end{pmatrix} \begin{pmatrix}V & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & V\end{pmatrix},
\end{align*}
so that $\norm{\left( S_A \right)_k}\leq \norm{A}$. Thus, $\norm{S_A}_{\cb}\leq \norm{A}$.\newline

However, this isn't a really good estimate. For instance, if $A$ is the matrix consisting of all $1$s, then the norm of $A$ is $n$, while $\norm{S_A} = 1$.\newline

We will prove that if $A$ is positive, then $S_A$ is completely positive. Thus, for positive matrices, we are able to obtain $\norm{S_A}_{\cb}$ by finding
\begin{align*}
  \norm{S_A} &= \norm{S_A(I)}\\
             &= \norm{S_A}_{\cb}\\
             &= \max\set{a_{ii} | i=1,\dots,n}.
\end{align*}
Now, if $A$ is not positive, then obtaining this norm is a bit more difficult. We can decompose $A = \left( P_1 - P_2 \right) + i\left( P_3 - P_4 \right)$, and get
\begin{align*}
  \norm{S_A}_{\cb} \leq \norm{S_{P_1}}_{\cb}  + \norm{S_{P_2}}_{\cb} + \norm{S_{P_3}}_{\cb} + \norm{S_{P_4}}_{\cb},
\end{align*}
but unfortunately this estimate isn't really enough.\newline

Now, we will characterize when the Schur product is completely positive.
\begin{theorem}
  Let $A = \left( a_{ij} \right)_{ij}\in \Mat_n\left( \C \right)$. The following are equivalent:
  \begin{enumerate}[(i)]
    \item $A$ is positive;
    \item $S_A\colon \Mat_n\left( \C \right)\rightarrow \Mat_n\left( \C \right)$ is positive;
    \item $S_A\colon \Mat_n\left( \C \right)\rightarrow \Mat_n\left( \C \right)$ is completely positive.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We have that (iii) implies (ii), and (ii) implies (i) by choosing $J$ to be the matrix consisting of $1$, which is positive, meaning $S_A\left( J \right) = A$. Thus, we must prove that (i) implies (iii).\newline

  Note that if $A$ and $B$ are positive, then $A\otimes B$ is positive. This follows from the fact that $A\otimes B = \left( A^{1/2}\otimes B^{1/2} \right)^2$.\newline

  Now, if $B\in \Mat_n\left( \C \right)$ is positive, then
  \begin{align*}
    S_A\left( B \right) &= V^{\ast}\left( A\otimes B \right)V\\
                        &= \left( \left( A^{1/2}\otimes B^{1/2} \right)V \right)^{\ast}\left( \left( A^{1/2}\otimes B^{1/2} \right)V \right)
  \end{align*}
  is positive, meaning (i) implies (ii).\newline

  Now, to see that (i) implies (iii), we let $B = \left( B_{ij} \right)_{ij}\in \Mat_k\left( \Mat_n\left( \C \right) \right)$, and write $B = \left( X_{ij} \right)_{ij}^{\ast}\left( X_{ij} \right)_{ij}$. We see that
  \begin{align*}
    \left( S_{A} \right)_k \left( B \right) &= \left( V^{\ast}\left( A\otimes B_{ij} \right)V \right)\\
                                            &= \left( \left( A^{1/2}\otimes X_{ij} \right)V \right)^{\ast}\left( \left( A^{1/2}\otimes X_{ij} \right)V \right),
  \end{align*}
  meaning $\left( S_A \right)_{k}$ is positive.
\end{proof}
There is an analogous theory of Schur products in the space $\B\left( \ell_2 \right)$, where we consider the bounded operators as infinite matrices. If we mandate that $A\in \B\left( \ell_2 \right)_{+}$, then we can use a similar line of argumentation to show that $S_A$ is completely positive., but this requires a bit more care as the matrix consisting of all $1$s regarded as an operator on $\ell_2$ is not a bounded operator.\newline

Now, we can show a pretty useful result, which is that bounded linear functionals are not only positive, but completely positive.
\end{document}
