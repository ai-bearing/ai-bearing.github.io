\documentclass[10pt]{mypackage}

% sans serif font:
%\usepackage{cmbright,sfmath,bbold}
%\renewcommand{\mathcal}{\mathtt}

%Euler:
\usepackage{newpxtext,eulerpx,eucal,eufrak}
\renewcommand*{\mathbb}[1]{\varmathbb{#1}}
\renewcommand*{\hbar}{\hslash}
\DeclareMathOperator{\cb}{cb}
\DeclareMathOperator{\hs}{HS}

%kp fonts:
%\usepackage{kpfonts}
%\renewcommand{\mathbb}{\mathds}
%\usepackage{homework}

\pagestyle{fancy} %better headers
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{More on $C^{\ast}$-Algebras}

\setcounter{secnumdepth}{0}

\begin{document}
\RaggedRight
\tableofcontents
\section{Introduction}%
Finally, the last part of my notes on $C^{\ast}$-algebras and amenability as part of my Honors Thesis independent study. Specifically, I am going to focus more on the theory of $C^{\ast}$-algebras, discussing ideas such as amenability and nuclearity in $C^{\ast}$-algebras. There are a few central results I'm going to be working on understanding and proving: almost-invariant vectors, Kesten's criterion, Hulanicki's criterion, nuclearity, and the equivalence of $C^{\ast}_{\lambda}\left(G\right)$ and $C^{\ast}\left(G\right)$.\newline

I will be using a variety of sources more focused on amenability, including but not limited to Volker Runde's \textit{Amenable Banach Algebras}, Kate Juschenko's \textit{Amenability of Discrete Groups by Examples}, and Brown and Ozawa's \textit{$C^{\ast}$-Algebras and Finite-Dimensional Approximations}.
\section{Review: Representations, the Reduced Group $C^{\ast}$-Algebra, and the Universal Group $C^{\ast}$-Algebra}%
\subsection{Left-Regular Representation}%
Let $\Gamma$ be a group. Consider the space $\ell_2\left(\Gamma\right)$. For every $s\in\Gamma$, we define the operator
\begin{align*}
  \lambda_s\left(\xi\right)\left(t\right) &= \xi\left(s^{-1}t\right).
\end{align*}
The map is linear, well-defined, and an isometry, as
\begin{align*}
  \norm{\lambda_s\left(\xi\right)}^2 &= \sum_{t\in\Gamma}\left\vert \lambda_s\left(\xi\right)\left(t\right) \right\vert^2\\
                                     &= \sum_{t\in\Gamma}\left\vert \xi\left(s^{-1}t\right) \right\vert^2\\
                                     &= \sum_{r\in\Gamma}\left\vert \xi\left(r\right) \right\vert^2\\
                                     &= \norm{\xi}^2.
\end{align*}
Additionally, each $\lambda_{s}$ admits an inverse, $\lambda_{s^{-1}} = \lambda_s^{\ast}$. Applying to the orthonormal basis $\set{\delta_t}_{t\in\Gamma}$, we get
\begin{align*}
  \lambda_s\left(\delta_t\right) &= \delta_{st}.
\end{align*}
Thus, $\lambda_{s}\circ \lambda_r = \lambda_{sr}$, and we have the unitary representation of $\Gamma$, $\lambda\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$, where $\lambda(s) = \lambda_s$, for $s\in \Gamma$. This is the left-regular representation of $\Gamma$.\newline
 
Note that the left regular representation is a faithful representation, hence injective.\newline

Because the $\lambda$ operator is linear, we may extend it to the case of any positive finitely supported function,
\begin{align*}
  \lambda_{f}\left(\xi\right)(t) &= \left(\sum_{s\in\Gamma}f(t)\lambda_{s}\left(\xi\right)\right)\left(t\right)\\
                                 &= \sum_{s\in\Gamma}f(s)\xi\left(s^{-1}t\right)
\end{align*}
Note that the space of finitely supported functions on $\Gamma$, $\C\left[\Gamma\right]$,\footnote{Also known as the free vector space over $\C$ with basis $\Gamma$.} is a $\ast$-algebra, where multiplication is given by convolution:
\begin{align*}
  f\ast g(t) &= \sum_{s\in\Gamma}f\left(s\right)g\left(s^{-1}t\right)\\
             &= \sum_{r\in\Gamma}f\left(tr^{-1}\right)g(r).
\end{align*}
Note that we are using $\ast$ both to refer to the involution (when as a superscript) as well as the group operation (when not a superscript). This is to maintain coherence with the traditional way that convolution is written. The involution on $\C\left[\Gamma\right]$ is given by
\begin{align*}
  f^{\ast}\left(t\right) &= \overline{f\left(t^{-1}\right)}.
\end{align*}
\subsection{A Bit on Representations and $C^{\ast}$-(Semi)norms}%
A $C^{\ast}$-seminorm on a $\ast$-algebra is a seminorm such that defined by
\begin{itemize}
  \item $\norm{ab}\leq \norm{a}\norm{b}$;
  \item $\norm{a^{\ast}} = \norm{a}$;
  \item $\norm{a^{\ast}a} = \norm{a}^2$.
\end{itemize}
If $A_0$ is a $\ast$-algebra, then a representation of $A_0$ is a pair $\left(\pi_0,\mathcal{H}\right)$, where $\mathcal{H}$ is a Hilbert space and $\pi\colon A_0\rightarrow \B\left(\mathcal{H}\right)$ is a $\ast$-homomorphism.\newline

Additionally, if $A_0$ is a $\ast$-algebra with representation $\pi_0$, then we have $C^{\ast}$-seminorm
\begin{align*}
  \norm{a}_{\pi_0} &= \norm{\pi_0\left(a\right)}_{\op}.
\end{align*}
If $\pi_0$ is injective, then $\norm{\cdot}_{\pi_0}$ is a $C^{\ast}$-norm. If $\pi_0$ is a $C^{\ast}$-norm, then the completion of $A_0$ with respect to $\norm{\cdot}_{\pi_0}$ is a $C^{\ast}$-algebra.\newline

The universal norm on $A_0$ is defined as
\begin{align*}
  \norm{a}_{u} &= \sup_{p\in \mathcal{P}}p(a),
\end{align*}
where $\mathcal{P}$ is the collection of all $C^{\ast}$-seminorms on $A_0$. If $\norm{a}_u < \infty$ for all $a\in A_0$, then $\norm{\cdot}_u$ is a $C^{\ast}$-seminorm on $A_0$. Note that if one of $p\in \mathcal{P}$ is a norm, then $\norm{\cdot}_{u}$ defines a $C^{\ast}$-norm on $A_0$.\newline

If we have the unitary representation $u\colon \C\left[\Gamma\right]\rightarrow \B\left(\mathcal{H}\right)$, then
\begin{align*}
  \pi_u(a) &= \sum_{s\in\Gamma}u_s
\end{align*}
is a representation of $\C\left[\Gamma\right]$. If $\lambda\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$ is the left-regular representation, then the left-regular group $C^{\ast}$-algebra is the group $\ast$-algebra with $C^{\ast}$-norm defined by $\norm{a} = \norm{\pi_{\lambda}(a)}$.\newline

The universal group $C^{\ast}$-algebra is defined as the norm completion of 
\begin{align*}
  \norm{a}_{\max} &= \sup\set{\norm{\pi\left(a\right)}_{\op} | \pi\colon \C\left[\Gamma\right]\rightarrow \B\left(\mathcal{H}_{\pi}\right) \text{ is a representation}}.
\end{align*}
Note that
\begin{align*}
  \norm{\pi\left(a\right)} &= \norm{\pi\left(\sum_{s\in\Gamma}a_s\delta_s\right)}\\
                           &= \norm{\sum_{s\in\Gamma}a_s\pi\left(\delta_s\right)}\\
                           &\leq \sum_{s\in\Gamma}\norm{a_s\pi\left(\delta_s\right)}\\
                           &= \sum_{s\in\Gamma}\left\vert a_s \right\vert.
\end{align*}
Note that since $\norm{\cdot}_{\lambda}$ is a norm, we must have $a=0$ if and only if $\norm{a}_{\max} = 0$. The full group $C^{\ast}$-algebra admits a universal property.
\begin{proposition}
  Let $\Gamma$ be a discrete group. If $u\colon \Gamma\rightarrow \B\left(\mathcal{H}\right)$, then there is a contractive $\ast$-homomorphism $\pi_u\colon C^{\ast}\left(\Gamma\right)\rightarrow \B\left(\mathcal{H}\right)$ that satisfies $\pi_u\left(\delta_s\right) = u(s)$.
\end{proposition}
\section{Using the Left-Regular Representation to Establish Amenability}%
If $\pi\colon \Gamma\rightarrow \mathcal{U}\left(\mathcal{H}\right)$ is a unitary representation of $\mathcal{H}$, then a vector $\xi\in \mathcal{H}$ is called invariant for $\pi$ if $\pi(g)\left(\xi\right) = \xi$ for all $g\in \Gamma$.
\begin{proposition}
  The left-regular representation for $\Gamma$ admits an invariant vector if and only if $\Gamma$ is finite.
\end{proposition}
\begin{proof}
  Let $\Gamma$ be finite. Since $\Gamma$ is finite, all functions $a\colon \Gamma\rightarrow \C$ are square-summable. Thus, $\xi = \1_{\Gamma}$ is square-summable, and since $s\Gamma = \Gamma$ for all $s\in\Gamma$, we have $\1_{\Gamma}$ is invariant for $\lambda$.\newline

  Now, let $\lambda\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$ be the left-regular representation, and suppose there is $\xi\in \ell_2\left(\Gamma\right)$ such that for all $s\in \Gamma$, we have
  \begin{align*}
    \lambda_s\left(\xi\right) &= \xi.
  \end{align*}
  In particular, this means that for any $t\in \Gamma$, we have
  \begin{align*}
    \lambda_s\left(\xi\right)\left(t\right) &= \xi\left(s^{-1}t\right)\\
                                            &= \xi\left(t\right).
  \end{align*}
  Since this holds for all $s\in \Gamma$, we have that $\xi = c\1_{\Gamma}$ for some $c\in \C$. However, since $\xi\in \ell_2\left(\Gamma\right)$, we must have that $\sum_{t\in\Gamma} \left\vert c \right\vert^2 < \infty$, which only holds if $\Gamma$ is finite.
\end{proof}
An almost-invariant vector for a representation $\pi\colon \Gamma\rightarrow \mathcal{U}\left(\ell_2\left(\Gamma\right)\right)$, as the name suggests,\footnote{I'm only mostly being facetious here.} a sequence (or net) of unit vectors $\left(\xi_i\right)_{i\in I}$ such that
\begin{align*}
  \lim_{i\in I}\norm{\pi(g)\left(\xi_i\right) - \xi_i} &= 0.
\end{align*}
\begin{theorem}
  A group $\Gamma$ is amenable if and only if the left-regular representation has an almost-invariant vector.
\end{theorem}
\begin{proof}
  Let $\Gamma$ be amenable, and let $F_i$ be a Følner sequence, where $\frac{\left\vert sF_i\triangle F_i \right\vert}{\left\vert F_i \right\vert}\rightarrow 0$ for all $s\in\Gamma$.\newline

  Define $\xi_i = \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{F_i}$. Then,
  \begin{align*}
    \norm{\lambda_{s}\left(\xi_i\right) - \xi_i}^2 &= \sum_{t\in\Gamma} \left\vert \lambda_{s}\left(\xi_i\right)\left(t\right) - \xi_i\left(t\right) \right\vert^2\\
                                                   &= \sum_{t\in\Gamma} \left\vert \lambda_s\left(\frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{F_i}\right)\left(t\right) - \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{F_i} \right\vert^2\\
                                                   &= \sum_{t\in\Gamma}\left\vert \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{sF_i}(t) - \frac{1}{\sqrt{\left\vert F_i \right\vert}}\1_{sF_i}(t) \right\vert^2\\
                                                   &= \frac{\left\vert sF_i\triangle F_i \right\vert}{\left\vert F_i \right\vert}.
  \end{align*}
  Thus, $\lambda$ has an almost-invariant vector.\newline

  Suppose there exists an almost-invariant vector $\left(\xi_i\right)_i\in \ell_2\left(\Gamma\right)$. It is sufficient to construct an approximate mean. Since $\xi_i\in \ell_2\left(\Gamma\right)$, we have that $\xi_i^2\in \ell_1\left(\Gamma\right)$. Setting $\mu_i = \xi_i^2$, we plug this into the expression for an approximate mean, and obtain
  \begin{align*}
    \norm{\lambda_s\left(u_i\right) - u_i}_{\ell_1} &= \sum_{t\in\Gamma}\left\vert \lambda_s\left(\xi_i^2\right)\left(t\right) - \xi_i^2\left(t\right) \right\vert\\
                                                    &= \sum_{t\in\Gamma}\left\vert \left(\lambda_s\left(\xi_i\right)\left(t\right) - \xi_i\left(t\right)\right)\left(\lambda_s\left(\xi_i\right)\left(t\right) + \xi_i\left(t\right)\right) \right\vert\\
                                                    &= \norm{\left(\lambda_s\left(\xi_i\right) - \xi_i\right)\left(\lambda_s\left(\xi_i\right) + \xi_i\right)}_{\ell_1}\\
                                                    &\leq \norm{\lambda_s\left(\xi_i\right) - \xi_i}_{\ell_2}\norm{\lambda_s\left(\xi_i\right) + \xi_{i}}\\
                                                    &\leq 2\norm{\lambda_s\left(\xi_i\right) - \xi_i}\\
                                                    &\rightarrow 0.
  \end{align*}
  Thus, $\mu_i$ is an approximate mean.
\end{proof}
Using the criterion of almost invariant vectors, we may show that a group is amenable if and only if the trivial representation --- defined by $1_{\Gamma}\colon \Gamma\rightarrow \C$, $1_{\Gamma}(g) = 1$ is what is known as weakly contained in the left-regular representation.\newline

A representation $\pi\colon \Gamma\rightarrow \mathcal{U}\left(\mathcal{H}\right)$ is weakly contained in another representation $\rho\colon \Gamma\rightarrow \mathcal{U}\left(\mathcal{H}\right)$, denoted $\pi\prec \rho$, if for every $\xi\in \mathcal{H}$, finite $E\subseteq \Gamma$, and $\ve > 0$, then there are $\eta_1,\dots,\eta_n\in \mathcal{K}$ such that
\begin{align*}
  \left\vert \iprod{\pi(g)\left(\xi\right)}{\xi} - \sum_{i=1}^{n} \iprod{\rho(g)\left(\eta_i\right)}{\eta_i} \right\vert < \ve.
\end{align*}
\begin{theorem}
  A discrete group $\Gamma$ is amenable if and only if $1_{\Gamma}\prec \lambda$, where $\lambda$ is the left-regular representation.
\end{theorem}
\begin{proof}
  We show that $1_{\Gamma}\prec \lambda$ is equivalent to the existence of an almost invariant vector for $\lambda$. We assume $\lambda$ admits an almost-invariant vector. It is sufficient to show that for every $\ve > 0$ and every finite set $E\subseteq \Gamma$, there are $\eta_1,\dots,\eta_n\in \ell_2\left(\Gamma\right)$ such that
  \begin{align*}
    \left\vert 1-\sum_{i=1}^{n} \iprod{\lambda_t\left(\eta_i\right)}{\eta_i} \right\vert < \ve
  \end{align*}
  for every $t\in E$. If we take $n = 1$ and $\eta_1 = \xi$, where $\xi$ is almost-invariant for all $g\in E$ --- i.e., $\norm{\lambda_g\left(\xi\right) - \xi}_{\ell_2} < \ve$ for all $g\in E$. Note that we have
  \begin{align*}
    \norm{\lambda_g\left(\xi\right) - \xi}^2 &= \iprod{\lambda_g\left(\xi\right) - \xi}{\lambda_g\left(\xi\right) - \xi}\\
                                             &= \iprod{\lambda_g\left(\xi\right)}{\lambda_g\left(\xi\right)} + \iprod{\xi}{\xi} - 2\re\left( \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                             &= 2 - 2\re\left( \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                             &= 2\re\left(1 -  \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                             &\leq 2\left\vert 1 - \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert.
  \end{align*}
  Additionally,
  \begin{align*}
    \left\vert 1- \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert^2 &= \left(1 - \iprod{\lambda_g\left(\xi\right)}{\xi}\right) \left( 1 - \overline{ \iprod{\lambda_g\left(\xi\right)}{\xi} } \right)\\
                                                                       &= 1 - \overline{ \iprod{\lambda_g\left(\xi\right)}{\xi} } - \iprod{\lambda_g\left(\xi\right)}{\xi} + \left\vert \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert^2\\
                                                                       &\leq 2 - 2\re\left( \iprod{\lambda_g\left(\xi\right)}{\xi}\right)\\
                                                                       &= \norm{\lambda_g\left(\xi\right) - \xi}^2.
  \end{align*}
  Thus, we have that
  \begin{align*}
    \left\vert 1- \iprod{\lambda_g\left(\xi\right)}{\xi} \right\vert &\leq \norm{ \lambda_g\left(\xi\right) - \xi }\\
                                                                     &< \ve.
  \end{align*}
%  Now, we suppose that $1_{\Gamma}\prec \lambda$. For every $\ve > 0$ and finite subset $E\subseteq \Gamma$, there are $\eta_1,\dots,\eta_n\in \ell_2\left(\Gamma\right)$ such that 
%  \begin{align*}
%    \left\vert 1 - \sum_{i=1}^{n} \iprod{\lambda_t\left(\eta_i\right)}{\eta_i} \right\vert &< \ve.
%  \end{align*}
%  for all $t\in E$. We may assume that $e_G\in E$, yielding
%  \begin{align*}
%    \left\vert 1-\sum_{i=1}^{n} \norm{\eta_i}^2 \right\vert < \ve.
%  \end{align*}
%  Furthermore, we may assume that $\sum_{i=1}^{n} \norm{\eta_i}^2 = 1$.\newline
%
%  Suppose toward contradiction that $\lambda$ does not have an almost-invariant vector. Then, there exists $C > 0$ and $S\subseteq \Gamma$ such that
%  \begin{align*}
%    \norm{\xi}^2\left\vert S \right\vert - \sum_{\gamma\in S} \iprod{\lambda_{\Gamma}\left(\xi\right)}{\xi} &> C\norm{\xi}^2.
%  \end{align*}
  We start by showing that $1_{\Gamma}\prec \lambda$ if and only if for every finite $S\subseteq \Gamma$ and every $\ve > 0$, there exists a unit vector $\xi\in \mathcal{H}$ such that
  \begin{align*}
    \norm{\lambda_s\left(\xi\right) - \xi}_{\ell_2} < \ve.
  \end{align*}
  In the forward direction, we see that there exists a unit vector $\xi$ such that $\left\vert 1 - \iprod{\lambda_s\left(\xi\right)}{\xi} \right\vert < \ve^2/2$, meaning $\norm{\lambda_s\left(\xi\right) - \xi} < \ve$ by above. Similarly, if $\norm{\lambda_s\left(\xi\right)-\xi} < \ve$, then $1_{\Gamma}\prec \lambda$.\newline

  Now, we assume $1_{\Gamma} \prec \lambda$. Thus, for a finite $E\subseteq \Gamma$ and $\ve > 0$, then there exists $f\in \ell_2\left(\Gamma\right)$ with $\norm{f}_{\ell_2} = 1$ such that $\norm{\lambda_s\left(f\right) - f} < \ve$ for all $s\in E$.\newline

  Setting $g = \left\vert f \right\vert^2$, we have $g\in \ell_1\left(\Gamma\right)$. From Hölder's inequality, we have
  \begin{align*}
    \norm{\lambda_s\left(g\right) - g}_{\ell_1} &\leq \norm{\lambda_s\left(\overline{f}\right) + \overline{f}}_{\ell_2} \norm{\lambda_s\left(f\right) - f}\\
                                                &\leq 2\norm{\lambda_s\left(f\right) - f}_{\ell_2}\\
                                                &< 2\ve.
  \end{align*}
  Thus, $\Gamma$ admits an approximate mean, hence is amenable.
\end{proof}
Having obtained some more resources on Kesten's criterion, we can now prove that.
\begin{definition}
  Let $\lambda\colon \Gamma\rightarrow \B\left(\ell_2\left(\Gamma\right)\right)$ be the left-regular representation. Then, for a finite set $E\subseteq \Gamma$, we define the Markov operator $M\left(E\right)$ by
  \begin{align*}
    M\left(E\right) &= \sum_{t\in E}\lambda_t.
  \end{align*}
\end{definition}
Note that since $\lambda_t$ is an isometry for each $t$, we have
\begin{align*}
  \norm{M\left(E\right)}_{\op} &= \norm{\frac{1}{\left\vert E \right\vert}\sum_{t\in E}\lambda_t}_{\op}\\
                               &= \frac{1}{\left\vert E \right\vert} \norm{\sum_{t\in E}\lambda_t}_{\op}\\
                               &\leq \frac{1}{\left\vert E \right\vert}\sum_{t\in E}\norm{\lambda_t}_{\op}\\
                               &= 1,
\end{align*}
so the Markov operator is a bounded operator (indeed, a contraction).
\begin{theorem}[Kesten's Criterion]
  Let $\Gamma$ contain a finite symmetric generating set $S$. Then, $\Gamma$ is amenable if and only if
  \begin{align*}
    \norm{M(S)}_{\op} &= 1.
  \end{align*}
\end{theorem}
\begin{proof}
  Let $\Gamma$ be amenable. Then, $\lambda$ admits an almost-invariant vector, $\left(\xi_n\right)_n\subseteq S_{\ell_2\left(\Gamma\right)}$, such that
  \begin{align*}
    \norm{\lambda_s\left(\xi_n\right) -\xi_n}_{\ell_2} \rightarrow 0
  \end{align*}
  for all $s\in \Gamma$. In particular, we have
  \begin{align*}
    \left\vert \left(\norm{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi_n\right)}_{\ell_2}\right)  - \norm{\xi_n}_{\ell_2}\right\vert &\leq \norm{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi_n\right) - \xi_n}_{\ell_2}\\
                                                                                                                                                                                  &= \frac{1}{\left\vert S \right\vert}\norm{\left(\sum_{t\in S}\lambda_t\right)\left(\xi_n\right) - \left\vert S \right\vert\xi_n}_{\ell_2}\\
                                                                                                                        &\leq \frac{1}{\left\vert S \right\vert} \sum_{t\in S}\norm{\lambda_t\left(\xi_n\right) - \xi_n}_{\ell_2}\\
                                                                                                                        &\rightarrow 0,
  \end{align*}
  meaning that
  \begin{align*}
    \sup_{\xi\in S_{\ell_2\left(\Gamma\right)}} \norm{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi\right)} &= \norm{\xi},
  \end{align*}
  and so the norm of the Markov operator is $1$.\newline

  Suppose
  \begin{align*}
    \norm{\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t}_{\op} &= 1,
  \end{align*}
  or
  \begin{align*}
    \norm{\sum_{t\in S}\lambda_t}_{\op} &= \left\vert S \right\vert.
  \end{align*}
  \begin{proposition}
    If $T\in \B\left(\mathcal{H}\right)$ is a self-adjoint operator, then
    \begin{align*}
      \norm{T}_{\op} &= \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T\left(x\right)}{x} \right\vert.
    \end{align*}
  \end{proposition}
  \begin{proof}
    We have that
    \begin{align*}
      \left\vert \iprod{T\left(x\right)}{x} \right\vert &\leq \norm{T\left(x\right)}\norm{x}\\
                                                        &\leq \norm{T}_{\op}\norm{x}^2\\
                                                        &= \norm{T}_{\op}.
    \end{align*}
    Now, we seek to establish the opposite direction. Note that since $T$ is self-adjoint, we know that $ \iprod{T\left(x\right)}{x}\in \R $ for any $x\in \mathcal{H}$, so by the polarization identity, we have that
    \begin{align*}
      \iprod{T\left(x\right)}{y} &= \frac{1}{4}\left( \iprod{T\left(x+y\right)}{x+y} - \iprod{T\left(x-y\right)}{x-y}\right).
    \end{align*}
    Note that we know that
    \begin{align*}
      \norm{T}_{\op} &= \sup_{x,y\in S_{\mathcal{H}}}\left\vert \iprod{T\left(x\right)}{y} \right\vert.
    \end{align*}
    Now, we set $\alpha = \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T(x)}{x} \right\vert$. Note that for any nonzero $x\in \mathcal{H}$, we have
    \begin{align*}
      \left\vert \iprod{T\left(\frac{x}{\norm{x}}\right)}{\frac{x}{\norm{x}}} \right\vert &\leq \alpha\\
      \left\vert \iprod{T\left(x\right)}{x} \right\vert &\leq \alpha \norm{x}^2.
    \end{align*}
    Now, for any $x,y\in \mathcal{H}$, we may assume that $ \iprod{T\left(x\right)}{y} \in \R $, as we may multiply $ \iprod{T\left(x\right)}{y} $ by $\sgn \left( \iprod{T\left(x\right)}{y}\right)$. Thus, by the polarization identity and the fact that $T$ is self-adjoint, we have
    \begin{align*}
      \iprod{T\left(x\right)}{y} &= \frac{1}{4}\left( \iprod{T\left(x+y\right)}{x+y} - \iprod{T\left(x-y\right)}{x-y}\right)\\
      \left\vert \iprod{T\left(x\right)}{y} \right\vert &= \left\vert \frac{1}{4}\left( \iprod{T\left(x+y\right)}{x+y} - \iprod{T\left(x-y\right)}{x-y}\right) \right\vert\\
                                                        &\leq \frac{1}{4}\left( \left\vert \iprod{T\left(x+y\right)}{x+y} \right\vert + \left\vert \iprod{T\left(x-y\right)}{x-y} \right\vert\right)\\
                                                        &\leq \frac{\alpha}{4} \left(\norm{x+y}^2 + \norm{x-y}^2\right)\\
                                                        &= \frac{\alpha}{4}\left(2\norm{x}^2 + 2\norm{y}^2\right)\\
                                                        &= \alpha.
    \end{align*}
    Thus, we have $\norm{T}_{\op}\leq \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T\left(x\right)}{x} \right\vert$.
  \end{proof}
  Now, since $S$ is symmetric, we have that $M(S)$ is self-adjoint. Therefore, we know that there is some $\xi_n\in S_{\mathcal{H}}$ such that
  \begin{align*}
    1-\frac{1}{n} &< \iprod{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\xi_n\right)}{\xi_n}\\
                  &\leq \iprod{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\left\vert \xi_n \right\vert\right)}{\left\vert \xi_n \right\vert}.
  \end{align*}
  Thus, rearranging, we have
  \begin{align*}
    1 - \iprod{\left(\frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t\right)\left(\left\vert \xi_n \right\vert\right)}{\left\vert \xi_n \right\vert} &< \frac{1}{n}.
  \end{align*}
  Since $M(S)$ is a self-adjoint operator, we have that $\re\left( \iprod{\left( \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t \right)\left( \xi_n \right)}{\xi_n} \right) = \iprod{\left( \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\lambda_t \right)\left( \xi_n \right)}{\xi_n}$. This gives
  \begin{align*}
    \norm{\left( \frac{1}{S}\sum_{t\in S}\lambda_t \right)\left( \xi \right) - \xi} &\leq \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\norm{\lambda_t\left( \xi \right) - \xi}\\
                                                                                    &\leq \frac{1}{\left\vert S \right\vert}\sum_{t\in S}\sqrt{2}\left\vert 1 - \iprod{\lambda_t\left( \xi \right)}{\xi} \right\vert\\
                                                                                    &= \sqrt{2}\left\vert 1 - \frac{1}{\left\vert S \right\vert} \sum_{t\in S} \iprod{\lambda_t\left( \xi \right)}{\xi}\right\vert\\
                                                                                    &\rightarrow 0.
  \end{align*}
  Thus, $\lambda$ admits an almost-invariant vector.
\end{proof}
Next, we turn to Hulanicki's Criterion.
\begin{definition}
  Let $f\in \ell_1\left( \Gamma \right)$. Then, we define the bounded operator
  \begin{align*}
    \lambda_{f(t)} &= \sum_{t\in\Gamma}f(t)\lambda_t.
  \end{align*}
\end{definition}
\begin{theorem}
  If $\Gamma$ is a discrete group, then $\Gamma$ is amenable if and only if for every positive finitely-supported $f\colon \Gamma\rightarrow \C$, we have
  \begin{align*}
    \sum f(t) &\leq \norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
\end{theorem}
\begin{proof}
  Suppose $\Gamma$ is amenable. Let $f \geq 0$ be a finitely supported function, and let $\left( F_n \right)_n$ be a Følner sequence such that for every $g\in \supp\left( f \right)$, we have
  \begin{align*}
    \frac{\left\vert gF_n\triangle F_n \right\vert}{\left\vert F_n \right\vert} &\leq \frac{1}{n}.
  \end{align*}
  Let $\xi_n = \frac{1}{\sqrt{\left\vert F_n \right\vert}}\1_{F_n}$. Note that $\norm{\xi_n}_{\ell_2} = 1$.\newline

  We will use the fact that
  \begin{align*}
    \sup_{x\in S_{\mathcal{H}}} \left\vert \iprod{T\left( x \right)}{x} \right\vert &\leq \norm{T}_{\op}.
  \end{align*}
  We see that
  \begin{align*}
    \left\vert \iprod{\left( \sum_{t\in\Gamma}f(t)\lambda_t \right)\left( \xi_n \right)}{\xi_n} \right\vert &= \left\vert \sum_{t\in\Gamma}f(t) \iprod{\lambda_t\left( \xi_n \right)}{\xi_n} \right\vert\\
                                                                                                            &= \left\vert \sum_{t,s\in\Gamma}f(t) \xi_n\left( t^{-1}s \right)\xi_n\left( s \right) \right\vert\\
                                                                                                            &\leq \norm{\lambda_{f(t)}},
                                                                                                            \intertext{meaning}
    \lim_{n\rightarrow\infty} \left\vert \iprod{\left( \sum_{t\in\Gamma}f(t)\lambda_t \right)\left( \xi_n \right)}{\xi_n} \right\vert &\leq \norm{\lambda_{f(t)}}.
  \end{align*}
  Notice that $\xi_n$ is an almost-invariant vector for $\lambda$, meaning that $\xi_n\left( t^{-1}s \right)\rightarrow \xi_n\left( s \right)$. Therefore, this means
  \begin{align*}
    \lim_{n\rightarrow\infty}\left\vert \sum_{t,s\in\Gamma}f(t)\xi_n\left( t^{-1}s \right)\xi_n\left( s \right) \right\vert &= \lim_{n\rightarrow\infty} \left\vert \sum_{t,s\in\Gamma}f(t)\left\vert \xi_n\left( s \right) \right\vert^2 \right\vert\\
                                                                                                                            &= \sum_{t\in\Gamma}f(t)\left\vert \sum_{s\in\Gamma}\left\vert \xi_n\left( s \right) \right\vert^2 \right\vert\\
                                                                                                                            &= \sum_{t\in\Gamma} f(t)\\
                                                                                                                            &\leq \norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  This establishes that there is some $C > 0$ such that
  \begin{align*}
    \sum_{t\in\Gamma}f(t) &\leq C\norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  To show that $C = 1$, we note that, by the definition of convolution, we must have
  \begin{align*}
    \left( \sum_{t\in\Gamma}f(t) \right)^{n} &= \sum_{t\in\Gamma} \left( f\ast \cdots \ast f \right)(t),
  \end{align*}
  and
  \begin{align*}
    \left( \lambda_{f(t)} \right)^{n} &= \left( \sum_{t\in\Gamma}f(t)\lambda)_t \right)^{n}\\
                                      &= \sum_{t\in\Gamma}\left( f\ast\cdots\ast f \right)\left( t \right)\lambda_t\\
                                      &= \lambda_{\left( f\ast\cdots\ast f \right)\left( t \right)}.
  \end{align*}
  Thus, we have
  \begin{align*}
    \left( \sum_{t\in\Gamma}f(t) \right)^{n}  &= \sum_{t\in\Gamma}\left( f\ast\cdots\ast f \right)\left( t \right)\\
                                              &\leq C\norm{\lambda_{\left( f\ast\cdots\ast f \right)\left( t \right)}}\\
                                              &= C\left( \norm{\lambda_{f(t)}}_{\op} \right)^n.
  \end{align*}
  This means we have
  \begin{align*}
    \sum_{t\in\Gamma}f(t) &\leq C^{1/n}\norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  Since $n$ is arbitrary, this means $C = 1$.\newline

  Now, if for all finitely supported $f$, we have
  \begin{align*}
    \sum_{t\in\Gamma} f(t) &\leq \norm{\lambda_{f(t)}}_{\op}.
  \end{align*}
  If $f = \1_{E}$ for some finite $E\subseteq \Gamma$, we see that
  \begin{align*}
    \norm{\sum_{t\in E}\lambda_{t}}_{\op} &= \left\vert E \right\vert,
  \end{align*}
  so by Kesten's criterion, we have that $\Gamma$ is amenable.
\end{proof}
\section{Completely [Property] Maps}%
We begin this section with an overview of positive maps, completely positive maps, and extensions. These will be useful for understanding the theorem that a group is amenable if and only if the reduced group $C^{\ast}$-algebra is nuclear. The ultimate goal here is to prove Arveson's extension theorem (i.e., that $\B\left( \mathcal{H} \right)$ is injective with respect to completely positive maps). The primary text for this purpose will be Vern Paulsen's \textit{Completely Bounded Maps and Operator Algebras}.\newline

The idea behind completely positive maps is that they are positive when subjected to a certain amplification process related to the matrix algebras.
\begin{definition}
  An element of a $C^{\ast}$-algebra is positive if and only if it is self-adjoint and its spectrum is contained in the nonnegative reals. Alternatively, $b\in A$ is of the form $b = a^{\ast}a$ for some $a\in A$.
\end{definition}
To introduce a norm such that $\Mat_n\left( A \right)$ becomes a $C^{\ast}$-algebra, we begin with the most basic $C^{\ast}$-algebra, $\B\left( \mathcal{H} \right)$, and consider the $n$-fold amplification of $\mathcal{H}$, $\mathcal{H}^{(n)}$. This is a Hilbert space equipped with inner product
\begin{align*}
  \iprod{ \begin{pmatrix}h_1\\\vdots\\h_n\end{pmatrix} }{ \begin{pmatrix}k_1\\\vdots\\k_n\end{pmatrix} } &= \sum_{j=1}^{n} \iprod{h_j}{k_j}.
\end{align*}
Meanwhile, we may consider $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$ as a linear map on $\mathcal{H}^{(n)}$, by taking
\begin{align*}
  \left( T_{ij} \right)_{ij} &= \begin{pmatrix}\sum_{j=1}^{n}T_{1j}\left( h_j \right)\\\vdots\\\sum_{j=1}^{n}T_{nj}\left( h_j \right)\end{pmatrix}.
\end{align*}
This yields a $\ast$-isomorphism between $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$ and $\B\left( \mathcal{H}^{(n)} \right)$.\newline

Given any $C^{\ast}$-algebra $A$, we may theorize $\Mat_n\left( A \right)$ by first isometrically representing $\mathcal{A}$ on some Hilbert space $\mathcal{H}$, letting $A$ be a $C^{\ast}$-subalgebra of $\B\left( \mathcal{H} \right)$, and then identifying $\Mat_n\left( A \right)$ as a $\ast$-subalgebra of $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$.\newline

Using a faithful $\ast$-representation of $A$, we now have a way to turn $\Mat_n\left( A \right)$ into a $C^{\ast}$-algebra. However, since the norm is unique on a $C^{\ast}$-algebra, the norm on $\Mat_n\left( A \right)$ defined in this fashion is independent of the representation of $A$ that we choose. Furthermore, since $\ast$-isomorphisms are positive maps, the positive elements of $\Mat_n\left( A \right)$ are uniquely determined. This means that every $C^{\ast}$-algebra carries with it a set of canonically defined norms and orders on each $\Mat_n\left( A \right)$.\newline

For example, consider $\Mat_k\left( \C \right)$, which can be identified with $\mathcal{L}\left( \mathcal{C}^{k} \right)$. We identify $\Mat_n\left( \Mat_{k}\left( \C \right) \right) \cong \Mat_{nk}\left( \C \right)$. With this identification, the usual multiplication and involution on $\Mat_n\left( \Mat_k\left( \C \right) \right)$ become multiplication and involution on $\Mat_{nk}\left( \C \right)$.\newline

Now, let $X$ be a compact Hausdorff space, and let $C(X)$ be the $C^{\ast}$-algebra of continuous functions with $f^{\ast}\left( x \right) = \overline{f(x)}$, equipped with the norm $\norm{f} = \sup_{x\in X}\left\vert f(x) \right\vert$. Then, an element $F = \left( f_{ij} \right)_{ij}$ of $\Mat_n\left( C(X) \right)$ can be considered as a continuous $\Mat_n\left( \C \right)$-valued function. Addition, multiplication, and involution in $\Mat_n\left( C(X) \right)$ are pointwise. Recalling that the norm on $\Mat_n\left( C(X) \right)$ is unique, we may let $\norm{F} = \sup_{x\in X}\norm{F(x)}$, where the latter norm is the canonical matrix norm on $\Mat_n\left( C(X) \right)$. The positive elements of $\Mat_n\left( C(X) \right)$ are those $F$ for which $F(x)$ is a positive matrix for all $x$.\newline

Now, given two $C^{\ast}$-algebras $A$ and $B$ and a map $\phi\colon A\rightarrow B$, there are maps $\phi_n\colon \Mat_n\left( A \right)\rightarrow \Mat_n\left( B \right)$, given by
\begin{align*}
  \phi_n\left( \left( a_{ij} \right)_{ij} \right) &= \left( \phi\left( a_{ij} \right) \right)_{ij}.
\end{align*}
In general, when we say that $\phi$ is completely [property], then we say that all the $\phi_n$ have that property. For instance, if $\phi$ is positive, in that it maps positive elements of $A$ to positive elements of $B$, then we say $\phi$ is completely positive if $\phi_n$ is a positive map for each $n$, where the positive elements of $\Mat_n\left( A \right)$ and $\Mat_n\left( B \right)$ are defined canonically.\newline

Unfortunately, it's not always the case that (e.g.) positive maps are completely positive, or even that $\norm{\phi_n}_{\op} = \norm{\phi}_{\op}$ for each $n$.\newline

There is an isomorphism between $\Mat_n\left( A \right)$ and the tensor product $\Mat_n\left( \C \right)\otimes A$. We detail it here. The proof is from Timothy Rainone's \textit{Functional Analysis-En Route to Operator Algebras}.
\begin{theorem}
  Let $A$ be an algebra, and let $\Mat_n\left( A \right)$ denote the matrix algebra of $A$. Then, there is a $\ast$-isomorphism
  \begin{align*}
    \Mat_n\left( A \right) &\cong \Mat_n\left( \C \right)\otimes A.
  \end{align*}
\end{theorem}
\begin{proof}
  Define $\varphi\colon \Mat_n\left( A \right)\rightarrow \Mat_n\left( \C \right)\otimes A$ by
  \begin{align*}
    \varphi\left( \left( a_{ij} \right)_{ij} \right) &= \sum_{i,j=1}^{n}e_{ij}\otimes x_{ij}.
  \end{align*}
  Recall that if $A$ and $B$ are two algebras, multiplication in $A\otimes B$ is defined by
  \begin{align*}
    \left( a\otimes b \right)\left( c\otimes d \right) &= ac\otimes bd,
  \end{align*}
  and if $A$ and $B$ are $\ast$-algebras, then the involution is defined by
  \begin{align*}
    \left( a\otimes b \right)^{\ast} &= a^{\ast}\otimes b^{\ast}.
  \end{align*}
  We start by showing that $\Mat_n\left( A \right)\cong \Mat_n\left( \C \right)\otimes A$ as vector spaces. By the definition of the tensor product, the map $\varphi$ is linear.\newline

  Now, suppose
  \begin{align*}
    \varphi\left( \left( a_{ij} \right)_{ij} \right) &= \sum_{i,j=1}^{n}e_{ij}\otimes a_{ij}\\
                                                     &= 0.
  \end{align*}
  Then, since $\set{e_{ij}}_{ij}$ is linearly independent, we know that $x_{ij} = 0$ for all $i,j$, so $\left( a_{ij} \right)_{ij} = 0$, so $\varphi$ is injective.\newline

  Now, let $t\in \Mat_n\left( \C \right)\otimes A$ be given by
  \begin{align*}
    t &= \sum_{k} m_k\otimes a_k,
  \end{align*}
  where $m_k\in \Mat_n\left( \C \right)$ and $a_k\in A$. Then, using the matrix units, we write each $m_k$ as
  \begin{align*}
    m_k &= \sum_{i,j=1}^{n}m_k\left( i,j \right)e_{ij}.
  \end{align*}
  This gives
  \begin{align*}
    t &= \sum_{k}\left( \sum_{i,j=1}^{n}m_k(i,j)e_{ij} \right)\otimes a_k\\
      &= \sum_{i,j=1}^{n}e_{ij}\otimes \left( \sum_{k}m_k(i,j)a_k \right).
  \end{align*}
  Defining $a_{ij}\coloneq \sum_{k}m_{k}(i,j)a_k$, we get
  \begin{align*}
    t &= \sum_{i,j=1}^{n}e_{ij}\otimes a_{ij},
  \end{align*}
  meaning that
  \begin{align*}
    \varphi\left( \left( x_{ij} \right)_{ij} \right) &= t.
  \end{align*}
  Thus, $\varphi$ is surjective.\newline

  We will show now that $\varphi$ is multiplicative and $\ast$-preserving. If $\left( a_{ij} \right)_{ij}$ and $\left( b_{ij} \right)_{ij}$ belong to $\Mat_n\left( A \right)$.
  \begin{align*}
    \varphi\left( \left( a_{ik} \right)_{ik} \right)\varphi\left( \left( b_{lj} \right)_{lj} \right) &= \left( \sum_{i,k=1}^{n}e_{ik}\otimes a_{ik} \right)\left( \sum_{l,j=1}^{n}e_{lj}\otimes b_{lj} \right)\\
                                                                                                     &= \sum_{i,j,k,l=1}^{n}\left( e_{ik}\otimes a_{ik} \right)\left( e_{lj}\otimes b_{lj} \right)\\
                                                                                                     &= \sum_{i,j,k,l=1}^{n} e_{ik}e_{lj}\otimes a_{ik}b_{lj}\\
                                                                                                     &= \sum_{i,j,k=1}^{n}e_{ik}e_{kj}\otimes a_{ik}b_{kj}\\
                                                                                                     &= \sum_{ij,k=1}^{n}e_{ij}\otimes a_{ik}b_{kj}\\
                                                                                                     &= \sum_{i,j=1}^{n}e_{ij}\otimes \left( \sum_{k=1}^{n}a_{ik}b_{kj} \right)\\
                                                                                                     &= \varphi\left( \left( \sum_{k=1}^{n}a_{ik}b_{kj} \right)_{ij} \right)\\
                                                                                                     &= \varphi\left( \left( a_{ij} \right)_{ij}\left( b_{ij}\right)_{ij} \right).
  \end{align*}
  Similarly,
  \begin{align*}
    \varphi\left( \left( a_{ij} \right)_{ij} \right)^{\ast} &= \left( \sum_{i=1}^{n}e_{ij}\otimes a_{ij} \right)^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}\left( e_{ij}\otimes a_{ij} \right)^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}e_{ij}^{\ast}\otimes a_{ij}^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}e_{ji}\otimes a_{ij}^{\ast}\\
                                                            &= \sum_{i,j=1}^{n}e_{ij}\otimes a_{ji}^{\ast}\\
                                                            &= \varphi\left( \left( a_{ji}^{\ast} \right)_{ij} \right)\\
                                                            &= \varphi\left( \left( a_{ij} \right)_{ij}^{\ast} \right).
  \end{align*}
\end{proof}
There are lots of useful results using amplification to the matrix algebras.
\begin{example}[Dilating an Isometry]
  Let $V$ be an isometry, and let $P = I_{\mathcal{H}} - VV^{\ast}$ be the projection onto $\Ran\left( V \right)^{\perp}$. Define $U$ on $\mathcal{K} = \mathcal{H} \oplus \mathcal{H}$ by
  \begin{align*}
    U &= \begin{pmatrix}V & P \\ 0 & V^{\ast}\end{pmatrix}.
  \end{align*}
  We find that
  \begin{align*}
    U^{\ast} &= \begin{pmatrix}V^{\ast} & 0 \\ P & V\end{pmatrix}\\
    UU^{\ast} &= \begin{pmatrix}V & P \\ 0 & V^{\ast}\end{pmatrix} \begin{pmatrix}V^{\ast} & 0 \\ P & V\end{pmatrix}\\
              &= \begin{pmatrix}VV^{\ast} + P  & PV \\ V^{\ast}P & V^{\ast}V\end{pmatrix}\\
              &= \begin{pmatrix}I_{\mathcal{H}} & 0 \\ 0 & I_{\mathcal{H}}\end{pmatrix}\\
              &= I_{\mathcal{K}}\\
    U^{\ast}U &= \begin{pmatrix}V^{\ast} & 0 \\ P & V\end{pmatrix} \begin{pmatrix}V & P \\ 0 & V^{\ast}\end{pmatrix}\\
              &= I_{\mathcal{K}}.
  \end{align*}
  Thus, $U$ is a unitary on $\mathcal{K}$. We may identify $\mathcal{H} \cong \mathcal{H}\oplus 0$, and take
  \begin{align*}
    V^n &= P_{\mathcal{H}}U^{n}|_{\mathcal{H}}
  \end{align*}
  for all $n\geq 0$. Thus, we are able to realize any isometry $V$ as the restriction of some unitary to a subspace that respects powers.
\end{example}
\begin{example}[Dilating a Contraction]
  Similarly, we may define the isometric dilation of a contraction. Let $T$ be an operator on $\mathcal{H}$ with $\norm{T}\leq 1$, and define $D_{T} = \left( I - T^{\ast}T \right)^{1/2}$. We see that
  \begin{align*}
    \norm{T\left( h \right)}^2 + \norm{D_T\left( h \right)}^2 &= \iprod{T^{\ast}T\left( h \right)}{h} + \iprod{D_T^2\left( h \right)}{h}\\
                                                              &= \norm{h}^2.
  \end{align*}
  We consider now the sequence space
  \begin{align*}
    \ell_2\left( \mathcal{H} \right) &= \set{\left( h_n \right)_{n\in\N} | h_n\in \mathcal{H},\sum_{n=1}^{\infty}\norm{h_n}^2 < \infty }.
  \end{align*}
  We have the norm
  \begin{align*}
    \norm{\left( h_n \right)_n}^2 &= \sum_{n=1}^{\infty}\norm{h_n}^2
  \end{align*}
  and the inner product
  \begin{align*}
    \iprod{\left( h_n \right)_n}{\left( k_n \right)_n} &= \sum_{n=1}^{\infty} \iprod{h_n}{k_n}.
  \end{align*}
  We define the operator $V\colon \ell_2\left( \mathcal{H} \right)\rightarrow \ell_2\left( \mathcal{H} \right)$ by
  \begin{align*}
    V\left( \left( h_n \right)_n \right) &= \left( T\left( h_1 \right),D_T\left( h_1 \right),h_2,\dots \right).
  \end{align*}
  It then follows that $V$ is an isometry on $\ell_2\left( \mathcal{H} \right)$, and that if we identify $\mathcal{H}\cong \mathcal{H}\oplus 0 \oplus \cdots$, then $T^n = P_{\mathcal{H}}V^n|_{\mathcal{H}}$.
\end{example}
\begin{theorem}[Sz.-Nagy's Dilation Theorem]
  Let $T$ be a contraction operator on $\mathcal{H}$. There is a Hilbert space $\mathcal{K}$ containing $\mathcal{H}$ as a subspace, and a unitary operator $U$ on $\mathcal{K}$ such that $T^n = P_{\mathcal{H}}U^{n}|_{\mathcal{H}}$.
\end{theorem}
\begin{proof}
  Take $\mathcal{K} = \ell_2\left( \mathcal{H} \right)\oplus \ell_2\left( \mathcal{H} \right)$, and identify $\mathcal{H}$ as $\left( \mathcal{H}\oplus 0 \oplus \cdots \right)\oplus 0$. Let $V$ be the isometric dilation of $T$ on $\ell_2\left( \mathcal{H} \right)$, and let $U$ be the unitary dilation of $V$ on $\ell_2\left( \mathcal{H} \right)\oplus \ell_2\left( \mathcal{H} \right)$. Then, since $\mathcal{H}\subseteq \ell_2\left( \mathcal{H} \right)\oplus 0$, we have that $P_{\mathcal{H}}U^{n}|_{\mathcal{H}} = P_{\mathcal{H}}V^{n}|_{\mathcal{H}} = T^n$ for all $n \geq 0$.
\end{proof}
Whenever $Y$ is an operator on $\mathcal{K}$, $\mathcal{H}$ a (closed) subspace of $\mathcal{K}$, and $X = P_{\mathcal{H}}Y|_{\mathcal{H}}$, then we say $X$ is a compression of $Y$.
\begin{corollary}[Von Neumann's Inequality]
Let $T$ be a contraction on a Hilbert space. Then, for any polynomial $p$,
\begin{align*}
  \norm{p(T)} &\leq \sup_{|z| \leq 1}\left\vert p(z) \right\vert.
\end{align*}
\end{corollary}
\begin{proof}
  Let $U$ be a unitary dilation of $T$. Since $T^n = P_{\mathcal{H}}U^n|_{\mathcal{H}}$, linearity means we have $p(T) = P_{\mathcal{H}}p(U)|_{\mathcal{H}}$. Since $U$ is defined on a larger space than $T$, then $\norm{p(T)}\leq \norm{p(U)}$. Furthermore, since unitaries are normal, we have
  \begin{align*}
    \norm{p(U)} &= \sup_{\lambda\in\sigma(U)} \left\vert p(\lambda) \right\vert,
  \end{align*}
  where $\sigma(U)$ is the spectrum of $U$. Since $U$ is unitary, $\sigma(U)\subseteq \mathbb{T}$, so von Neumann's inequality follows.
\end{proof}
\section{Positive and Completely Positive Maps}%
\subsection{Positive Maps}%
There are certain results on positive maps that are useful in the study of completely positive maps. We introduce them here.
\begin{definition}
  If $S$ is a subset of a $C^{\ast}$-algebra $A$, we say $S$ is an operator system if $A$ is unital and $S$ is a self-adjoint sub\textit{space} of $A$ with $1_A\in S$.
\end{definition}
Note that if $S$ is an operator system and $h\in S$ is self-adjoint, then though the values $h_{+}$ and $h_{-}$, defined by the continuous functional calculus with
\begin{align*}
  f^+(x) &= \max\set{0,x}\\
  f^{-}(x) &= \min_{0,-x}
\end{align*}
may not belong to $S$, we can write $h$ as the difference of two positive elements in $s$ by
\begin{align*}
  h &= \frac{1}{2}\left( \norm{h}1_A + h \right) - \frac{1}{2}\left( \norm{h}1_A - h \right).
\end{align*}
\begin{definition}
  If $S$ is an operator system, $B$ is a $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ is a linear map, then $\phi$ is called positive if it maps positive elements of $S$ to positive elements of $B$.
\end{definition}
\begin{theorem}
  If $\phi$ is a positive linear functional on an operator system $S$, then $\norm{\phi} = \phi\left( 1_A \right)$.
\end{theorem}
When the range of $\phi$ is not $\C$, but rather a $C^{\ast}$-algebra, then the situation is a bit different.
\begin{proposition}
  Let $S$ be an operator system, and let $B$ be a $C^{\ast}$-algebra. If $\phi\colon S\rightarrow B$ is a positive map, then $\phi$ is bounded, with
  \begin{align*}
    \norm{\phi} &\leq 2\norm{\phi\left( 1_A \right)}.
  \end{align*}
\end{proposition}
\begin{proof}
  Note that if $p$ is positive, then $0\leq p \leq \norm{p}1_A$, so $0\leq \phi(p)\leq \norm{p}\phi\left( 1_A \right)$ since positive functions are order-preserving. Thus, we get $\norm{\phi(p)}\leq \norm{p}\norm{\phi(1)}$ when $p\geq 0$.\newline

  Note that when $p_1$ and $p_2$ are positive, then $\norm{p_1 - p_2}\leq \max\set{\norm{p_1},\norm{p_2}}$. If $h$ is self-adjoint, then we have
  \begin{align*}
    \norm{\phi(h)} &= \frac{1}{2}\phi\left( \norm{h}1_A + h \right) - \frac{1}{2}\phi\left( \norm{h}1_A - h \right),
  \end{align*}
  which is the difference of two positive elements in $B$. Thus, we have
  \begin{align*}
    \norm{\phi(h)} &\leq \frac{1}{2}\max\set{\norm{\phi\left( \norm{h}1_A + h \right)},\norm{\phi\left( \norm{h}1_A - h \right)}}\\
                   &\leq \norm{h}\norm{\phi(1)}.
  \end{align*}
  Finally, if $a$ is arbitrary then write $a = h + ik$ via the Cartesian decomposition, where $\norm{h},\norm{k}\leq \norm{a}$, and $h,k$ are self-adjoint. Thus, we have
  \begin{align*}
    \norm{\phi(a)} &\leq \norm{\phi(h)} + \norm{\phi(k)}\\
                   &\leq 2\norm{a}\norm{\phi\left(1_A\right)}.
  \end{align*}
\end{proof}
As it turns out, $2$ is the best constant.
\begin{example}
  Let $\T$ be the unit circle in $\C$, and $C(\T)$ be the continuous functions on $z$. Let $z$ be the cordinate function, and let $S\subseteq C\left( \T \right)$ be the subspace spanned by $1,z,\overline{z}$. Defining
  \begin{align*}
    \phi\left( a + bz + c\overline{z} \right) &= \begin{pmatrix}a & 2b \\ 2c & a\end{pmatrix},
  \end{align*}
  An element of $S$ is positive if and only if $c = \overline{b}$ and $a \geq 2\left\vert b \right\vert$, and an element of $\Mat_2\left( \C \right)$ is positive if and only if its diagonal entries and determinant are nonnegative real numbers. Thus, it is the case that $\phi$ is a positive map, but also 
  \begin{align*}
    2\norm{\phi(1)} &= 2\\
                    &= \norm{\phi(z)}\\
                    &\leq \norm{\phi},
  \end{align*}
  meaning $\norm{\phi} = 2\norm{\phi(1)}$.
\end{example}
We are interested in seeing when unital, positive maps are contractive.
\begin{lemma}
  Let $A$ be a $C^{\ast}$-algebra, and let $p_i$ be positive elements of $A$ such that
  \begin{align*}
    \sum_{i=1}^{n}p_i \leq 1.
  \end{align*}
  If $\lambda_i$ are scalars with $\left\vert \lambda_i \right\vert \leq 1$, then
  \begin{align*}
    \norm{\sum_{i=1}^{n}\lambda_ip_i} &\leq 1.
  \end{align*}
\end{lemma}
\begin{proof}
  Note that
  \begin{align*}
    \begin{pmatrix}\sum_{i=1}^{n}\lambda_ip_i & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 \end{pmatrix} &= \begin{pmatrix}p_1^{1/2} &\cdots & p_n^{1/2} \\ 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0\end{pmatrix} \operatorname{diag}\left( \lambda_1,\dots,\lambda_n \right) \begin{pmatrix}p_1^{1/2} & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ p_n^{1/2} & 0 & \cdots & 0\end{pmatrix}.
  \end{align*}
  The norm on the matrix on the left is $\norm{\sum_{i=1}^{n}\lambda_ip_i}$, while the three matrices on the right have norm less than $1$, using the fact that $\norm{a^{\ast}a} = \norm{a}^2$.
\end{proof}
\begin{theorem}
  Let $B$ be a $C^{\ast}$-algebra, $X$ a compact Hausdorff space, and $C(X)$ the continuous functions on $X$. Let $\phi\colon C(X)\rightarrow B$ be a positive map. Then, $\norm{\phi} = \norm{\phi(1)}$.
\end{theorem}
\begin{proof}
  We may assume $\phi(1)\leq 1$. Let $f\in C(X)$ with $\norm{f}\leq 1$, and let $\ve > 0$. Now, we may choose a finite open cover $\set{U_i}_{i=1}^{n}$ of $X$ such that $\left\vert f(x) - f\left( x_i \right) \right\vert < \ve$ for all $x\in U_i$, and let $\set{p_i}_{i = 1}^{n}$ be a partition of unity subordinate to the cover. That is, $\set{p_i}_{i=1}^{n}$ are nonnegative continuous functions satisfying $\sum_{i=1}^{n}p_i = 1$ and $p_i(x) = 0$ for $x\notin U_i$.\newline

  Set $\lambda_i = f\left( x_i \right)$, and note that if $p_i(x)\neq 0$ for some $i$, then $x\in U_i$ and $\left\vert f(x) - \lambda_i \right\vert < \ve$. Hence, for any $x$, we have
  \begin{align*}
    \left\vert f(x) - \sum_{i=1}^{n}\lambda_ip_i(x) \right\vert &= \left\vert \sum_{i=1}^{n}\left( f(x) - \lambda_i \right)p_i(x) \right\vert\\
                                                                &\leq \sum_{i=1}^{n}\left\vert f(x) - \lambda_i \right\vert p_i(x)\\
                                                                &< \sum_{i=1}^{n}\ve p_i(x)\\
                                                                &= \ve.
  \end{align*}
  By above, we know that $\norm{\sum_{i=1}^{n}\lambda_ip_i} \leq 1$, we have
  \begin{align*}
    \norm{\phi(f)} &\leq \norm{\phi\left( f - \sum_{i=1}^{n}\lambda_ip_i \right)} + \norm{\sum_{i=1}^{n}\phi\left( p_i \right)}\\
                   &< 1 + \ve\norm{\phi}.
  \end{align*}
  Since $\ve$ was arbitrary, we have $\norm{\phi} \leq 1$.
\end{proof}
\begin{lemma}[Riesz--Fejér Theorem]
  Let $\tau\left( e^{i\theta} \right) = \sum_{n=-N}^{N} a_ne^{in\theta}$ be a strictly positive function on $\T$. Then, there is a polynomial $p(z) = \sum_{n=0}^{n}p_nz^n$ such that
  \begin{align*}
    \tau\left( e^{i\theta} \right) &= \left\vert p\left( e^{i\theta} \right) \right\vert^{2}.
  \end{align*}
\end{lemma}
\begin{proof}
  Note that $\tau$ is real-valued, so $a_{-n} = \overline{a_n}$, and $a_0$ is real. Assuming $a_{-N} \neq 0$, we take $g(z) = \sum_{n=-N}^{N}a_nz^{n+N}$, so that $g$ is a polynomial of degree $2n$, $g(0)\neq 0$.\newline

  We have $g\left( e^{i\theta} \right) = \tau\left( e^{i\theta} \right)e^{iN\theta}\neq 0$, and that $\overline{g\left( 1/\overline{z} \right)} = z^{-2N}g(z)$.\newline

  We write the $2N$ zeros of $g$ as $z_1,\dots,z_N,1/\overline{z_1},\dots,1/\overline{z_N}$.\newline

  Set $q(z) = \left( z-z_1 \right)\cdots \left( z-z_N \right)$ and $h(z) = \left( z-1/\overline{z_1} \right)\cdots \left( z-1/\overline{z_N} \right)$. We have that
  \begin{align*}
    g(z) &= a_Nq(z)h(z),
  \end{align*}
  where
  \begin{align*}
    \overline{h(z)} &= \frac{\left( -1 \right)^N\overline{z}^Nq\left( 1/\overline{z} \right)}{z_1\cdots z_N}.
  \end{align*}
  Thus, we have
  \begin{align*}
    \tau\left( e^{i\theta} \right) &= e^{-iN\theta}g\left( e^{i\theta} \right)\\
                                   &= \left\vert g\left( e^{i\theta} \right) \right\vert\\
                                   &= \left\vert a_Nq\left( e^{i\theta} \right)\overline{h}\left( e^{i\theta} \right) \right\vert\\
                                   &= \frac{a_N}{z_1\cdots z_N}\left\vert q\left( e^{i\theta} \right) \right\vert^2.
  \end{align*}
\end{proof}
\begin{theorem}
  Let $T$ be an operator on $\mathcal{H}$ with $\norm{T}\leq 1$, and let $S\subseteq C\left( \T \right)$ be the operator system defined by
  \begin{align*}
    S &= \set{p\left( e^{i\theta} \right) + \overline{q\left( e^{i\theta} \right)} | p,q\text{ are polynomials}}.
  \end{align*}
  Then, $\phi\colon S\rightarrow \B\left( \mathcal{H} \right)$, given by $\phi\left( p + \overline{q} \right) = p(T) + q(T)^{\ast}$ is positive.
\end{theorem}
\begin{proof}
  It is enough to prove that $\phi(\tau)$ is positive for every \textit{strictly} positive $\tau$.\newline

  Let $\tau\left( e^{i\theta} \right)$ be strictly positive in $S$, meaning $\tau\left( e^{i\theta} \right) = \sum_{\ell,k=0}^{n}\alpha_{\ell}\overline{\alpha_{k}}e^{i\left( \ell - k \right)\theta}$. We must prove that
  \begin{align*}
    \phi\left( \tau \right) &= \sum_{\ell,k=0}^{n}\alpha_{\ell}\overline{\alpha_{k}} T\left( \ell - k \right),
  \end{align*}
  where
  \begin{align*}
    T\left( j \right) &= \begin{cases}
      T^j & j \geq 0\\
      \left( T^{\ast} \right)^{-j} & j < 0.
    \end{cases}
  \end{align*}
  Fix $x\in \mathcal{H}$. Note that
  \begin{align*}
    \iprod{\phi\left(\tau\right)(x)}{x} &= \iprod{ \begin{pmatrix}I & T^{\ast} & \cdots & \left( T^{\ast} \right)^{n}\\ T & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & T^{\ast} \\ T^{n} & \cdots & T & I\end{pmatrix} \begin{pmatrix}\overline{\alpha_1}x \\ \overline{\alpha_2}x \\ \vdots \\ \overline{\alpha_n} x\end{pmatrix} }{ \begin{pmatrix}\overline{\alpha_1}x \\ \overline{\alpha_2}x \\ \vdots \\ \overline{\alpha_n}x\end{pmatrix} },\tag*{(\textasteriskcentered)}
  \end{align*}
  where our matrix operator acts on $\mathcal{H}^{(n)}$. Thus, we only need to show that this matrix operator is positive.\newline

  To that end, define the $n\times n$ matrix
  \begin{align*}
    R &= \begin{pmatrix}0 & \cdots & \cdots & \cdots & 0 \\ T & \ddots & \ddots & \ddots & \vdots \\ 0 & \ddots & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots & \vdots \\ 0 & \cdots & 0 & T & 0\end{pmatrix},
  \end{align*}
  and note that $R^{n+1} = 0$, with $\norm{R}_{\op} \leq 1$ (as $T$ is a contraction).\newline

  We let $I$ denote the identity operator on $\mathcal{H}^{(n)}$. The matrix operator (\textasteriskcentered) can be written as
  \begin{align*}
    I + R + R^2 + \cdots + R^{n} + R^{\ast} + \cdots + \left( R^{\ast} \right)^{n} &= \left( I-R \right)^{-1} + \left( I-R^{\ast} \right)^{-1} - I,
  \end{align*}
  where we used the fact that $R^{n+1} = 0$ in the geometric series for $\left( I-R \right)^{-1}$ and $\left( I-R^{\ast} \right)^{-1}$. To see that this operator is positive, we let $h\in \mathcal{H}^{(n)}$, and let $h = \left( I-R \right)y$ for some $y\in \mathcal{H}^{(n)}$. Then,
  \begin{align*}
    \iprod{\left( \left( I-R \right)^{-1} + \left( I-R^{\ast} \right)^{-1} - I \right)\left( h \right)}{h} &= \iprod{y}{\left( I-R \right)y} + \iprod{\left( I-R \right)\left( y \right)}{y} - \iprod{\left( I-R \right)\left( y \right)}{\left( I-R \right)\left( y \right)}\\
                                                                                                           &= \norm{y}^2 - \norm{R\left( y \right)}^2\\
                                                                                                           &\geq 0,
  \end{align*}
  since $R$ is a contraction.
\end{proof}
Now, we may prove von Neumann's inequality in a different way.
\begin{theorem}[von Neumann's Inequality] 
  Let $T$ be an operator on a Hilbert space with $\norm{T}_{\op}\leq 1$. Then, for any polynomial $p$, we have
  \begin{align*}
    \norm{p(T)}_{\op} &\leq \norm{p},
  \end{align*}
  where $\norm{p} = \sup_{\theta} \left\vert p\left( e^{i\theta} \right) \right\vert$.
\end{theorem}
\begin{proof}
  The operator system defined by
  \begin{align*}
    S &= \set{p\left( e^{i\theta} \right) + \overline{q\left( e^{i\theta} \right)} | p,q\text{ polynomials}}
  \end{align*}
  is a $\ast$-algebra that separates points, so by the Stone--Weierstrass theorem, $S$ is dense in $C\left(\mathbb{T}\right)$. We know that $\phi$ is bounded, so it extends $C(\T)$. The extension to $\overline{S} = C(\T)$ also positive, so $\phi$ is contractive.
\end{proof}
Note that if $A\left( \D \right)$ denotes the functions analytic on $\D$ and continuous on $\overline{\D}$, we know that by the maximum modulus principle that the supremum of any function in $A(\D)$ occurs on $\T$. We may thus consider $A(\D)$ as a closed subalgebra of $C(\T)$.\newline

Furthermore, polynomials are dense in $A(\D)$. Thus, the homomorphism $p\mapsto p(T)$ extends to a homomorphism $f\mapsto f(T)$ that satisfies $\norm{f(T)}_{\op}\leq \norm{f}$ for all $f\in A(\D)$.\newline

Another consequence is that if $a$ is an element of some unital $C^{\ast}$-algebra $A$ with $\norm{a}\leq 1$, then there is a unital, positive map $\phi\colon C(\T)\rightarrow A$ such that $\phi(p) = p(a)$.
\begin{corollary}
  Let $B$ and $C$ be unital $C^{\ast}$-algebras. Let $A$ be a unital subalgebra of $B$, and let $S = A + A^{\ast}$ be an operator space. If $\phi\colon S\rightarrow C$ is positive, then $\norm{\phi(a)}\leq \norm{\phi(1)}\norm{a}$.
\end{corollary}
\begin{proof}
  Let $a\in A$ with $\norm{a}\leq 1$. We may extend $\phi$ to a positive map on $\overline{S}$. There is also a positive map $\psi\colon C(\T)\rightarrow B$ with $\psi\left( p \right) = p(a)$. Since $A$ is an algebra, we must have $\Ran\left( \psi \right)\subseteq \overline{S}$.\newline

  The composition of positive maps is positive, so we have
  \begin{align*}
    \norm{\phi(a)} &= \norm{\phi\circ\psi\left( e^{i\theta} \right)}\\
                   &\leq \norm{\phi\circ \psi(1)}\norm{e^{i\theta}}\\
                   &= \norm{\phi(1)}.
  \end{align*}
\end{proof}
If $\phi(1) = 1$, then $\phi$ is a contraction on $A$, though $\phi$ may not be a contraction on all of $S$.
\begin{corollary}
  Let $A$ and $B$ be unital $C^{\ast}$-algebras with $\phi\colon A\rightarrow B$ a positive map. Then, $\norm{\phi}_{\op}= \norm{\phi(1)}$.
\end{corollary}
\begin{lemma}
  Let $A$ be a $C^{\ast}$-algebra, $S\subseteq A$ an operator system, and $f\colon S\rightarrow \C$ a linear functional with $f(1) = 1 = \norm{f}$. If $a$ is a normal element of $A$, and $a\in S$, then $f(a)\in \overline{\operatorname{conv}}\left( \sigma\left( a \right) \right)$.
\end{lemma}
\begin{proof}
  Suppose not.\newline

  The convex hull of a compact set is the intersection of all closed disks containing the set. Then, there exists $\lambda$ and $r > 0$ such that $\left\vert f(a) - \lambda \right\vert > r$, where
  \begin{align*}
    \sigma\left( a \right) &\subseteq \set{z | \left\vert z - \lambda \right\vert \leq r}.
  \end{align*}
  Then, $\sigma\left( a - \lambda 1 \right)\subseteq \set{z | \left\vert z \right\vert \leq r}$. Since norm and spectral radius agree for normal elements, we have $\norm{a - \lambda 1} \leq r$, while $\left\vert f\left( a - \lambda 1 \right) \right\vert > r$. This contradicts the fact that $\norm{f}\leq 1$.
\end{proof}
\begin{proposition}
  Let $S$ be an operator system, $B$ a unital $C^{\ast}$-algebra, and let $\phi\colon S\rightarrow B$ be a unital contraction. Then, $\phi$ is positive.
\end{proposition}
\begin{proof}
  Since we can represent $B$ on $\B\left( \mathcal{H} \right)$, we assume $B = \B\left( \mathcal{H} \right)$ for some Hilbert space $\mathcal{H}$. Fix $x\in \mathcal{H}$ with $\norm{x} = 1$.\newline

  Setting $f(a) = \iprod{\phi(a)(x)}{x}$, we have $f(1) = 1$ and $\norm{f}\leq \norm{\phi}$. If $a$ is positive, then $f(a)$ is positive by the previous lemma, so since $x$ was arbitrary, $\phi(a)$ is also positive.
\end{proof}
\begin{proposition}
  Let $A$ be a unital $C^{\ast}$-algebra, and let $M$ be a unital subspace of $A$. If $B$ is a unital $C^{\ast}$-algebra, and $\phi\colon M\rightarrow B$ is a unital contraction, then the map $ \widetilde{\phi}\colon M + M^{\ast}\rightarrow B $, given by
  \begin{align*}
    \widetilde{\phi}\left( a + b^{\ast} \right) = \phi\left( a \right) + \phi\left( b \right)^{\ast}
  \end{align*}
  is well-defined and the unique positive extension of $\phi$ to $M + M^{\ast}$.
\end{proposition}
\begin{proof}
  To prove that $ \widetilde{\phi} $ is well-defined, it is enough to prove that if $a$ and $a^{\ast}$ belong to $M$, then $\phi\left( a \right)^{\ast} = \phi\left( a^{\ast} \right)$. Set
  \begin{align*}
    S_1 &= \set{a | a\in M\text{ and }a^{\ast}\in M}.
  \end{align*}
  Then, $S_1$ is an operator system, and $\phi$ is a unital, contractive map on $S_1$, hence positive by the previous proposition. Since $\phi$ is positive, $\phi$ is self-adjoint, so $\phi\left( a^{\ast} \right) = \phi\left( a \right)^{\ast}$, meaning $\widetilde{\phi}$ is well-defined.\newline

  To see that $\widetilde{\phi}$ is positive, we may assume $B = \B\left( \mathcal{H} \right)$. Fix $x\in S_{\mathcal{H}}$, and set $\widetilde{\rho}\left( a \right) = \iprod{\widetilde{\phi}\left( a \right)\left( x \right)}{x}$. We will show that $\widetilde{\rho}$ is positive.\newline

  Let $\rho\colon M\rightarrow C$ be defined by $\rho(a) = \iprod{\phi(a)(x)}{x}$. Then, $\norm{\rho} = 1$, and so by the Hahn--Banach theorem, $\rho$ extends to $\rho_1\colon M + M^{\ast}\rightarrow \C$ with $\norm{\rho_1} = 1$. Since $\rho_1$ is positive, $\rho_1\left( a + b^{\ast} \right) = \rho\left( a \right) + \overline{\rho\left( b \right)} = \widetilde{\rho}\left( a + b^{\ast} \right)$. Thus $\widetilde{\rho}$ is positive.
\end{proof}
\subsection{Completely Positive Maps}%
\begin{definition}
  If $A$ is a $C^{\ast}$ algebra and $M\subseteq A$ is a linear subspace, then we call $M$ an operator space. 
\end{definition}
We may regard $\Mat_n\left( M \right)$ as a subspace of $\Mat_n\left( A \right)$, with the norm structure inherited from the unique norm structure on $\Mat_n\left( A \right)$. The primary distinguishing feature of an operator space is the fact that $\Mat_n\left( M \right)$ has a unique norm for all $n\geq 1$.\newline

Similarly, if $S\subseteq A$ is an operator system, then we endow $\Mat_n\left( S \right)$ with the norm and order it inherits from $\Mat_n\left( A \right)$.
\begin{definition}
  If a matrix $S\in \Mat_n\left( \C \right)$ is positive definite and Hermitian, then $S$ is positive.
\end{definition}
\begin{proof}
  If $S$ is Hermitian, then we know that all the eigenvalues of $S$ are real and that $S$ is diagonalizable with orthonormal vectors $\set{v_1,\dots,v_n}$. Therefore, if 
  \begin{align*}
    \iprod{S\left( x \right)}{x} &\geq 0
  \end{align*}
  for all $x\in \C^n$, then so too does this hold for $v_j$ and corresponding $\lambda_j$. Thus, $\lambda_j\geq 0$ for all $j$, so $S$ is positive.
\end{proof}
\begin{lemma}[Ordering of $\Mat_n\left( \B\left( \mathcal{H} \right) \right)$]
  We have that $\left( T_{ij} \right)_{ij}\in \Mat_{n}\left( \B\left( \mathcal{H} \right) \right)_{+}$ if and only if, for all $x_1,\dots,x_n\in \mathcal{H}$, we have $\left( \iprod{T_{ij}\left( x_j \right)}{x_i} \right)_{ij}\in \Mat_n\left( \C \right)_{+}$.
\end{lemma}
\begin{definition}
  If $B$ is a $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ is a linear map, then $\phi_n\colon \Mat_n\left( S \right)\rightarrow \Mat_n\left( B \right)$ is defined by $\phi_n\left( \left( a_{ij} \right)_{ij} \right) = \left( \phi\left( a_{ij} \right) \right)_{ij}$. We call $\phi$ $n$-positive if $\phi_n$ is positive, and $\phi$ is called completely positive if it is $n$-positive for all $n$.\newline

  We call $\phi$ completely bounded if $\sup_{n}\norm{\phi_n}$ is finite. We set
  \begin{align*}
    \norm{\phi}_{\cb} &= \sup_{n}\norm{\phi_n}.
  \end{align*}
  We say $\phi$ is completely isometric or completely contractive if each $\phi_n$ is isometric and that $\norm{\phi}_{\cb} \leq 1$ respectively.
\end{definition}
We investigate some of the properties of classes of completely positive maps such that we may prove when they are automatically completely positive.
\begin{lemma}
  Let $A$ be a $C^{\ast}$-algebra, and let $a,b\in A$. Then, the following hold.
  \begin{enumerate}[(i)]
    \item We have $\norm{a}\leq 1$ if and only if
      \begin{align*}
        \begin{pmatrix}1 & a \\ a^{\ast} & 1\end{pmatrix}
      \end{align*}
      is positive in $\Mat_2\left( A \right)$.
    \item We have
      \begin{align*}
        \begin{pmatrix}1 & a \\ a^{\ast} & b\end{pmatrix}
      \end{align*}
      is positive in $\Mat_2\left( A \right)$ if and only if $a^{\ast}a \leq b$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  Let $A$ be represented by $\pi\colon A\rightarrow \B\left( \mathcal{H} \right)$, and set $T = \pi(a)$. If $\norm{T}\leq 1$, then for any $x,y\in \mathcal{H}$, we have
  \begin{align*}
    \iprod{ \begin{pmatrix}I & T \\ T^{\ast} & I\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{T\left( y \right)}{x} + \iprod{x}{T\left( y \right)} + \iprod{y}{y}\\
                             &\geq \norm{x}^2 - 2\norm{T}_{\op}\norm{y}\norm{x} + \norm{y}^2\\
                             &\geq 0.
  \end{align*}
  Conversely, if $\norm{T}_{\op} > 1$, then there exist unit vectors $x$ and $y$ such that $ \iprod{T\left( y \right)}{x} < -1 $, and the above inner product is negative.\newline

  %Now, to see (ii), we begin by assuming that $b\geq a^{\ast}a$. Just as before, we represent $A$ on $\B\left( \mathcal{H} \right)$, and we use the abuse of notation as $b = \pi(b)$ and $a = \pi(a)$. Since $b\geq a^{\ast}a$, we have, for all $y\in \mathcal{H}$,
  %\begin{align*}
  %  \iprod{\left( b-a^{\ast}a \right)\left( y \right)}{y} &\geq 0,
  %\end{align*}
  %so that
  %\begin{align*}
  %  \iprod{b\left( y \right)}{y} \geq \norm{a\left( y \right)}^2.
  %\end{align*}
  %Thus, in the $2\times 2$ case, we have, for any $ \begin{pmatrix}x \\ y\end{pmatrix} \in \mathcal{H}^{(2)} $,
  %\begin{align*}
  %  \iprod{ \begin{pmatrix}1 & a \\ a^{\ast} & b\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{a(y)}{x} + \iprod{a^{\ast}\left( x \right)}{y} + \iprod{b\left( y \right)}{y}\\
  %                           &\geq \iprod{x}{x} + \iprod{a(y)}{x} + \iprod{a^{\ast}(x)}{y} + \iprod{a(y)}{a(y)}\\
  %                           &= \iprod{x}{x} + \iprod{a(y)}{a(y)} + 2\re\left( \iprod{a(y)}{x} \right)\\
  %                           &\geq \iprod{x}{x} + \iprod{a(y)}{a(y)} - 2 \norm{a(y)}\norm{x}\\
  %                           &= \norm{x}^2 + \norm{a(y)}^2 - 2\norm{a(y)}\norm{x}\\
  %                           &\geq 0.
  %\end{align*}
  %Thus, the matrix is positive.\newline

  %Now, we suppose that $b\ngeq a^{\ast}a$. Then, there exists some $y\in \mathcal{H}$ such that $ \iprod{b\left( y \right)}{y} < \norm{a(y)}^2 $.
\end{proof}
\begin{exercise}[Exercise 3.2]
  Let $P,Q,A$ be operators on a Hilbert space $\mathcal{H}$, with $P,Q$ positive.
  \begin{enumerate}[(i)]
    \item Show that
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix}\geq 0
      \end{align*}
      if and only if
      \begin{align*}
        \left\vert \iprod{Ax}{y} \right\vert^2 &\leq \iprod{Py}{y} \iprod{Qx}{x}.
      \end{align*}
    \item Show that
      \begin{align*}
        \begin{pmatrix}1 & A \\ A^{\ast} & B\end{pmatrix} &\geq 0
      \end{align*}
      if and only if $B\geq A^{\ast}A$.
    \item Show that if
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} &\geq 0,
      \end{align*}
      then for any $x\in \mathcal{H}$, we have
      \begin{align*}
        0 &\leq \iprod{\left( P + A + A^{\ast} + Q \right)x}{x}\\
          &\leq \left( \sqrt{ \iprod{Px}{x} } + \sqrt{ \iprod{Qx}{x} }\right) ^2,
      \end{align*}
      hence
      \begin{align*}
        \norm{P + AA^{\ast} + Q} &\leq \left( \norm{P}^{1/2} + \norm{Q}^{1/2} \right)^2.
      \end{align*}
    \item Show that if
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & P\end{pmatrix} &\geq 0,
      \end{align*}
      then $A^{\ast}A \leq \norm{P}P$, implying $\norm{A}\leq \norm{P}$.
  \end{enumerate}
\end{exercise}
\begin{solution}\hfill
  \begin{enumerate}[(i)]
    \item We see that
      \begin{align*}
        \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} \geq 0
      \end{align*}
      if and only if, for any $x,y\in \mathcal{H}$, we have
      \begin{align*}
        \begin{pmatrix} \iprod{Px}{x} & \iprod{Ay}{x} \\ \iprod{A^{\ast}x}{y} & \iprod{Qy}{y}\end{pmatrix} &\geq 0.
      \end{align*}
      Thus, we have
      \begin{align*}
        \det \begin{pmatrix} \iprod{Px}{x} & \iprod{Ay}{x} \\ \iprod{A^{\ast}x}{y} & \iprod{Qy}{y}\end{pmatrix} &= \iprod{Px}{x} \iprod{Qy}{y} - \left\vert \iprod{Ay}{x} \right\vert^2\\
                                           &\geq 0,
      \end{align*}
      so that
      \begin{align*}
        \left\vert \iprod{Ay}{x} \right\vert^2 &\leq \iprod{Px}{x} \iprod{Qy}{y}.
      \end{align*}
      Suppose that
      \begin{align*}
        \left\vert \iprod{Ay}{x} \right\vert^2 &\leq \iprod{Px}{x} \iprod{Qy}{y}.
      \end{align*}
      Now, for any $x,y\in \mathcal{H}$, we have
      \begin{align*}
        \iprod{ \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{Px}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{Qy}{y}\\
                                 &= \iprod{Px}{x} + 2\re \left( \iprod{Ay}{x} \right) + \iprod{Qy}{y}\\
                                 &\geq \iprod{Px}{x} - 2\left\vert \iprod{Ay}{x} \right\vert + \iprod{Qy}{y}\\
                                 &\geq \iprod{Px}{x} - 2 \iprod{Px}{x}^{1/2} \iprod{Qy}{y}^{1/2} + \iprod{Qy}{y}\\
                                 &= \left( \iprod{Px}{x}^{1/2}  + \iprod{Qy}{y}^{1/2} \right)^{2}\\
                                 &\geq 0.
      \end{align*}
    \item We begin by assuming that $B\geq A^{\ast}A$. Since $B\geq A^{\ast}A$, we have
    \begin{align*}
      \iprod{\left( B-A^{\ast}A \right)\left( y \right)}{y} &\geq 0,
    \end{align*}
    so that
    \begin{align*}
      \iprod{By}{y} \geq \norm{Ay}^2.
    \end{align*}
    Thus, in the $2\times 2$ case, we have, for any $ \begin{pmatrix}x \\ y\end{pmatrix} \in \mathcal{H}^{(2)} $,
    \begin{align*}
      \iprod{ \begin{pmatrix}1 & A \\ A^{\ast} & B\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{By}{y}\\
                               &\geq \iprod{x}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{Ay}{Ay}\\
                               &= \iprod{x}{x} + \iprod{Ay}{Ay} + 2\re\left( \iprod{Ay}{x} \right)\\
                               &\geq \iprod{x}{x} + \iprod{Ay}{Ay} - 2 \norm{Ay}\norm{x}\\
                               &= \norm{x}^2 + \norm{Ay}^2 - 2\norm{Ay}\norm{x}\\
                               &\geq 0.
    \end{align*}
    Thus, the matrix is positive.\newline

    For the converse direction, we suppose $B\ngeq A^{\ast}A$. Then, there is some $y\in \mathcal{H}$ such that $ \iprod{\left( B-A^{\ast}A \right)(y)}{y} < 0$. This gives $ \iprod{By}{y} < \norm{Ay}^2 $. We may select $y$ such that $\norm{Ay}^2 = 1$. Setting $x = -Ay$, we have
    \begin{align*}
      \iprod{ \begin{pmatrix}1 & A \\ A^{\ast} & B\end{pmatrix} \begin{pmatrix}x\\y\end{pmatrix} }{ \begin{pmatrix}x\\y\end{pmatrix} } &= \iprod{x}{x} + \iprod{Ay}{x} + \iprod{A^{\ast}x}{y} + \iprod{By}{y}\\
                               &= \iprod{x}{x} + \iprod{Ay}{x} + \iprod{x}{Ay} + \iprod{By}{y}\\
                               &= \iprod{-Ay}{-Ay} + \iprod{Ay}{-Ay} + \iprod{-Ay}{Ay} + \iprod{By}{y}\\
                               &= \norm{Ay}^2 - 2\norm{Ay}^2 + \iprod{By}{y}\\
                               &= -1 + \iprod{By}{y}\\
                               &< -1 + \norm{Ay}^2\\
                               &=0.
    \end{align*}
    Thus, the matrix is negative.
  \item We apply the result in (i) to the vector $ \begin{pmatrix}x\\x\end{pmatrix} $. This gives
    \begin{align*}
      \iprod{ \begin{pmatrix}P & A \\ A^{\ast} & Q\end{pmatrix} \begin{pmatrix}x\\x\end{pmatrix} }{ \begin{pmatrix}x\\x\end{pmatrix} } &= \iprod{Px}{x} + \iprod{Ax}{x} + \iprod{A^{\ast}x}{x} + \iprod{Qx}{x}\\
                               &= \iprod{Px}{x} + 2\re \left( \iprod{Ax}{x} \right) + \iprod{Qx}{x}\\
                               &\leq \iprod{Px}{x} + 2 \left\vert \iprod{Ax}{x} \right\vert + \iprod{Qx}{x}\\
                               &\leq \iprod{Px}{x} + 2 \iprod{Px}{x}^{1/2} \iprod{Qx}{x}^{1/2} + \iprod{Qx}{x}\\
                               &= \left( \iprod{Px}{x}^{1/2} + \iprod{Qx}{x}^{1/2} \right)^2.
    \end{align*}
  \item Setting $Q = P$ in the result from (i), we have
    \begin{align*}
      \left\vert \iprod{Ay}{x} \right\vert^2 &\leq \iprod{Px}{x} \iprod{Py}{y},
    \end{align*}
    which holds for all $x,y\in \mathcal{H}$. In particular, setting $x = Ay$, we have
    \begin{align*}
      \left\vert \iprod{Ay}{Ay} \right\vert &\leq \iprod{PAy}{Ay} \iprod{Py}{y}\\
                                            &\leq \norm{PAy}\norm{Ay} \iprod{Py}{y}\\
                                            &\leq \norm{P} \norm{Ay}^2 \iprod{Py}{y}.
    \end{align*}
    This gives
    \begin{align*}
      \norm{Ay}^4 &\leq \norm{P}\norm{Ay}^2 \iprod{Py}{y}\\
      \norm{Ay}^2 &\leq \norm{P} \iprod{Py}{y}\\
      \iprod{A^{\ast}Ay}{y} &\leq \norm{P} \iprod{Py}{y},
    \end{align*}
    or that $A^{\ast}A \leq \norm{P}P$.
  \end{enumerate}
\end{solution}
\begin{proposition}
  Let $S$ be an operator system, $B$ a unital $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ a unital $2$-positive map. Then, $\phi$ is contractive.
\end{proposition}
\begin{proof}
  Let $a\in S$ with $\norm{a}\leq 1$. Then,
  \begin{align*}
    \phi_2 \begin{pmatrix}1 & a \\ a^{\ast} & 1\end{pmatrix} &= \begin{pmatrix}1 & \phi(a) \\ \phi(a)^{\ast} & 1\end{pmatrix}
  \end{align*}
  is positive, hence $\norm{\phi(a)}\leq 1$.
\end{proof}
\begin{proposition}[Cauchy--Schwarz for 2-positive Maps]
  Let $A,B$ be unital $C^{\ast}$-algebras, and let $\phi\colon A\rightarrow B$ be a unital $2$-positive map. Then,
  \begin{align*}
    \phi(a)^{\ast}\phi(a) &\leq \phi\left( a^{\ast}a \right)
  \end{align*}
  for all $a\in A$.
\end{proposition}
\begin{proof}
  We have that
  \begin{align*}
    \begin{pmatrix}1 & a \\ 0 & 0\end{pmatrix}^{\ast} \begin{pmatrix}1 & a \\ 0 & 0\end{pmatrix} &= \begin{pmatrix}1 & \phi(a) \\ \phi(a)^{\ast} & \phi\left( a^{\ast}a \right)\end{pmatrix}\\
                     &\geq 0,
  \end{align*}
  meaning that $\phi(a)^{\ast}\phi(a)\leq \phi\left( a^{\ast}a \right)$ by above.
\end{proof}
\begin{proposition}
  Let $A$ and $B$ be unital $C^{\ast}$-algebras, and let $M$ be a unital subspace of $M$, with $S = M + M^{\ast}$. If $\phi\colon M\rightarrow B$ is unital and $2$-contractive, then $\widetilde{\phi}\colon S\rightarrow B$ given by $\widetilde{\phi}\left( a + b^{\ast} \right) = \phi(a) + \phi(b)^{\ast}$ is $2$-positive and contractive.
\end{proposition}
\begin{proof}
  Since $\phi$ is contractive, we know from above that $\widetilde{\phi}$ is well-defined. Furthermore, note that
  \begin{align*}
    \Mat_2\left( S \right) &= \Mat_2\left( M \right) + \Mat_2\left( M \right)^{\ast},
  \end{align*}
  and
  \begin{align*}
    \left( \widetilde{\phi} \right)_2 &= \left( \widetilde{\phi}_2 \right).
  \end{align*}
  Now, since $\phi_2$ is contractive, we have that $\widetilde{\phi}_2$ is positive, so $\widetilde{\phi}$ is contractive.
\end{proof}
\begin{proposition}
  Let $A$ and $B$ be unital $C^{\ast}$-algebras, let $M$ be a unital subspace, and let $S = M + M^{\ast}$. If $\phi\colon M\rightarrow B$ is unital and completely contractive, then $\widetilde{\phi}\colon S\rightarrow B$ is completely positive and completely contractive.
\end{proposition}
\begin{proof}
  Since $\phi_n$ is unital and contractive, $\widetilde{\phi}_n$ is positive. Additionally, since $\left( \widetilde{\phi}_n \right)_2$ is positive, $\widetilde{\phi}_n$ is contractive.
\end{proof}
Note that since $\Mat_{2}\left( \Mat_n\left( A \right) \right) \cong \Mat_{2n}\left( A \right)$ are $\ast$-isomorphic, the norm on $\Mat_2\left( \Mat_n\left( A \right) \right)$ is equal to the norm on $\Mat_{2n}\left( A \right)$.\newline

Now, we may see some examples that belong to these categories.
\begin{example}
  If $A$ and $B$ are $C^{\ast}$-algebras, and $\pi\colon A\rightarrow B$ is a $\ast$-homomorphism, then $\pi$ is completely positive and completely contractive, since each $\pi_n\colon \Mat_n\left( A \right)\rightarrow \Mat_n\left( B \right)$ is a $\ast$-homomorphism, and $\ast$-homomorphisms are both positive and contractive.
\end{example}
\begin{example}
  Fixing $x,y\in A$, we may define $\phi\colon A\rightarrow A$ by $\phi(a) = xay$. Note that if $\left( a_{ij} \right)_{ij}\in \Mat_n\left( A \right)$, then
  \begin{align*}
    \norm{\phi_n\left( \left( a_{ij} \right)_{ij} \right)} &= \norm{\left( xa_{ij}y \right)_{ij}}\\
                                                           &= \norm{\left( xI_{n} \right) \begin{pmatrix}a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn}\end{pmatrix} \left( yI_n \right)}\\
                                                           &\leq \norm{x}\norm{\left( a_{ij} \right)_{ij}}\norm{y}.
  \end{align*}
  This means $\phi$ is completely bounded with $\norm{\phi}_{\cb}\leq \norm{x}\norm{y}$. Similarly, if $x = y^{\ast}$, then $\phi_n$ is positive.\newline

  This gives us the archetype of a completely bounded map. If $\mathcal{H}_1$ and $\mathcal{H}_2$ are Hilbert spaces, and $v_i\colon \mathcal{H}_1\rightarrow \mathcal{H}_2$ are bounded operators for $i=1,2$, then if $\pi\colon A\rightarrow \B\left( \mathcal{H}_2 \right)$ is a $\ast$-homomorphism, we may define $\phi\colon A\rightarrow \B\left( \mathcal{H}_1 \right)$ by $\phi(a) = v_2^{\ast}\pi(a)v_1$. This function $\phi$ is completely bounded with $\norm{\phi}_{\cb}\leq \norm{v_1}\norm{v_2}$.\newline

  In fact, we will show that \textit{every} completely bounded map is of this form.
\end{example}
\begin{proposition}
  Let $S\subseteq A$ be an operator system, $B$ a $C^{\ast}$-algebra, and $\phi\colon S\rightarrow B$ completely positive. Then, $\phi$ is completely bounded, and $\norm{\phi(1)} = \norm{\phi} = \norm{\phi}_{\cb}$.
\end{proposition}
\begin{proof}
  We have $\norm{\phi(1)}\leq \norm{\phi}\leq \norm{\phi}_{\cb}$, so it is sufficient to show that $\norm{\phi}_{\cb}\leq \norm{\phi(1)}$. Let $A = \left( a_{ij} \right)_{ij}$ be in $\Mat_n\left( S \right)$ with $\norm{A} \leq 1$, and let $I_n$ be the unit of $\Mat_n\left( A \right)$. Then, since
  \begin{align*}
    T = \begin{pmatrix}I_n & A \\ A^{\ast} & I_n\end{pmatrix}
  \end{align*}
  is positive, the map
  \begin{align*}
    \phi_{2n} \begin{pmatrix}\begin{pmatrix}I_n & A \\ A^{\ast} & I_n\end{pmatrix}\end{pmatrix}  &= \begin{pmatrix} \begin{pmatrix}\phi_n\left( I_n \right) & \phi_n\left( A \right) \\ \phi_n\left( A \right)^{\ast} & \phi_n\left( I_n \right)\end{pmatrix} \end{pmatrix}
  \end{align*}
  is positive, so $\norm{\phi_n\left( A \right)} \leq \norm{\phi_n\left( I_n \right)} = \norm{\phi(1)}$.
\end{proof}
\subsubsection{Schur Products and Tensor Products}%
We will apply the previous results on positive and completely positive maps on the Schur product.
\begin{definition}
  If $A = \left( a_{ij} \right)_{ij}$ and $B = \left( b_{ij} \right)_{ij}$, then the Schur product is defined by
  \begin{align*}
    A\ast B &= \left( a_{ij}b_{ij} \right)_{ij}.
  \end{align*}
\end{definition}
Note that for a fixed $A$, we get a linear map
\begin{align*}
  S_A\left( B \right) &= A\ast B.
\end{align*}
To study the Schur product, we review some results on tensor products.\newline

Let $A\in \Mat_n\left( \C \right)$ and $B\in \Mat_m\left( \C \right)$. Then, $A\otimes B$ is the linear transformation on $\C^n\otimes \C^m = \C^{nm}$, defined by $A\otimes B\left( x\otimes y \right) = Ax\otimes By$ with the unique linear extension provided by the tensor product.\newline

Note that we have $\norm{A\otimes B} = \norm{A}\norm{B}$, which is shown by writing $A\otimes B = \left( A\otimes I \right)\left( I\otimes B \right)$.\newline

Now, letting $\set{e_1,\dots,e_n}$ and $\set{f_1,\dots,f_m}$ be our canonical orthonormal bases for $\C^n$ and $\C^m$ respectively, we may order our basis as $e_1\otimes f_i$, then $e_2\otimes f_i$, etc., yielding the block matrices for $A\otimes B$ is
\begin{align*}
  \begin{pmatrix}a_{11}B & \cdots & a_{1n}B \\ \vdots & \ddots & \vdots \\ a_{n1}B &\cdots & a_{nn}B\end{pmatrix}.
\end{align*}
This matrix is known as the Kronecker product of $A$ and $B$. Now, similarly, we may order our basis by $e_i\otimes f_1$, then $e_i\otimes f_2$, etc., yielding a different block matrix of the form
\begin{align*}
  \begin{pmatrix}b_{11}A &\cdots & b_{1m}A \\ \vdots & \ddots & \vdots \\ b_{m1}A & \cdots & b_{mm} A\end{pmatrix},
\end{align*}
which is the Kronecker product of $B$ and $A$.\newline

Now, since both of these matrices represent the same linear transformation, they are unitarily equivalent, given by the permutation matrix that reorders the basis vectors. One obtains the $(k,\ell)$ entry of the $(i,j)$ block of $b_{ij}A$ by taking the $(i,j)$ entry of the $(k,\ell)$ block $a_{k,\ell}B$. We will call this the \textit{canonical shuffle}.\newline

Now, we let $A$ and $B$ be elements of $\Mat_n\left( \C \right)$, and define $V\colon \C^n\rightarrow \C^n\otimes \C^n$ to be the isometry given by $V\left( e_i \right) = e_i\otimes e_i$. We will show that $V^{\ast}\left( A\otimes B \right)V = A\ast B$. Note that
\begin{align*}
  \iprod{V^{\ast}\left( A\otimes B \right)Ve_j}{e_i} &= \iprod{\left( A\otimes B \right)\left( e_j\otimes e_j \right)}{e_i\otimes e_i}\\
                                                     &= \iprod{Ae_j}{e_i} \iprod{Be_j}{e_i}\\
                                                     &= a_{ij}b_{ij}\\
                                                     &= \iprod{A\ast B e_{j}}{e_i}.
\end{align*}
Thus,
\begin{align*}
  \norm{S_A(B)} &\leq \norm{V^{\ast}\left( A\otimes B \right)V}\\
                &\leq \norm{A}\norm{B},
\end{align*}
so that
\begin{align*}
  \norm{S_A}\leq \norm{A}.
\end{align*}
Now, if $\left( B_{ij} \right)_{ij}\in \Mat_k\left( \Mat_n\left( \C \right) \right)$, then
\begin{align*}
  \left( S_{A} \right)\left( \left( B_{ij} \right)_{ij} \right) &= \left( V^{\ast}\left( A\otimes B_{ij} \right)V \right)_{ij}\\
                                                                &= \begin{pmatrix}V^{\ast} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & V^{\ast}\end{pmatrix} A\otimes \begin{pmatrix}B_{11} & \cdots & B_{1n}\\\vdots & \ddots & \vdots \\ B_{n1} & \cdots & B_{nn}\end{pmatrix} \begin{pmatrix}V & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & V\end{pmatrix},
\end{align*}
so that $\norm{\left( S_A \right)_k}\leq \norm{A}$. Thus, $\norm{S_A}_{\cb}\leq \norm{A}$.\newline

However, this isn't a really good estimate. For instance, if $A$ is the matrix consisting of all $1$s, then the norm of $A$ is $n$, while $\norm{S_A} = 1$.\newline

We will prove that if $A$ is positive, then $S_A$ is completely positive. Thus, for positive matrices, we are able to obtain $\norm{S_A}_{\cb}$ by finding
\begin{align*}
  \norm{S_A} &= \norm{S_A(I)}\\
             &= \norm{S_A}_{\cb}\\
             &= \max\set{a_{ii} | i=1,\dots,n}.
\end{align*}
Now, if $A$ is not positive, then obtaining this norm is a bit more difficult. We can decompose $A = \left( P_1 - P_2 \right) + i\left( P_3 - P_4 \right)$, and get
\begin{align*}
  \norm{S_A}_{\cb} \leq \norm{S_{P_1}}_{\cb}  + \norm{S_{P_2}}_{\cb} + \norm{S_{P_3}}_{\cb} + \norm{S_{P_4}}_{\cb},
\end{align*}
but unfortunately this estimate isn't really enough.\newline

Now, we will characterize when the Schur product is completely positive.
\begin{theorem}
  Let $A = \left( a_{ij} \right)_{ij}\in \Mat_n\left( \C \right)$. The following are equivalent:
  \begin{enumerate}[(i)]
    \item $A$ is positive;
    \item $S_A\colon \Mat_n\left( \C \right)\rightarrow \Mat_n\left( \C \right)$ is positive;
    \item $S_A\colon \Mat_n\left( \C \right)\rightarrow \Mat_n\left( \C \right)$ is completely positive.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We have that (iii) implies (ii), and (ii) implies (i) by choosing $J$ to be the matrix consisting of $1$, which is positive, meaning $S_A\left( J \right) = A$. Thus, we must prove that (i) implies (iii).\newline

  Note that if $A$ and $B$ are positive, then $A\otimes B$ is positive. This follows from the fact that $A\otimes B = \left( A^{1/2}\otimes B^{1/2} \right)^2$.\newline

  Now, if $B\in \Mat_n\left( \C \right)$ is positive, then
  \begin{align*}
    S_A\left( B \right) &= V^{\ast}\left( A\otimes B \right)V\\
                        &= \left( \left( A^{1/2}\otimes B^{1/2} \right)V \right)^{\ast}\left( \left( A^{1/2}\otimes B^{1/2} \right)V \right)
  \end{align*}
  is positive, meaning (i) implies (ii).\newline

  Now, to see that (i) implies (iii), we let $B = \left( B_{ij} \right)_{ij}\in \Mat_k\left( \Mat_n\left( \C \right) \right)$, and write $B = \left( X_{ij} \right)_{ij}^{\ast}\left( X_{ij} \right)_{ij}$. We see that
  \begin{align*}
    \left( S_{A} \right)_k \left( B \right) &= \left( V^{\ast}\left( A\otimes B_{ij} \right)V \right)\\
                                            &= \left( \left( A^{1/2}\otimes X_{ij} \right)V \right)^{\ast}\left( \left( A^{1/2}\otimes X_{ij} \right)V \right),
  \end{align*}
  meaning $\left( S_A \right)_{k}$ is positive.
\end{proof}
There is an analogous theory of Schur products in the space $\B\left( \ell_2 \right)$, where we consider the bounded operators as infinite matrices. If we mandate that $A\in \B\left( \ell_2 \right)_{+}$, then we can use a similar line of argumentation to show that $S_A$ is completely positive., but this requires a bit more care as the matrix consisting of all $1$s regarded as an operator on $\ell_2$ is not a bounded operator.\newline

Now, we can show a pretty useful result, which is that bounded linear functionals are not only positive, but completely positive.
\begin{proposition}
  Let $S$ be an operator space, and let $f\colon S\rightarrow \C$ be a bounded linear functional. Then,
  \begin{align*}
    \norm{f}_{\cb} &= \norm{f}_{\op},
  \end{align*}
  and if $S$ is an operator system with $f$ positive, then $f$ is completely positive.
\end{proposition}
\begin{proof}
  Let $\left( a_{ij} \right)_{ij}\in \Mat_n\left( S \right)$, and let $x,y\in \C^n$ be unit vectors. Then,
  \begin{align*}
    \left\vert \iprod{f\left( \left( a_{ij} \right)_{ij} \right)\left( x \right)}{y} \right\vert &= \left\vert \sum_{i,j=1}^{n}f\left( a_{ij} \right)\left( x_j \right)\overline{y_i} \right\vert\\
                                                                                                 &= \left\vert f\left( \sum_{i,j=1}^{n}a_{ij}x_j\overline{y_i} \right) \right\vert\\
                                                                                                 &\leq \norm{f}_{\op} \norm{\sum_{i,j=1}^{n}a_{ij}x_j\overline{y_i}}.
  \end{align*}
  Now, all we need to show is that the latter element has norm less than $\norm{\left( a_{ij} \right)_{ij}}$. Note that this sum is the entry on the first row and column of the matrix that represents the product
  \begin{align*}
    \begin{pmatrix}\overline{y_1} 1 & \cdots & \overline{y_n}1 \\ 0 & \cdots & \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0\end{pmatrix} \begin{pmatrix}a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n_1} & \cdots & a_{nn}\end{pmatrix} \begin{pmatrix}x_1 1 & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ x_n 1 & 0 & \cdots & 0\end{pmatrix}.
  \end{align*}
  The outer two factors have norm $1$, since $x$ and $y$ are chosen to be unit vectors.\newline

  To show that $f$ is completely positive, we only need to show that
  \begin{align*}
    \iprod{f_n\left( \left( a_{ij} \right)_{ij} \right)\left( x \right)}{x} &= f\left( \sum_{i,j=1}^{n}a_{ij}x_j\overline{x_i} \right)
  \end{align*}
  is positive whenever $\left( a_{ij} \right)_{ij}$ is positive. However, using the above product, we see that the summation is equal to the first row and column entry of a positive matrix, hence positive.
\end{proof}
Now, we examine the positivity and boundedness of maps with codomain $C(X)$, where $X$ is a compact Hausdorff space.\newline

Note that every element $F = \left( f_{ij} \right)_{ij}$ of $\Mat_n\left( C(X) \right)$ can be considered as a continuous matrix-valued function, with multiplication and $\ast$-operation as pointwise multiplication and involution of the matrix-valued functions.\newline

To make $\Mat_n\left( C(X) \right)$ into a $C^{\ast}$-algebra is to set $\norm{f} = \sup\set{\norm{F(x)} | x\in X}$, and by uniqueness of $C^{\ast}$-norms, this is the only way to create a $C^{\ast}$-norm.
\begin{theorem}
  Let $S$ be an operator space, and let $\phi\colon S\rightarrow C(X)$ be a bounded linear map. Then, $\norm{\phi}_{\cb} = \norm{\phi}_{\op}$. Furthermore, if $S$ is an operator system and $\phi$ is positive, then $\phi$ is completely positive.
\end{theorem}
\begin{proof}
  Let $x\in X$, and define $\phi^{x}$ to be pointwise evaluation --- i.e., $\phi^x(a) = \phi(a)(x)$. Then,
  \begin{align*}
    \norm{\phi_n} &= \sup\set{\norm{\phi_n^x} | x\in X} \\
                  &= \sup\set{\norm{\phi^x} | x\in X} \\
                  &= \norm{\phi}_{\op}.
  \end{align*}
  Similarly, $\phi_n\left( \left( a_{ij} \right)_{ij} \right)$ is positive if and only if $\phi_n^x\left( \left( a_{ij} \right)_{ij} \right)$ is positive for all $x\in X$.
\end{proof}
Thus, when the codomain $C^{\ast}$-algebra is commutative, boundedness and complete boundedness, as well as positivity and complete positivity, coincide. A commutative domain \textit{is} enough to show that positive maps are completely positive, but unfortunately a commutative domain is not enough to guarantee that bounded maps are completely bounded.
\begin{lemma}
  Let $\left( p_{ij} \right)_{ij}$ be a positive scalar matrix, and let $q$ be a positive element of some $C^{\ast}$-algebra $B$. Then, $\left( p_{ij}q \right)_{ij}$ is positive in $\Mat_n\left( B \right)$.
\end{lemma}
\begin{proof}
  We write $\left( p_{ij} \right)_{ij}$ as $\left( s_{ij} \right)_{ij}^{\ast}\left( s_{ij} \right)_{ij}$, and write $q = n^{\ast}n$. This gives
  \begin{align*}
    \left( p_{ij}q \right) &= \left( p_{ij} \right)_{ij} \diag\left( q,\dots,q \right)\\
                           &= \left( s_{ij} \right)_{ij}^{\ast}\left( s_{ij} \right) \diag\left( n^{\ast}n,\dots,n^{\ast}n \right)\\
                           &= \left( s_{ij} \right)_{ij}^{\ast}\left( s_{ij} \right)_{ij}\diag\left( n^{\ast},\dots,n^{\ast} \right)\diag\left( n,\dots,n \right)\\
                           &= \left( s_{ij} \right)_{ij}^{\ast}\left( s_{ij} \right)_{ij}\diag\left( n,\dots,n \right)^{\ast}\diag\left( n,\dots,n \right)\\
                           &= \diag\left( n,\dots,n \right)^{\ast}\left( s_{ij} \right)_{ij}^{\ast}\left( s_{ij} \right)_{ij}\diag\left( n,\dots,n \right) \tag*{$\diag\left( n,\dots,n \right)\in \Mat_n\left( B \right),\left( s_{ij} \right)_{ij}\in \Mat_n\left( \C \right)$}\\
                           &= \left( \left( s_{ij} \right)_{ij}\diag\left( n,\dots,n \right) \right)^{\ast}\left( \left( s_{ij} \right)_{ij}\diag\left( n,\dots,n \right) \right).
  \end{align*}
  Thus, $\left( p_{ij}q \right)_{ij}$ is positive in $\Mat_n\left( B \right)$.
\end{proof}
\begin{theorem}
  Let $B$ be a $C^{\ast}$-algebra, and let $\phi\colon C(X)\rightarrow B$ be a positive map. Then, $\phi$ is completely positive.
\end{theorem}
\begin{proof}
  Let $P(x)$ be positive in $\Mat_n\left( C(X) \right)$. We prove that $\phi_n(P)$ is positive.\newline

  Given $\ve > 0$, we may find a partition of unity $\set{u_{\ell}(x)}_{\ell=1}^{m}$ and positive matrices $P_{\ell} = \left( p_{ij}^{\ell} \right)_{ij}$ such that
  \begin{align*}
    \left\vert P - \sum_{\ell=1}^{m} u_{\ell}(x)P_{\ell}\right\vert &< \ve.
  \end{align*}
  However, we know that
  \begin{align*}
    \phi_n\left( u_{\ell}P_{\ell} \right) &= \phi_n\left( \left( u_{\ell}p_{ij}^{\ell} \right)_{ij} \right)\\
                                          &= \left( \phi\left( u_{\ell} \right)p_{ij}^{\ell} \right)_{ij},
  \end{align*}
  which is positive. Therefore, $\phi_n(P)$ is within $\ve \norm{\phi_n}\norm{P}$ of a sum of positive elements. Since $\Mat_n\left( B \right)_{+}$ is a closed set, we have that $\phi_n\left( P \right)$ is positive.
\end{proof}
\begin{corollary}
  Let $T$ be a contractive operator on $\mathcal{H}$, and let $\left( p_{ij} \right)_{ij}$ be a $n\times n$ matrix of polynomials. Then,
  \begin{align*}
    \norm{\left( p_{ij}(T) \right)_{ij}}_{\op} &\leq \sup\set{\norm{\left( p_{ij}(z) \right)_{ij}} | \left\vert z \right\vert = 1}.
  \end{align*}
\end{corollary}
\begin{proof}
  The map given by $\phi\left( p + \overline{q} \right) = p(T) + q(T)^{\ast}$ extends to a positive map $\varphi\colon C(\mathbb{T})\rightarrow \B\left( \mathcal{H} \right)$. This map is completely positive as $C(\mathbb{T})$ is a commutative $C^{\ast}$-algebra. Thus, $\norm{\varphi}_{\cb} = \norm{\varphi(1)} = 1$. Thus,
  \begin{align*}
    \norm{\left( p_{ij}(T) \right)_{ij}} &= \norm{\phi_n\left( \left( p_{ij} \right)_{ij} \right)}\\
                                         &\leq \norm{\left( p_{ij}(1) \right)_{ij}}.
  \end{align*}
\end{proof}
\begin{lemma}
  Let $A$ be a $C^{\ast}$-algebra. Then, every positive element of $\Mat_n\left( A \right)$ is a sum of $n$ positive elements of the form $\left( a_i^{\ast}a_j \right)_{ij}$, where $\set{a_1,\dots,a_n}\subseteq A$.
\end{lemma}
\begin{proof}
  Note that if $R$ is the element of $\Mat_n\left( A \right)$ whose $k$th row is $a_1,\dots,a_n$ and $0$ elsewhere, then $R^{\ast}R = \left( a_i^{\ast}a_j \right)_{ij}$, so such an element is positive.\newline

  Now, let $P$ be positive, yielding $P = B^{\ast}B$. Write $B = R_1 + \cdots R_n$, where $R_k$ is the $k$th row of $B$ and $0$ elsewhere.\newline

  Then, since $R_i^{\ast}R_j = 0$ whenever $i\neq j$, we have that $P = R_1^{\ast}R_1 + \cdots + R_n^{\ast}R_n$.
\end{proof}
Thus, it suffices to check that $\phi\colon A\rightarrow B$ is $n$-positive by verifying that $\left( \phi\left( a_i^{\ast}a_j \right) \right)_{ij}$ is positive for all $a_1,\dots,a_n\in A$.
\begin{theorem}
  Let $B$ be a $C^{\ast}$-algebra, let $\phi\colon \Mat_n\left( \C \right)\rightarrow B$ be a linear map, and let $\set{e_{ij}}_{i,j=1}^{n}$ denote the standard matrix units for $\Mat_n\left( \C \right)$. The following are equivalent:
  \begin{enumerate}[(i)]
    \item $\phi$ is completely positive;
    \item $\phi$ is $n$-positive;
    \item $\left( \phi\left( e_{ij} \right) \right)_{ij}$ is positive in $\Mat_n\left( B \right)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  It suffices to show that (iii) implies (i), as (i) implies (ii) and $\left( e_{ij} \right)_{ij}$ is positive for each $i,j$, giving (ii) implies (iii).\newline

  It is sufficient to assume that $B = \B\left( \mathcal{H} \right)$. Fix $k$, and let $x_1,\dots,x_k\in \mathcal{H}$, $B_1,\dots,B_k\in \Mat_n\left( \C \right)$. It is sufficient to prove that
  \begin{align*}
    \sum_{i,j}^{k} \iprod{\phi\left( B_i^{\ast}B_j \right)x_j}{x_j} &\geq 0.
  \end{align*}
  Write $B_{\ell} = \sum_{r,s=1}^{n}b_{rs,\ell}e_{rs}$, such that
  \begin{align*}
    B_i^{\ast}B_j &= \sum_{r,s,t=1}^{n}\overline{b_{rs,i}}b_{rt,j}e_{st}.
  \end{align*}
  Set $y_{t,r} = \sum_{j=1}^{k}b_{rt,j}x_j$. Then,
  \begin{align*}
    \sum_{i,j=1}^{k} \iprod{\phi\left( B_i^{\ast}B_j \right)x_j}{x_i} &= \sum_{r=1}^{n}\sum_{s,t=1} \iprod{\phi\left( e_{st} \right)\left( \sum_{i,j=1}^{k}b_{rs,i}b_{rt,j}x_j \right)}{x_i}\\
                                                                      &= \sum_{r=1}^{n}\sum_{s,t} \iprod{\phi\left( e_{st} \right)y_{t,r}}{y_{s,r}}.
  \end{align*}
  However, this latter sum is positive, since $\left( \phi\left( e_{st} \right) \right)st$ is positive, so we have expressed our original sum as the sum of $n$ positive quantities.
\end{proof}
Now, we may obtain some fairly deep results in operator theory via the properties of positive maps.
\begin{definition}
  If $T\in \B\left( \mathcal{H} \right)$, we define the numerical radius of $T$ by
  \begin{align*}
    w(T) &= \sup_{x\in B_{\mathcal{H}}} \left\vert \iprod{Tx}{x} \right\vert.
  \end{align*}
\end{definition}
\begin{exercise}
  Let $S_n$ be the cyclic forward shift on $\C^n$. That is, $S_n e_j = e_{j+1}$ mod $n$, where $e_0,\dots,e_{n-1}$ is the canonical basis for $\C^n$.
  \begin{enumerate}[(i)]
    \item Show that $S_n$ is unitarily equivalent to a diagonal matrix whose entries are the $n$th roots of unity.
    \item Let $T\in \B\left( \mathcal{H} \right)$. Show that $w\left( T \right) = w\left( T\otimes S_n \right)$.
    \item Let $R_n$ be the $n\times n$ matrix of operators whose subdiagonals are $T$ and $0$ elsewhere. Show that $w\left(R_n\right)\leq w\left( T\otimes S_n \right)$.
    \item Show that $\re\left( \iprod{R_ny}{y} \right) \leq 1$ for all $\norm{y} = 1$ if and only if $w\left( R_n \right) \leq 1$.
  \end{enumerate}
\end{exercise}
\begin{solution}\hfill
  \begin{enumerate}[(i)]
    \item By the definition of the cyclic forward shift, defining $A \coloneq S_n$, we have $A^n = I_n$, or $A^n - I = 0$. This means that the minimal polynomial for $S_n$ is $m_{S_n}\left( x \right) = x^n - 1$, meaning that the $n$th roots of unity are eigenvalues for $S$. Since $S_n$ is an operator acting on $\C^n$, there are at $n$ eigenvalues (with multiplicity) for $S_n$, meaning that the $n$th roots of unity are in fact \textit{the} eigenvalues of $S_n$. Thus, $S_n$ is unitarily equivalent to a diagonal matrix with the $n$th roots of unity on the diagonal.
    \item The operator $T\otimes S_n$ acts on $\mathcal{H}\otimes \C^n$ such that $\left( T\otimes S_n \right)\left( y\otimes v \right) = Ty \otimes S_n v$. Thus, we have
      \begin{align*}
        w\left( T\otimes S_n \right) &= \sup_{y\otimes v\in \B_{\mathcal{H}\otimes \C^n}} \left\vert \iprod{\left( T\otimes S_n \right)\left( y\otimes v \right)}{y\otimes v} \right\vert\\
                                     &= \sup_{y\otimes v\in B_{\mathcal{H}\otimes \C^n}}\left\vert \iprod{Ty\otimes S_n v}{y\otimes v} \right\vert\\
                                     &= \sup_{y\in B_{\mathcal{H}}}\sup_{v\in B_{\C^n}} \left\vert \iprod{Ty}{y} \right\vert \left\vert \iprod{S_n v}{v} \right\vert\\
                                     &= w\left( T \right)w\left( S_n \right)\\
                                     &= w\left( T \right).
      \end{align*}
    \item We consider the non-cyclic shift $S_n'$, and note that $w\left( S_n' \right)\leq w\left( S_n \right)$, as applying the non-cyclic shift will yield zero in the first entry of the vector $v$. We have that $R_n \cong T\otimes S_n'$, meaning that $w\left( R_n \right)\leq w\left( T \right)$.
    \item If $w\left( R_n \right)\leq 1$ for all $\norm{y} = 1$, then since $\re\left( \iprod{R_ny}{y} \right)\leq \left\vert \iprod{R_ny}{y} \right\vert \leq 1$, it is clear that $\re\left( \iprod{R_n y}{y} \right)\leq 1$ for all $\norm{y} = 1$.\newline

      Now, suppose $\re\left( \iprod{R_ny}{y} \right)\leq 1$ for all $\norm{y} = 1$.
  \end{enumerate}
\end{solution}


\begin{theorem}
  Let $T\in \B\left( \mathcal{H} \right)$, let $S\subseteq C(\T)$ be the operator system defined by $S = \set{p + \overline{q} | p,q\text{ polynomials}}$. The following are equivalent:
  \begin{enumerate}[(i)]
    \item $w(T)\leq 1$;
    \item the map $\phi\colon S\rightarrow \B\left( \mathcal{H} \right)$, defined by
      \begin{align*}
        \phi\left( p + \overline{q} \right) &= p(T) + q\left( T \right)^{\ast} + \left( p(0) + \overline{q(0)} \right) I
      \end{align*}
      is positive.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We start by showing that (i) implies (ii).\newline

  Let $R_n$ be the $n\times n$ operator matrix with subdiagonal entry $T$ and remaining entries $0$. Note that $w\left( R_n \right)\leq w(T)$.\newline

  Now, we see that $\phi$ is positive so long as the matrix
  \begin{align*}
    \begin{pmatrix}2 & T^{\ast} & \cdots & \left( T^{\ast} \right)^n\\ T & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & T^{\ast} \\ T^n & \cdots & T & 2\end{pmatrix}\tag{\textasteriskcentered}
  \end{align*}
  is positive for all $n$.\newline

  Note that $R_n^{n+1} = 0$, so (\textasteriskcentered) can be written as $\left( I - R_n \right)^{-1} + \left( I - R_n^{\ast} \right)^{-1}$.\newline

  Fix $x = \left( I - R_n \right)y$, and compute
  \begin{align*}
    \iprod{\left( \left( I-R_n \right)^{-1} + \left( I - R_n^{\ast} \right)^{-1} \right)x}{x} &= 2\norm{y}^2 - 2\re \left( \iprod{R_n y}{y} \right).
  \end{align*}
  Thus, (\textasteriskcentered) is positive if and only if $w\left( R_n \right) \leq 1$. Since $w(T)\leq 1$ implies $w\left( R_n \right)\leq 1$, we have (\textasteriskcentered) is positive, meaning $\phi$ is positive.\newline

  Conversely, if $\phi$ is positive, since $\overline{S} = C(\T)$, $\phi$ is completely positive by the fact that if $\phi\colon C(X)\rightarrow B$ is positive, then $\phi$ is completely positive.\newline

  Note that
  \begin{align*}
    \begin{pmatrix}1 & \overline{z} & \cdots & \overline{z}^n \\ z & 1 & \cdots & \ddots & \vdots \\ \vdots & \ddots & \ddots & \overline{z} \\ z^n & \cdots & z & 1\end{pmatrix} &= \begin{pmatrix}1 & 0 & \cdots & 0 \\ 0 & z & \ddots & \vdots \\ \vdots & \ddots & \ddots & 0 \\ 0 & \cdots & 0 & z^n\end{pmatrix} \begin{pmatrix}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} \begin{pmatrix}1 & 0 & \cdots & 0 \\ 0 & \overline{z} & \ddots & \vdots \\ \vdots & \ddots & \ddots & 0 \\ 0 & \cdots & 0 & \overline{z}^n \end{pmatrix}
  \end{align*}
  is positive in $\Mat_n\left( C\left( \T \right) \right)$, so its image under $\phi_n$ is also positive. However, since this image is equal to (\textasteriskcentered), we have that (\textasteriskcentered) is positive for all $n$, meaning $w\left( R_n \right)\leq 1$.\newline

  Let $x\in \mathcal{H}$, $\norm{x} = 1$, and $y = \frac{1}{\sqrt{n}}\left( x\oplus\cdots\oplus x \right)$ be a unit vector $\mathcal{H}\oplus\cdots\oplus \mathcal{H}$.  Then, we have
  \begin{align*}
    1 &\leq \left\vert \iprod{R_ny}{y} \right\vert\\
      &= \frac{n-1}{n}\left\vert \iprod{Tx}{x} \right\vert,
  \end{align*}
  meaning $w(t)\leq \frac{n}{n-1}$ for all $n$, meaning $w(T)\leq 1$.
\end{proof}
If $w(T)\leq 1$, we may extend the functional calculus from the circle to the disk algebra, $A(\mathbb{D})$.
\begin{corollary}
  Let $T\in \B\left( \mathcal{H} \right)$ with $w(T)\leq 1$. Let $f\in A\left( \mathbb{D} \right)$ with $f(0) = 0$. Then, $w\left( f(T) \right)\leq \norm{f}$.
\end{corollary}
\begin{proof}
  It is sufficient to assume that $f$ is a polynomial, and $\norm{f}\leq 1$. 
\end{proof}
\section{Dilations}%
We saw our first example of a dilation theorem earlier in our first proof of von Neumann's inequality when we showed that if $T$ is contractive, there is some projection $P$ from $\mathcal{K}\supseteq \mathcal{H}$ and some unitary $U\in \B\left( \mathcal{K} \right)$ such that $T^n = PU^n|_{\mathcal{H}}$.\newline

Now, we will show an incredibly powerful result that characterizes all the completely positive maps, known as Stinespring's dilation theorem.
\begin{theorem}[Stinespring's Dilation]
  Let $A$ be a unital $C^{\ast}$-algebra, and let $\phi\colon A\rightarrow \B\left( \mathcal{H} \right)$ be a completely positive map. Then, there exists a Hilbert space $\mathcal{K}$, a unital $\ast$-homomorphism $\pi\colon \B\left( \mathcal{K} \right)$, and a bounded operator $V\colon \mathcal{H}\rightarrow \mathcal{K}$ with $\norm{\phi(1)} = \norm{V}^2_{\op}$, such that
  \begin{align*}
    \phi(a) &= V^{\ast}\pi(a)V
  \end{align*}
  for all $a\in A$.
\end{theorem}
\begin{proof}
  Consider the algebraic tensor product $A\otimes \mathcal{H}$, and define the symmetric bilinear map $ \iprod{\cdot}{\cdot} $ on the space by setting
  \begin{align*}
    \iprod{a\otimes x}{b\otimes y} &= \iprod{\phi\left( b^{\ast}a \right)x}{y}_{\mathcal{H}},
  \end{align*}
  and extending linearly.\newline

  Since we have
  \begin{align*}
    \iprod{\sum_{j=1}^{n}a_j\otimes x_j}{\sum_{i=1}^{n}a_i\otimes x_i} &= \iprod{\phi_n\left( \left( a_i^{\ast}a_j \right)_{ij} \right) \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix}}{ \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} }_{\mathcal{H}^{(n)}}\\
                                                                       &\geq 0,
  \end{align*}
  and $\phi$ is completely positive, we have that $ \iprod{a\otimes x}{b\otimes y} $ is a positive semidefinite bilinear form.\newline

  Since positive semidefinite bilinear forms satisfy the Cauchy--Schwarz inequality, $\left\vert \iprod{u}{v} \right\vert\leq \iprod{u}{u} \iprod{v}{v}$, we may define the ``null set''
  \begin{align*}
    N &\coloneq \set{u\in A\otimes \mathcal{H} | \iprod{u}{u} = 0}
  \end{align*}
  as a subspace of $A\otimes \mathcal{H}$. The induced inner product  on $\left( A\otimes \mathcal{H} \right)/N$ is
  \begin{align*}
    \iprod{u + N}{v + N} &= \iprod{u}{v}.
  \end{align*}
  \begin{remark}
    The construction here is very similar to the GNS construction.
  \end{remark}
  We will let $\mathcal{K}$ be the completion of $\left( A\otimes \mathcal{H} \right)/N$.\newline

  Now, if $a\in A$, define $\pi(a)\colon A\otimes \mathcal{H}\rightarrow A\otimes \mathcal{H}$ by
  \begin{align*}
    \pi(a)\left( \sum_{i=1}^{n}a_i\otimes x_i \right) &= \sum_{i=1}^{n}\left( aa_i \right)\otimes x_i.
  \end{align*}
  We begin by showing that
  \begin{align*}
    \left( a_i^{\ast}a^{\ast}a a_j \right)_{ij} &\leq \norm{a^{\ast}a}\left( a_i^{\ast}a_j \right)_{ij},
  \end{align*}
  where the inequality is in $\Mat_n\left( A \right)_{+}$. This follows from the fact that
  \begin{align*}
    \left( a_i^{\ast}a^{\ast}a a_j \right)_{ij} &= \left( aI_n\left( a_{ij} \right)_{ij} \right)^{\ast}\left( aI_n\left( a_{ij} \right)_{ij} \right)\\
                                                &\leq \norm{\left( aI_n \right)^{\ast}aI_n} \left( a_i^{\ast}a_j \right)_{ij}\\
                                                &= \norm{a^{\ast}a} \left( a_{i}^{\ast}a_j \right)_{ij},
  \end{align*}
  where the last line follows from the fact that for any elements of a $C^{\ast}$-algebra, $a,b$, we have $0\leq b^{\ast}a^{\ast}ab\leq \norm{a^{\ast}a}b^{\ast}b$.\newline

  Now, this gives
  \begin{align*}
    \iprod{\pi(a)\left( \sum_{j=1}^{n}a_j\otimes x_j \right)}{\pi(a)\left( \sum_{i=1}^{n}a_i\otimes x_i \right)} &= \sum_{i,j=1}^{n} \iprod{\pi\left( a_i^{\ast}a^{\ast}aa_j \right)x_j}{x_i}_{\mathcal{H}}\\
                                                                                                                 &\leq \norm{a^{\ast}a}\sum_{i,j=1}^{n} \iprod{\phi\left( a_i^{\ast}a_j \right)x_j}{x_i}\\
                                                                                                                 &= \norm{a}^2 \iprod{\sum_{j=1}^{n}a_j\otimes x_j}{\sum_{i=1}^{n}a_i\otimes x_i}.
  \end{align*}
  Thus, $\pi(a)$ vanishes on $N$, meaning it induces a quotient map that we will write as $\overline{\pi}(a)$. The above inequality shows that $\overline{\pi}(a)$ is bounded, with $\norm{\overline{\pi}\left( a \right)}\leq \norm{a}$. Thus, $\overline{\pi}\left( a \right)$ extends to a bounded linear operator on $\mathcal{K}$, denoted $\widetilde{\pi}\left( a \right)$.\newline

  Now, the map $\widetilde{\pi}\colon A\rightarrow \B\left( \mathcal{K} \right)$ is a unital $\ast$-homomorphism.\newline

  Define $V\colon \mathcal{H}\rightarrow \mathcal{K}$ by $V(x) = 1\otimes x + N$. Then, since
  \begin{align*}
    \norm{Vx}^2 &= \iprod{1\otimes x}{1\otimes x}\\
                &= \iprod{\phi(1)x}{x}_{\mathcal{H}}\\
                &\leq \norm{\phi(1)} \norm{x}^2,
  \end{align*}
  $V$ is bounded. Furthermore,
  \begin{align*}
    \norm{V}_{\op}^2 &= \sup_{x\in B_{\mathcal{H}}} \iprod{\phi(1)x}{x}\\
                     &= \norm{\phi(1)}.
  \end{align*}
  Finally, we see that
  \begin{align*}
    \iprod{V^{\ast}\widetilde{\pi}(a)V x}{y} &= \iprod{\left( \pi(a)1 \right)\otimes x}{1\otimes y}_{\mathcal{H}}\\
                                             &= \iprod{\phi(a)x}{y}_{\mathcal{H}},
  \end{align*}
  so that $V^{\ast}\widetilde{\pi}(a)V = \phi(a)$.
\end{proof}
There are some remarks to be made. First, any map of the form $\phi(a) = V^{\ast}\pi(a)V$ is already completely positive, so Stinespring's dilation is a complete characterization of completely positive maps from any $C^{\ast}$-algebra into any $\B\left( \mathcal{H} \right)$. Furthermore, if $\phi$ is unital, then $V$ is an isometry, and we may identify $\mathcal{H}$ with $V\mathcal{H}\subseteq \mathcal{K}$. This identification gives $V^{\ast}$ as the projection of $\mathcal{K}$ onto $\mathcal{H}$, or $P_{\mathcal{H}}$. Thus,
\begin{align*}
  \phi(a) = P_{\mathcal{H}}\pi(a)|_{\mathcal{H}}.
\end{align*}
If $T\in \B\left( \mathcal{K} \right)$, then $P_{\mathcal{H}}T|_{\mathcal{H}}$ is called the compression of $T$ to $\mathcal{H}$. We may decompose $\mathcal{K} = \mathcal{H}\oplus \mathcal{H}^{\perp}$, and consider $T$ as the $2\times 2$ operator matrix whose compression is equal to the $(1,1)$ entry of the operator matrix. Thus, Stinespring's dilation shows that every completely positive map into $\B\left( \mathcal{H} \right)$ is the compression to $\mathcal{H}$ of a $\ast$-homomorphism into a Hilbert space that contains $\mathcal{H}$.\newline

Additionally, Stinespring's dilation is a generalization of the GNS construction, which was used to convert from states to representations of $C^{\ast}$-algebras as subalgebras of $\B\left( \mathcal{H} \right)$. In particular, if $\mathcal{H} = \C$, then the isometry $V\colon \C\rightarrow \mathcal{K}$ is determined by $V(1) = x$< and>
\begin{align*}
  \phi(a) &= \phi(a)(1)\cdot 1\\
          &= V^{\ast}\pi(a)V(1)\cdot 1\\
          &= \iprod{\pi(a)V(1)}{V(1)}_{\mathcal{K}}\\
          &= \iprod{\pi(a)x}{x}.
\end{align*}
Furthermore, if we reread the proof with $\mathcal{H}= \C$  and $A\otimes \C = A$, we recover the proof of the GNS representation of states.\newline

Finally, if $\mathcal{H}$ and $A$ are separable, then so too is $\mathcal{K}$, and if $\mathcal{H}$ and $A$ are finite-dimensional, then so too is $\mathcal{K}$.\newline

Now, we turn our attention to the uniqueness of the Stinespring representations, $\left( \pi,V,\mathcal{K} \right)$. Given one of these Stinespring representations, $\left( \pi,V,\mathcal{K} \right)$, we may consider $\mathcal{K}_1$ to be the closed linear span of $\pi(A)V\mathcal{H}$, which reduces $\pi(A)$, so that the restriction of $\pi$ to $\mathcal{K}_1$ defines a $\ast$-homomorphism $\pi_1\colon A\rightarrow \B\left( \mathcal{K}_1 \right)$.\newline

Now, $V\mathcal{H}\subseteq \mathcal{K}_1$, so that $\phi(a)=V^{\ast}\pi(a)V$. Therefore, $\left( \pi_1,V,\mathcal{K}_1 \right)$ is also a Stinespring representation, where $\mathcal{K}_1$ is the closed linear span of $\pi_1(A)V\mathcal{H}$.\newline

If our Stinespring representation also has the property that $\mathcal{K}_1 = \overline{\Span}\left( \pi(A)V\mathcal{H} \right)$, then we call the triple a \textit{minimal} Stinespring representation.
\begin{proposition}
  Let $A$ be a $C^{\ast}$-algebra, and let $\phi\colon A\rightarrow \B\left( \mathcal{H} \right)$ be a completely positive map. Let $\left( \pi_i,V_i,\mathcal{K}_i \right)$ be two minimal Stinespring representations for $\phi$.\newline

  Then, there exists a unitary map $U\colon \mathcal{K}_1\rightarrow \mathcal{K}_2$ such that $UV_1 = V_2$, and $U\pi_1U^{\ast} = \pi_2$.
\end{proposition}
\begin{proof}
  If $U$ exists, then we must necessarily have
  \begin{align*}
    U\left( \sum_{i}\pi_1\left( a_i \right)V_1h_i \right) &= \sum_{i}\pi_2\left( a_i \right)V_2h_i,
  \end{align*}
  so it is sufficient to verify that the above formula gives a well-defined isometry. By the minimality condition, $U$ will have dense range, hence onto.\newline

  Now, note that
  \begin{align*}
    \norm{\sum_{i}\pi_1\left( a_i \right)V_1h_i}^2 &= \sum_{i,j} \iprod{V_1\pi_1\left( a_i^{\ast}a_j \right)V_1h_j}{h_i}\\
                                                   &= \sum_{i,j} \iprod{\phi\left( a_i^{\ast}a_j \right)h_j}{h_i}\\
                                                   &= \norm{\sum_{i}\pi_2\left( a_i \right)V_2h_i}^2,
  \end{align*}
  so $U$ is isometric.
\end{proof}
\section{Arveson's Extension Theorem(s)}%
We start by recalling the fact that we can identify elements of the tensor product of two Banach spaces, $X\otimes Y$, with maps in $\B\left( X,Y^{\ast} \right)$.\newline

Fix $x\in X$ and $y\in Y$. Define a linear functional $x\otimes y\in \B\left( X,Y^{\ast} \right)^{\ast}$ by $\left( x\otimes y \right)\left( L \right) = L(x)(y)$. Here, $L(x)$ is a linear functional on $Y$.\newline

Since $\left\vert x\otimes y(L) \right\vert \leq \norm{L}\norm{x}\norm{y}$. We see that $x\otimes y\in \B\left( X,Y^{\ast} \right)^{\ast}$, with $\norm{x\otimes y} \leq \norm{x}\norm{y}$. In fact, it can be shown that $\norm{x\otimes y} = \norm{x}\norm{y}$.\newline

We may verify that $x\otimes y$ is a bilinear map. Let $Z$ be the closed linear span in $\B\left( X,Y^{\ast} \right)^{\ast}$ of the elementary tensors, and we may identify $Z$ with a cross-norm completion of $X\otimes Y$ (but we will not use that here). For now, we use the following result.
\begin{lemma}
  The space $\B\left( X,Y^{\ast} \right)$ is isometrically isomorphic to $Z^{\ast}$with duality given by
  \begin{align*}
    \iprod{L}{x\otimes y} &= \left( x\otimes y \right)(L).
  \end{align*}
\end{lemma}
\begin{proof}
  We show that this map is surjective. Let $f\in Z^{\ast}$ be fixed, and for each $x$, define $f_x\colon Y\rightarrow \C$ by $f_x(y) = f\left( x\otimes y \right)$. Then, since $\left\vert f_x(y) \right\vert\leq \norm{f}\norm{x}\norm{y}$, $f_x\in Y^{\ast}$.\newline

  If we set $L(x) = f_x$, then $L\colon X\rightarrow Y^{\ast}$ is bounded with $\norm{L}\leq \norm{f}$, so $L\in \B\left( X,Y^{\ast} \right)$ with the correspondence $L\mapsto F$.
\end{proof}
We call the weak* topology on $\B\left( X,Y^{\ast} \right)$ the bounded weak topology (or BW topology).
\begin{lemma}
  Let $\left( L_{\lambda} \right)_{\lambda}$ be a bounded net in $\B\left( X, Y\right)$. Then, $L_{\lambda}$ converges to $L$ in the BW topology if and only if $L_{\lambda}(x)$ converges weakly to $L(x)$ for all $x\in X$.
\end{lemma}
\begin{proof}
  If $L_{\lambda}\xrightarrow{\text{BW}} L$, then
  \begin{align*}
    L_{\lambda}\left( x \right)\left( y \right) &= \iprod{L_{\lambda}}{x\otimes y}\\
                                                &\rightarrow \iprod{L}{x\otimes y}\\
                                                &= L\left( x \right)\left( y \right)
  \end{align*}
  for all $y\in Y$, so that $L_{\lambda}(x)\xrightarrow{w} L(x)$ for all $x$.\newline

  Conversely, if $L_{\lambda}(x)\xrightarrow{w} L(x)$ for all $x\in X$, then $ \iprod{L_{\lambda}}{x\otimes y} $ converges to $ \iprod{L}{x\otimes y} $ for all $x$ and $y$, hence on the linear span of the elementary tensors. However, since the net is bounded, it converges on the closed linear span.
\end{proof}
If $\mathcal{H}$ is a Hilbert space, then $\B\left( \mathcal{H} \right)$ is the dual of a Banach space, known as the trace class operators, $L_1\left( \B\left( \mathcal{H} \right) \right)$, with the trace norm $\norm{T}_{\tr} = \tr\left( \left\vert T \right\vert \right)$.\newline

Under the duality, an operator $A\in \B\left( \mathcal{H} \right)$ is identified with the linear functional $\tr\left( AT \right)$ for some $T\in L_1\left( \B\left( \mathcal{H} \right) \right)$. If $h,k\in \mathcal{H}$, define $\theta_{h,k}$ to be the rank-one bounded operator $\theta_{h,k}\left( x \right) = \iprod{x}{k}h$.\newline

The linear span of the $\theta_{x,y}$ is dense in $L_1\left( \B\left( \mathcal{H} \right) \right)$ with the trace norm. For $A\in \B\left( \mathcal{H} \right)$, we have
\begin{align*}
  \tr\left( A\theta_{h,k} \right) &= \iprod{Ah}{k}.
\end{align*}
\begin{proof}
  Let $X$ be a Banach space, and let $\mathcal{H}$ be a Hilbert space. A bounded net $\left( L_{\lambda} \right)_{\lambda}$ in $\B\left( X,\B\left( \mathcal{H} \right) \right)$ converges in the BW topology to $L$ if and only if $ \iprod{L_{\lambda}(x)h}{k} \rightarrow \iprod{L(x)h}{k} $ for all $h,k\in \mathcal{H}$ and $x\in X$.
\end{proof}
\begin{proof}
  We know that $\left( L_{\lambda} \right)_{\lambda}\xrightarrow{\text{BW}} L$ if and only if $\tr\left( L_{\lambda}(x)T \right)\rightarrow \tr\left( L(x)T \right)$ for all $T\in L_1\left( \B\left( \mathcal{H} \right) \right)$ and $x\in X$. However, since the net is bounded, we only need to consider the case of $T = \theta_{h,k}$.
\end{proof}
In other words, BW convergence is pointwise WOT convergence.\footnote{Yes, there are two modifiers here.} Now, we consider some subspace we will use to establish Arveson's extension theorem.
\begin{definition}
  Let $A$ be a $C^{\ast}$-algebra, $S$ an operator system, and $M$ a subspace. We define
  \begin{align*}
    B_r\left( M,\mathcal{H} \right) &\coloneq \set{L\in \B\left( M,\B\left( \mathcal{H} \right) \right) | \norm{L} \leq r}\\
    CB_{r}\left( M,\mathcal{H} \right) &\coloneq \set{L\in \B\left( M,\B\left( \mathcal{H} \right) \right) | \norm{L}_{\cb}\leq r}\\
    CP_{r}\left( S,\mathcal{H} \right) &\coloneq \set{L\in \B\left( S,\B\left( \mathcal{H} \right) \right) | L\text{ is completely positive, }\norm{L}\leq r}\\
    CP\left( S,\mathcal{H};P \right) &\coloneq \set{L\in \B\left( S,\B\left( \mathcal{H} \right) \right) | L\text{ is completely positive, }L(1) = P}.
  \end{align*}
\end{definition}
\begin{theorem}
  Let $A$ be a $C^{\ast}$-algebra, $S$ a closed operator system, and $M$ a closed subspace. Then, each of the above four sets is BW-compact.
\end{theorem}
\begin{proof}
  Since BW is a weak* topology, the set $B_r\left( M,\mathcal{H} \right)$, being BW-closed and norm-bounded, is thus compact by the Banach--Alaoglu theorem. Thus, it is enough to show that the remaining sets are subsets of this set.\newline

  Let $\left( L_{\lambda} \right)_{\lambda}$ be a net in $CB_r\left( M,\mathcal{H} \right)$, and let $\left( L_{\lambda} \right)_{\lambda}\rightarrow \lambda$.\newline

  If $\left( a_{ij} \right)_{ij}\in \Mat_n\left( M \right)$, and $x = x_1\oplus\cdots\oplus x_n$, $y = y_1\oplus\cdots\oplus y_n$, are in $\mathcal{H}\oplus\cdots\oplus \mathcal{H}$, then
  \begin{align*}
    \iprod{\left( L\left( a_{ij} \right) \right)_{ij}x}{y} &= \lim_{\lambda} \iprod{\left( L_{\lambda}\left( a_{ij} \right) \right)_{ij}x}{y},
  \end{align*}
  so that
  \begin{align*}
    \norm{\left( L\left( a_{ij} \right) \right)_{ij}} &\leq r\norm{\left( a_{ij} \right)_{ij}}
  \end{align*}
  for all $n$, meaning
  \begin{align*}
    \norm{L}_{\cb}\leq r.
  \end{align*}
  A similar process holds for the other sets.
\end{proof}
Before we go to Arveson's extension theorem in the general case, we start with the case of maps into $\Mat_n\left( \C \right)$.
\begin{definition}
  Let $M$ be an operator space, and let $\set{e_{j}}_{j=1}^{n}$ be the canonical basis for $\C^n$. If $A\in \Mat_n\left( \C \right)$, we let $A_{\left( i,j \right)} \coloneq \iprod{Ae_j}{e_i}$.\newline

  If $\phi\colon M\rightarrow \Mat_n\left( \C \right)$ is a linear map, we associate a linear functional $s_{\phi}$, defined by
  \begin{align*}
    s_{\phi}\left( \left( a_{ij} \right)_{ij} \right) &= \frac{1}{n}\sum_{i,j=1}^{n} \iprod{\phi\left( \left( a_{ij} \right)_{ij} \right)e_j}{e_i}.
  \end{align*}
  Alternatively, if we let $x\in \C^n\oplus\cdots\oplus \C^n$ be defined by $x = e_1\oplus\cdots\oplus e_n$, we may define
  \begin{align*}
    s_{\phi}\left( \left( a_{ij} \right)_{ij} \right) &= \frac{1}{n} \iprod{\phi_n\left( \left( a_{ij} \right)_{ij} \right)x}{x}
  \end{align*}
\end{definition}
It can be verified that $\phi\mapsto s_{\phi}$ is a linear map between $\mathcal{L}\left( M,\Mat_n\left( \C \right) \right)$ and $\mathcal{L}\left( \Mat_n\left( M \right),\C \right)$. If $M$ is unital, and $\phi(1) = 1$, then $s_{\phi}(1) = 1$.\newline

Finally, if $s\colon \Mat_n\left( M \right)\rightarrow \C$, then we may define $\phi_s\colon M\rightarrow \Mat_n\left( \C \right)$ by
\begin{align*}
  \iprod{\phi_s\left( a \right)e_j}{e_i} &= ns\left( a\otimes e_{ij} \right),
\end{align*}
where $a\otimes e_{ij}$ is the element of $\Mat_n\left( M \right)$ that has $a$ in row $i$ and column $j$, and is $0$ elsewhere. The maps $\phi\mapsto s_{\phi}$ and $s\mapsto \phi_s$ are mutual inverses.
\begin{exercise}[Krein's Theorem]
Let $S$ be an operator system contained in the $C^{\ast}$-algebra $A$. Let $\phi\colon S\rightarrow \C$ be positive. Show that $\phi$ can be extended to a positive map on $A$.
\end{exercise}
\begin{solution}
  We may extend $\phi$ to a positive map on $A$ by defining $\psi\colon A\rightarrow \C$ by defining $\psi|_{S} = \phi$ and $\psi|_{A\setminus S} = 0$.
\end{solution}
\begin{theorem}
  Let $A$ be a unital $C^{\ast}$-algebra, let $S$ be an operator system in $A$, and let $\phi\colon S\rightarrow \Mat_n\left( \C \right)$. The following are equivalent:
  \begin{enumerate}[(i)]
    \item $\phi$ is completely positive;
    \item $\phi$ is $n$-positive;
    \item $s_{\phi}$ is positive.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Since (i) implies (ii), and (ii) implies (iii) by the definition of $s_{\phi}$ as
  \begin{align*}
    s_{\phi}\left( \left( a_{ij} \right)_{ij} \right) &= \frac{1}{n} \iprod{\phi_n\left( \left( a_{ij} \right)_{ij} \right)x}{x},
  \end{align*}
  where $x = e_1\oplus\cdots\oplus e_n$, we have that $s_{\phi}$ is positive.\newline

  Suppose $s_{\phi}$ is positive. Then, we may extend $s_{\phi}$ from $\Mat_n\left( S \right)$ to a positive linear functional $s$ on $\Mat_n\left( A \right)$. The map $\psi\colon A\rightarrow \Mat_n\left( \C \right)$ associated to $s$ extends $s_{\phi}$.\newline

  If we can prove that $\psi$ is completely positive, then $\phi = \psi|_{S}$ is completely positive.\newline

  To show that $\psi$ is $m$-positive, we may consider an element of the form $\left( a^{\ast}_ia_j \right)_{ij} \in \Mat_m\left( A \right)$.\newline

  Since $\psi_m\left( \left( a^{\ast}_ia_j \right)_{ij} \right)$ acts on $\C^{mn}$, it is sufficient to take $x = x_1\oplus\cdots\oplus x_m$, where $x_j = \sum_{k=1}^{n}\lambda_{jk}e_k$, and find
  \begin{align*}
    \iprod{\psi_m\left( \left( a^{\ast}_ia_j \right)_{ij} \right)x}{x} &= \sum_{i,j} \iprod{\psi\left( a_i^{\ast}a_j \right)x}{x}\\
                                                                       &= \sum_{i,j,k,\ell} \lambda_{jk}\overline{\lambda_{i\ell}} \iprod{\psi\left( a_i^{\ast}a_j \right)e_k}{e_{\ell}}\\
                                                                       &= \sum_{i,j,k,\ell}\lambda_{jk}\overline{\lambda_{i\ell}} s\left( a_i^{\ast}a_j\otimes e_{\ell k} \right).
  \end{align*}
  Let $A_i$ be the $n\times n$ matrix with row $\lambda_{i1},\dots,\lambda_{in}$ and $0$ elsewhere. Then,
  \begin{align*}
    A_i^{\ast}A_j &= \sum_{k,\ell}\overline{\lambda_{i\ell}}\lambda_{jk}e_{\ell k}.
  \end{align*}
  Therefore, we obtain
  \begin{align*}
    \iprod{\psi_m\left( \left( a_i^{\ast}a_j \right)_{ij} \right)x}{x} &= \sum_{i,j}s\left( a_i^{\ast}a_j\otimes  A_i^{\ast}A_j\right)\\
                                                                       &= s\left( \left( \sum_{i}a_i\otimes A_i \right)^{\ast}\left( \sum_{j}a_j\otimes A_j \right) \right),
  \end{align*}
  which is positive since $s$ is positive. Thus, $\psi$ is $m$-positive for all $m$.
\end{proof}
\begin{theorem}
  Let $A$ be a unital $C^{\ast}$-algebra, $S\subseteq A$ an operator system, and $\phi\colon S\rightarrow \Mat_n\left( \C \right)$ completely positive.\newline

  Then, there exists a completely positive map $\psi\colon A\rightarrow \Mat_n\left( \C \right)$ that extends $\phi$.
\end{theorem}
\begin{proof}
  Let $s_{\phi}\colon \Mat_n\left( S \right)\rightarrow \C$ associated with $\phi$. Extend $s_{\phi}$ to a positive linear functional $s$ on $\Mat_n\left( A \right)$ by Krein's theorem.\newline

  The map $\psi\colon A\rightarrow \Mat_n\left( \C \right)$ is completely positive. Since $s$ extends $s_{\phi}$, $\psi$ extends $\phi$.
\end{proof}
We can now show the case for $\B\left( \mathcal{H} \right)$, not just the case of matrix algebras.
\begin{theorem}[Arveson's Extension Theorem]
  Let $A$ be a $C^{\ast}$-algebra, $S\subseteq A$ an operator system, and $\phi\colon S\rightarrow \B\left( \mathcal{H} \right)$ a completely positive map.\newline

  Then, there exists $\psi\colon A\rightarrow \B\left( \mathcal{H} \right)$ extending $\phi$. This implies that $\B\left( \mathcal{H} \right)$ is injective in the category of $C^{\ast}$-algebras with completely positive maps.
  \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQBlEAX1PU1z5CKAEwVqdJq3YBBHnxAZseAkTLEJDFm0QgAOnoBCACgMBbejgAWAYybAAEtwCU8-sqFFy4mlum7yHgkYKABzeCJQADMAJwgzJG8QHAgkMUltdgM0KywQGkZ6ACMYRgAFARVhEBisUKscNxBY+KQyZNTEdMKS8srPXUYYKMbfKR19PTRsJpaExCSUtrHM3QN8HHpZuPmAZholhe5KbiA
    \begin{tikzcd}
      0 \arrow[r] & S \arrow[d, "\phi"'] \arrow[r, "\iota"] & A \arrow[ld, "\psi"] \\
                  & \B(\mathcal{H})                         &                     
    \end{tikzcd}
  \end{center}
\end{theorem}
\begin{theorem}
  Let $\mathcal{F}$ be a finite-dimensional subspace of $\mathcal{H}$, and let $\phi_{\mathcal{F}}\colon S\rightarrow \B\left( \mathcal{F} \right)$ be the compression $\phi_{\mathcal{F}}\left( a \right) = P_{\mathcal{F}}\phi(a)|_{\mathcal{F}}$, where $P_{\mathcal{F}}$ is the projection onto $\mathcal{F}$.\newline

  Since $\B\left( \mathcal{F} \right)\cong \Mat_n\left( \C \right)$ for some $n$, there exists a completely positive map $\psi_{\mathcal{F}}\colon A\rightarrow \B\left( \mathcal{F} \right)$ that extends $\phi_{\mathcal{F}}$. Let $\psi_{\mathcal{F}}'\colon A\rightarrow \B\left( \mathcal{H} \right)$ be defined by $\psi_{\mathcal{F}}'\left( a \right) = \psi_{\mathcal{F}}(a)$ on $\mathcal{F}$ and equal to $0$ on $\mathcal{F}^{\perp}$.\newline

  The set of finite-dimensional subspaces of $\mathcal{H}$ is a directed set under inclusion, so $\left( \psi_{\mathcal{F}}' \right)_{\mathcal{F}}$ is a net in $\text{CP}_{r}\left( A,\mathcal{H} \right)$, where $r = \norm{\phi}$.\newline

  Since the set $\text{CP}_{r}\left( A,\mathcal{H} \right)$ is compact, we may choose a subnet which converges to some $\psi\in \text{CP}_{r}\left( A,\mathcal{H} \right)$.\newline

  We claim that $\psi$ is the desired extension. If $a\in S$ and $x,y\in \mathcal{H}$, let $\mathcal{F} = \Span\left( x,y \right)$. Then, for any $\mathcal{F}_1\supseteq \mathcal{F}$, we have $ \iprod{\phi(a)x}{y} = \iprod{\psi_{\mathcal{F}}'(a)x}{y} $, and since $\mathcal{F}_1$ is cofinal, $ \iprod{\phi(a)x}{y} = \iprod{\psi(a)x}{y} $.
\end{theorem}
\begin{corollary}
  Let $A$ be a $C^{\ast}$-algebra, $M\subseteq A$ a unital subspace, and $\phi\colon M\rightarrow \B\left( \mathcal{H} \right)$ a unital, completely contractive map. Then, there exists a completely positive map $\psi\colon A\rightarrow \B\left( \mathcal{H} \right)$ that extends $\phi$.
\end{corollary}
\section{Ideals of Operators}%
We need a bit more theory about $\B\left( \mathcal{H} \right)$, specifically related to the trace class/Hilbert--Schmidt operators and the noncommutative dual spaces, as well as their induced topologies. 
\subsection{Polar and Singular Value Decomposition}%
Just as a nonzero complex number $z\in \C$ can be written as $z = \omega \left\vert z \right\vert$, where $\omega\in \T$ is equal to $\sgn(z)$, or $\frac{z}{\left\vert z \right\vert}$, we can decompose an operator into a product of a partial isometry and a positive operator.
\begin{lemma}
  Let $\mathcal{H}$ be a Hilbert space, $T\in \B\left( \mathcal{H} \right)$.
  \begin{enumerate}[(1)]
    \item For all $\xi\in \mathcal{H}$, $\norm{T\xi} = \norm{\left\vert T \right\vert\xi}$, so that $\ker\left( \left\vert T \right\vert \right)= \ker\left( T \right)$.
    \item We have $\overline{\Ran}\left( \left\vert T \right\vert \right) = \ker\left( \left\vert T \right\vert \right)^{\perp} = \ker\left( T \right)^{\perp}$.
  \end{enumerate}
\end{lemma}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item If $\xi\in \mathcal{H}$, then
      \begin{align*}
        \norm{\left\vert T \right\vert\xi} ^2 &= \iprod{\left\vert T \right\vert\xi}{\left\vert T \right\vert\xi}\\
                                              &= \iprod{\left\vert T \right\vert^{\ast}\left\vert T \right\vert\xi}{\xi}\\
                                              &= \iprod{\left\vert T \right\vert^2\xi}{\xi}\\
                                              &= \iprod{T^{\ast}T\xi}{\xi}\\
                                              &= \norm{T\xi}^2,
      \end{align*}
      as $T$ is self-adjoint, and $T^{\ast}T = \left\vert T \right\vert^2$. Thus, $\ker\left( \left\vert T \right\vert \right) = \ker\left( T \right)$.
    \item We have
      \begin{align*}
        \overline{\Ran}\left( T \right) &= \ker\left( \left\vert T \right\vert^{\ast} \right)^{\perp}\\
                                        &= \ker\left( \left\vert T \right\vert \right)^{\perp}.
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{proposition}[Polar Decomposition]
  Let $\mathcal{H}$ be a Hilbert space, and let $T\in \B\left( \mathcal{H} \right)$. Then, there exists a unique partial isometry $V\in \B\left( \mathcal{H} \right)$ such that $T = V\left\vert T \right\vert$ and $\ker\left( T \right) = \ker\left( V \right)$. Moreover, $V^{\ast}T = \left\vert T \right\vert$, and if $T$ is invertible, then $V = T\left\vert T \right\vert^{-1}$ is unitary, and belongs to $C^{\ast}\left( T \right)$.
\end{proposition}
\begin{proof}
  We see that the map $\left\vert T \right\vert\xi \mapsto T\xi$ from $\Ran\left( \left\vert T \right\vert \right)$ to $\Ran\left( T \right)$, is well-defined, linear, and isometric, so it extends by continuity to an isometric linear map $V_0\colon \overline{\Ran}\left( \left\vert T \right\vert \right)\rightarrow \overline{\Ran}\left( T \right)$. Define
  \begin{align*}
    V\colon \ker\left( T \right)^{\perp}\oplus \ker\left( T \right)\rightarrow \mathcal{H}\\
    \xi\oplus \eta \mapsto V_0\xi.
  \end{align*}
  By construction, $V\left\vert T \right\vert = T$ and $\ker\left( V \right) = \ker\left( T \right)$. Since $V|_{\ker\left( V \right)^{\perp}} = V_0$ is isometric, $V$ is a partial isometry with initial space $\ker\left( T \right)^{\perp}$ and final space $\overline{\Ran}\left( T \right)$.\newline

  Now, since $V^{\ast}V$ is the orthogonal projection onto the initial space $\overline{\Ran}\left( \left\vert T \right\vert \right)$, we have
  \begin{align*}
    V^{\ast}T &= V^{\ast}V\left\vert T \right\vert\\
              &= \left\vert T \right\vert.
  \end{align*}
  For uniqueness, suppose $W$ is a partial isometry satisfying $T = W\left\vert T \right\vert$ with $\ker\left( W \right) = \ker\left( T \right)$. Then, for all $\xi\in \mathcal{H}$, we have
  \begin{align*}
    W\left( \left\vert T \right\vert\xi \right) &= T\xi\\
                                                &= V\left( \left\vert T \right\vert\xi \right),
  \end{align*}
  so $W$ and $V$ agree on $\Ran\left( \left\vert T \right\vert \right)$, and by continuity, $\overline{\Ran}\left( \left\vert T \right\vert \right)$. Since both $W$ and $V$ send $\ker\left( T \right)$ to $0$, $W$ and $V$ agree on the orthogonal complement $\overline{\Ran}\left( \left\vert T \right\vert \right)^{\perp}$ as well. Thus, $W = V$.\newline

  Now, if $T$ is invertible, then so is $\left\vert T \right\vert$, so $T\left\vert T \right\vert^{-1} = V$. Since $\Ran\left( T \right) = \Ran\left( \left\vert T \right\vert \right) = \mathcal{H}$, the operator $V$ is a surjective isometry, hence unitary.\newline

  Furthermore, $\left\vert T \right\vert\in C^{\ast}\left( T \right)$, and $\left\vert T \right\vert^{-1}\in C^{\ast}\left( \left\vert T \right\vert \right)\subseteq C^{\ast}\left( T \right)$, so that $V = T\left\vert T \right\vert^{-1}\in C^{\ast}\left( T \right)$.
\end{proof}
\begin{fact}
  Let $T = V\left\vert T \right\vert$ be the polar decomposition of $T\in \B\left( \mathcal{H} \right)$. Then, $\left\vert T^{\ast} \right\vert = V\left\vert T \right\vert V^{\ast}$.
\end{fact}
\begin{proof}
  We see that
  \begin{align*}
    \left( V\left\vert T \right\vert V^{\ast} \right)\left( V \left\vert T \right\vert V^{\ast} \right) &= TV^{\ast}TV^{\ast}\\
                                                                                                        &= T\left\vert T \right\vert V^{\ast}\\
                                                                                                        &= T\left( V\left\vert T \right\vert \right)^{\ast}\\
                                                                                                        &= TT^{\ast}.
  \end{align*}
  Since $\left\vert T \right\vert$ is positive, and positivity is preserved by conjugation, $V\left\vert T \right\vert V^{\ast}$ is also positive, and since $\left( V\left\vert T \right\vert V^{\ast} \right)^2 = TT^{\ast}$, uniqueness of square roots implies that $V \left\vert T \right\vert V^{\ast} = \left( TT^{\ast} \right)^{1/2} = \left\vert T^{\ast} \right\vert$.
\end{proof}
One of the primary uses of the polar decomposition is in establishing the singular value decomposition.
\begin{proposition}[Singular Value Decomposition]
  Let $T \in\K\left( \mathcal{H} \right)$ be compact. There is a possibly finite sequence $\left( \mu_k \right)_k\in c_0$ with $\mu_k > 0$, and two possibly finite orthonormal sequences $\left( e_k \right)_k$ and $\left( f_k \right)_k$ in $\mathcal{H}$ such that
  \begin{align*}
    T &= \sum_{k}\mu_k\theta_{f_k,e_k},
  \end{align*}
  with convergence in operator norm. Moreover, $\norm{T}_{\op} = \max_{k}\left\vert \mu_k \right\vert$.\newline

  If $T$ is finite-rank, then the sum is finite.\newline

  The scalars $\left( \mu_k \right)_k$ are called the singular values of $T$.
\end{proposition}
\begin{proof}
  Let $T = V\left\vert T \right\vert$ be the polar decomposition. Then, $\left\vert T \right\vert = V^{\ast}T$ is compact, as $\K\left( \mathcal{H} \right)$ is an ideal. Similarly, if $T$ is finite-rank, then so is $\left\vert T \right\vert$.\newline

  From the spectral theorem, we know that
  \begin{align*}
    \left\vert T \right\vert &= \sum_{k}\mu_k\theta_{e_k,e_k},
  \end{align*}
  where $\left( e_k \right)_k$ is an orthonormal sequence and $\left( \mu_k \right)_k$ is a sequence of necessarily positive scalars (as $\left\vert T \right\vert$ is a positive operator). The sum is finite if $T$ is finite rank.\newline

  Now, we set $f_k = Ve_k$ for each $k$. Thus, we have
  \begin{align*}
    T &= V\left\vert T \right\vert\\
      &= V\left( \sum_{k}\mu_k\theta_{e_k,e_k} \right)\\
      &= \sum_{k}\mu_kV\theta_{e_k,e_k}\\
      &= \sum_{k}\mu_{k}\theta_{Ve_k,e_k}\\
      &= \sum_{k}\mu_k\theta_{f_k,e_k}.
  \end{align*}
  Now, we must show that $\left( f_k \right)_k$ is orthonormal. Since $e_k$ is an eigenvector for $\left\vert T \right\vert$, and each $\mu_k$ is nonzero, each $e_k$ belongs to $\Ran\left( \left\vert T \right\vert \right)$. Since $V^{\ast}V$ is the projection onto the initial space $\overline{\Ran}\left( \left\vert T \right\vert \right)$, we have $V^{\ast}Ve_k = e_k$, so
  \begin{align*}
    \iprod{f_k}{f_{\ell}} &= \iprod{Ve_k}{Ve_{\ell}}\\
                          &= \iprod{V^{\ast}Ve_k}{e_{\ell}}\\
                          &= \iprod{e_k}{e_{\ell}}\\
                          &= \delta_{k\ell}.
  \end{align*}
  Now, since $\left\vert T \right\vert$ is diagonalizable, its operator norm is the supremum of its eigenvalues, $\norm{\left\vert T \right\vert}_{\op} = \sup_{k}\left\vert \mu_k \right\vert$, and since $\norm{T}_{\op} = \norm{\left\vert T \right\vert}_{\op}$, and $\left( \mu_k \right)_{k}\in c_0$, the supremum is attained.
\end{proof}
\subsection{Hilbert--Schmidt Operators}%
We start with the noncommutative analogue of $\ell_2$.
\begin{lemma}
  Let $\mathcal{H}$ be a Hilbert space, $T,S\in \B\left( \mathcal{H} \right)$, and let $\left( e_{\alpha} \right)_{\alpha}$ and $\left( f_{\gamma} \right)_{\gamma}$ be orthonormal bases for $\mathcal{H}$. Then,
  \begin{enumerate}[(1)]
    \item $\sum_{\alpha}\norm{Te_{\alpha}}^2 = \sum_{\alpha}\norm{T^{\ast}e_{\alpha}}^2$ (invariant under adjoints);
    \item $\sum_{\alpha}\norm{Te_{\alpha}}^2 = \sum_{\gamma}\norm{Tf_{\gamma}}^2$ (independent of basis choice).
  \end{enumerate}
\end{lemma}
\begin{proof}
  Note that by Parseval's identity, we have
  \begin{align*}
    \norm{Te_{\alpha}}^2 &= \sum_{\beta}\left\vert \iprod{Te_{\alpha}}{e_{\beta}} \right\vert^2\label{eq:parseval1}\tag{\textasteriskcentered}\\
                         &= \sum_{\gamma}\left\vert \iprod{Te_{\alpha}}{f_{\gamma}} \right\vert^2.\label{eq:parseval2}\tag{\textasteriskcentered\textasteriskcentered}
  \end{align*}
  \begin{enumerate}[(1)]
    \item By Fubini's theorem and \eqref{eq:parseval1}, we have
      \begin{align*}
        \sum_{\alpha}\norm{Te_{\alpha}}^2 &= \sum_{\alpha}\sum_{\beta}\left\vert \iprod{Te_{\alpha}}{e_{\beta}} \right\vert^2\\
                                          &= \sum_{\alpha}\sum_{\beta}\left\vert \iprod{e_{\alpha}}{T^{\ast}e_{\beta}} \right\vert^2\\
                                          &= \sum_{\beta}\sum_{\alpha}\left\vert \iprod{T^{\ast}e_{\beta}}{e_{\alpha}} \right\vert^2\\
                                          &= \sum_{\beta}\norm{T^{\ast}e_{\beta}}^2\\
                                          &= \sum_{\alpha}\norm{T^{\ast}e_{\alpha}}^2.
      \end{align*}
    \item The proof commences similarly, substituting \eqref{eq:parseval2} instead of \eqref{eq:parseval1}, and using the result in (1).
  \end{enumerate}
\end{proof}
\begin{definition}
  Let $\mathcal{H}$ be a Hilbert space. We define
  \begin{align*}
    \norm{T}_{\operatorname{HS}} &= \left( \sum_{\alpha\in A}\norm{Te_{\alpha}}^2 \right)^{1/2},
  \end{align*}
  where $\left( e_{\alpha} \right)_{\alpha}$ is an orthonormal basis for $\mathcal{H}$. The class of Hilbert--Schmidt operators is
  \begin{align*}
    L_2\left( \B\left( \mathcal{H} \right) \right) &\coloneq \set{T\in \B\left( \mathcal{H} \right) | \norm{T}_{\operatorname{HS}} < \infty}.
  \end{align*}
\end{definition}
\begin{lemma}
  Let $\mathcal{H}$ be a Hilbert space with orthonormal bases $\left( e_{\alpha} \right)_{\alpha}$ and $\left( f_{\gamma} \right)_{\gamma}$. Let $S,T\in L_2\left( \B\left( \mathcal{H} \right) \right)$. Then,
  \begin{enumerate}[(1)]
    \item The series $\sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}}$ is summable;
    \item $\sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}} = \sum_{\alpha} \iprod{S^{\ast}e_{\alpha}}{Te_{\alpha}}$;
    \item $\sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}} = \sum_{\gamma} \iprod{Tf_{\gamma}}{Sf_{\gamma}}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}[(1)]
    \item For a finite collection $F$ of basis elements, we use the Cauchy--Schwarz inequality in $\mathcal{H}$ and $\ell_2^{\left\vert F \right\vert}$ to get
      \begin{align*}
        \sum_{\alpha\in F}\left\vert \iprod{Te_{\alpha}}{Se_{\alpha}} \right\vert &\leq \sum_{\alpha\in F}\norm{Te_{\alpha}}\norm{Se_{\alpha}}\\
                                                                                  &\leq \left( \sum_{\alpha\in F}\norm{Te_{\alpha}}^2 \right)^{1/2}\left( \sum_{\alpha\in F}\norm{Se_{\alpha}}^2 \right)^{1/2}\\
                                                                                  &\leq \norm{T}_{\hs}\norm{S}_{\hs}.
      \end{align*}
      Taking the supremum over all finite subsets, we have $\sum_{\alpha}\left\vert \iprod{Te_{\alpha}}{Se_{\alpha}} \right\vert$ is summable, so $\sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}}$ is summable.
    \item Recalling the identity that
      \begin{align*}
        \iprod{x}{y} &= \sum_{\alpha} \iprod{x}{e_{\alpha}} \iprod{e_{\alpha}}{y},
      \end{align*}
      we have
      \begin{align*}
        \sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}} &= \sum_{\alpha}\sum_{\beta} \iprod{Te_{\alpha}}{e_{\beta}} \iprod{e_{\beta}}{Se_{\alpha}}\\
                                                       &= \sum_{\alpha}\sum_{\beta} \iprod{e_{\alpha}}{T^{\ast}e_{\beta}} \iprod{S^{\ast}e_{\beta}}{e_{\alpha}}\\
                                                       &= \sum_{\alpha}\sum_{\beta} \iprod{S^{\ast}e_{\beta}}{e_{\alpha}} \iprod{e_{\alpha}}{T^{\ast}e_{\beta}}\\
                                                       &= \sum_{\beta}\sum_{\alpha} \iprod{S^{\ast}e_{\beta}}{e_{\alpha}} \iprod{e_{\alpha}}{T^{\ast}e_{\beta}}\\
                                                       &= \sum_{\beta} \iprod{S^{\ast}e_{\beta}}{T^{\ast}e_{\alpha}}\\
                                                       &= \sum_{\alpha} \iprod{S^{\ast}e_{\alpha}}{T^{\ast}e_{\beta}}.
      \end{align*}
    \item Similarly,
      \begin{align*}
        \sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}} &= \sum_{\alpha}\sum_{\gamma} \iprod{Te_{\alpha}}{f_{\gamma}} \iprod{f_{\gamma}}{Se_{\alpha}}\\
                                                       &= \sum_{\gamma}\sum_{\alpha} \iprod{S^{\ast}f_{\gamma}}{e_{\alpha}} \iprod{e_{\alpha}}{T^{\ast}f_{\gamma}}\\
                                                       &= \sum_{\gamma} \iprod{S^{\ast}f_{\gamma}}{T^{\ast}f_{\gamma}}\\
                                                       &= \sum_{\gamma} \iprod{Tf_{\gamma}}{Sf_{\gamma}}.
      \end{align*}
  \end{enumerate}
\end{proof}
\begin{theorem}[Properties of Hilbert--Schmidt Operators]
  Let $\mathcal{H}$ be a Hilbert space.
  \begin{enumerate}[(1)]
    \item The vector space $L_2\left( \B\left( \mathcal{H} \right) \right)$ admits $\norm{\cdot}_{\hs}$ as a norm.
    \item For $T\in \B\left( \mathcal{H} \right)$, we have $\norm{T}_{\op}\norm{T}_{\hs}$, and $\norm{T}_{\hs} = \norm{T^{\ast}}_{\hs}$.
    \item If $T\in \B\left( \mathcal{H} \right)$ and $S\in L_2\left( \B\left( \mathcal{H} \right) \right)$,
      \begin{align*}
        \norm{TS}_{\hs}&\leq \norm{T}_{\op}\norm{S}_{\hs}\\
        \norm{ST}_{\hs} &\leq \norm{S}_{\hs}\norm{T}_{\op}.
      \end{align*}
      Thus, $L_2\left( \B\left( \mathcal{H} \right) \right)\subseteq \B\left( \mathcal{H} \right)$ is an algebraic ideal.
    \item The form
      \begin{align*}
        \iprod{T}{S}_{\hs} &= \sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}},
      \end{align*}
      where $\left( e_{\alpha} \right)_{\alpha}$ is an orthonormal basis for $\mathcal{H}$, defines an inner product satisfying $ \iprod{T}{T}_{\hs} = \norm{T}_{\hs}^2 $ and $ \iprod{T}{S} = \iprod{S^{\ast}}{T^{\ast}} $.
    \item The pair $\left( L_2\left( \B\left( \mathcal{H} \right) \right), \iprod{\cdot}{\cdot}_{\hs} \right)$ is a Hilbert space.
  \end{enumerate}
\end{theorem}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item If $\lambda\in \C$, then $\norm{\lambda T}_{\hs} = \left\vert \lambda \right\vert\norm{T}_{\hs}$, so that $\lambda T$ is in $L_2\left( \B\left( \mathcal{H} \right) \right)$ if $T\in L_2\left( \B\left( \mathcal{H} \right) \right)$.\newline

      Now, let $\left( e_{\alpha} \right)_{\alpha}$ be an orthonormal basis for $\mathcal{H}$, and let $T,S\in L_2\left( \B\left( \mathcal{H} \right) \right)$. Then,
      \begin{align*}
        \norm{T + S}_{\hs} &= \left( \sum_{\alpha} \norm{\left( T + S \right)e_{\alpha}}^2 \right)^{1/2}\\
                           &\leq \left( \sum_{\alpha}\left( \norm{Te_{\alpha}} + \norm{Se_{\alpha}} \right)^2 \right)^{1/2}\\
                           &\leq \left( \sum_{\alpha}\norm{Te_{\alpha}}^2 \right)^{1/2} + \left( \sum_{\alpha}\norm{Se_{\alpha}} \right)^{1/2}\tag{\textasteriskcentered}\\
                           &= \norm{T}_{\hs} + \norm{S}_{\hs},
      \end{align*}
      where in (\textasteriskcentered), we used the triangle inequality in $\ell_2\left( A \right)$.\newline

      Finally, if $\norm{T}_{\hs} = 0$, then $Te_{\alpha} = 0$ for all $\alpha \in A$, whence $T = 0$.
    \item Let $x\in S_{\mathcal{H}}$. We may extend $\set{x}$ to an orthonormal basis $\left( e_{\alpha} \right)_{\alpha}$ such that $e_{\alpha_0} = x$ for some $\alpha_0$. This gives
      \begin{align*}
        \norm{Tx} &\leq \left( \sum_{\alpha}\norm{Te_{\alpha}}^2 \right)^{1/2}\\
                  &= \norm{T}_{\hs}.
      \end{align*}
      Taking suprema, we get $\norm{T}_{\op}\leq \norm{T}_{\hs}$.\newline

      We showed $\norm{T}_{\hs} = \norm{T^{\ast}}_{\hs}$ implicitly in the earlier lemma.
    \item If $T\in \B\left( \mathcal{H} \right)$ and $S\in L_2\left( \B\left( \mathcal{H} \right) \right)$, we have
      \begin{align*}
        \norm{TS}_{\hs}^2 &= \sum_{\alpha}\norm{TSe_{\alpha}}^2\\
                          &\leq \sum_{\alpha}\norm{T}_{\op}^2\norm{Se_{\alpha}}^2\\
                          &= \norm{T}_{\op}^2 \norm{S}_{\hs}^2.
      \end{align*}
      Taking square roots yields our desired results.\newline

      In the opposite direction, we have
      \begin{align*}
        \norm{ST}_{\hs} &= \norm{\left( ST \right)^{\ast}}_{\hs}\\
                        &\leq \norm{T^{\ast}}_{\op}\norm{S}_{\hs}\\
                        &= \norm{T}_{\hs}\norm{S}_{\hs}.
      \end{align*}
      Thus, $L_2\left( \B\left( \mathcal{H} \right) \right)$ is a two-sided $\ast$-ideal in $\B\left( \mathcal{H} \right)$. Furthermore, if $T,S\in L_2\left( \B\left( \mathcal{H} \right) \right)$, then
      \begin{align*}
        \norm{TS}_{\hs} &\leq \norm{T}_{\op}\norm{S}_{\hs}\\
                        &\leq \norm{T}_{\hs}\norm{S}_{\hs},
      \end{align*}
      so $L_2\left( \B\left( \mathcal{H} \right) \right)$ is a normed $\ast$-algebra as well.
    \item We know that $ \iprod{T}{S}_{\hs} = \sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}} $ is summable and basis-independent. Linearity in the first coordinate follows from the definition of the inner product.\newline

      To see conjugate symmetry, we have
      \begin{align*}
        \iprod{S}{T}_{\hs} &= \sum_{\alpha} \iprod{Se_{\alpha}}{Te_{\alpha}}\\
                           &= \sum_{\alpha} \overline{\iprod{Te_{\alpha}}{Se_{\alpha}}}\\
                           &= \overline{\sum_{\alpha} \iprod{Te_{\alpha}}{Se_{\alpha}}}\\
                           &= \overline{ \iprod{T}{S}_{\hs} }.
      \end{align*}
      Finally,
      \begin{align*}
        \iprod{T}{T}_{\hs} &= \sum_{\alpha} \iprod{Te_{\alpha}}{Te_{\alpha}}\\
                           &= \sum_{\alpha} \norm{Te_{\alpha}}^2\\
                           &= \norm{T}_{\hs}^2.
      \end{align*}
    \item Let $\left( T_n \right)_n$ be a $\norm{\cdot}_{\hs}$-Cauchy sequence in $L_2\left( \B\left( \mathcal{H} \right) \right)$. Thus, $\left( T_n \right)_n$ is $\norm{\cdot}_{\op}$-Cauchy, so there is $T\in \B\left( \mathcal{H} \right)$ such that $\left( T_n \right)_n\xrightarrow{\norm{\cdot}_{\op}} T$.\newline

      We claim that $T\in L_2\left( \B\left( \mathcal{H} \right) \right)$. Let $\left( e_{\alpha} \right)_{\alpha}$ be an orthonormal basis in $\mathcal{H}$, and let $F\subseteq A$ be a finite subset. Since $\left( T_n \right)_n$ is $\norm{\cdot}_{\hs}$-Cauchy, it is $\norm{\cdot}_{\hs}$-bounded. Set $C \coloneq \sup_{n}\norm{T_n}_{\hs} < \infty$.\newline

      Using the triangle inequality in $\mathcal{H}$ and $\ell_2^{\left\vert F \right\vert}$, we get
      \begin{align*}
        \left( \sum_{\alpha \in F}\norm{Te_{\alpha}}^2 \right)^{1/2} &\leq \left( \sum_{\alpha\in F}\left( \norm{Te_{\alpha} - T_ne_{\alpha}} + \norm{T_ne_{\alpha}} \right) \right)^{1/2}\\
                                                                     &\leq \left( \sum_{\alpha\in F}\norm{Te_{\alpha} - T_ne_{\alpha}}^2 \right)^{1/2} + \left( \sum_{\alpha \in F}\norm{T_ne_{\alpha}}^2 \right)^{1/2}\\
                                                                     &\leq \left( \sum_{\alpha \in F}\norm{Te_{\alpha} - T_ne_{\alpha}}^2 \right)^{1/2} + C.
      \end{align*}
      Letting $n\rightarrow\infty$, we have
      \begin{align*}
        \left( \sum_{\alpha\in F}\norm{Te_{\alpha}}^2 \right)^{1/2} &\leq C.
      \end{align*}
      Now, taking the supremum over all finite $F\subseteq A$, we have $\norm{T}_{\hs}\leq C$.\newline

      Now, we claim that $\norm{T_n - T}_{\hs}\rightarrow 0$ as $n\rightarrow\infty$. Since $\left( T_n \right)_n$ is $\norm{\cdot}_{\hs}$-Cauchy, we may choose $N\in \N$ such that for all $m,n\geq N$, we have $\norm{T_m - T_n}_{\hs} < \ve$.\newline

      If $F\subseteq A$ is finite, with $m,n\geq N$, then
      \begin{align*}
        \sum_{\alpha \in F}\norm{T_me_{\alpha} - T_ne_{\alpha}}^2 &< \ve^2.
      \end{align*}
      Taking $m\rightarrow\infty$, we have
      \begin{align*}
        \sum_{\alpha\in F}\norm{Te_{\alpha} - T_ne_{\alpha}}^2 &\leq \ve^2,
      \end{align*}
      and taking the supremum over all finite $F\subseteq A$, we get $\norm{T - T_n}_{\hs}^2 \leq \ve$ for all $n\geq N$.
  \end{enumerate}
\end{proof}
\begin{proposition}
  Let $\mathcal{H}$ be a Hilbert space. The following inclusions hold:
  \begin{align*}
    \F\left( \mathcal{H} \right) \subseteq L_2\left( \B\left( \mathcal{H} \right) \right) \subseteq \mathbb{K}\left( \mathcal{H} \right).
  \end{align*}
  Moreover, $\overline{\F\left( \mathcal{H} \right)}^{\norm{\cdot}_{\hs}} = L_2\left( \B\left( \mathcal{H} \right) \right)$.
\end{proposition}
\begin{proof}
  Let $\xi,\eta\in \mathcal{H}$. Let $\theta_{\xi,\eta}$ be the rank-one bounded operator given by $\theta_{\xi,\eta}\left( x \right) = \iprod{x}{\eta}\xi$. We will show that $\theta_{\xi,\eta}\in L_2\left( \B\left( \mathcal{H} \right) \right)$.\newline

  Let $\left( e_{\alpha} \right)_{\alpha}$ be an orthonormal basis for $\mathcal{H}$. Using Parseval's identity, we have
  \begin{align*}
    \norm{\theta_{\xi,\eta}}_{\hs} &= \left( \sum_{\alpha}\norm{\left( \theta_{\xi,\eta} \right)\left( e_{\alpha} \right)}^2 \right)^{1/2}\\
                                   &= \left( \sum_{\alpha}\norm{ \iprod{e_{\alpha}}{\eta}\xi }^2 \right)^{1/2}\\
                                   &= \left( \norm{\xi}^2\sum_{\alpha}\left\vert \iprod{e_{\alpha}}{\eta} \right\vert^2 \right)^{1/2}\\
                                   &= \left( \norm{\xi}^2\norm{\eta}^2 \right)^{1/2}\\
                                   &= \norm{\xi}\norm{\eta}.
  \end{align*}
  Now, since $L_2\left( \B\left( \mathcal{H} \right) \right)$ is a vector space, we have $\F\left( \mathcal{H} \right) = \Span\left( \set{\theta_{\xi,\eta} | \xi,\eta\in \mathcal{H}} \right)\subseteq L_2\left( \B\left( \mathcal{H} \right) \right)$.\newline

  Finally, let $T\in L_2\left( \B\left( \mathcal{H} \right) \right)$. There is a finite $F\subseteq A$ with $\sum_{\alpha\notin F}\norm{Te_{\alpha}}^2 < \ve^2$. We define $S\in \F\left( \mathcal{H} \right)$ by
  \begin{align*}
    S &\coloneq \sum_{\beta\in F}T\theta_{e_{\beta},e_{\beta}}.
  \end{align*}
  Note that
  \begin{align*}
    Se_{\alpha} &= \begin{cases}
      0 & \alpha\notin F\\
      Te_{\alpha} & \alpha \in F
    \end{cases}.
  \end{align*}
  Therefore,
  \begin{align*}
    \norm{T - S}_{\hs}^2 &= \sum_{\alpha\in A} \norm{Te_{\alpha}-Se_{\alpha}}^2\\
                         &= \sum_{\alpha \in F}\norm {Te_{\alpha}-Se_{\alpha}}^2 + \sum_{\alpha\notin F}\norm{Te_{\alpha}-Se_{\alpha}}^2\\
                         &= \sum_{\alpha\notin F} \norm{Te_{\alpha}}^2\\
                         &< \ve^2.
  \end{align*}
  Since $L_2\left( \B\left( \mathcal{H} \right) \right)$ is $\norm{\cdot}_{\hs}$-complete, we have $\overline{\F\left( \mathcal{H} \right)}^{\norm{\cdot}_{\hs}} = L_2\left( \B\left( \mathcal{H} \right) \right) $.\newline

  Furthermore, since $\norm{T - S}_{\op} < \ve$, as the operator norm is dominated by the HS norm, we have $T\in \F\left( \mathcal{H} \right)^{\norm{\cdot}_{\op}} = \mathbb{K}\left( \mathcal{H} \right)$.
\end{proof}
\begin{proposition}
  Let $\mathcal{H}$ be a Hilbert space. There is a unitary map $U\colon \mathcal{H}\otimes \overline{\mathcal{H}} \rightarrow L_2\left( \B\left( \mathcal{H} \right) \right)$, where $\mathcal{H}\otimes \overline{\mathcal{H}}$ is endowed with the $2$-norm, $ \norm{x\otimes y}^2 = \iprod{x\otimes y}{x\otimes y} = \iprod{x}{x} \iprod{y}{y} $.\newline

  The map $U$ is defined by $U\left( \xi\otimes \overline{\eta} \right) = \theta_{\xi,\eta}$.
\end{proposition}
\begin{proof}
  Consider the bilinear map $\mathcal{H}\times \overline{\mathcal{H}} \rightarrow \F\left( \mathcal{H} \right)$ given by $\left( \xi,\overline{\eta} \right) \mapsto \theta_{\xi,\eta}$. This induces a linear map $U_0\colon \mathcal{H}\otimes \overline{\mathcal{H}} \rightarrow \F\left( \mathcal{H} \right)$, given by $\xi\otimes \overline{\eta} \mapsto \theta_{\xi,\eta}$. This map is surjective since $\F\left( \mathcal{H} \right)$ is the linear span of the rank-one operators.\newline

  Now, let $\xi,\xi',\eta,\eta'\in \mathcal{H}$, and suppose $\left( e_{\alpha} \right)_{\alpha}$ is an orthonormal basis for $\mathcal{H}$. We compute
  \begin{align*}
    \iprod{\theta_{\xi,\eta}}{\theta_{\xi',\eta'}} &= \sum_{\alpha \in A} \iprod{\theta_{\xi,\eta}\left( e_{\alpha} \right)}{\theta_{\xi',\eta'}\left( e_{\alpha} \right)}\\
                                                   &= \sum_{\alpha\in A} \iprod{ \iprod{e_{\alpha}}{\eta}\xi }{ \iprod{e_{\alpha}}{\eta'}\xi' }\\
                                                   &= \sum_{\alpha \in A} \iprod{e_{\alpha}}{\eta} \iprod{\eta'}{e_{\alpha}} \iprod{\xi}{\xi'}\\
                                                   &= \iprod{\eta'}{\eta} \iprod{\xi}{\xi'}\\
                                                   &= \iprod{\xi}{\xi'} \overline{ \iprod{\eta}{\eta'} }\\
                                                   &= \iprod{ \xi\otimes\eta }{\xi'\otimes\eta'}.
  \end{align*}
  By linearity, the map $U_0\colon \mathcal{H}\otimes \overline{\mathcal{H}} \rightarrow \F\left( \mathcal{H} \right)$ is an isomorphism of inner product spaces. By continuity, it extends to the map $U\colon \mathcal{H}\otimes \overline{\mathcal{H}} \rightarrow \overline{\F\left( \mathcal{H} \right)}^{\norm{\cdot}_2} = L_2\left( \B\left( \mathcal{H} \right) \right)$.
\end{proof}
\subsection{Trace-Class Operators}%
Now, we will examine the noncommutative analogue of $\ell_1$. Recall that for any positive operator $T\in \B\left( \mathcal{H} \right)_{+}$, there is a unique $S\in \B\left( \mathcal{H} \right)_{+}$ such that $S^2 = T$, called $T^{1/2}\coloneq S$. We define the absolute value of an operator to be $\left\vert T \right\vert\coloneq \left( T^{\ast}T \right)^{1/2}$.
\begin{definition}
  Let $\mathcal{H}$ be a Hilbert space. We define
  \begin{align*}
    \norm{T}_{\tr} &= \norm{\left\vert T \right\vert^{1/2}}_{\hs}^2.
  \end{align*}
  The collection of trace-class operators is the set
  \begin{align*}
    L_1\left( \B\left( \mathcal{H} \right) \right) &\coloneq \set{T\in \B\left( \mathcal{H} \right) | \norm{T}_{\tr} < \infty}.
  \end{align*}
\end{definition}
Using the Hilbert--Schmidt inner product formula, we may find
\begin{align*}
  \norm{T}_{\tr} &= \norm{\left\vert T \right\vert^{1/2}}_{\hs}^2\\
                 &= \iprod{\left\vert T \right\vert^{1/2}}{\left\vert T \right\vert^{1/2}}_{\hs}\\
                 &= \sum_{\alpha \in A} \iprod{\left\vert T \right\vert^{1/2}e_{\alpha}}{\left\vert T \right\vert^{1/2}e_{\alpha}}\\
                 &= \sum_{\alpha \in A} \iprod{\left\vert T \right\vert e_{\alpha}}{e_{\alpha}}.
\end{align*}
\begin{lemma}
  Let $\mathcal{H}$ be a Hilbert space, and suppose $T\in \B\left( \mathcal{H} \right)$. The following are equivalent:
  \begin{enumerate}[(i)]
    \item $T\in L_1\left( \B\left( \mathcal{H} \right) \right)$;
    \item $\left\vert T \right\vert\in L_1\left( \B\left( \mathcal{H} \right) \right)$;
    \item $\left\vert T \right\vert^{1/2}\in L_2\left( \B\left( \mathcal{H} \right) \right)$;
    \item there exists $S,R\in L_2\left( \B\left( \mathcal{H} \right) \right)$ such that $T = SR$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The statement (i) if and only if (iii) follows from definitions. The statement (i) if and only if (ii) follows from the fact that $\left\vert \left\vert T \right\vert \right\vert = \left\vert T \right\vert$.\newline

  Now, to show (iii) implies (iv), we use the polar decomposition, $T = V\left\vert T \right\vert$. Notice that
  \begin{align*}
    T &= V\left\vert T \right\vert\\
      &= \left( V\left\vert T \right\vert^{1/2} \right) \left\vert T \right\vert^{1/2},
  \end{align*}
  where both factors are in $L_2\left( \B\left( \mathcal{H} \right) \right)$. That the first factor is in $L_2\left( \B\left( \mathcal{H} \right) \right)$ follows from the fact that $L_2\left( \B\left( \mathcal{H} \right) \right)$ is a $\ast$-ideal in $\B\left( \mathcal{H} \right)$.\newline

  Now, if $T = SR$ with $S,R\in L_2\left( \B\left( \mathcal{H} \right) \right)$, we let $T = V\left\vert T \right\vert$ be the polar decomposition of $T$, meaning $\left\vert T \right\vert = V^{\ast}T$. Then,
  \begin{align*}
    \norm{T}_{\tr} &= \sum_{\alpha} \iprod{\left\vert T \right\vert e_{\alpha}}{e_{\alpha}}\\
                   &= \sum_{\alpha} \iprod{ V^{\ast}T e_{\alpha} }{e_{\alpha}}\\
                   &= \sum_{\alpha} \iprod{ SR e_{\alpha} }{V e_{\alpha}}\\
                   &= \sum_{\alpha} \iprod{Re_{\alpha}}{S^{\ast}Ve_{\alpha}}\\
                   &= \iprod{R}{S^{\ast}V}_{\hs}\\
                   &< \infty,
  \end{align*}
  where we once again use the fact that $L_2\left( \B\left( \mathcal{H} \right) \right)$ is a $\ast$-ideal in $\B\left( \mathcal{H} \right)$.
\end{proof}
\begin{lemma}
  Let $\mathcal{H}$ be a Hilbert space. If $T\in L_1\left( \B\left( \mathcal{H} \right) \right)$, and $\left( e_{\alpha} \right)_{\alpha}$ is an orthonormal basis for $\mathcal{H}$, then
  \begin{align*}
    \sum_{\alpha \in A} \iprod{Te_{\alpha}}{e_{\alpha}}
  \end{align*}
  is summable, and any choice of orthonormal basis yields the same result.
\end{lemma}
\begin{proof}
  We know there are $S,R\in L_2\left( \B\left( \mathcal{H} \right) \right)$ such that $T = SR$. Thus,
  \begin{align*}
    \sum_{\alpha\in A} \iprod{Te_{\alpha}}{e_{\alpha}} &= \sum_{\alpha \in A} \iprod{Re_{\alpha}}{S^{\ast}e_{\alpha}}\\
                                                       &= \iprod{R}{S^{\ast}}_{\hs}.
  \end{align*}
\end{proof}
\begin{definition}
  Let $\mathcal{H}$ be a Hilbert space. We define the trace as the map
  \begin{align*}
    \tr\left( T \right) &= \sum_{\alpha \in A} \iprod{Te_{\alpha}}{e_{\alpha}},
  \end{align*}
  where $\left( e_{\alpha} \right)_{\alpha}$ is any orthonormal basis for $\mathcal{H}$.
\end{definition}
\begin{example}
  If $\theta_{x,y}$ is a rank-one operator, and $\left( e_{\alpha} \right)_{\alpha}$ is an orthonormal basis, then
  \begin{align*}
    \tr\left( \theta_{x,y} \right) &= \sum_{\alpha \in A} \iprod{\theta_{x,y}\left( e_{\alpha} \right)}{e_{\alpha}}\\
                                   &= \sum_{\alpha \in A} \iprod{ \iprod{e_{\alpha}}{y}x }{e_{\alpha}}\\
                                   &= \sum_{\alpha \in A} \iprod{x}{e_{\alpha}} \iprod{e_{\alpha}}{y}\\
                                   &= \iprod{x}{y}.
  \end{align*}
\end{example}
\begin{proposition}
  The map $\tr\colon L_1\left( \B\left( \mathcal{H} \right) \right)\rightarrow \C$ is tracial, in that if
  \begin{enumerate}[(i)]
    \item $R,S\in L_2\left( \B\left( \mathcal{H} \right) \right)$, or
    \item $S\in L_1\left( \B\left( \mathcal{H} \right) \right)$ and $R\in \B\left( \mathcal{H} \right)$,
  \end{enumerate}
  then 
  \begin{align*}
    \tr\left( RS \right) &= \tr\left( SR \right).
  \end{align*}
\end{proposition}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item Using the correspondence between trace and Hilbert--Schmidt inner product, we have
      \begin{align*}
        \tr\left( SR \right) &= \iprod{R}{S^{\ast}}_{\hs}\\
                             &= \iprod{S}{R^{\ast}}_{\hs}\\
                             &= \tr\left( RS \right).
      \end{align*}
    \item Letting $S = L_1\left( \B\left( \mathcal{H} \right) \right)$, we may write $S = T_1T_2$ as a product of Hilbert--Schmidt operators. Note that $T_2R,RT_1\in L_2\left( \B\left( \mathcal{H} \right) \right)$ since Hilbert--Schmidt operators form a $\ast$-ideal. Thus,
      \begin{align*}
        \tr\left( SR \right) &= \tr\left( T_1T_2R \right)\\
                             &= \tr\left( T_2RT_1 \right)\\
                             &= \tr\left( RT_1T_2 \right)\\
                             &= \tr\left( RS \right).
      \end{align*}
  \end{enumerate}
\end{proof}
Note that
\begin{align*}
  \norm{T}_{\tr} &= \tr\left( \left\vert T \right\vert \right)\\
  \norm{T}_{\hs} &= \tr\left( \left\vert T \right\vert^2 \right)^{1/2}\\
  \iprod{S}{R}_{\hs} &= \tr\left( R^{\ast}S \right) \\
                     &= \tr\left( SR^{\ast} \right).
\end{align*}
\begin{theorem}
  Let $\mathcal{H}$ be a Hilbert space. 
  \begin{enumerate}[(1)]
    \item The vector space $L_1\left( \B\left( \mathcal{H} \right) \right)$ has the norm $\norm{\cdot}_{\tr}$.
    \item For $T\in L_1\left( \B\left( \mathcal{H} \right) \right)$, we have $\norm{T}_{\op}\leq \norm{T}_{\tr}$ and $\norm{T^{\ast}}_{\tr} = \norm{T}_{\tr}$.
    \item For $T\in \B\left( \mathcal{H} \right)$ and $S\in L_1\left( \B\left( \mathcal{H} \right) \right)$, we have
      \begin{align*}
        \norm{TS}_{\tr} &\leq \norm{T}_{\op}\norm{S}_{\tr}\\
        \norm{ST}_{\tr} &\leq \norm{T}_{\op}\norm{S}_{\tr},
      \end{align*}
      so $L_1\left( \B\left( \mathcal{H} \right) \right)$ is a two-sided $\ast$-ideal in $\B\left( \mathcal{H} \right)$.
    \item The space $L_1\left( \B\left( \mathcal{H} \right) \right)$ is complete when equipped with $\norm{\cdot}_{\tr}$, making it a Banach $\ast$-algebra.
  \end{enumerate}
\end{theorem}
\begin{proof}\hfill
  \begin{enumerate}[(1)]
    \item If $\norm{T}_{\tr} = 0$, then $\left\vert T \right\vert^{1/2} = 0$ as $\norm{\cdot}_{\hs}$ is a norm. Then, $\left\vert T \right\vert = 0$, so $T^{\ast}T = 0$, and $\norm{T}^2_{\op} = \norm{T^{\ast}T}_{\op} = 0$, so $T = 0$.\newline

      Since $\left\vert \lambda T \right\vert = \left\vert \lambda \right\vert\left\vert T \right\vert$, we have 
      \begin{align*}
        \norm{\lambda T}_{\tr} &= \sum_{\alpha\in A} \iprod{\left\vert \lambda T \right\vert e_{\alpha}}{e_{\alpha}}\\
                               &= \sum_{\alpha\in A} \iprod{\left\vert \lambda \right\vert\left\vert T \right\vert e_{\alpha}}{e_{\alpha}}\\
                               &= \left\vert \lambda \right\vert\sum_{\alpha \in A} \iprod{\left\vert T \right\vert e_{\alpha}}{e_{\alpha}}\\
                               &= \left\vert \lambda \right\vert\norm{T}_{\tr}.
      \end{align*}
      Now, suppose $T,S\in L_1\left( \B\left( \mathcal{H} \right) \right)$, with polar decompositions $T = V\left\vert T \right\vert$ and $S = U\left\vert S \right\vert$, $\left( T+S \right) = W\left\vert T+S \right\vert$. Since $\left\vert T \right\vert^{1/2}\in L_2\left( \B\left( \mathcal{H} \right) \right)$, we have
      \begin{align*}
        \norm{T + S}_{\tr} &= \tr\left( \left\vert T+S \right\vert \right)\\
                           &= \tr\left( W^{\ast}\left( T+S \right) \right)\\
                           &= Tr\left( W^{\ast}T \right) + \tr\left( W^{\ast}S \right)\\
                           &= \tr\left( W^{\ast}V\left\vert T \right\vert \right) + \tr\left( W^{\ast}U\left\vert S \right\vert \right)\\
                           &= \tr\left( W^{\ast}V\left\vert T \right\vert^{1/2}\left\vert T \right\vert^{1/2} \right) + \tr\left( W^{\ast}U\left\vert S \right\vert^{1/2}\left\vert S \right\vert^{1/2} \right)\\
                           &= \iprod{\left\vert T \right\vert^{1/2}}{\left\vert T \right\vert^{1/2}V^{\ast}W} + \iprod{\left\vert S \right\vert^{1/2}}{\left\vert S \right\vert^{1/2}U^{\ast}W}\\
                           &\leq \norm{\left\vert T \right\vert^{1/2}}_{\hs}\norm{\left\vert T \right\vert^{1/2}V^{\ast}W}_{\hs} + \norm{\left\vert S \right\vert^{1/2}}_{\hs}\norm{\left\vert S \right\vert^{1/2}U^{\ast}W}_{\hs}\\
                           &\leq \norm{\left\vert T \right\vert^{1/2}}_{\hs}\norm{\left\vert T \right\vert^{1/2}}_{\hs}\norm{V^{\ast}W}_{\op} + \norm{\left\vert S \right\vert^{1/2}}_{\hs}\norm{\left\vert S \right\vert^{1/2}}_{\hs}\norm{U^{\ast}W}_{\op}\\
                           &\leq \norm{\left\vert T \right\vert^{1/2}}_{\hs}^2 + \norm{\left\vert S \right\vert^{1/2}}_{\hs}^2\\
                           &= \norm{T}_{\tr} + \norm{S}_{\tr}.
      \end{align*}
    \item Let $T$ be a trace-class operator on $\mathcal{H}$. Using the fact that $\left\vert T \right\vert$ is positive, we have
      \begin{align*}
        \norm{T}_{\tr} &= \norm{\left\vert T \right\vert^{1/2}}_{\hs}^2\\
                       &\geq \norm{\left\vert T \right\vert^{1/2}}_{\op}^2\\
                       &= \norm{\left\vert T \right\vert}_{\op}\\
                       &= \norm{T}_{\op}.
      \end{align*}
      Now, applying the polar decomposition to $T$ to obtain the partial isometry $V\in \B\left( \mathcal{H} \right)$ with $T = V\left\vert T \right\vert$, we have
      \begin{align*}
        \norm{T^{\ast}}_{\tr} &= \tr\left( \left\vert T^{\ast} \right\vert \right)\\
                              &= \tr\left( V\left\vert T \right\vert V^{\ast} \right)\\
                              &= \tr\left( TV^{\ast} \right)\\
                              &= \tr\left( V^{\ast}T \right)\\
                              &= \tr\left( \left\vert T \right\vert \right)\\
                              &= \norm{T}_{\tr}.
      \end{align*}
    \item We have
      \begin{align*}
        \norm{TS}_{\tr} &= \sum_{\alpha\in A} \iprod{\left\vert TS \right\vert e_{\alpha}}{e_{\alpha}}\\
                        &\leq \sum_{\alpha\in A} \iprod{\norm{T}_{\op} \left\vert S \right\vert e_{\alpha}}{e_{\alpha}}\\
                        &= \norm{T}_{\op}\sum_{\alpha \in A} \iprod{\left\vert S \right\vert e_{\alpha}}{e_{\alpha}}\\
                        &= \norm{T}_{\op}\norm{S}_{\tr}.
      \end{align*}
      Similarly to the case of Hilbert--Schmidt operators, we have
      \begin{align*}
        \norm{ST}_{\tr} &= \norm{\left( ST \right)^{\ast}}_{\tr}\\
                        &= \norm{T^{\ast}S^{\ast}}_{\tr}\\
                        &\leq \norm{T^{\ast}}_{\op}\norm{S^{\ast}}_{\tr}\\
                        &= \norm{T}_{\op}\norm{S}_{\tr}.
      \end{align*}
    \item Let $\left( T_n \right)_n$ be a $\norm{\cdot}_{\tr}$-Cauchy sequence in $L_1\left( \B\left( \mathcal{H} \right) \right)$. Let $C > 0$ be such that $\norm{T_n}_{\tr} \leq C$ for all $n$. By (2), $\left( T_n \right)_n$ is $\norm{\cdot}_{\op}$-Cauchy, so there is $T\in \B\left( \mathcal{H} \right)$ such that $\norm{T_n - T}_{\op}\rightarrow 0$.\newline

      We claim that $T$ is trace-class. Let $\left( e_{\alpha} \right)_{\alpha}$ be an orthonormal basis for $\mathcal{H}$, and suppose $F\subseteq A$ is finite. We see that $\left( \left\vert T_n \right\vert \right)_{n}\rightarrow \left\vert T \right\vert$ in operator norm, so that
      \begin{align*}
        \sum_{\alpha \in F} \iprod{\left\vert T \right\vert e_{\alpha}}{e_{\alpha}} &= \lim_{n\rightarrow\infty} \left( \sum_{\alpha \in F} \iprod{\left\vert T_n \right\vert e_{\alpha}}{e_{\alpha}} \right)\\
                                                                                    &\leq \limsup_{n\rightarrow\infty} \norm{T_n}_{\tr}\\
                                                                                    &\leq C.
      \end{align*}
      Taking the supremum over all finite $F\subseteq A$, we have $\norm{T}_{\tr} \leq C < \infty$.\newline

      Now, we claim that $\norm{T - T_n}_{\tr}\rightarrow 0$. Letting $\ve > 0$, we find $N\in \N$ such that for $m,n\geq N$, $\norm{T_n - T_m}_{\tr} < \ve$. We find that $\left( \left\vert T_m - T-n \right\vert \right)_{m}\rightarrow \left\vert T - T_n \right\vert$ in operator norm as $m\rightarrow\infty$ with fixed $n$. Thus, if $F\subseteq A$ is finite, for all $n\geq N$, we have
      \begin{align*}
        \sum_{\alpha \in F} \iprod{\left\vert T - T_n \right\vert e_{\alpha}}{e_{\alpha}} &= \lim_{m\rightarrow\infty} \left( \sum_{\alpha \in F} \iprod{\left\vert T_m - T_n \right\vert e_{\alpha}}{e_{\alpha}} \right)\\
                                                                                          &\leq \limsup_{m\rightarrow\infty} \norm{T_m - T_n}_{\tr}\\
                                                                                          &\leq \ve.
      \end{align*}
      Taking the supremum over all finite subsets $F\subseteq A$, we have $\norm{T - T_n}_{\tr} \leq \ve$ for all $n\geq N$.
  \end{enumerate}
\end{proof}
\begin{proposition}
  Let $\mathcal{H}$ be a Hilbert space. The following inclusions hold.
  \begin{align*}
    \F\left( \mathcal{H} \right) \subseteq L_1\left( \B\left( \mathcal{H} \right) \right) \subseteq L_2\left( \B\left( \mathcal{H} \right) \right)\subseteq \B\left( \mathcal{H} \right).
  \end{align*}
  Moreover, $\overline{\F\left( \mathcal{H} \right)}^{\norm{\cdot}_{\tr}} = L_1\left( \B\left( \mathcal{H} \right) \right)$.
\end{proposition}
\begin{proof}
  Note that $L_2\left( \B\left( \mathcal{H} \right) \right)$ is an ideal, so that $L_1\left( \B\left( \mathcal{H} \right) \right)\subseteq L_2\left( \B\left( \mathcal{H} \right) \right)$.\newline

  We have shown that every rank-one operator $\xi\otimes \overline{\eta}$ is trace-class, so $\F\left( \mathcal{H} \right)\subseteq L_1\left( \B\left( \mathcal{H} \right) \right)$.\newline

  Let $T \in L_1\left( \B\left( \mathcal{H} \right) \right)$, and let $\ve > 0$. Then, there is an orthonormal basis $\left( e_{\alpha} \right)_{\alpha}$ and a finite subset $F\subseteq A$ such that
  \begin{align*}
    \sum_{\alpha\notin F} \iprod{\left\vert T \right\vert e_{\alpha}}{e_{\alpha}} < \ve.
  \end{align*}
  Now, let $P$ be the projection onto $\Span\left( \set{e_{\alpha} | \alpha \in F} \right)$, and let $T - TP = V\left\vert T-TP \right\vert$ be the polar decomposition, so $V^{\ast}\left( T-TP \right) = \left\vert T-TP \right\vert$. Thus, we have
  \begin{align*}
    \norm{T-TP}_{\tr} &= \sum_{\alpha \in A} \iprod{\left\vert T-TP \right\vert e_{\alpha}}{e_{\alpha}}\\
                      &= \sum_{\alpha \in A} \iprod{\left( T-TP \right) e_{\alpha}}{Ve_{\alpha}}\\
                      &= \sum_{\alpha \in F} \iprod{\left( T-TP \right) e_{\alpha}}{Ve_{\alpha}} + \sum_{\alpha \notin F} \iprod{\left( T- TP \right)e_{\alpha}}{e_{\alpha}}\\
                      &= \sum_{\alpha\notin F} \iprod{Te_{\alpha}}{Ve_{\alpha}}\\
                      &= \sum_{\alpha\notin F} \iprod{\left\vert T \right\vert e_{\alpha}}{e_{\alpha}}\\
                      &< \ve.
  \end{align*}
  Since $TP$ is of finite rank, and the trace class operators are complete, we have $\overline{\F\left( \mathcal{H} \right)}^{\norm{\cdot}_{\tr}} = L_1\left( \B\left( \mathcal{H} \right) \right)$.
\end{proof}
\begin{proposition}
  Let $\mathcal{H}$ be a Hilbert space. The trace, $\tr\colon L_1\left( \B\left( \mathcal{H} \right) \right)\rightarrow \C$ is a contractive, positive, and faithful linear functional, with
  \begin{align*}
    \left\vert \tr\left( ST \right) \right\vert &\leq \norm{S}_{\op}\norm{T}_{\tr}
  \end{align*}
  for all $S\in \B\left( \mathcal{H} \right)$ and $T\in L_1\left( \B\left( \mathcal{H} \right) \right)$.
\end{proposition}
\begin{proof}
  Linearity is clear by definition, and the fact that the trace is a contraction follows from taking $S = I_{\mathcal{H}}$.\newline

  Note that
  \begin{align*}
    \tr\left( T^{\ast}T \right) &= \iprod{T}{T}_{\hs}\\
                                &= \norm{T}_{\hs}^2\\
                                &\geq 0,
  \end{align*}
  so $\tr\left( T^{\ast}T \right) = 0$ implies $T = 0$, meaning that it is faithful.\newline

  Now, let $T\in L_1\left( \B\left( \mathcal{H} \right) \right)$ have the polar decomposition $T = V\left\vert T \right\vert$. Using the Cauchy--Schwarz inequality for $ \iprod{\cdot}{\cdot}_{\hs} $ along with some other properties, we have
  \begin{align*}
    \left\vert \tr\left( ST \right) \right\vert &= \left\vert \tr\left( SV\left\vert T \right\vert^{1/2}\left\vert T \right\vert^{1/2} \right) \right\vert\\
                                                &= \left\vert \iprod{\left\vert T \right\vert^{1/2}V^{\ast}S^{\ast}}{\left\vert T \right\vert^{1/2}}_{\hs} \right\vert\\
                                                &\leq \norm{\left\vert T \right\vert^{1/2}V^{\ast}S^{\ast}}_{\hs}\norm{\left\vert T \right\vert^{1/2}}_{\hs}\\
                                                &\leq \norm{\left\vert T \right\vert^{1/2}}_{\hs}\norm{\left\vert T \right\vert^{1/2}}_{\hs}\norm{V^{\ast}S^{\ast}}_{\op}\\
                                                &\leq \norm{\left\vert T \right\vert^{1/2}}_{\hs}^2 \norm{S^{\ast}}_{\op}\\
                                                &= \norm{T}_{\tr}\norm{S}_{\op}.
  \end{align*}
\end{proof}
\begin{example}
  Let $T\in \F\left( \mathcal{H} \right)$ be a finite-rank operator. We may compute the trace-class norm of $T$ by looking at the singular values,
  \begin{align*}
    T &= \sum_{k=1}^{n}\mu_k\theta_{f_k,e_k},
  \end{align*}
  where $\left\vert T \right\vert = \sum_{k=1}^{n}\mu_k\theta_{e_k,e_k}$ is the diagonalization of the positive finite-rank operator $\left\vert T \right\vert$.\newline

  Since the trace is linear, we calculate
  \begin{align*}
    \norm{T}_{\tr} &= \tr\left( \left\vert T \right\vert \right)\\
                   &= \tr\left( \sum_{k=1}^{n}\mu_k\theta_{e_k,e_k} \right)\\
                   &= \sum_{k=1}^{n}\mu_k\tr\left( \theta_{e_k,e_k} \right)\\
                   &= \sum_{k=1}^{n}\mu_k.
  \end{align*}
\end{example}
Now, similar to the case of the compact and Hilbert--Schmidt operators, where we realize them as tensor products of $\mathcal{H} \otimes \overline{\mathcal{H}}$ with respect to the injective norm and $2$-norm respectively, we may realize the trace-class operators as a tensor product.
\begin{proposition}
  Let $\mathcal{H}$ be a Hilbert space. Then, there is an isometric isomorphism
  \begin{align*}
    T\colon \mathcal{H}\hat\otimes \overline{\mathcal{H}},
  \end{align*}
  given by $x\otimes \overline{y} \mapsto \theta_{x,y}$.
\end{proposition}
\begin{proof}
  We have established a linear isomorphism that maps $x\otimes \overline{y}\mapsto \theta_{x,y}$ between $\mathcal{H}\otimes \overline{\mathcal{H}}$ and $\F\left( \mathcal{H} \right)$.\newline

  Now, let $t\in \mathcal{H}\otimes \overline{\mathcal{H}}$. Then, for any representation $t = \sum_{k=1}^{n}x_k\otimes \overline{y_k}$, we obtain
  \begin{align*}
    \norm{T(t)}_{\tr} &= \norm{\sum_{k=1}^{n}\theta_{x_k,y_k}}_{\tr}\\
                      &\leq \sum_{k=1}^{n}\norm{\theta_{x_k,y_k}}_{\tr}\\
                      &= \sum_{k=1}^{n}\norm{x_k}\norm{y_k}.
  \end{align*}
  Taking the infimum over representations, we get $\norm{T(t)}_{\tr}\leq \norm{t}_{\wedge}$.\newline

  On the other hand, using the singular value decomposition, we may express
  \begin{align*}
    T(t) &= \sum_{k=1}^{n}\mu_k\theta_{f_k,e_k},
  \end{align*}
  where $\mu_k > 0$ are nonzero eigenvalues for $\left\vert T(t) \right\vert$ and $\left( e_k \right)_{k=1}^{n}$ and $\left( f_k \right)_{k=1}^{n}$ are orthonormal families. This gives
  \begin{align*}
    \norm{t}_{\wedge} &= \norm{\sum_{k=1}^{n}\mu_k\left( f_k\otimes \overline{e_k} \right)}_{\wedge}\\
                      &\leq \sum_{j=1}^{n}\norm{\mu_kf_k}\norm{e_k}\\
                      &= \sum_{k=1}^{n}\mu_k\\
                      &= \norm{T(t)}_{\tr},
  \end{align*}
  so $T$ is an isometry when $\mathcal{H}\otimes \overline{\mathcal{H}}$ is equipped with the projective norm and $\F\left( \mathcal{H} \right)$ with the trace-class norm. By continuity, we have the isometric isomorphism $T\colon \mathcal{H}\hat\otimes \overline{\mathcal{H}}\rightarrow L_1\left( \B\left( \mathcal{H} \right) \right)$.
\end{proof}
\subsection{Interlude: Tensor Products of Operators}%
Recall that if $\mathcal{H}$ and $\mathcal{K}$ are Hilbert spaces, the Hilbert space tensor product $\mathcal{H}\otimes \mathcal{K}$ is endowed with the norm induced by the inner product
\begin{align*}
  \iprod{x\otimes y}{x'\otimes y'} &= \iprod{x}{x'} \iprod{y}{y'}.
\end{align*}
We usually call this the $2$-norm.
\begin{proposition}
  Let $\mathcal{H}$ and $\mathcal{K}$ be Hilbert spaces, and suppose $T\in \B\left( \mathcal{H} \right)$ and $S\in \B\left( \mathcal{K} \right)$. The linear map $T\otimes S\colon \mathcal{H}\otimes \mathcal{K}\rightarrow \mathcal{H}\otimes \mathcal{K}$, given by $\left( T\otimes S \right)\left( x\otimes y \right)= Tx\otimes Sy$, is bounded with respect to the $2$-norm. The continuous extension, $T\otimes S\in \B\left( \mathcal{H}\otimes \mathcal{K} \right)$, has the cross norm $\norm{T\otimes S}_{\op} = \norm{T}_{\op}\norm{S}_{\op}$.
\end{proposition}
\begin{proof}
  Consider the linear map $T\otimes I\colon \mathcal{H}\otimes \mathcal{K}\rightarrow \mathcal{H}\otimes \mathcal{K}$, where $T\otimes I\left( h\otimes k \right) = Th\otimes k$ for $h\in \mathcal{H}$ and $k\in \mathcal{K}$.\newline

  We will show that $T\otimes I$ is bounded. Let $t = \sum_{j=1}^{n}h_j\otimes k_j\in \mathcal{H}\otimes \mathcal{K}$, and regard $\set{k_1,\dots,k_n}$ as orthonormal. Then, the famiies $\set{Th_j\otimes k_j}_{j=1}^{n}$ and $\set{h_j\otimes k_j}_{j=1}^{n}$ are orthogonal. Using the Pythagorean theorem and the cross norm on $\mathcal{H}\otimes \mathcal{K}$, we get
  \begin{align*}
    \norm{\left( T\otimes I \right)\left( t \right)}^2 &= \norm{\sum_{j=1}^{n}Th_j\otimes k_j}^2\\
                                                       &= \sum_{j=1}^{n}\norm{Th_j\otimes k_j}^2\\
                                                       &= \sum_{j=1}^{n} \norm{Th_j}^2\norm{k_j}^2\\
                                                       &\leq \sum_{j=1}^{n}\norm{T}_{\op}^2\norm{h_j}^2\norm{k_j}^2\\
                                                       &= \norm{T}_{\op}^2\sum_{j=1}^{n}\norm{h_j}^2\norm{k_j}^2\\
                                                       &= \norm{T}_{\op}^2 \sum_{j=1}^{n}\norm{h_j\otimes k_j}^2\\
                                                       &= \norm{T}_{\op}^2 \norm{\sum_{j=1}^{n}h_j\otimes k_j}^2\\
                                                       &= \norm{T}_{\op}^2\norm{t}^2.
  \end{align*}
  Thus, $T\otimes I$ is bounded, with $\norm{T\otimes I}_{\op}\leq \norm{T}_{\op}$, and similarly with $I\otimes S$. Note that $T\otimes S = \left( T\otimes I \right)\circ \left( I\otimes S \right)$, so $T\otimes S$ is bounded with
  \begin{align*}
    \norm{T\otimes S}_{\op} &\leq \norm{T\otimes I}_{\op}\norm{I\otimes S}_{\op}\\
                            &\leq \norm{T}_{\op}\norm{S}_{\op}.
  \end{align*}
  Moreover,
  \begin{align*}
    \norm{T\otimes S}_{\op} &\geq \sup_{\substack{x\in B_{\mathcal{H}}\\y\in B_{\mathcal{K}}}}\norm{\left( T\otimes S \right)\left( x\otimes y \right)}\\
                            &= \sup_{\substack{x\in B_{\mathcal{H}}\\y\in B_{\mathcal{K}}}}\norm{Tx\otimes Sy}\\
                            &= \sup_{x\in B_{\mathcal{H}}}\norm{Tx}\sup_{y\in B_{\mathcal{K}}}\norm{Sy}\\
                            &= \norm{T}_{\op}\norm{S}_{\op},
  \end{align*}
  so $\norm{T\otimes S}_{\op} = \norm{T}_{\op}\norm{S}_{\op}$. Thus, $T\otimes S$ extends continuously to a bounded operator $T\otimes S\in \B\left( \mathcal{H}\otimes \mathcal{K} \right)$.
\end{proof}
The traditional rules of tensor calculus extend to operators.
\begin{lemma}
  Let $\mathcal{H}$ and $\mathcal{K}$ be Hilbert spaces. Suppose $T,T_1,T_2\in\B\left( \mathcal{H} \right)$, and $S,S_1,S_2\in \B\left( \mathcal{K} \right)$, with $\alpha \in \C$. Then,
  \begin{enumerate}[(1)]
    \item $\left( T_1 + \alpha T_2 \right)\otimes S = T_1\otimes S + \alpha \left( T_2\otimes S \right)$;
    \item $T\otimes \left( S_1 + \alpha S_2 \right) = T\otimes S_1 + \alpha\left( T\otimes S_2 \right)$;
    \item $\left( T_1\otimes S_1 \right)\left( T_2\otimes S_2 \right) = T_1T_2\otimes S_1S_2$;
    \item $\left( T\otimes S \right)^{\ast} = T^{\ast}\otimes S^{\ast}$.
  \end{enumerate}
\end{lemma}

\subsection{Duality and the Weak* Topology}%

\section{Nuclear Maps}%
Now, we will use results related to completely positive maps to understand nuclearity in $C^{\ast}$-algebras.
\begin{definition}
  Let $\theta\colon A\rightarrow B$ be a map between $C^{\ast}$-algebras. We say $\theta$ is nuclear if there exist completely positive contractions $\varphi_n\colon A\rightarrow \Mat_{k(n)}\left( \C \right)$ and $\psi_n\colon \Mat_{k(n)}\left( \C \right)\rightarrow B$ such that
  \begin{align*}
    \norm{\psi_n\circ \varphi_n\left( a \right) - \theta(a)}\rightarrow 0
  \end{align*}
  for all $a\in A$.
\end{definition}
There is a similar definition for von Neumann algebras.
\begin{definition}
  If $A$ is a $C^{\ast}$-algebra, and $N$ is a von Neumann algebra, a map $\theta\colon A\rightarrow N$ is called weakly nuclear if there exist completely positive contractions $\varphi_n\colon A\rightarrow \Mat_{k(n)}\left( \C \right)$ and $\psi_n\colon \Mat_{k(n)} \left( \C \right)\rightarrow N$ such that, for all $\eta\in N_{\ast}$, where $N_{\ast}$ is the predual of $N$, we have
  \begin{align*}
    \eta\left( \psi_n\circ \varphi_{n}\left( a \right) \right) \rightarrow \eta\left( \theta\left( a \right) \right).
  \end{align*}
  We call convergence in the weak* topology induced by the predual of $N$ \textit{point-ultraweak} convergence.
\end{definition}
Recall that when we are checking convergence of bounded nets, we have $\psi_n\circ\varphi_n\xrightarrow{\text{BW}} \theta$, if and only if
\begin{align*}
  \iprod{\psi_n\circ\varphi_n(a)v}{w} \rightarrow \iprod{\theta(a)v}{w}
\end{align*}
for all $a\in A$ and $v,w\in \Omega\subseteq \mathcal{H}$.\newline

What makes nuclearity interesting is that it is dependent on the range. For instance, there are von Neumann algebras for which the identity map is not nuclear, while the natural inclusion is always nuclear.
\begin{proposition}
  Let $M\subseteq \B\left( \mathcal{H} \right)$ be a von Neumann algebra. The natural inclusion map, $M\hookrightarrow \B\left( \mathcal{H} \right)$ is always weakly nuclear.
\end{proposition}
\begin{proof}
  Let $\set{P_i}_{i\in I}$ be a net of finite rank projections increasing to the identity. That is, if $i\leq j$, then $P_i\leq P_j$, and also $\norm{P_iv - v}\rightarrow 0$ for all $v\in \mathcal{H}$.\newline

  Now, if $P_i$ has rank $k(i)$, we define the completely positive contractions $\varphi_i\colon M\rightarrow \Mat_{k(i)}\left( \C \right)$ via the identification $\Mat_{k(i)}\left( \C \right) \cong P_i\B\left( \mathcal{H} \right)P_i$ with the compression $\varphi_i(T) = P_iTP_i$.\newline

  Let $\psi_i\colon \Mat_{k(i)}\left( \C \right) = \B\left( \mathcal{H} \right)$ be the natural inclusion map --- i.e., $\varphi_i(T) = P_iTP_i$ as the matrix representation $\left( T_{ij} \right)_{ij}$, and $\psi_i\left( \left( T_{ij} \right)_{ij} \right) = P_iTP_i$, where $T$ is the operator for the matrix $\left( T_{ij} \right)_{ij}$.\newline

  Now, since the predual of $\B\left( \mathcal{H} \right)$ is the trace class operators, and the $\left( P_i \right)_{i}$ converge in the to the identity, the net $\left( \psi_i\circ \varphi_i \right)_i$ converges pointwise (i.e., ultraweakly) to the identity.
\end{proof}
\begin{exercise}[Exercise 2.1.1]
  Show that $\theta\colon A\rightarrow B$ is nuclear if and only if for each finite subset $F\subseteq A$ and $\ve > 0$, there exist $n\in \N$ and completely positive contractions $\varphi\colon A\rightarrow \Mat_n\left( \C \right)$, $\psi\colon \Mat_n\left( \C \right)\rightarrow B$ such that $\norm{\theta(a) - \psi\circ\varphi\left( a \right)} < \ve$ for all $a\in F$.
\end{exercise}
\begin{solution}
  If $\theta\colon A\rightarrow B$ is nuclear, then we have nets $\varphi_i\colon A\rightarrow \Mat_{n(i)}\left( \C \right)$ and $\psi_i\colon \Mat_{n(i)}\left( \C \right)\rightarrow B$ such that $\norm{\theta(a) - \psi_i\circ \varphi_i(a)} \rightarrow 0$ as a net. Therefore, for any $\ve > 0$, there is some $i$ such that for all $i > I$ we may find some finite subset $F$ such that $a\in F$ and $\norm{\theta(a) - \psi_i\circ\varphi_i(a)} < \ve$. The value $n(i)$ is thus our desired value of $n$.\newline

  Let $\mathcal{F}$ be the set
  \begin{align*}
    \mathcal{F} &= \set{\left( F,\ve \right) | F\subseteq A\text{ is finite, }\ve > 0},
  \end{align*}
  directed by
  \begin{align*}
    \left( F,\ve \right) \preceq \left( F',\ve' \right) \text{ if and only if } F\subseteq F'\text{ and }\ve' < \ve.
  \end{align*}
  Suppose $\theta$ satisfies the condition of the theorem. We define the net of completely positive contractions with respect to $\mathcal{F}$ to be $\varphi_{(F,\ve)}\colon A\rightarrow \Mat_{n(F,\ve)}\left( \C \right)$ and $\psi_{(F,\ve)}\colon \Mat_{n(F,\ve)}\left( \C \right)\rightarrow B$. Then, we have
  \begin{align*}
    \norm{\theta(a) - \psi_{n(F,\ve)}\circ \varphi_{n(F,\ve)}(a)} \rightarrow 0,
  \end{align*}
  so that $\theta$ is nuclear.
\end{solution}
\begin{exercise}[Exercise 2.1.2]
  Prove that $\theta\colon A\rightarrow B$ is nuclear if and only if there exist completely positive contractions $\varphi_n\colon A\rightarrow C_n$ and $\psi_n\colon C_n\rightarrow B$, where the $C_n$ are finite-dimensional $C^{\ast}$-algebras.
\end{exercise}
\begin{exercise}[Exercise 2.1.3]
  If $\theta\colon A\rightarrow B$ is nuclear and $C\subseteq A$ is a $C^{\ast}$-algebra, then $\theta|_{C}\colon C\rightarrow B$ is also nuclear.
\end{exercise}
\begin{exercise}[Exercise 2.1.4]
  If $\theta\colon A\rightarrow B$ and $\sigma\colon B\rightarrow C$ are completely positive contractions such that either $\theta$ or $\sigma$ is nuclear, then so is $\sigma\circ\theta$.
\end{exercise}
\begin{exercise}[Exercise 2.1.5]
  If $A$ is a $C^{\ast}$-algebra such that the identity map on $A$ is nuclear, than any other completely positive contraction $\theta\colon A\rightarrow B$ is also nuclear.
\end{exercise}
\begin{exercise}[Exercise 2.1.6]
  Assume $A\subseteq \B\left( \mathcal{H} \right)$ is a concretely represented $C^{\ast}$-algebra such that the inclusion map $A\hookrightarrow \B\left( \mathcal{H} \right)$ is nuclear. Show that if $\theta\colon A\rightarrow \B\left( \mathcal{K} \right)$ is any completely positive contraction, then $\theta$ is nuclear.
\end{exercise}
\begin{exercise}[Exercise 2.1.7]
  If $\theta\colon A\rightarrow B$ is nuclear, and $C\subseteq B$ is a $C^{\ast}$-subalgebra with the properties that
  \begin{enumerate}[(a)]
    \item $\theta(A)\subseteq C$; and
    \item there exists a conditional expectation $\Phi\colon B\rightarrow C$,
  \end{enumerate}
  then $\theta\colon A\rightarrow C$ is nuclear.
\end{exercise}
\begin{exercise}[Exercise 2.1.8]
  If $\theta\colon A\rightarrow B$ and $C\subseteq B$ is a $C^{\ast}$-subalgebra with properties that 
  \begin{enumerate}[(a)]
    \item $\theta(A)\subseteq C$; and
    \item there exist a sequence of completely positive contractions $\Phi_n\colon B\rightarrow C$ such that $\Phi_n|_{C}\rightarrow \id_{C}$ in the point-norm topology,
  \end{enumerate}
  then $\theta\colon A\rightarrow C$ is nuclear.
\end{exercise}
\begin{definition}
  If $A$ is a unital $C^{\ast}$-algebra, an extension is called locally split if, for each finite-dimensional operator system $E\subseteq A/J$, there exists a unital completely positive map $\sigma\colon E\rightarrow A$ such that $\pi\circ \sigma = \id_{E}$.
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRGJAF9T1Nd9CKAIzkqtRizYApLjxAZseAkQBMo6vWatEIAIKzeigUQDM68Vra6A9DO6H+ylABZzmyTo6cxMKAHN4IlAAMwAnCABbJDIQHAgkIXsQMMiE6jikFSSUqMQ1WPjEMwsPEAAdMrQsA2Tw3OKMxGdvTiA
\begin{tikzcd}
0 \arrow[r] & J \arrow[r] & A \arrow[r, "\pi"] & A/J \arrow[r] & 0
\end{tikzcd}
  \end{center}
\end{definition}
\begin{exercise}[Exercise 2.1.9]
  Let $\theta\colon A\rightarrow B$ be a unital nuclear map, such that $\theta|_{J} = 0$ for some ideal $J\subseteq A$. Show that $\theta$ descends to a unital completely positive map $\dot{\theta}\colon A/J\rightarrow B$.\newline

  Next, show that if the extension is locally split, then $\dot{\theta}$ is nuclear.
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRGJAF9T1Nd9CKAIzkqtRizYApLjxAZseAkQBMo6vWatEIAIKzeigUQDM68Vra6A9DO6H+ylABZzmyTo6cxMKAHN4IlAAMwAnCABbJDIQHAgkIXsQMMiE6jikFSSUqMQ1WPjEMwsPEAAdMrQsA2Tw3OKMxGdvTiA
\begin{tikzcd}
0 \arrow[r] & J \arrow[r] & A \arrow[r, "\pi"] & A/J \arrow[r] & 0
\end{tikzcd}
  \end{center}
\end{exercise}

\section{Amenability with \texorpdfstring{$C^{\ast}$-Algebras}{C*-Algebras}}%

\end{document}
