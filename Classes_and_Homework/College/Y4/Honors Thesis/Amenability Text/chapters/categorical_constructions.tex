In this book, we cover certain structures --- like the free group, free $\ast$-algebra, tensor product, etc. --- that are usually not covered in the undergraduate algebra or analysis curriculum in depth. We discuss these ``free'' constructions\footnote{Hence the name of this chapter.} here, with the general theme that these constructions allow us to, in a ``universal'' manner, convert one type of map (a set-map or a bilinear map) into another type of map (a group homomorphism or a linear map).
\section{Free Groups}%
Given a set $A$, we want to know how exactly we can create a group structure from the elements in $A$ such that they extend from $A$ to a group generated by $A$ in a particularly ``natural'' way. This will be the free group, whose properties we will discuss in Chapter \ref{ch:paradoxical_decompositions}.
\begin{definition}\label{def:generating_sets}
  Let $G$ be a group, and $S\subseteq G$ be a subset. We define the subgroup \textit{generated by} $S$ to be
  \begin{align*}
    \left\langle S \right\rangle_{G} &= \bigcap \set{H | S\subseteq H,~H\text{ a subgroup}}.
  \end{align*}
  We say $S$ generates $G$ if $\left\langle S \right\rangle_{G} = G$.\newline

  We say $\left\langle S \right\rangle_{G}$ is \textit{finitely generated} if $\Card(S) < \infty$.\newline

  If $S$ is such that, for any $x\in S$, we have $x^{-1}\in S$, then we say $S$ is \textit{symmetric}.
\end{definition}
\begin{fact}
  If $S = \set{s_1,\dots,s_n}\subseteq G$, then the picture of $\left\langle S \right\rangle$ is as follows:
  \begin{align*}
    \left\langle S \right\rangle_{G} &= \set{s_1^{a_1}s_2^{a_2}\cdots s_n^{a_n} | n\in\N,~s_1,\dots,s_n\in S,~a_1,\dots,a_n\in \set{-1,1}}.
  \end{align*}
\end{fact}
To construct a free group, we begin by stating its universal property --- that is, its innate nature as an ``extension'' of a set-function into a group structure. Then, we will show that a more constructive definition of the free group satisfies this universal property. The following section draws heavily from \cite{loh_geometric_group_theory}, but we will mostly focus on the construction of the free group rather than the proof of uniqueness.
\begin{definition}\label{def:free_group}
  Let $S$ be a set. A group $F$ containing $S$ is said to be \textit{freely generated} if, for every group $G$, and every map $\phi\colon S\rightarrow G$, there is a unique group homomorphism $\varphi\colon F\rightarrow G$ that extends $\varphi$. The following diagram, where $\iota$ denotes the inclusion of $S$ into $F$, commutes:
    \begin{center}
      \begin{tikzcd}
        S \arrow[d, "\iota"', hook] \arrow[r, "\phi"] & G \\
        F \arrow[ru, "\varphi"']                      &  
      \end{tikzcd}
    \end{center}
  We say $F$ is the \textit{free group} generated by $S$.
\end{definition}
Intuitively, to construct the free group, if we have $a\mapsto \phi(a)$ between $S$ and $G$, then we will define $\varphi\left(a^n\right) = \phi(a)^n$ inside $F(S)$. Uniqueness will follow from the fact that we can take two groups that satisfy the universal property, $F$ and $F'$, and apply the universal property on set-valued functions between $S$ and $F$ and $S$ and $F'$ respectively. 
\begin{theorem}\label{thm:free_group_existence}
  If $S$ is some set, then there is some freely generated group $F(S)$ that satisfies \ref{def:free_group}.
\end{theorem}
\begin{proof}
  We will construct a group consisting of ``words'' made up of elements of $S$ and their inverses. This starts by considering the alphabet $A = S\cup \widehat{S}$, where $\widehat{S}$ is a disjoint copy of $S$ --- every $\hat{s}\in \widehat{S}$ will play the role of an inverse to $s$ in our group.
  \begin{itemize}
    \item Define $A^{\ast}$ to be the set of all words over the alphabet $A$, including the empty word, $\epsilon$. We define the operation $A^{\ast}\times A^{\ast}\rightarrow A^{\ast}$ by concatenating words, which is an associative operation with neutral element $\epsilon$.
    \item Define the equivalence relation $\sim$ generated by the following two relations, where for all $x,y\in A^{\ast}$ and $s\in S$, we have
      \begin{align*}
        xs\hat{s}y &\sim xy\\
        x\hat{s}sy &\sim xy.
      \end{align*}
      The equivalence classes with respect to $\sim$ will be denoted $\left[\cdot\right]$.\newline

      We have a well-defined composition $\left[x\right]\left[y\right] = \left[xy\right]$ mapping $F(S) \times F(S) \rightarrow F(S)$ for all $x,y\in A^{\ast}$.
  \end{itemize}
  We show that $F(S)$ with the concatenation operation is a group. Here, we see that $\left[\epsilon\right]$ is the neutral element for the composition, and associativity is inherited from associativity of concatenation in $A^{\ast}$. To show the existence of inverses, we define the inverse map inductively by taking $I\left(\epsilon\right) = \epsilon$, and
  \begin{align*}
    I\left(sx\right) &= I(x)\hat{s}\\
    I\left(\hat{s}x\right) &= I(x)s
  \end{align*}
  for all $x\in A^{\ast}$  and $s\in S$. Inductively, we can see that $I\left(I\left(x\right)\right) = x$ and
  \begin{align*}
    \left[I(x)\right]\left[x\right] &= \left[I(x)x\right]\\
                                    &= \left[\epsilon\right]\\
    \left[x\right]\left[I(x)\right] &= \left[xI(x)\right]\\
                                    &= \left[\epsilon\right].
  \end{align*}
  Thus, $F(S)$ is a group.\newline

  Now, we show $F(S)$ is freely generated. Let $i\colon S\rightarrow F(S)$ be the map that sends $s\mapsto \left[s\right]$. By our construction, we know that $i(S)\subseteq F(S)$ is a generating set for $F(S)$. We will show the universal property holds for $F(S)$.\newline

  To start, let $\phi\colon S\rightarrow G$ be a set-valued map between $S$ and an arbitrary group $G$. We construct $\phi^{\ast}\colon A^{\ast}\rightarrow G$ by taking
  \begin{align*}
    \epsilon &\mapsto e\\
    sx &\mapsto \phi(s)\phi^{\ast}\left(x\right)\\
    \hat{s}x &\mapsto \left(\phi(s)\right)^{-1}\phi^{\ast}\left(x\right)
  \end{align*}
  for all $x\in A^{\ast}$ and $s\in S$. This definition of $\phi^{\ast}$ is compatible with the equivalence relation on $A^{\ast}$, and we see that $\phi^{\ast}\left(xy\right) = \phi^{\ast}\left(x\right)\phi^{\ast}\left(y\right)$. Thus, we get a well-defined map $\varphi\colon F(S)\rightarrow G$, taking $\left[x\right]\mapsto \left[\phi^{\ast}\left(x\right)\right]$.\newline

  It remains to be shown that the map $i\colon S\rightarrow F(S)$ is injective, which will show that $F(S)$ is freely generated by $S$. Let $s_1,s_2\in S$, and consider the set-function $\phi\colon S\rightarrow \Z$ given by $\phi\left(s_1\right) = 1$ and $\phi\left(s_2\right) = -1$. Then, we must have
  \begin{align*}
    \varphi\left(i\left(s_1\right)\right) &= \phi\left(s_1\right)\\
                                          &= 1\\
                                          &\neq -1\\
                                          &= \phi\left(s_2\right)\\
                                          &= \varphi\left(i\left(s_2\right)\right).
  \end{align*}
  Thus, we have $i\left(s_1\right)\neq i\left(s_2\right)$, so $i$ is injective.
\end{proof}
Most of the definitions of the free group automatically default to the characterization of $F(S)$ as the set of reduced words in $S\cup S^{-1}$. This is the characterization we will be using in the future, but it is still important to understand where exactly the ``free'' in free group comes from, and how it relates to the particular universal property that actually characterizes $F(S)$ uniquely up to isomorphism.
\section{Free Vector Spaces}%
Given a set $A$, just as we are able to construct a free group, $F(A)$, we can take any set $A$ and construct a ``universal'' vector space out of the set.\newline

The free vector space (as it is known) is the universal object that extends any set-valued function into a linear map, treating elements of the set as its basis (see Definition \ref{def:basis}). We are interested in the case of the free vector space over the complex numbers, but note that the following definition of the free vector space applies over any field. 
\begin{theorem}\label{thm:free_vector_space}
  Let $\Gamma$ be a nonempty set. There is a vector space, $\C\left[\Gamma\right]$, with $\Dim\left(\C\left[\Gamma\right]\right) = \Card\left(\Gamma\right)$, and an injective map $\delta\colon \Gamma\rightarrow \C\left[\Gamma\right]$ such that the following universal property holds: if $V$ is a $\C$-vector space, and $\phi\colon \Gamma\rightarrow V$ is a set-valued function, then there is a unique linear map $T_{\phi}\colon \C\left[\Gamma\right]\rightarrow V$ such that $T_{\phi}\circ \delta = \phi$.
  \begin{center}
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB12BxOgW17ogAvqXSZc+QigCM5KrUYs2nAMLJOPfnQrDRIDNjwEis6fPrNWiEADVh8mFADm8IqABmAJwi8kZEDgQSLIKlsrssAw4giIe3r6IIYFIAEzUDHQARjAMAAriRlIgDDDuOCDUFkrWACoA+sCcaAAWWEK6cT5+1MmIacVZOfmGkmyeWE7N5ZWKVhzsLVj2QkA
\begin{tikzcd}
\Gamma \arrow[r, "\delta"] \arrow[rd, "\phi"'] & {\C[\Gamma]} \arrow[d, "T_{\phi}"] \\
                                               & V                                 
\end{tikzcd}
  \end{center}
\end{theorem}
\begin{proof}
  Consider the linear subspace of finitely supported functions, $\C\left[\Gamma\right]\subseteq \mathcal{F}\left(\Gamma,\C\right)$. For each $t\in \Gamma$, we define
  \begin{align*}
    \delta_t\left(s\right) &= \begin{cases}
      1 & s=t\\
      0 & \text{else}
    \end{cases}.
  \end{align*}
    We see that $\delta_t\neq \delta_s$ whenever $s\neq t$, meaning that the map $\delta\colon \Gamma\rightarrow \C\left[\Gamma\right]$, defined by $s \mapsto \delta_s$, is injective.\newline

    We will show that $\set{\delta_s}_{s\in \Gamma}$ is a linear basis for $\C\left[\Gamma\right]$. If $f\in \C\left[\Gamma\right]$, with $\supp\left(f\right) = \set{s_1,\dots,s_n}\subseteq \Gamma$, we set $\alpha_j = f\left(t_j\right)$, and see that
    \begin{align*}
      f &= \sum_{j=1}^{n}\alpha_j\delta_{s_j},
    \end{align*}
    which shows that $\set{\delta_s}_{s\in\Gamma}$ is a spanning set.\newline

    To show that $\set{\delta_s}_{s\in\Gamma}$ is linearly independent, consider $g = \sum_{j=1}^{n}\alpha_j\delta_{s_j}\in \C\left[\Gamma\right]$ such that $g = 0$. Then, $g(t) = 0$ for all $t\in\Gamma$, and in particular, $g\left(s_i\right) = 0$ for every $1 \leq i \leq n$. Thus, we have
    \begin{align*}
      0 &= g\left(s_j\right)\\
        &= \sum_{j=1}^{n}\alpha_j\delta_{s_j}\left(s_i\right)\\
        &= \alpha_i,
    \end{align*}
    so $\alpha_j = 0$ for each $j$. Thus, $\set{\delta_s}_{s\in \Gamma}$ is linearly independent.\newline

    Turning to the universal property, we define $T_{\phi}\colon \C\left[\Gamma\right]\rightarrow V$ in terms of $\phi$ as follows:
    \begin{align*}
      T_{\phi}\left(\sum_{j=1}^{n}\alpha_j\delta_{s_j}\right) &= \sum_{j=1}^{n}\alpha_j\phi\left(s_j\right).
    \end{align*}
    This yields an expression of $T_{\phi}$ uniquely in terms of $\phi$ and $\delta$.
\end{proof}
\begin{example}
  Let $z$ be an abstract variable, and consider the set of ``formal powers'' of $z$, $\set{z^k}_{k\in\N}$. Then, the free vector space generated by this set, $\C\left[z\right]$, is the set of all polynomials with coefficients in $\C$. By the universal property, we know that every polynomial $p\in \C\left[z\right]$ has a unique expression $p = \sum_{j=0}^{n}a_jz^j$.
\end{example}
One of the primary uses of the free vector space is that, via this construction, we can show that vector spaces are particularly nice algebraic objects. We often use these properties implicitly in linear algebra.
\begin{theorem}\label{thm:injective_projective_objects}
  Let $X$, $Y$, and $Z$ be vector spaces.
  \begin{enumerate}[(a)]
    \item If $\iota\colon Y\hookrightarrow X$ is an injective linear map, and $\varphi\colon Y\rightarrow Z$ is a linear map, then there is a (not necessarily unique) map $T\colon X\rightarrow Y$ such that $T\circ\iota = \varphi$.
      \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRGJAF9T1Nd9CKAIzkqtRizYBNLjxAZseAkRFCx9Zq0QgAWrN6KBRAEyjqGydoAaXMTCgBzeEVAAzAE4QAtkhEgcEEgAzOYSWiAAOhH4OHQg1LFYDGwAFhAQANb6IB7evgmBiKYgDHQARjAMAAp8SoIg7lgOKTjx4ppsUfTuaClY2bk+iCH+hcWlFdW1RtoMMK6toR3aACoDnkNko76cFJxAA
\begin{tikzcd}
0 \arrow[r] & Y \arrow[r, "\iota", hook] \arrow[d, "\varphi"'] & X \arrow[ld, "T"] \\
            & Z                                                &                  
\end{tikzcd}
      \end{center}
This shows that vector spaces are injective objects --- any linear map factors through an injective map.
    \item If $\pi\colon X\rightarrow Z$ is a surjective linear map, and $\varphi\colon Y\rightarrow Z$ is a linear map, then there is a (not necessarily unique) map $\delta\colon Y\rightarrow X$ such that $\pi\circ\delta = \varphi$.
      \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZARgBoAGAXVJADcBDAGwFcYkQBNEAX1PU1z5CKMsWp0mrdgC0efEBmx4CRcqTE0GLNohAANOfyVCiAJnXitU3eR7iYUAObwioAGYAnCAFskakDgQSOYgjPQARjCMAAoCysIgHliOABY4IJqSOiAAOjmwjDj0hiCePsE0gUhkoRFRscYqukmp6Zna7HloWCVlvoj+VYg1Vtl5DB5oKT00YZExcSa6jDBu6bzuXv01QwDM3JTcQA
        \begin{tikzcd}
                            & Y \arrow[ld, "\delta"'] \arrow[d, "\varphi"] &   \\
        X \arrow[r, "\pi"'] & Z \arrow[r]                                  & 0
        \end{tikzcd}
      \end{center}
      This shows that vector spaces are projective objects --- any linear map factors through a surjective map.
  \end{enumerate}
\end{theorem}
\begin{proof}\hfill
  \begin{enumerate}[(a)]
    \item Let $\mathcal{A}$ be a basis for $Y$. Then, since $\iota$ is an injective linear map, the set $\mathcal{B}_0 = \set{\iota(y) | y\in \mathcal{A}}$ can be extended to a basis $\mathcal{B}$ for $X$.\newline

      We set $t\colon \mathcal{B}\rightarrow Z$ to be
      \begin{align*}
        t\left(x\right) &= \begin{cases}
          \varphi(x) & x\in \mathcal{B}_0\\
          0 & x\in \mathcal{B}\setminus \mathcal{B}_0
        \end{cases}.
      \end{align*}
      By the universal property of the free vector space, this extends to a linear map $T\colon X\rightarrow Y$. Since $T\circ\iota$ agrees with $\varphi$ on $\mathcal{A}$, the universal property of the free vector space states that $T\circ\iota$ agrees with $\varphi$ on all of $Y$.
    \item Let $\set{y_i}_{i\in I}$ be a basis for $Y$. We define $d\left( y_i \right) = x_i\in  \pi^{-1}\circ \varphi\left( y_i \right)$ for each $i\in I$, where $x_i\in \pi^{-1}\circ \varphi\left( y_i \right)$ is some representative.  By the universal property of the free vector space, this extends to a unique linear map $\delta\colon Y\rightarrow X$ that agrees on the basis of $Y$.
  \end{enumerate}
\end{proof}

\section{Free Algebras}%
Chapter 8 of this thesis will be focused on understanding the properties of the (reduced) group $C^{\ast}$-algebra. This will require some background in the theory of algebras, so we will understand the purely algebraic properties here before diving into the analytic properties in Chapter 5.
%\begin{definition}\label{def:star_algebra}
%  Let $A$ be an algebra over $\C$ (see Definition \ref{def:vector_space_and_algebra}). An involution on $A$ is a unary operation $\ast\colon A\rightarrow A$ that satisfies the following, for all $a,b\in A$ and $\alpha\in\C$:
%  \begin{itemize}
%    \item $\left(a^{\ast}\right)^{\ast} = a$;
%    \item $\left(a+b\right)^{\ast} = a^{\ast} + b^{\ast}$;
%    \item $\left(\alpha a\right)^{\ast} = \overline{\alpha} a^{\ast}$;
%    \item $\left(ab\right)^{\ast} = b^{\ast}a^{\ast}$.
%  \end{itemize}
%  If $A$ is equipped with an involution, we say $A$ is a $\ast$-algebra.
%\end{definition}
%\begin{example}
%  Consider the algebra of $n\times n$ matrices over $\C$, $\Mat_n\left(\C\right)$, with element-wise addition and scalar multiplication, as well as traditional matrix multiplication. From linear algebra, we know that if $T\in \Mat_n\left(\C\right)$, then $T$ admits a unique adjoint map (or conjugate transpose), $T^{\ast}$, such that for any $x,y\in \C^n$, $ \iprod{T\left(x\right)}{y} =  \iprod{x}{T^{\ast}\left(y\right)}$, where $ \iprod{\cdot}{\cdot} $ denotes the complex inner product. The map $\ast\colon \Mat_n\left(\C\right)\rightarrow \Mat_n\left(\C\right)$, given by $T\mapsto T^{\ast}$, satisfies the definition of an involution.\newline
%
%  If $x,y\in \C^n$, $\alpha\in\C$, and $T,S\in \Mat_n\left(\C\right)$,
%  \begin{align*}
%    \iprod{T\left(x\right)}{y} &= \iprod{x}{T^{\ast}\left(y\right)}\\
%                               &= \overline{ \iprod{T^{\ast}\left(y\right)}{x} }\\
%                               &= \overline{ \iprod{y}{T^{\ast\ast}\left(x\right)} }\\
%                               &= \iprod{T^{\ast\ast}\left(x\right)}{y}\\
%                               \\
%    \iprod{\left(T+S\right)\left(x\right)}{y} &= \iprod{T\left(x\right)}{y} + \iprod{S\left(x\right)}{y}\\
%                                              &= \iprod{x}{T^{\ast}\left(y\right)} + \iprod{x}{S^{\ast}\left(y\right)}\\
%                                              &= \iprod{x}{\left(T^{\ast} + S^{\ast}\right)\left(y\right)}\\
%                                              \\
%    \iprod{\alpha T\left(x\right)}{y} &= \iprod{T\left(\alpha x\right)}{y}\\
%                                      &= \iprod{\alpha x}{T^{\ast}\left(y\right)}\\
%                                      &= \iprod{x}{\overline{\alpha}T^{\ast}\left(y\right)}\\
%                                      \\
%    \iprod{TS\left(x\right)}{y} &= \iprod{S\left(x\right)}{T^{\ast}\left(y\right)}\\
%                                &= \iprod{x}{S^{\ast}T^{\ast}\left(y\right)}.
%  \end{align*}
%  Thus, we can see that $\Mat_n\left(\C\right)$ is a $\ast$-algebra.
%\end{example}
%\begin{definition}
%  Let $A$ be a $\ast$-algebra, and let $B\subseteq A$.
%  \begin{itemize}
%    \item We say $B$ is self-adjoint (or $\ast$-closed) if for any $x\in B$, $x^{\ast}\in B$.
%    \item We say $B$ is a subalgebra of $A$ if $B\subseteq A$ is a linear subspace and for any $b_1,b_2\in B$, $b_1b_2\in B$. If $B$ is $\ast$-closed, then we say $B$ is a $\ast$-subalgebra.
%    \item If $B$ is a subalgebra such that, for any $b\in B$ and $a\in A$, $ab\in B$ and $ba\in B$, then $B$ is an ideal. If $B$ is $\ast$-closed, then we say $B$ is a $\ast$-ideal.
%  \end{itemize}
%  If $J\subseteq A$ is a $\ast$-ideal, the linear space $A/J$ admits multiplication and involution, defined by
%  \begin{align*}
%    \left(a+J\right)\left(b+J\right) &= ab + J\\
%    \left(a+J\right)^{\ast} &= a^{\ast}+J.
%  \end{align*}
%  For any $a\in A$, the $\ast$-ideal generated by $a$ is denoted
%  \begin{align*}
%    \operatorname{ideal}\left(a\right) &= \bigcap\set{J\subseteq A | a\in J\text{ and }J\text{ is a $\ast$-ideal} }.
%  \end{align*}
%  We say an ideal $J\subseteq A$ is maximal if $J$ is a proper ideal in $A$ and, if $J\subseteq I$ for some ideal $I$, then $I = J$ or $I = A$.
%\end{definition}
%\begin{definition}
%  Let $A$ and $B$ be $\ast$-algebras.
%  \begin{itemize}
%    \item An algebra homomorphism between $A$ and $B$ is a linear map $\varphi\colon A\rightarrow B$ that preserves the multiplication structure.
%      \begin{align*}
%        \varphi\left(a + \alpha b\right) &= \varphi\left(a\right) + \alpha \varphi\left(b\right)\\
%        \varphi\left(ab\right) &= \varphi\left(a\right)\varphi\left(b\right)
%      \end{align*}
%      for all $a,b\in A$ and $\alpha\in \C$.
%    \item If the algebra homomorphism $\varphi\colon A\rightarrow B$ preserves the involution structure --- i.e., $\varphi\left(a^{\ast}\right) = \varphi\left(a\right)^{\ast}$ --- then we say $\varphi$ is a $\ast$-homomorphism.
%    \item A ($\ast$)-isomorphism is a bijective ($\ast$)-homomorphism.
%    \item A $\ast$-automorphism is a $\ast$-isomorphism $\varphi\colon A\rightarrow A$. We write $\operatorname{Aut}\left(A\right)$ to denote the set of all $\ast$-automorphisms.
%  \end{itemize}
%\end{definition}
Just as there are free groups and free vector spaces, we can also talk about free algebras. In Chapter 8, we will construct special norms on free algebras to elucidate properties of the underlying group.\newline

Similar to a free group, the free algebra (or free $\ast$-algebra) is constructed by taking a certain collection of ``words'' over a set of symbols, and then, if desired, ``modding out'' by the ideal generated by a set of relations. We formalize this in steps.
\begin{definition}
  Let $E=\set{x_i}_{i\in I}$ be a collection of symbols that may not commute. The space of all polynomials over $E$ is the free vector space over the set of words formed by symbols in $E$,
  \begin{align*}
    \Gamma_E &= \set{x_{i_1}x_{i_2}\cdots x_{i_n} | n\in\N,i_1,\dots,i_n\in I}.
  \end{align*}
  We denote this space $\C \left\langle E \right\rangle$.\newline

  In the free vector space $\C \left\langle E \right\rangle$, we may define multiplication by concatenation:
  \begin{align*}
    \left(x_{i_1}x_{i_2}\cdots x_{i_n}\right)\left(x_{j_1}x_{j_2}\cdots x_{j_m}\right) &= x_{i_1}x_{i_2}\cdots x_{i_n}x_{j_1}x_{j_2}\cdots x_{j_m},
  \end{align*}
  where $i_1,\dots,i_n,j_1,\dots,j_m\in I$. The space $\C\left\langle E \right\rangle$, equipped with multiplication by concatenation, is known as the \textit{free algebra} on $E$.\newline

  To turn $\C\left\langle E \right\rangle$ into a $\ast$-algebra, we define the formal set $E^{\ast} = \set{x_{i}^{\ast}}_{i\in I}$, and define the involution on $\C\left\langle E\cup E^{\ast} \right\rangle$ by taking
  \begin{align*}
    \left(\alpha x_{i_1}^{\ve_1}x_{i_2}^{\ve_2}\cdots x_{i_n}^{\ve_n}\right)^{\ast} &= \overline{\alpha}x_{i_n}^{\delta_n}x_{i_{n-1}}^{\delta_{n-1}}\cdots x_{i_2}^{\delta_2}x_{i_1}^{\delta_1},
  \end{align*}
  where
  \begin{align*}
    \delta_j &= \begin{cases}
      \ast & \ve_j = 1\\
      1 & \ve_j = \ast
    \end{cases}.
  \end{align*}
  The set $\C\left\langle E\cup E^{\ast} \right\rangle$ with the involution defined above is known as the \textit{free $\ast$-algebra} on $E$, and is usually denoted $\mathbb{A}^{\ast}\left(E\right)$.\newline

  If $R\subseteq \mathbb{A}^{\ast}\left(E\right)$ is a collection of relations, we let $I(R) = \operatorname{ideal}\left(R\right)$. Then, the quotient algebra
  \begin{align*}
    \mathbb{A}^{\ast}\left(E|R\right) &= \mathbb{A}^{\ast}\left(E\right)/I(R)
  \end{align*}
  is known as the \textit{universal $\ast$-algebra on $E$ with relations $R$}.
\end{definition}
Evident from the name, the universal $\ast$-algebra(s) admit universal properties that characterize them as unique.
\begin{theorem}[Universal Properties]
  Let $E = \set{x_i}_{i\in I}$ be a set of abstract symbols, and let $B$ be a $\ast$-algebra. Let $\phi\colon E\rightarrow B$ be an injective map, and define $b_i = \phi\left(x_i\right)$.
  \begin{itemize}
    \item There is a unique $\ast$-homomorphism $\varphi\colon \mathbb{A}^{\ast}\left(E\right) \rightarrow B$ such that $x_i \mapsto b_i$. The following diagram commutes.
      \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAFEQBfU9TXfIRQBGclVqMWbAELdeIDNjwEiZYePrNWiEAB1dAWzo4AFgCMzwAIJcAFOwCU3cTCgBzeEVAAzAE4QDJDIQHAgkUQktNn00Eyw5H39AxGDQpAAmagY6MxgGAAV+ZSEQXyw3ExwQak0pHX18HDoEkD8AjOo0xAjs3IKiwTYyiqqayW09XXpfWPiuCi4gA
        \begin{tikzcd}
        E \arrow[r, "\phi"] \arrow[d, "\iota"'] & B \\
        \mathbb{A}^{\ast}(E) \arrow[ru, "\varphi"']    &  
        \end{tikzcd}
      \end{center}
    \item If $R\subseteq \mathbb{A}^{\ast}\left(E\right)$ is a set of relations, and $\set{b_i}_{i\in I}$ satisfies the relations $R$, then there is a unique $\ast$-homomorphism $\mathbb{A}^{\ast}\left(E|R\right) \rightarrow B$ such that $x_i + I(R) \mapsto b_i$. The following diagram commutes.
      \begin{center}
        % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAFEQBfU9TXfIRQBGclVqMWbAELdeIDNjwEiZYePrNWiEAB1dAWzo4AFgCMzwAIJcAesH104OLgAp2AHwBKASm7iYKABzeCJQADMAJwgDJDIQHAgkUQktNn00Eyw5COjYxHjEpAAmagY6MxgGAAV+ZSEQSKwgkxwQak0pHX18HDockCiYkuoixBTyypq6wTYmlraOyW09XXpIzOyuCi4gA
\begin{tikzcd}
E \arrow[r, "\phi"] \arrow[d, "\iota"']       & B \\
\mathbb{A}^{\ast}(E|R) \arrow[ru, "\varphi"'] &  
\end{tikzcd}
      \end{center}
  \end{itemize}
\end{theorem}
One of the most important $\ast$-algebras we will study is generated from a group by taking the free vector space over the group.
\begin{definition}
  Let $\Gamma$ be a group with identity element $e$, and let $\C\left[\Gamma\right]$ be the free vector space generated by $\Gamma$. We define a multiplication $f \ast g$, where $f,g\in \C\left[\Gamma\right]$ are finitely supported functions, by convolution:
  \begin{align*}
    f\ast g(s) &= \sum_{t\in\Gamma}f(t)g\left(t^{-1}s\right)\\
               &= \sum_{r\in\Gamma}f\left(sr^{-1}\right)g\left(r\right).
  \end{align*}
  The involution on $\C\left[\Gamma\right]$ is defined by $f^{\ast}\left(t\right) = \overline{f\left(t^{-1}\right)}$. The multiplicative identity is $\delta_e$, and multiplication satisfies $\delta_s\ast \delta_t = \delta_{st}$.
\end{definition}
\section{Tensor Products}%
Given two vector spaces $V,W$, and a bilinear map $b\colon V\times W \rightarrow Z$ (for some vector space $Z$), it's tempting to use the property of the free vector space to find a linear map on some structure that incorporates both $V$ and $W$ and stays faithful to the bilinear map $b$. Indeed, this is what the tensor product of the vector spaces $V$ and $W$ is --- a universal construction that ``turns'' bilinear maps into linear maps.\newline

In this section, we detail the construction of the tensor product $V\otimes W$, and apply it to the specific case when $V$ and $W$ are Banach spaces (see definition \ref{def:norms}) to obtain certain norms on the tensor product that ``play nicely'' with the norms on $V$ and $W$.
\begin{definition}\label{def:bilinear_map}
  Let $V,W,Z$ be vector spaces, and let $b\colon V\times W\rightarrow Z$ be a map such that, for all $\alpha\in \C$, $v,v_1,v_2\in V$, and $w,w_1,w_2\in W$,
  \begin{align*}
    b\left( \alpha v_1 + v_2,w \right) &= \alpha b\left( v_1,w \right) + b\left( v_2,w \right)\\
    b\left( v,\alpha w_1 + w_2 \right) &= b\left( v,w_1 \right) + \alpha b\left( v,w_2 \right).
  \end{align*}
  Then, we say $b$ is \textit{bilinear}.\newline

  If $V$ and $W$ are normed vector spaces, then we say $b$ is \textit{bounded} bilinear if
  \begin{align*}
    \norm{b}_{\op} &\coloneq \sup_{\substack{v\in B_{V}\\w\in B_{W}}}\norm{b\left( v,w \right)}\\
                   &< \infty.
  \end{align*}
\end{definition}
Just as we defined the free vector space and free group, we define the tensor product through a universal property --- and, just as with the case of the free group, we will focus more on the construction of the tensor product than on showing uniqueness.
\begin{theorem}[Universal Property of Tensor Products]\label{thm:tensor_product_existence}
  Let $V,W,Z$ be vector spaces, and let $b\colon V\times W \rightarrow Z$ be a bilinear map. Then, there exists a vector space, $V\otimes W$ and a linear map $T\colon V\otimes W \rightarrow Z$ such that for any $v\in V$ and $w\in W$, $T\left( v\otimes w \right) = b\left( v,w \right)$. The following diagram, where $\iota\colon V\times W \hookrightarrow V\otimes W$ is defined by $\left( v,w \right)\mapsto v\otimes w$, commutes.
  \begin{center}
      % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRADUAdTvAW3gAEAdRABfUuky58hFAEZyVWoxZsunCH0Ejxk7HgJEFcpfWatEIAFpilMKAHN4RUADMAThF5IyIHBCQAJmoGOgAjGAYABSkDWRB3LAcACxwQajNVSzCxCRAPLx9qfyQFZXM2bnwcOnS-OiwGNkgwVl18z29EMpLEYJBQiOjYmTYGGFc0jJULEAAVW1EgA
    \begin{tikzcd}
    V\times W \arrow[rd, "b"'] \arrow[r, "\iota"] & V\otimes W \arrow[d, "T"] \\
                                                  & Z                        
    \end{tikzcd}
  \end{center}
  The vector space $V\otimes W$ is unique up to linear isomorphism, and is known as the \textit{tensor product} of $V$ and $W$.
\end{theorem}
\begin{proof}
  We focus on showing existence. With $V$ and $W$ as in Theorem \ref{thm:tensor_product_existence}, we consider the free vector space (Theorem \ref{thm:free_vector_space}) on $V\times W$, $\C\left[ V\times W \right]$. Elementary elements of $V\times W$ are of the form $\delta_{(v,w)}$, where
  \begin{align*}
    \delta_{(v,w)} \left( s,t \right) &= \begin{cases}
      1 & v=s,w=t\\
      0 & \text{else}
    \end{cases}.
  \end{align*}
  Intuitively, from the way we have defined the tensor product as a linear map that extends a bilinear map, we would find the following properties of tensors desirable, for any $v,v_1,v_2\in V$, $w,w_1,w_2\in W$, and $\alpha\in \C$
  \begin{align*}
    \left( v_1 + v_2 \right)\otimes w &= v_1\otimes w + v_2\otimes w\label{eq:rel_1}\tag{1}\\
    v\otimes\left( w_1 + w_2 \right) &= v\otimes w_1 + v\otimes w_2\label{eq:rel_2}\tag{2}\\
    \left( \alpha v \right)\otimes w &= \alpha \left( v\otimes w \right)\label{eq:rel_3}\tag{3}\\
    v\otimes \left( \alpha w \right) &= \alpha \left( v\otimes w \right)\label{eq:rel_4}\tag{4}.
  \end{align*}
  With these four desirable properties in mind, we define a certain set of relations on the free vector space that we will ``mod out'' to obtain our desired tensor product. 
  \begin{itemize}
    \item To satisfy \eqref{eq:rel_1}, we define the set $N_1 = \set{\delta_{\left( v_1 + v_2,w \right)} - \delta_{\left( v_1,w \right)} - \delta_{\left( v_2,w \right)} | v_1,v_2\in V,w\in W}$, as this will be equivalent to the statement $\left( v_1 + v_2 \right)\otimes w - v_1\otimes w - v_2\otimes w = 0$.
    \item To satisfy \eqref{eq:rel_2}, we define the set $N_2 = \set{\delta_{\left( v,w_1 + w_2 \right)} - \delta_{\left( v,w_1 \right)} - \delta_{\left( v,w_2 \right)} | v\in V,w_1,w_2\in W}$, as this will be equivalent to the statement $v\otimes \left( w_1 + w_2 \right) - v\otimes w_1 - v\otimes w_2 = 0$.
    \item To satisfy \eqref{eq:rel_3}, we define the set $N_3 = \set{\delta_{\left( \alpha v,w \right)} - \alpha \delta_{\left( v,w \right)} | \alpha\in\C,v\in V,w\in W}$, as this will be equivalent to the statement $\left( \alpha v \right)\otimes w - \alpha \left( v\otimes w \right) = 0$.
    \item To satisfy \eqref{eq:rel_4}, we define the set $N_4 = \set{\delta_{\left(  v,\alpha w \right)} - \alpha \delta_{\left( v,w \right)} | \alpha\in\C,v\in V,w\in W}$, as this will be equivalent to the statement $v\otimes \left( \alpha w \right) - \alpha \left( v\otimes w \right) = 0$.
  \end{itemize}
  We define the ``zero set'' of our tensor product to be
  \begin{align*}
    N &= \Span\left( N_1 \cup N_2\cup N_3\cup N_4 \right),
  \end{align*}
  and consider the quotient space (Definition \ref{def:subspace_quotient_space_direct_sum}) $\C\left[ V\times W \right]/N$. We define
  \begin{align*}
    v\otimes w &\coloneq \delta_{(v,w)} + N.
  \end{align*}
  It can be verified that this definition is faithful to our requirements in \eqref{eq:rel_1}--\eqref{eq:rel_4}. Elements of $V\otimes W$ are of the form $\sum_{i\in I}v_i \otimes w_i$. We call elements of the form $v\otimes w$ \textit{elementary tensors}.\newline

  Define $\iota\colon V\times W \rightarrow V\otimes W$ by $\left( v,w \right) \mapsto v\otimes w$, and set $b = T\circ \iota$.\newline

  We verify that this definition satisfies the universal property of tensor products. We let $v_1,v_2,v\in V$, $w_1,w_2,w\in W$, and $\alpha\in \C$. Then,
  \begin{align*}
    b\left( v_1 + cv_2,w \right) &= T\left( \iota\left( v_1 + cv_2,w \right) \right)\\
                                 &= T\left( \left( v_1 + cv_2 \right)\otimes w \right)\\
                                 &= T\left( v_1\otimes w + c\left( v_2\otimes w \right) \right)\\
                                 &= T\left( v_1\otimes w \right) + cT\left( v_2\otimes w \right)\\
                                 &= b\left( v_1,w \right) + cb\left( v_2,w \right)\\
                                 \\
    b\left( v,w_1 + cw_2 \right) &= T\left( \iota\left( v,w_1 + cw_2 \right) \right)\\
                                 &= T\left( v\otimes \left( w_1 + cw_2 \right) \right)\\
                                 &= T\left( v\otimes w_1 + c\left( v\otimes w_2 \right) \right)\\
                                 &= T\left( v\otimes w_1 \right) + cT\left( v\otimes w_2 \right)\\
                                 &= b\left( v,w_1 \right) + cb\left( v,w_2 \right).
  \end{align*}
  Thus, by the universal property of the free vector space, there is a unique linear map $\tilde{b}\colon \C\left[ V\times W \right]\rightarrow Z$, defined by $\tilde{b}\left( \delta_{(v,w)} \right) = b\left( v,w \right)$. Note that $\tilde{b}$ vanishes on $N$, so by the first isomorphism theorem there is a unique linear map $T_{b}\colon \C\left[ V\times W \right]/N\rightarrow Z$ that is defined by $T_{b} \circ \pi = \tilde{b}$, where $\pi\colon \C\left[ V\times W \right] \rightarrow \C\left[ V\times W \right]/N$ is the canonical projection.\newline

  Thus, we know that $T = T_{b}$ satisfies the universal property of tensor products.
\end{proof}
In linear algebra, we often use the universal property of tensor products to convert from bilinear maps to linear maps. This universal property is actually used to show that the tensor product is compatible with the multiplication structure on algebras. We state the structure on the tensor product of algebras here, but do not provide its full proof.
\begin{proposition}\label{prop:tensor_product_algebras}
  Let $A$ and $B$ be algebras. The vector space $A\otimes B$ admits a multiplication $\left( A\otimes B \right)\times \left( A\otimes B \right)\rightarrow A\otimes B$, given by
  \begin{align*}
    \left( a\otimes b \right)\left( c\otimes d \right) &= ac\otimes bd.
  \end{align*}
  If $A$ and $B$ are $\ast$-algebras, then $A\otimes B$ admits an involution given by
  \begin{align*}
    \left( a\otimes b \right)^{\ast} &= a^{\ast}\otimes b^{\ast}.
  \end{align*}
\end{proposition}
The proof of Proposition \ref{prop:tensor_product_algebras} is carried out by ``amplifying'' to the vector spaces $\mathcal{L}\left( A \right)$ and $\mathcal{L}\left( B \right)$, and establishing the linear map $L\colon A\otimes B \rightarrow \mathcal{L}\left( A \right)\otimes \mathcal{L}\left( B \right)$.\newline

When our vector spaces are equipped with a norm --- specifically, if they are Banach spaces --- not only does it matter that the tensor product preserves the vector space structure, but also that it preserves the norm structure in a particular manner. This is the domain of the injective and projective norms. The injective and projective norms will become more relevant when we discuss $C^{\ast}$-algebras, nuclearity, and amenability in Chapter \ref{ch:nuclearity}. First, we provide some background before elaborating on their definitions.
