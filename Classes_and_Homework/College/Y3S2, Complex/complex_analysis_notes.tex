\documentclass[10pt]{extarticle}
\title{}
\author{}
\date{}
\usepackage[shortlabels]{enumitem}


%paper setup
\usepackage{geometry}
\geometry{letterpaper, portrait, margin=1in}
\usepackage{fancyhdr}
% sans serif font:
\usepackage{cmbright}
%symbols
\usepackage{amsmath}
\usepackage{bigints}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbold}
\usepackage[hidelinks]{hyperref}
\usepackage{gensymb}
\usepackage{multirow,array}
\usepackage{multicol}

\newtheorem*{remark}{Remark}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%chemistry stuff
%\usepackage[version=4]{mhchem}
%\usepackage{chemfig}

%plotting
\usepackage{pgfplots}
\usepackage{tikz}
\tikzset{middleweight/.style={pos = 0.5}}
%\tikzset{weight/.style={pos = 0.5, fill = white}}
%\tikzset{lateweight/.style={pos = 0.75, fill = white}}
%\tikzset{earlyweight/.style={pos = 0.25, fill=white}}

%\usepackage{natbib}

%graphics stuff
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[style=numeric, backend=biber]{biblatex} % Use the numeric style for Vancouver
\addbibresource{the_bibliography.bib}
%code stuff
%when using minted, make sure to add the -shell-escape flag
%you can use lstlisting if you don't want to use minted
%\usepackage{minted}
%\usemintedstyle{pastie}
%\newminted[javacode]{java}{frame=lines,framesep=2mm,linenos=true,fontsize=\footnotesize,tabsize=3,autogobble,}
%\newminted[cppcode]{cpp}{frame=lines,framesep=2mm,linenos=true,fontsize=\footnotesize,tabsize=3,autogobble,}

%\usepackage{listings}
%\usepackage{color}
%\definecolor{dkgreen}{rgb}{0,0.6,0}
%\definecolor{gray}{rgb}{0.5,0.5,0.5}
%\definecolor{mauve}{rgb}{0.58,0,0.82}
%
%\lstset{frame=tb,
%	language=Java,
%	aboveskip=3mm,
%	belowskip=3mm,
%	showstringspaces=false,
%	columns=flexible,
%	basicstyle={\small\ttfamily},
%	numbers=none,
%	numberstyle=\tiny\color{gray},
%	keywordstyle=\color{blue},
%	commentstyle=\color{dkgreen},
%	stringstyle=\color{mauve},
%	breaklines=true,
%	breakatwhitespace=true,
%	tabsize=3
%}
% text + color boxes
\renewcommand{\mathbf}[1]{\mathbb{#1}}
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable}
\tcbuselibrary{skins}
\newtcolorbox{problem}[1]{colback=white,enhanced,title={\small #1},
          attach boxed title to top center=
{yshift=-\tcboxedtitleheight/2},
boxed title style={size=small,colback=black!60!white}, sharp corners, breakable}
%including PDFs
%\usepackage{pdfpages}
\setlength{\parindent}{0pt}
\usepackage{cancel}
\pagestyle{fancy}
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Complex Analysis: Class Notes}
\newcommand{\card}{\text{card}}
\newcommand{\ran}{\text{ran}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\setcounter{secnumdepth}{0}
\begin{document}
\section{Complex Numbers}%
  A complex number is an ordered pair of real numbers, $(a,b)=a+bi$. A vector in $\R^2$ is also an ordered pair, $(a,b)$ of real numbers.\\

  Indeed, vector addition and scalar multiplication on complex numbers are defined just as with $\R^2$. However, unlike vectors in $\R^2$, there is also an operation $\cdot$. We desire for $(0,1)\cdot (0,1) = (-1,0)$; essentially, $i^2 = -1$. We say that $i$ is a square foot of $-1$; every complex number except $0$ has two square roots.
  \begin{align*}
    (a,b)\cdot (c,d) &= (a+bi) + (c+di)\\
                     &:= a(c) + adi + bci + bd(i^2)\\
                     &:= (ac-bd) + (ad+bc)i\\
                     &= (ac-bd,ad+bc)
  \end{align*}
  Thus, $\R^2$ with the operations $+$ and the above defined complex multiplication is known as $\C$. We write as $a+bi$ instead of $(a,b)$.\\

  Given $z=(a+bi)\in \C$, we write $\text{Re}(z) = a$ and $\text{Im}(z) = b$. If $\text{Im}(z) = 0$, then $z\in \R\times\{0\} \subset \C$. However, many people say that $\R\subseteq \C$, even if $\C$ isn't defined as such.
  \subsection{Reciprocals of Complex Numbers}%
  Let $z\in \C$, where $z\neq 0$. Then, $\exists w\in \C$ such that $zw = 1$.\\

  Let $w = c+di$. We want to show that $zw = 1$.
  \begin{align*}
    (a+bi) + (c+di) &= (ac-bd) + (ad+bc)i\\
    \shortintertext{with the condition that}
    ac-bd &= 1\\
    ad + bc &= 0.\\
    \shortintertext{Thus, let $w = c+di$, with $a,b\neq 0$}
    c &= \frac{a}{a^2 + b^2}\\
    d &= \frac{-b}{a^2 + b^2}
  \end{align*}
  For every $z\neq 0$, with $z = a+bi$, the \textit{reciprocal} of $z$ is defined as $\frac{1}{z} = \frac{a}{a^2 + b^2} + \frac{-b}{a^2 + b^2}i$. Then, for $w\in \C$, we define
  \begin{align*}
    \frac{w}{z} &:= w \left(\frac{1}{z}\right).
  \end{align*}
  \section{Properties of Complex Numbers}%
  Let $z = a+bi\in C$. Then, the (Euclidean) norm (or absolute value) of $z$ is defined as
  \begin{align*}
    |z| &= \sqrt{a^2 + b^2}.
  \end{align*}
  The conjugate of $z = a+bi$ is $\overline{z} = a-bi$.
  \begin{enumerate}[(i)]
    \item $z\overline{z} = |z|^2$
    \item $\overline{(\overline{z})}=z$
    \item $\overline{(z+w)} = \overline{z} + \overline{w}$
    \item $\overline{zw} = \overline{z}\cdot\overline{w}$
    \item $z + \overline{z} = 2\text{Re}(z)$, so $\text{Re}(z) = \frac{z + \overline{z}}{2}$
    \item $z - \overline{z} = 2\text{Im}(z)i$, so $\text{Im}(z) = \frac{z-\overline{z}}{2i}$
  \end{enumerate}
  \subsection{Polar Representation}%
  Let $z = a+bi$ (or $z = (a,b)$). Then, $|z| = \sqrt{a^2 + b^2}$ is the \textit{radius}, and the \textit{argument} is found by $\theta = \arctan(b/a)$ for $a\neq 0$. Therefore, the full polar representation is as follows:
  \begin{align*}
    z = |z|\left(\cos\theta + i\sin\theta\right) \tag*{$\theta \in [0,2\pi)$}.
  \end{align*}
  If $z = 0$, then $|z| = 0$, and $\arg z$ is undefined.\\

  For example, we can find $\arg i$ in $[\pi,3\pi)$ as $\frac{5\pi}{2}$.\\

  For $z_1$ and $z_2$ in polar form, we have:
  \begin{align*}
    |z_1z_2| &= |z_1||z_2| \tag*{(1)}\\
    \arg(z_1z_2) &= \arg z_1 + \arg z_2 \mod 2\pi \tag*{(2)}
  \end{align*}
  Proof of (1):
  \begin{align*}
    |z_1z_2|^2 &= (z_1z_2)\overline{(z_1z_2)}\\
               &= z_1z_2\overline{z_1}\overline{z_2}\\
               &= z_1\overline{z_1}z_2\overline{z_2}\\
               &= |z_1|^2|z_2|^2
  \end{align*}
  Since $|z|\geq 0$, we get $|z_1z_2| = |z_1||z_2|$.\\

  Let $z = 2(\cos \pi/6 + i\sin\pi/6)$, and let $f: \C \rightarrow \C$ defined as $f(w) = zw$. Then, $f$ rotates $w$ by $\pi/6$ and scales $w$ by $2$.\\

  \begin{description}
    \item[Theorem:] For $n\in \N$, if $z = r(\cos\theta + i\sin\theta)$, then $z^n = r^n(\cos(n\theta) + i\sin(n\theta))$.
    \item[Proof:] Induct on $n$. For the base case, we know that $n=1$ satisfies this property. For $n > 1$, we have:
      \begin{align*}
        z^{n+1} &= (z^n)(z)\\
                &= \left(r^n(\cos(n\theta) + i\sin(n\theta))\right)r(\cos\theta + i\sin\theta)\\
                &= (r^n)(r)\left(\cos(n\theta + \theta) + i\sin(n\theta + \theta)\right) \tag*{Polar Representation Definition}\\
                &= r^{n+1}(\cos\left((n+1)\theta\right) + i\sin((n+1)\theta))
      \end{align*}
  \end{description}
  We can use this technique to find the ``roots of unity.'' For example, to find all $z$ such that $z^3 = 1$, we use our technique:
  \begin{align*}
    z^3 &= 1\\
    |z| &= 1\\
    \arg z^3 &= 0\\
    3\arg z &= 0 \mod 2\pi\\
    \arg z &= \frac{k2\pi}{3}\\
           &= 0,\frac{2\pi}{3},\frac{4\pi}{3}\\
    z_1 &= 1\\
    z_2 &= (\cos 2\pi/3 + i\sin 2\pi/3)\\
    z_3 &= (\cos 4\pi/3 + i\sin 4\pi/3)
  \end{align*}
  We can see that $z_2^2 = z_3$.\\

  For the $n$ case, we find $z_2 = \cos(2\pi/n) + i\sin(2\pi/n)$, and $z_{k} = z_2^{k-1}$.
  \section{Exponential, Logarithm, and Trigonometric Functions in $\C$}%
  \subsection{Exponential}%
  Let $z = a+bi$. We define $e^{a+bi}$ as follows:
  \begin{align*}
    e^{a+bi} &= e^a \left(\cos b + i\sin b\right)
  \end{align*}
  Recall that for every nonzero complex number, $z = |z|\left(\cos \theta + i\sin\theta\right)$, where $\theta = \arg z$. Thus,
  \begin{align*}
    z &= |z|e^{i\theta}\\
      &= |z|e^{i\arg z}.
  \end{align*}
  The function $e^z$ has some properties similar to the function $e^x$ in real numbers, and some properties varying with the real numbers.
  \begin{align*}
    e^ze^w &= e^{z+w}\\
    e^z &\neq 0
  \end{align*}
  However, there are some differences:
  \begin{align*}
    |e^{i\theta}| &= 1 \tag*{$\forall \theta$}\\
    e^{a+bi} &= e^a
  \end{align*}
  From these properties, we find Euler's equation:
  \begin{align*}
    e^{i\pi} + 1 &= 0
  \end{align*}
  Additionally, $e^z$ is periodic, while $f(x) = e^x$ is injective:
  \begin{align*}
    e^{z + 2n\pi} &= e^{z}\left(\cos(2n\pi) + i\sin{2n\pi}\right)\\
                  &= e^z
  \end{align*}
  When examining the function $f: \C\rightarrow\C\setminus\{0\}$, $z \mapsto e^z$, we find that the following happen:
  \begin{itemize}
    \item $f(\R) = (0,\infty)$ --- we apply $f(x) = e^x$.
    \item $f(a+bi) = e^ae^{bi}$ --- $e^a$ is rotated by $b$.
    \item $f(\R + bi)$ is expressed as the line along $b$ radians through the origin.
    \item Therefore, $f(A_0) = \C\setminus\{0\}$, where $A_0 = \{a+bi\mid a\in \R, b\in [0,2\pi)\}$.
  \end{itemize}
  \subsection{Logarithm}%
  
  Recall that for a function $f: A\rightarrow B$, $f^{-1}$ is a function if $f$ is injective. However, for any $f$, it is the case that $f^{-1}(b)$ does exist, defined as follows:
  \begin{align*}
    f^{-1}(b) &= \{a\mid f(a) = b\}.
  \end{align*}

  For the function $f(z) = e^z$, $f$ is not one to one, so for $w = e^z$, $f^{-1}(w) = \{z'\in\C \mid e^{z'} = w\}$. We can find this as $f^{-1}(w) = \{z + 2n\pi i\mid n\in\Z\}$.\\

  We define $\log(w) := \{z\in\C\mid e^z = w\}$. For a fixed $\theta\in\R$, we define $\log_{A_{\theta}}(w) := \{z\mid e^{z}=w,z\in A_{\theta}\}$.\\

  Let $z = 1 + \frac{5\pi}{2}i$. Then,
  \begin{align*}
    \log_{A_{-\pi}}e^z &= 1 + \frac{\pi}{2}i
  \end{align*}
  Let $w\in \C\setminus\{0\}$. To find $\log w$ (all values), then
  \begin{align*}
    z&\in \log w\\
    e^z &= w\\
        &= |w|e^{i\arg w}\\
    e^{a+bi} &= |w|e^{i\arg w}\\
    e^ae^{ib} &= |w|e^{i\arg w}.
  \end{align*}
  Therefore, $a = \ln|w|$ and $b = \arg w$. Additionally, the following hold, for $z_1,z_2\in \C$:
  \begin{align*}
    \log_{A_{\theta}}(z_1z_2) &= \log_{A_{\theta}}(z_1) + \log_{A_{\theta}}(z_2) + 2n\pi i
  \end{align*}
  \subsection{Cosine and Sine}%
  \begin{align*}
    e^{ib} &= \cos b + i\sin b\\
    e^{-ib} &= \cos b - i\sin b\\
    \cos z &:= \frac{e^{iz} + e^{-iz}}{2}\\
    \sin z &:= \frac{e^{iz}-e^{-iz}}{2i}
  \end{align*}
  \subsection{Complex Powers}%
  Recall that for $s,t\in\R$, $s^t = e^{t\ln s}$, where $s > 0$. For $z,w\in \C$, $z^w = e^{w\log z}$., where $z\neq 0$.
  \begin{align*}
    (-2)^{i} &= e^{i\log(-2)}\\
             &= e^{i\left(\ln(2) + i\pi\right)}\\
             &= e^{i\ln 2 - (\pi + 2\pi n)}\\
             &= e^{-\pi + 2\pi n + i\ln 2}
  \end{align*}
  This has \textit{infinitely} many values.\\

  Let $\alpha = u + vi$. Then,
  \begin{align*}
    z^{\alpha} &= e^{\alpha \log z}\\
               &= e^{(u+vi)(\ln|z| + i\arg z)}\\
               &= e^{(u\ln|z| - v\arg z)}e^{i(v\ln|z| + u\arg z)}\\
   \intertext{Since $\arg z = \theta + 2\pi n$ for some real $\theta\in[0,2\pi)$,}\\
               &= e^{u\ln z} e^{-v(\theta + 2\pi n)} e^{iv\ln|z|} e^{iu(\theta + 2\pi n)}
  \end{align*}
  Therefore, complex exponentiation is single-valued if $\alpha\in \R$. If $\alpha\in \Z$, then $z^{\alpha}$ has only one value; if $\alpha\in \Q$, where $\alpha = \frac{p}{q}$ and $\gcd(p,q) = 1$, then $z^{\alpha}$ takes $q$ distinct values, which are the $q$th-roots.
  \section{Continuous Functions with Complex Domains}%
  Let $z\in \C$, let $r > 0$.
  \begin{itemize}
    \item The set $D(z;r) := \{w\mid w\in\C,|z-w| < r\}$ is the $r$-neighborhood of $z$.
    \item A subset $A\subseteq \C$ is open if $\left(\forall z\in A\right)\left(\exists r > 0\right) \ni D(z;r)\subseteq A$.
  \end{itemize}
  For example, if $A = \{z\mid \text{Re}(z) > 0\}$, we can find $r$ equal to half the magnitude of the real component of $z$ for any $z\in A$, meaning $A$ is open.\\

  Meanwhile, if $A = \{z\mid \text{Re}(z) \geq 0\}$, this is not the case. If $z = 0$, then $\nexists r > 0$ such that $D(z;r)\subseteq A$, as any open ball of radius $r$ will have some element in $\overline{A}$.
  \begin{itemize}
    \item A subset $B\subseteq \C$ is closed if $\overline{B}\subseteq \C$ is open.
  \end{itemize}
  For example, $A = \emptyset$ is open, by vacuous truth, so $\overline{A} = \C$ is closed. Similarly, since $\C$ is open, $\emptyset$ is closed.\\

  Meanwhile, $A = \{x + iy\mid -1\leq x < 1\}$ is neither open nor closed.
  \subsubsection{Limits}%
  Let $A\subseteq \C$, $f: A\rightarrow \C$, $z_0\in \C$. Then,
  \begin{align*}
    \lim_{z\rightarrow z_{0}}f(z) = \ell
  \end{align*}
  means both of the following hold:
  \begin{enumerate}[(i)]
    \item for some $r > 0$, $D(z_{0};r)\setminus\{z_0\}\subseteq \text{dom}(f)$,
    \item $\forall \varepsilon > 0$, $\exists \delta > 0$ such that $f(D(z_0;\delta)\setminus \{z_0\}) \subseteq D(\ell;\varepsilon)$.
  \end{enumerate}
  For example, if
  \begin{align*}
    f(z) &= \begin{cases}
      z & z\in \C\setminus\R\\
      3i & z\in\R
    \end{cases}
  \end{align*}
  Then, $\lim_{z\rightarrow 0}f(z)$ does not exist, as there is no $\ell$ that satisfies both conditions. Specifically, if $\ell = 3i$, and we set $\varepsilon = 1$, then a disc of any radius around $0$ has some $z\in \C\setminus\R$ that maps to itself. Similarly, if we set $\ell = 0$, then there is a real number in a disc of any radius around $0$.
  \begin{description}
    \item[Note:] $f$ does not have to be defined at $z_0$ for the limit to be defined at $z_0$.
  \end{description}
  Let $A\subseteq \C$ be open, $f: A\rightarrow\C$, and $z_0\in A$. We say $f$ is continuous at $z_0$ if $\lim_{z\rightarrow z_{0}}f(z) = f(z_0)$. We say $f$ is continuous on $A$ if $\forall z_0\in A$, $f$ is continuous at $z_0$.\\

  We will show that $f: \C\rightarrow\C$, $z\mapsto 3z$ is continuous. 
  \begin{description}
    \item[Scratch Work:] We want $\delta$ such that $f(D(z_0;\delta))\subseteq D(3z_0;\varepsilon)$. Let $z\in D(z_0;\delta)$, meaning $f(z) = 3z$. We want $3z \in D(3z_0;\varepsilon)$, meaning we want $|3z-3z_0| < \varepsilon$, or $|z-z_0| < \frac{\varepsilon}{3}$.
    \item[Proof:] Let $\varepsilon > 0$. Set $\delta = \frac{\varepsilon}{3}$. We show $f(D(z_0;\delta))\subseteq D(f(z_0);\varepsilon)$. Let $z\in D(z_0;\delta)$. Then, $|z-z_0| < \delta=\varepsilon/3$, meaning $3|z-z_0| < \varepsilon$, meaning $|3z-3z_0| < \varepsilon$, so $|f(z)-f(z_0)| < \varepsilon$. Therefore, $f(z)\in D(f(z_0);\varepsilon)$. Since $f$ is continuous at arbitrary $z_0$, $f$ is continuous on $\C$.
  \end{description}
  \subsubsection{Sequences}%
  A sequence $z_1,z_2,\dots \in \C$. A sequence converges to $z_0\in\C$ if 
  \begin{align*}
    (\forall \varepsilon > 0)(\exists M\in \N) \ni \forall z_{n > M},~|z_n-z_0| < \varepsilon
  \end{align*}
  In words, for any radius around $z_0$, we can find $z_n$ arbitrarily close to $z_0$ for sufficiently large $n$. We write $z_n\rightarrow z_0$ if this is the case.\\

  Let $f: \C\rightarrow\C$. Then, $f$ is continuous on $\C$ if and only if the following equivalent conditions are met:
  \begin{enumerate}[(i)]
    \item the inverse image of every open set is open ($f^{-1}(B):=\{a\in \C\mid f(a)\in B\}$);
    \item the inverse image of every closed set is closed;
    \item for every sequence $(z_n)_n$ such that $(z_n)_n\rightarrow z_0$, $f(z_n)\rightarrow f(z_0)$.
  \end{enumerate}
  Let
  \begin{align*}
    f(z) &= \begin{cases}
      0&z=0\\
      1 & z\neq 0
    \end{cases}.
  \end{align*}
  This function is not continuous. We will check that (i)--(iii) fail.
  \begin{enumerate}[(i)]
    \item Let $B = D(0;1)$. Then, $f^{-1}(B) = \{0\}$, which is not open set.
    \item Let $B = \text{cl}(D(1;0.5))$. Then, $f^{-1}(B) = \C\setminus\{0\}$, which is not closed.
    \item Let $z_n = \frac{1}{n}$. Then, $(z_n)_n\rightarrow 0$, but $f(z_n) = 1$ for all $n$, meaning $f(z_n)\rightarrow 1\neq f(0)$.
  \end{enumerate}
  To show limit divergence, recall the definition of limit convergence:
  \begin{align*}
    \lim_{n\rightarrow\infty} z_n = z_0 \Leftrightarrow (\forall \varepsilon > 0)(\exists M\in \N) \ni \forall z_{n > M},~|z_n-z_{0}| < \varepsilon.
  \end{align*}
  Let $z_1,\dots,\in \C$ be a sequence. Then, $\lim_{n\rightarrow\infty} = \infty$ means
  \begin{align*}
    (\forall M > 0)(\exists N\in \N) \ni \forall n > N,~|z_n| > M.
  \end{align*}
  In words, $|z_n|$ is arbitrarily large for sufficiently large $n$.
  \subsubsection{Connected Sets}%
  Let $a,b\in\C$. A path from $a$ to $b$ is a continuous function $p:[0,1]\rightarrow \C$ such that $p(0) = a$ and $p(1) = b$. Let $S\subseteq \C$. If $p([0,1])\subseteq S$, then $p$ is a path in $S$.\\

  We say $S$ is \textit{path-connected} if for any $s,t\in S$, there is a path in $S$ from $s$ to $t$.\\

  Every set that is path-connected is connected, but not necessarily the other way around --- if $A$ is open and path connected, then $A$ is connected.\\

  An open, path-connected subset of $\C$ is known as a region, or a domain.\\

  Let $A = \R\times\{0\}$ (or the $x$ axis in $\C$). $A$ is not a region, as $A$ is not an open set, even if $A$ is path-connected.\\

  $A\subseteq \C$ is bounded if there exists $r > 0$ such that $A \subseteq D(0;r)$. $A = \R\times\{0\}$ is not bounded.\\

  If $A\subseteq \C$, then $A$ is compact if $A$ is closed and bounded. There are various properties of compact sets that make them particularly amenable towards analysis.
  \begin{description}
    \item[Extreme Value Theorem:] Every real-valued continuous function on a compact domain attains its maximum and minimum values.
    \item[Uniform Continuity Theorem:] Elaborated below.
  \end{description}
  \subsection{Uniform Continuity}%
  Recall that if $f: A\rightarrow \C$, $f$ is continuous if $\forall a\in A$, $\lim_{z\rightarrow a}f(z) = f(a)$.
  \begin{align*}
    (\forall a\in A)(\forall \varepsilon > 0)(\exists \delta_{a} > 0) \ni f(D(a;\delta_{a})) \subseteq D(f(a);\varepsilon)\tag*{$\delta$ depends on $a$}
  \end{align*}
  When $f$ is uniformly continuous, there is one value of $\delta$, dependent on $\varepsilon$, that applies for every value of $a$.
  \begin{align*}
    (\forall \varepsilon > 0)(\exists \delta_{\varepsilon} > 0) \ni (\forall a\in A), f(D(a;\delta_{\varepsilon}))\subseteq D(f(a);\varepsilon)
  \end{align*}
  \subsection{Riemann Sphere}%
  Let $S^2 = \{(x,y,z)\in \R^3\mid x^2 + y^2 + z^2\}$. Let $N = (0,0,1)$ denote the north pole. Then, there is a continuous bijection from $S^2 \setminus \{N\} \rightarrow \C$.\\

  We can visualize this by picking a random point on the sphere and drawing a line from the north pole through the sphere to this point, and finding the point that intersects the plane.\\

  Consider the sequence $z_n = n^2 i$ for $n=1,2,\dots$. We can see that, on the projection from $z_n$ to the sphere, all the values of $p$ converge to $N$. Therefore, we write $\lim_{n\rightarrow\infty}z_n = \infty$, where $\infty$ corresponds to $N$ on $S^2$.\\

  We can define $\overline{\C} = \C\cup \{\infty\}$ to be the complex plane that includes the ``point at infinity'' (from the projection on $S^2$ that corresponds to the north pole).
  \section{Analytic Functions}%
  Let $f: A\subseteq \C \rightarrow \C$ where $A$ is open. Let $z_0\in A$. We say $f$ is differentiable at $z_0$ if
  \begin{align*}
    \lim_{z\rightarrow z_0}\frac{f(z)-f(z_0)}{z-z_0}
  \end{align*}
  exists.
  \subsection{Rules of Differentiation}%
  \begin{itemize}
    \item $(f+g)' = f' + g'$
    \item $(fg)' = f'g + fg'$
    \item $\left(\frac{f}{g}\right)' = \frac{f'g - fg'}{(g)^2}$
    \item $(f\circ g)' = g'(f'\circ g)$
    \item For $n\in\Z$, $(z^n)' = nz^{n-1}$
  \end{itemize}
  Let $f(z) = \overline{z}$. We will find this value by directly applying the definition of the derivative.
  \begin{align*}
    f'(z_0) &= \lim_{z\rightarrow z_0} \frac{\overline{z}-\overline{z_0}}{z-z_0}\\
            &= \lim_{z\rightarrow z_0}\frac{\overline{z-z_0}}{z-z_0}.
    \intertext{Let's approach $z_0$ from the horizontal direction. Suppose $z = z_0 + t$ for some $t\in\R$. Then,}
    \lim_{z\rightarrow z_0} \frac{\overline{z_0 + t} - \overline{z_0}}{z_0 + t - z_0} &= 1.
    \intertext{Let's approach $z_0$ from the horizontal direction. Suppose $z = z_0 + ti$ for some $t\in\R$. Then,}
    \lim_{z\rightarrow z_0}\frac{\overline{z_0 + ti} - \overline{z_0}}{z_0 + ti - z_0} &= \frac{-ti}{ti}\\
                                                                                       &= -1.
     \intertext{Since $1 \neq -1$, we find that the limit does not exist.}
  \end{align*}
  We see that complex-differentiability is a strong condition.\\

  Suppose that $f'(z_0) = 2i$, meaning
  \begin{align*}
    \lim_{z\rightarrow z_0} \frac{f(z)-f(z_0)}{z-z_0} &= 2i.
  \end{align*}
  If $z$ is close to $z_0$, then $f(z)-f(z_0) \approx 2i(z-z_0)$. Pictorially, we can visualize this as, for $z_0$ sufficiently close to $z$, the vector $z_0-z$ is akin to a counterclockwise rotation and a scaling by $2$. This is applicable for \textit{all} $z$ in sufficient proximity to $z_0$.\\

  Specifically, we can see that the complex differentiable function is \textit{angle-preserving}. The technical name for $f$ is that $f$ is \textit{conformal}.
  \subsection{Analytic Function}%
  Let $f: A\subseteq C \rightarrow \C$. If $f$ is differentiable at every $z_0 \in A$, we say $f$ is \textit{analytic} on $A$.\\

  If $f$ is analytic on $A$, then $f$ is infinitely differentiable on $A$.\\

  If $f$ is analytic on $A$ and $f'(z_0)\neq 0$ for some $z_0\in A$, then $f$ is conformal at $z_0\in A$. 
  \subsection{Cauchy-Riemann Theorem}%
  Given a function $f(x,y): \R^2\rightarrow \R$. Recall that we can take partial derivatives, $\frac{\partial f}{\partial x}$, and directional derivative $\frac{\partial f}{\partial u}$ for some unit vector $u$.\\

  However, for $\C$, there is only one derivative, $f'(z_0)$, meaning that regardless of direction, $f'(z_0)$ exists and has one value. We can contextualize $f(z) = f(x+yi) = u(x,y) + iv(x,y)$, where $u(x,y)\in\R$ and $v(x,y)\in\R$. Then,
  \begin{align*}
    \frac{\partial u}{\partial x} &\neq \frac{\partial u}{\partial y}\\
    \intertext{and}
    \frac{\partial v}{\partial x} &\neq \frac{\partial v}{\partial y}\\
    \intertext{but}
    \frac{\partial f}{\partial x} &= \frac{\partial f}{\partial y}.
  \end{align*}
  We can see this by first letting $z = z_0 + \delta x$.
  \begin{align*}
    f'(z_0) &= \lim_{z\rightarrow z_0} \frac{f(z_0 + \delta x) - f(z_0)}{z_0 + \delta x - z_0}\\
            &= \lim_{z\rightarrow z_0} \frac{u(x_0 + \delta x, y_0) + iv(x_0 + \delta x, y_0) - (u(x_0,y_0) + iv(x_0,y_0))}{\delta x}\\
            &= \frac{\partial u}{\partial x} + i\frac{\partial v}{\partial x}
  \end{align*}
  and in the $y$ direction,
  \begin{align*}
    f'(z_0) &= \frac{1}{i} \frac{\partial u}{\partial y} + \frac{\partial v}{\partial y}\\
            &= -i \frac{\partial u}{\partial y} + \frac{\partial v}{\partial y}.
  \end{align*}
  We set these two values equal to find
  \begin{align*}
    \frac{\partial u}{\partial x} &= \frac{\partial v}{\partial y}\\
    \frac{\partial v}{\partial x} &= -\frac{\partial u}{\partial y},
  \end{align*}
  which are the Cauchy-Riemann equations. The corresponding theorem states that if $f'(z_0)$ exists, then the Cauchy-Riemann equations must hold.\\

  For example, if $f(z) = \overline{z}$, with $f(x + yi) = x - yi$, we have $u(x,y) = x$ and $v(x,y) = -y$. Then,
  \begin{align*}
    \frac{\partial u}{\partial x} &= 1\\
    \frac{\partial v}{\partial v} &= -1,
  \end{align*}
  meaning $f$ is not complex-differentiable.\\

  If $f: A\rightarrow \C$ satisfies the Cauchy-Riemann equations at every $z_0 \in A$, then $f$ is analytic on $A$.\\

  If $f: A\subseteq \C \rightarrow \C$ is analytic on $A$, then we know $f'$ and $f''$ are continuous. From multivariable calculus, we know that $u_{xy} = u_{yx}$ if both are continuous. So,
  \begin{align*}
    u_{xy} &= \frac{\partial}{\partial y}(u_x)\\
           &= \frac{\partial}{\partial y}(v_y)\\
           &= v_{yy}\\
    u_{yx} &= \frac{\partial}{\partial x}(u_y)\\
           &= \frac{\partial }{\partial x}(-v_x)\\
           &= -v_{xx}
  \end{align*}
  Therefore, $v_{xx} + v_{yy} = 0$. Similarly, $u_{xx} + u_{yy} = 0$.\\

  If $\varphi: \R^2\rightarrow \R$ If $\varphi_{xx} + \varphi_{yy} = 0$, then we say $\varphi$ is a harmonic function. Therefore, if $f$ is an analytic function, then both the real and imaginary parts of $f$ are harmonic.\\

  Let $A\subseteq \R^2$. If $u: A\rightarrow \R$ and $v: A\rightarrow \R$. Then, $u$ and $v$ are harmonic conjugates if $u + iv$ is an analytic function. Additionally, $u$ and $v$ are harmonic conjugates if and only if they satisfy the Cauchy-Riemann equations.\\

  We may ask if there exists an analytic function $f$ such that $\text{Re}(f) = x^3 - 3xy^2 + y$. Then,
  \begin{align*}
    v_y = u_x &= 3x^2 - 3y^2\\
    -v_x = u_y &= 1-6xy.
  \end{align*}
  Therefore, we find $v = -x + 3x^2y - y^3 + c$ through integration. Therefore, we have
  \begin{align*}
    f(z) &= (x^3 - 3xy^2 + y) + i(3x^2y - y^3 - x + c)\\
         &= (x-iy)^3 + y-ix + ic\\
         &= z^3 + i(-iy + x) + ic\\
         &= \overline{z}^3  + i\left(\overline{z} + c\right)
  \end{align*}
  Recall from from multivariable calculus that $\nabla u \perp $ contour lines of $u$. Similarly, $\nabla v \perp$ contour lines of $v$. Then, using the Cauchy-Riemann equations, we find
  \begin{align*}
    \nabla u \cdot \nabla v &= (-u_xu_y) + u_xu_y\\
                            &= 0,
  \end{align*}
  meaning the gradients are orthogonal to each other, meaning the contours of $u$ are perpendicular to the contours of $v$.
  \subsection{Inverse Functions}%
  Let $f: A\subseteq \C \rightarrow \C$. Let $z_0\in A$. If $f$ is analytic on $A$ and $f'(z_0) \neq 0$, then $f$ is one to one on some neighborhood of $z_0$. Then, $f^{-1}: f(N)\rightarrow N$ is analytic on $f(N)$, and
  \begin{align*}
    (f^{-1})'(f(z_0)) &= \frac{1}{f'(z_0)}.
  \end{align*}
  \section{Derivatives of Elementary Functions}%
  Specifically, we will be working with complex exponentiation, complex trigonometric functions, and complex logarithms.
  \subsection{Complex Exponential}%
  \begin{align*}
    \frac{d}{dz}e^z &= e^z,\\
    \intertext{since, letting $z = x+iy$,}
    e^z &= e^{x}e^{iy}\\
        &= e^x(\cos(y) + i\sin(y)).\\
    \frac{d}{dz} e^z &= \frac{\partial}{\partial x}e^z \tag*{treating $y$ as constant}\\
                     &= e^x(\cos(y) + i\sin(y))\\
                     &= e^{x + iy}\\
                     &= e^z.
  \end{align*}
  We know that $e^z$ is continuous on $\C$, but this doesn't imply differentiability at every $z_0\in \C$. We can verify by checking the Cauchy-Riemann equations, where $u(x,y) = e^x\cos(y)$ and $v(x,y) = e^x\sin(y)$. Then,
  \begin{align*}
    \frac{\partial u}{\partial x} &= e^x\cos(y)\\
                                  &= \frac{\partial v}{\partial y}\\
    \frac{\partial v}{\partial y} &= -e^x\sin(y)\\
                                  &= -\frac{\partial v}{\partial x}.
  \end{align*}
  If a function is analytic on $\C$, then $f$ is known as entire.
  \subsection{Complex Logarithm}%
  We might ask where $\log z$ is analytic. Let $f(z) = e^z$. Then, $\log z = f^{-1}(z)$; since $f$ is not one to one, we restrict the domain of $f$ to $A_{\theta} = \{z\mid \text{Im}(z)\in [\theta,\theta + 2\pi)\}$ for any $\theta$.\\

  Since $f\big|_{A_\theta}$ is one to one, then
  \begin{align*}
    \left(f\big|_{A_{\theta}}\right)^{-1} &= \log_{A_{\theta}}.
  \end{align*}
  Fixing $\theta$, set $g = f\big|_{A_{\theta}}$. Then,
  \begin{align*}
    g^{-1}(g(z)) &= z.
  \end{align*}
  Because $g$ is analytic on $A_{\theta}$, $g^{-1}$ is analytic on $A_{\theta}$. By chain rule, we have
  \begin{align*}
    \frac{d}{dz} (g^{-1}(g(z))) &= \frac{d}{dz}z\\
    g^{-1'}(g(z)) &= \frac{1}{g'(z)} \tag*{$g'(z)\neq 0$}\\
    g^{-1}(w) &= \frac{1}{g'(z)}\tag*{$w = e^z$}\\
              &= \frac{1}{e^{z}}\\
              &= \frac{1}{w}.
  \end{align*}
  Therefore, $\frac{d}{dw}\log_{A_{\theta}}(z) = \frac{1}{z}$. Therefore, $\text{dom}(\log_{A_{\theta}}) = \text{ran}(e^{z}_{A_{\theta}}) = \C\setminus \{0\}$. However, $\log_{A_{0}}$ (setting $\theta = 0$) is not even continuous on $\C\setminus \{0\}$!\\

  Specifically, at $z = 0$, $e^z = 1$. Travelling around the unit circle counterclockwise in the image, we see that the preimage of these points travels along the imaginary axis. Approaching $1$ ``from the bottom,'' we find that the preimage of the points approaches $2\pi$ in the domain. However, they ought to be approaching $0$. Therefore, the limit doesn't exist.\\

  However, notice that the domain is not open! To fix this, we will let $B_{\theta} = \{z\in \C\mid \text{Im}(z) \in (\theta,\theta + 2\pi)\}$.\\

  Our $\log$ function \textit{is} when $e^z$ is restricted to $B_{\theta}$. Then, $\log_{B_{\theta}}$ is analytic on $\C\setminus\{re^{i\theta}\mid r\geq 0\}$. When $\theta = -\pi$, then $\log_{B_{-\pi}}$ is the principle branch of $\log z$.\\

  Then, the domain is $C\setminus \{z\mid z = x + 0i, x < 0\}$ and the range is $B_{-\pi}$.
  \subsection{Powers}%
  Let $\alpha \in \C$. We might ask
  \begin{align*}
    \frac{d}{dz}\alpha^{z}\\
    \frac{d}{dz}z^{\alpha}.
  \end{align*}
  Recall that $a^b = e^{b \log a}$. Specifically, $a^b = e^{b (\ln|a| + i\arg a)}$.
  \begin{align*}
    \frac{d}{dz}\alpha^{z} &= \frac{d}{dz}e^{z\log \alpha}\\
    \intertext{Fix $\theta$. Then,}
                           &= \frac{d}{dz}e^{z\log_{A_{\theta}}\alpha}\\
                           &= \log_{A_{\theta}}\alpha e^{z\log_{A_{\theta}}\alpha} \tag*{assuming analytic domain}\\
                           &= \alpha^{z}\log_{A_{\theta}}\alpha.
  \end{align*}
  Specifically, as long as $\alpha \notin \{re^{i\theta}\mid r \geq 0\}$, $z\log_{A_{\theta}}\alpha$ is analytic, meaning $e^{z\log_{A_{\theta}}\alpha}$ is analytic (composition of analytic functions).
  \begin{align*}
    z^{\alpha} &= e^{\alpha \log z}\\
               &= e^{\alpha \log_{B_{\theta}} z}\\
               &= e^{\alpha \log_{B_{\theta}}z} \frac{\alpha}{z}\\
               &= \alpha z^{\alpha - 1}.
  \end{align*}
  Specifically, this holds for $z\notin \{re^{i\theta}\mid r\geq 0\}$.\\

  We know that $\frac{d}{dz}\log_{B_{-\pi}(z)} = \frac{1}{z}$. The domain of $\log_{B_{-\pi}}$ is $\C\setminus (-\infty,0]$.
  \section{Contour Integrals}%
  Recall from multivariable that $\gamma: [a,b]\rightarrow \R^n$ is called a curve.\\

  For example, $\gamma: [0,\pi] \rightarrow \R^2$, defined as $\gamma(\theta) = (\cos\theta,\sin\theta)$. The image of the given curve is a half circle.\\

  We want to have $\gamma$ be continuous and differentiable. Then,
  \begin{align*}
    \gamma(t) &= (\gamma_1(t),\dots,\gamma_n(t))
  \end{align*}
  is continuous/differentiable if and only if every $\gamma_i$ is continuous/differentiable.
  \begin{align*}
    \gamma'(t) &= (\gamma_1'(t),\dots,\gamma_n'(t))
  \end{align*}
  If $\gamma'$ is continuous, we say $\gamma$ is smooth. For us, $\gamma\in C^{1}$ is enough, $\gamma \in C^{\infty}$ is not necessary.\\

  For $\gamma:[a,b]\rightarrow \R^n$ and $f: \R^n\rightarrow \R^n$, we define
  \begin{align*}
    \int_{\gamma}f &:= \int_{a}^{b}f(\gamma(t))\cdot\gamma'(t)dt
  \end{align*}
  as the line integral of $f$ over $\gamma$.\\

  Let $f: A\subseteq \C\rightarrow \C$ for $A$ open, where $\gamma: [a,b]\rightarrow A$. Then,
  \begin{align*}
    \int_{\gamma}f &:= \int_{a}^{b}f(\gamma(t))\gamma'(t)dt\\
                   &= \lim_{n\rightarrow \infty}\sum_{k=1}^{n}f(z_k)\Delta z
  \end{align*}
  Rather than the dot product, we use complex multiplication.\\

  To define $\gamma'(t)$, we can imagine it as
  \begin{align*}
    \gamma(t) &= \gamma_1(t) + i\gamma_2(t)\\
    \gamma'(t_0) &= \lim_{t\rightarrow t_0}\frac{\gamma(t)-\gamma(t_0)}{t-t_0}\\
                 &= \gamma_1'(t_0) + i\gamma_2'(t_0).
  \end{align*}
  Therefore,
  \begin{align*}
    \int_{\gamma}f &= \int_{\gamma}\underbrace{f(\gamma(t))\gamma'(t)}_{u(t) + iv(t)} dt\\
                   &= \int_{a}^{b}u(t)dt + i\int_{a}^{b}v(t)dt
  \end{align*}
  Let $\gamma$ be the line from $i$ to $2$, and $f$ as $\text{Im}(z)$. Find $\int_{\gamma}f$.\\

  To solve, we need a formula for $\gamma: [0,1]\rightarrow \C$. We can consider $\gamma(t) = i(1-t) + 2t$. For any straight line, we can define $\gamma: [0,1]\rightarrow \C$ as $\gamma(t) = p(1-t) + qt$, or $p + t(q-p)$.\\

  So,
  \begin{align*}
    \int_{\gamma}f &= \int_{0}^{1}f(\gamma(t))\gamma'(t)dt\\
                   &= \int_{0}^{1}\text{Im}(2t + i(1-t))(2-i)dt\\
                   &= (2-i)\int_{0}^{1}(1-t)dt\\
                   &= (2-i)\left(t-\frac{t^2}{2}\right)\biggr\vert_{0}^{1}\\
                   &= \frac{1}{2}\left(2-i\right)
  \end{align*}
  We could also have $\tilde{\gamma}:[0,1]\rightarrow \C$, $\tilde{\gamma}(t) = 2t^2 + i(1-t^2)$. The image of $\tilde{\gamma}$ is the same as the image of $\gamma$, and (not coincidentally), so is its line integral.
  \subsection{Theorem: Reparametrization}%
  Let $f: A\rightarrow \C$ be analytic, $\gamma: [a,b]\rightarrow A$ and $\tilde{\gamma}:[\tilde{a},\tilde{b}]\rightarrow A$ smooth curves such that $\tilde{\gamma}$ is a reparametrization of $\gamma$. Then, 
  \begin{align*}
    \int_{\gamma}f &= \int_{\tilde{\gamma}}f.
  \end{align*}
  If $\gamma: [a,b]\rightarrow A$, then $\tilde{\gamma}[\tilde{a},\tilde{b}]\rightarrow A$ is a reparametrization if $\exists r: [a,b]\rightarrow [\tilde{a},\tilde{b}]$ such that $r(a) = \tilde{a}$ and $r(b) = \tilde{b}$, and $\tilde{\gamma}\circ r = \gamma$.\\

  For a quick proof, we look at
  \begin{align*}
    \int_{\gamma}f &= \int_{a}^{b}f(\gamma(t))\gamma'(t)dt\\
                   &= \int_{a}^{b}f(\tilde{\gamma}\circ r(t))(\tilde{\gamma}\circ r)(t)dt\\
                   &= \int_{a}^{b}f(\tilde{\gamma}\circ r(t))\tilde{\gamma}'(r(t))r'(t)dt\\
                   \intertext{$u = r(t), du = r'(t)dt$}
                   &= \int_{r(a)}^{r(b)}f(\tilde{\gamma}(u))\tilde{\gamma}'(u)du\\
                   &= \int_{\tilde{a}}^{\tilde{b}}f(\tilde{\gamma}(u))\tilde{\gamma}(t)du
  \end{align*}
  \subsection{Cauchy's Theorem: A Generalization}%
  \begin{description}
    \tiny
    \item[Note:]I was out of class the previous week so we jumped to this location
  \end{description}
  So far, we know that if $\gamma$ is a simple closed curve and $f$ is analytic on and inside $\gamma$, then $\int_{\gamma}f = 0$. However, the theorem is much stronger.\\

  If $\gamma$ is a closed curve, and $f$ is analytic on $A\subseteq \C$, with $\gamma$ contained in $A$, and $\gamma$ is homotopic to a point in $A$, then $\int_{\gamma}f = 0$.\\

  Let $A\subseteq \C$, with $j = 0,1$, and $\gamma_j: [0,1]\rightarrow A$ closed curves. We say $\gamma_0$ is homotopic in $A$ to $\gamma_1$ if there exists continuous $H: [0,1]\times [0,1]\rightarrow A$ such that
  \begin{itemize}
    \item $H_{t}: [0,1]\rightarrow A$ defined by $x\mapsto H(x,t)$ is a closed curve
    \item $H_0 = \gamma_0$ and $H_1 = \gamma_1$.
  \end{itemize}
  If such $H$ exists, we write $\gamma_0 \sim \gamma_1$.\\

  For example, if $\gamma_0(\theta) = e^{2\pi i \theta}$ and $\gamma_3(\theta) = 3e^{2\pi i \theta}$, we can show they are homotopic by using a linear homotopy:
  \begin{align*}
    H_t(\theta) &= (1-t)e^{2\pi i \theta} + t\left(3e^{2\pi i \theta}\right),
  \end{align*}
  which is both continuous and satisfies the given requirements.\\

  In general, for two arbitrary closed curves $\gamma_0$ and $\gamma_1$, we can't go wrong by trying the linear homotopy $H_t(\theta) := (1-t)\gamma_0 + t\gamma_1$.\\

  If a closed curve $\gamma$ is homotopic in $A$ to a point in $A$ (i.e., the curve is homotopic to a constant map), we say $\gamma$ is null-homotopic.\\

  A set in $\C$ is simply connected if it is path-connected and every closed curve in the set is null-homotopic in the set. A set $A\subseteq \C$ is convex if $\forall z_0,z_1\in A, t\in [0,1]$, $tz_1 + (1-t)z_0 \in A$.\\

  Let $f: A\rightarrow \C$, where $f$ is analytic on $A$. If $\gamma_0$ and $\gamma_1$ are curves in $A$ such that $\gamma_0 \sim \gamma_1$ in $A$, then
  \begin{align*}
    \int_{\gamma_0}f &= \int_{\gamma_1} f
  \end{align*}
  Consider $\rho$, a path connecting some point in $\gamma_0$ to some point in $\gamma_1$ (if they are closed), which exists by the homotopy. Then, $\Gamma := \gamma_0 + \rho - \gamma_1 - \rho$ (where we traverse along $\gamma_0$, then $\rho$, then $\gamma_1$, then reverse $\rho$.) is null-homotopic. So, Cauchy's Theorem implies that
  \begin{align*}
    \int_{\Gamma}f &= \int_{\gamma_0} + \int_{\rho} f - \int_{\gamma_1}f -\int_{\rho} f\\
                   &= 0\\
    \int_{\gamma_0} f &= \int_{\gamma_1}f .
  \end{align*}
  \section{Cauchy's Integral Formula}%
  We know that
  \begin{align*}
    \int_{\gamma} f(z) dz = 0
  \end{align*}
  occurs if one of these conditions is satisfied.
  \begin{enumerate}[(i)]
    \item If $\gamma$ is a simple closed curve and $\gamma$ is analytic on and inside $\gamma$.
    \item If $\gamma$ is homotopic in a region $R$ to a point, where $f$ is analytic on $R$.
    \item If $f$ has an antiderivative in the region, and $\gamma$ is a closed curve.
    \item If $\gamma$ is closed and contained in a simply connected region $R$ that $f$ is analytic on.
  \end{enumerate}
  We can also show that
  \begin{align*}
    \int_{\gamma}\frac{1}{z-z_0}dz &= 2\pi i
  \end{align*}
  where $\gamma$ is a simple closed curve and $z_0$ is contained within the region with boundary $\gamma$.\\

  Let $f$ be analytic on a simply connected open set $D\subseteq \C$. Then, for every piecewise smooth closed curve $\gamma\in D$ and every point $z_0\in D\setminus \text{im}(\gamma)$,
  \begin{align*}
    \int_{\Gamma}\frac{f(z)}{z-z_0}dz &= 2\pi i f(z_0)\\
    f(z_0) &= \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{z-z_0}dz
  \end{align*}
  For every $z_0$ inside $\Gamma$, $f(z_0)$ is determined by the values of $f$ on $\Gamma$.\\

  For an outline of the proof, consider $C$, a circle of radius $\varepsilon > 0$ centered at $z_0$. Since $\Gamma \sim C$ in $D\setminus \{z_0\}$, we know that
  \begin{align*}
    \int_{\Gamma}\frac{f(z)}{z-z_0}dz &= \int_{C}\frac{f(z)}{z-z_0}dz
    \intertext{Therefore, on $C$, $f(z)\approx f(z_0)$ if $\varepsilon$ is small. So,}
                                      &\approx f(z_0)\int_{C}\frac{1}{z-z_0}dz\\
                                      &= 2\pi if(z_0)
  \end{align*}
  For example, we can find
  \begin{align*}
    \int_{|z|=4}\frac{\cos(z)}{(z-\pi)(z-5)}dz &= \int_{|z|=4}\left(\frac{\cos z}{z-5}\right)\frac{1}{z-\pi}dz\\
                                               &= 2\pi i \frac{\cos(\pi)}{\pi-5}\\
                                               &= \frac{2\pi i}{5-\pi}
  \end{align*}
  Suppose $f(z)$ is continuous on a contour $\Gamma$ (not necessarily closed). Let
  \begin{align*}
    g(w) &= \int_{\Gamma}\frac{f(z)}{z-w}dz.
  \end{align*}
  Then, $g$ is defined for all $w\notin \text{im}(\Gamma)$, and $g$ is differentiable at every $w\notin \text{im}(\Gamma)$. In other words, $g$ is analytic on $\C\setminus \text{im}(\Gamma)$. Additionally, $g'$ is also analytic on $\C\setminus \text{im}(\Gamma)$, with
  \begin{align*}
    g'(w) &= \frac{d}{dw} \int_{\Gamma} \frac{f(z)}{z-w}dz\\
          &= \int_{\Gamma}\frac{d}{dw}\frac{f(z)}{z-w} dz\\
          &= \int_{\Gamma}\frac{f(z)}{(z-w)^2}dz
  \end{align*}
  This is what we use to prove that any complex-differentiable function is infinitely complex-differentiable.\\

  If $f$ is analytic on $D$, then $f'$ is analytic on $D$. Since $f$ is analytic, then
  \begin{align*}
    f(w) &= \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{z-w}dz
    \intertext{where $\Gamma$ is a circle centered at $w$. So,}
    f'(w) &= \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{(z-w)^2}dz\\
          &= \frac{1}{2\pi i}\int_{\Gamma}\frac{(f(z)/(z-w))}{z-w}dz
  \end{align*}
  The numerator $\frac{f(z)}{z-w}$ is continuous on $\Gamma$ because $w\notin \Gamma$, so by the previous theorem, the integral is analytic on $D\setminus \text{im}(\Gamma)$. Therefore, $f'$ is differentiable at $w$, so $f'$ is analytic on $D$.\\

  If $\Gamma$ is a simple closed curve, $w$ is inside $\Gamma$, and $f$ is analytic on $D$ with $\Gamma \subseteq D$. Then,
  \begin{align*}
    f'(w) &= \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{(z-w)^2}dz\\
    f''(w) &= \frac{2}{2\pi i}2! \int_{\Gamma}\frac{f(z)}{(z-w)^3}dz\\
    f^{(n)}(w) &= \frac{n!}{2\pi i}\int_{\Gamma}\frac{f(z)}{(z-w)^{n+1}}dz
  \end{align*}
  For example,
  \begin{align*}
    \int_{|z|=2}\frac{e^{-z}}{(z+1)^3}dz &= e^{-2}\pi i
  \end{align*}
  If $f$ is continuous on a domain $D$ and $\int_{\Gamma}f = 0$ for every closed $\Gamma$ in $D$, then $f$ is analytic on $D$.\\

  By the path independence theorem, $f$ has an antiderivative $F$ on $D$. So, $F$ is analytic on $D$ as $F' = f$. Thus, $F^{(n)}$ is analytic for all $n$, so $F'$ is analytic, meaning $f$ is analytic. The converse does not hold.\\

  Recall that $\varphi(x,y)$ is harmonic on $D$ if $\varphi_{xx} + \varphi_{yy} = 0$. If $f(z) = u(x,y) + iv(x,y)$, then $f' = u_x + iv_x$, or $f' = v_y - iu_y$. If $f$ is analytic, then both $u$ and $v$ are harmonic. Similarly, $u_x$, $v_x$ are harmonic, and $u_y$, $v_y$ are harmonic (since the analyticity of $f$ implies that $f'$ is also analytic).
  \section{Bounds for Analytic Functions and the Fundamental Theorem of Algebra}%
  Liouville's Theorem: every non-constant entire function is unbounded.\\

  Recall that
  \begin{align*}
    f^{(n)}(w) &= \frac{n!}{2\pi i}\int_{\Gamma}\frac{f(z)}{(z-w)^{n+1}}dz.
  \end{align*}
  Suppose that $f$ is analytic on $C_{R}(z_0) = \{z\mid |z-z_0| = R\}$ and $f$ is bounded on $C_R$. Then, $|f^{(n)}(z_0)| \leq \frac{n!M}{R^n}$.
  \begin{align*}
    |f^{(n)}(z_0)| &= \left|\frac{n!}{2\pi i}\int_{C_R}\frac{f(z)}{(z-w)^{n+1}}dz\right|\\
                   &= \frac{n!}{2\pi}\left|\int_{C_R}\frac{f(z)}{(z-w)^{n+1}}dz\right|\\
                   \intertext{given $|f(z)| \leq M$,}
    \left|\frac{f(z)}{(z-z_0)^{n+1}}\right| &= \frac{|f(z)|}{R^{n+1}}\\
                                            &\leq \frac{M}{R^{n+1}}\\
                                            \intertext{So}
    |f^{(n)}(z_0)| &\leq \frac{n!}{2\pi}\frac{M}{R^{n+1}}2\pi R\\
                   &= \frac{n!M}{R^{n}}
  \end{align*}
  To show Liouville's Theorem, by the above result, $|f'(z_0)| \leq \frac{M}{R}$. Since $f$ is entire and $M$ is fixed, we can make $R$ arbitrarily large. So, $|f'(z_0)| = 0$, with $z_0$ arbitrary. Thus, $f$ is constant.
  \subsection{Fundamental Theorem of Algebra}%
  Every non-constant polynomial has at least one root in the complex plane.\\

  To prove this, suppose $P(z) = a_nz^n + \cdots + a_1 z + a_0$ has no root. Then, $\frac{1}{P(z)}$ is also entire. We have that
  \begin{align*}
    \lim_{|z|\rightarrow\infty}\left|\frac{1}{P(z)}\right| &= \lim_{|z|\rightarrow\infty}\left|\frac{1}{z^n(a_0/z^n + \cdots + a_n)}\right|\\
                                                           &= \lim \frac{1}{|z^n|} \left|\frac{1}{a_0/z^n + \cdots + a_n}\right|\\
                                                           &\rightarrow 0.
  \end{align*}
  Therefore, there exists $M$ such that for $|z| > M$, $|P(z)| < 1$. Examining $D_M:= \{z\mid |z|\leq M\}$. Since $\left|\frac{1}{P(z)}\right|$ is a real-valued continuous function, it attains a maximum value $A$ on $D_M$ since $D_M$ is compact. Thus, $\left|1/P(z)\right| \leq \max\{1,A\}$ for all $z\in \C$. Thus, $1/P(z)$ is bounded and entire, meaning $P(z)$ is constant. $\bot$
  \subsection{Extrema of Non-Constant Analytic Functions}%
  Let $f$ be analytic on $A\subseteq \C$ open. Then, $|f|$ admits a local maximum at $z_0\in A$ only if $f$ is constant.\\

  $f$ has a local maximum at $z_0$ if $\exists \varepsilon > 0$ such that for all $z\in D_\varepsilon(z_0) := \{z\mid |z-z_0| < \varepsilon\}$, $|f(z)| \leq |f(z_0)|$.\\

  Maximum modulus principle: If $f$ is analytic on a bounded domain $D$ and continuous on $\partial D$, then $f$ attains its maximum on $\partial D$.\\

  For example, if $f(z) = z^2 - 1$, then to find the absolute extrema of $f$ on $D_2(0)$ (the closed disk of radius $2$ about $0$), we know that $f$ attains its absolute extrema on the boundary of $D_2(0)$.
  \begin{align*}
    |f(z)| &= |z^2 - 1|\\
           &\leq |z^2| + |1| = 5\\
           &\geq |z^2| - |1| = 3
  \end{align*}
  We have that $|f(2)| = 3$ and $|f(2i)| = 5$.\\

  If $f$ is a non-constant, non-zero analytic function on a bounded domain $D$, $f$ has no local minimum.
  \begin{description}
    \item[Proof:] Let $g(z) = \frac{1}{f(z)}$. Since $f(z)$ is non-zero on $D$, and $f$ is analytic on $D$, so too is $g$. Therefore, $|g|$ admits its maximum on $\partial D$. Since $\max|g| = \min|f|$, $|f|$ attains its minimum on $\partial D$.
  \end{description}
  To prove the maximum modulus principle, we use the following lemma:
  \begin{description}
    \item[Lemma:] If $f$ is analytic, and $|f|$ is non-constant on a disk $|z-z_0| < r$, then $|f(z_0)|$ is not maximal on $D$.
    \item[Proof of Lemma:] Suppose toward contradiction that $|f(z_0)|$ is the maximum of $|f(z)|$. By the hypothesis, there exists $z_1\in D$ with $|f(z_1)| < |f(z_0)|$. Let $\Gamma$ be the circle $|z-z_0| = |z_1-z_0|$. Since $f$ is analytic,
      \begin{align*}
        2\pi if(z_0) &= \int_{\Gamma} \frac{f(z)}{z-z_0}dz.
      \end{align*}
      On $\Gamma$, $|z-z_0| = |z_1-z_0|$, so
      \begin{align*}
        \left|\int_{\Gamma}\frac{f(z)}{z-z_0}dz\right| &\leq \int_{\Gamma}\left|\frac{f(z)}{z-z_0}\right|dz\\
                                                       &= \int_{\Gamma}\frac{|f(z)|}{|z-z_0|}dz\\
                                                       &= \frac{1}{|z_1-z_0|}\int_{\Gamma}|f(z)|dz\\
                                                       &< \frac{1}{|z_1-z_0|} \int_{\Gamma}|f(z_0)|dz \tag*{(\textasteriskcentered)}\\
                                                       &= \frac{\ell(\Gamma)|f(z_0)|}{|z_1-z_0|}\\
                                                       &= \frac{2\pi|z_1-z_0||f(z_0)|}{|z_1-z_0|}\\
                                                       &= 2\pi |f(z_0)|
      \end{align*}
      (\textasteriskcentered): There must exist $\varepsilon > 0$ such that for $|z-z_1| < \varepsilon$, $|f(z)| < |f(z_0)|$, since $f$ is continuous and $|f(z_1)| < |f(z_0)|$. Let $\Gamma_1 = \Gamma \cap D(z_1,\varepsilon)$, and $\Gamma_2 = \Gamma \setminus \Gamma_1$. Then,
      \begin{align*}
        \left|\int_{\Gamma}\right| &= \left|\int_{\Gamma_1} + \int_{\Gamma_2}\right|\\
                                   &\leq \left|\int_{\Gamma_1}\right| + \left|\int_{\Gamma_2}\right|\\
                                   &< \left|\int_{\Gamma_1}|f(z_0)|\right| + \left|\int_{\Gamma_2}|f(z_0)|\right|.
      \end{align*}
      However, this means $|f(z_0)| < |f(z_0)|$, which is a contradiction.\\

      Alternatively, if $f'(z_0)\neq 0$, then $f$ approximately rotates and stretches or contracts a small disk around $z_0$. If we draw a line from $0$ to $f(z_0)$ through the disk, then there is some point in $\text{im}(f)$ in the disk that has a larger modulus than $f(z_0)$.
  \end{description}
  \subsection{Winding Number}%
  Recall the Cauchy Integral Formula: if $f$ is analytic on a simply connected domain $D$, and $\Gamma$ is a simple closed curve in $D$, with $z_0$ inside $\Gamma$, then
  \begin{align*}
    f(z_0) &= \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{z-z_0}dz.
  \end{align*}
  There is a generalized version: if $f$ is analytic on any domain $D$, and $\Gamma$ is any closed curve that is null-homotopic in $D$. If $z_0\notin \Gamma$, then
  \begin{align*}
    f(z_0)I(\Gamma,z_0) &= \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{z-z_0}dz,
  \end{align*}
  where $I(\Gamma,z_0)$ denotes the winding number of $\Gamma$ about $z_0$.\\

  We define
  \begin{align*}
    I(\Gamma,z_0)\frac{1}{2\pi i}\int_{\Gamma}\frac{1}{z-z_0}dz
  \end{align*}
  for $z_0\notin \Gamma$. We assert that $I(\Gamma,z_0)$ is always an integer.
  \begin{align*}
    \frac{1}{2\pi i}\int_{\Gamma}\frac{f(z)}{z-z_0}dz &= f(z_0)I(\Gamma,z_0)\\
    \int_{\Gamma}\frac{f(z)}{z-z_0}dz &= \int_{\Gamma}\frac{f(z_0)}{z-z_0}dz
  \end{align*}
  \section{Series and Sequences}%
  A sequence in $\C$ is a function $a: \N\rightarrow \C$. We denote $a(n) = a_n$.\\

  A sequence $(a_n)_n$ converges to $L\in \C$ if $\forall \varepsilon > 0$, $\exists N\in \N$ such that for all $n\geq N$, $|a_n - L| < \varepsilon$. In other words, $(a_n)_n$ converges to $L$ if $a_n$ is arbitrarily close to $L$ for all sufficiently large $n$.\\

  A series $\displaystyle\sum_{n=1}^{\infty}a_n$ converges to some $S$ if the sequence of partial sums converges to $S$, where $\displaystyle s_n := \sum_{k=1}^{n}a_n$.
  \subsection{Tests for Convergence and Divergence}%
  \begin{description}
    \item[Divergence Test:] In real numbers, if $\lim_{n\rightarrow\infty}x_n \neq 0$, then $\sum x_n$ diverges.\\

      Similarly, in complex numbers, if $\lim_{n\rightarrow\infty}|a_n| \rightarrow 0$, then $\sum a_n$ diverges.
    \item[Ratio Test:] Let
      \begin{align*}
        L &= \lim_{n\rightarrow\infty}\left|\frac{a_{n+1}}{a_n}\right|.
      \end{align*}
      If $L < 1$, then $\sum a_n$ converges, and if $L > 1$, then $\sum a_n$ diverges. If $L=1$, then the test is inconclusive.
    \item[Comparison Test:] Given $\sum a_n$ and $\sum b_n$ series. If $|a_n| \leq |b_n|$ for sufficiently large $n$, and $\sum b_n$ converges, then $\sum |a_n|$ converges (so $\sum a_n$ converges).\\
    \item[Geometric Series:] If $a_{n+1}/a_n = c$ for all $n$, then $\sum a_n = \sum a_0c^n$, and we say $a_n$ is a geometric series. If $|c| < 1$, then $a_n$ converges, and if $|c| > 1$, then $\sum a_n$ converges.\\

      The partial sums
      \begin{align*}
        s_n &= a_0 + \cdots + a_0c^n\\
        cs_n &= a_0c + \cdots + a_0c^{n+1}\\
        s_n(1-c) &= a_0 - a_0c^{n+1}\\
        s_n &= \frac{a_0 - a_0c^{n+1}}{1-c}\\
            &= a_0 \frac{1-c^{n+1}}{1-c}\\
        \lim_{n\rightarrow\infty} &= a_0 \lim_{n\rightarrow\infty}\frac{1-c^{n+1}}{1-c}\\
                                  &= a_0\frac{1}{1-c} \tag*{since $|c|<1$}
      \end{align*}
  \end{description}
  \subsection{Convergence of Functions}%
  To find for which $z\in \C$ does $\sum \frac{1}{z^n}$ converge, we use the geometric series, meaning $\left|\frac{1}{z}\right| < 1$, meaning $|z| > 1$ is necessary for the series to converge. When $|z| > 1$, the series converges to $\frac{1}{1-(1/z)}$.\\

  Letting $f_n(z) = s_n(z)$, we have that $f_n$ is itself a sequence of functions. Letting $g(z) = \frac{1}{1-(1/z)}$. Then, for each fixed $z$ with $|z|>1$, we see that $\lim f_n(z) = g(z)$. So, on the set $|z|>1$, the sequence $f_n$ converges \textit{pointwise} to $g$.\\

  Let $(f_n)_n$ be a sequence of functions with $f_n: A\rightarrow \C$, $A\subseteq \C$. We say $(f_n)_n$ converges pointwise to $g$ on $A$ if $\forall z \in A$, $\forall \varepsilon > 0$, $\exists M\in\N$ such that for all $n\geq M$, $|f_n(z)-g(z)| < \varepsilon$.\\

  We say $f_n$ converges uniformly to $g$ on $A$ if $\forall \varepsilon > 0$, $\exists M$ such that for all $z\in A$ and $\forall n\geq M$, $|f_n(z)-g(z)| < \varepsilon$.\\

  Let $f_n = \sum_{k=0}^{n}\frac{1}{z}$. Does $f_n$ converge to $g(z) = \frac{1}{1-(1/z)}$ uniformly on $|z|>1$?\\

  We want to show that for some $\varepsilon_0 > 0$, there does not exist $M$ such that $\forall z\in A, \forall n > M, |f_n(z)-g(z)| < \varepsilon_0$. Let $\varepsilon_0 = 1$. Fix $M\in \N$. We will show $\exists z$ with $|z| > 1$ such that for some $n > M$, $|f_n(z) - g(z)| \geq 1$. We have
  \begin{align*}
    |f_n(z)-g(z)| &= \left|\frac{1-\frac{1}{z^{n+1}}}{1-\frac{1}{z}} - \frac{1}{1-\frac{1}{z}}\right|\\
                  &= \frac{1}{z^{n+1}\left(1-\frac{1}{z}\right)}\\
                  \intertext{Let $z = 1 + \delta$ for $\delta > 0$ sufficiently small. Then,}
                  &\geq 1
  \end{align*}
  \subsection{Taylor Series}%
  Recall from Calc II that for $f: \R\rightarrow \R$, a Taylor series for $f$ centered at $x_0$ is
  \begin{align*}
    T_{x_0}(x) &= \sum_{k=0}^{\infty}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n.
  \end{align*}
  If $f$ is infinitely differentiable at $x_0$, we have that $T_{x_0}(x)$ will converge to $f$ in an interval of convergence about $x_0$. For a finite-degree polynomial, we have that
  \begin{align*}
    P_k(x) := \sum_{n=0}^{k}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
  \end{align*}
  approximates $f$. Specifically, we can see that $P_k^{(j)}(x_0) = f^{(j)}(x_0)$ for $j \leq k$.\\

  We say that $f(z)$ is analytic on $z_0$ if $f(z)$ is analytic on $D(z_0;\delta)$ for some $\delta > 0$. If $f(z)$ is analytic at $z_0$, then the Taylor series for $f(z)$ around $z_0$ is
  \begin{align*}
    \sum_{n=0}^{\infty}\frac{f^{(n)}(z_0)}{n!}(z-z_0).
  \end{align*}
  If $f(z)$ is analytic on an open disk $D(z_0;r)$, then the Taylor series for $f(z)$ around $z_0$ converges to $f(z)$ on $D(z_0;r)$, and converges uniformly on $D(z_0;r')\subset D(z_0;r)$.\\

  For example, if $f(z) = (c-z)^{-1}$, we can find a Taylor series for $f$ about $0$, and find the disk of convergence.
  \begin{align*}
    f'(z_0) &= (c-z)^{-2}\\
    f''(z_0) &= 2(c-z)^{-3}\\
    f^{(3)}(z_0) &= 3!(c-z)^{-4}\\
               &\vdots\\
    f^{(n)}(z_0) &= n!(c-z)^{-(n+1)}.
  \end{align*}
  Therefore,
  \begin{align*}
    T(f,z_0) &= \sum_{n=0}^{\infty}\frac{n!(c-z_0)^{-(n+1)}}{n!}(z-z_0)^{n}\\
             &= \sum_{n=0}^{\infty}c^{-(n+1)}z^n.
  \end{align*}
  To find the radius of convergence, we find that $f$ is analytic on $\C\setminus \{c\}$. Thus, $T(f,z_0)$ is convergent about $D(0;|c|)$.\\

  Considering $f(z) = (c-z)^{-1}$ again, we find 
  \begin{align*}
    f(z) &= \frac{1}{c}\frac{1}{1-\frac{z}{c}}\\
         &= \frac{1}{c}\sum_{n=0}^{\infty}c^{-n}z^n \tag*{true iff $|z/c|<1$}\\
         &= \sum_{n=0}^{\infty}c^{-(n+1)}z^n.
  \end{align*}
  To find a Taylor series for $g(z) = (c-z)^{-2}$, we have that $g(z) = f'(z)$, so we can take the Taylor series for $f$ and differentiate it. Since $f$ is analytic on $|z|<|c|$, and $g$ is equal to $f'$, we have that $g$ is convergent on the same disk that $f$ is convergent on.\\

  If $f$ is analytic at $z_0$, and $f(z) = \sum_{n=0}^{\infty}c_n(z-z_0)^n$ on some disk $D(z_0;r)$, then $f'(z) = \sum_{n=1}^{\infty}c_n n (z-z_0)^{n-1}$, and this series converges on $D(z_0;r)$. We can also do integration term-by-term.\\

  For example, to find the Taylor series for $f(z) = \text{Log}(z)$ around $z_0\in \C\setminus (\infty,0]$, we take integrals term-by-term on $g(z) = 1/z$.
  \begin{align*}
    g(z) &= \frac{1}{z}\\
         &= \frac{1}{z_0-(z_0 - z)}\\
         &= \frac{1}{z_0}\frac{1}{1-\left(1-\frac{z}{z_0}\right)}\\
         &= \frac{1}{z_0}\sum_{n=0}^{\infty}\left(1-\frac{z}{z_0}\right)^n.\\
   \intertext{We have that the series converges if $|1-z/z_0| < 1$.}\\
         &= \frac{1}{z_0}\sum_{n=0}^{\infty}\left(\frac{z_0-z}{z_0}\right)\\
         &= \sum_{n=0}^{\infty}\frac{(-1)^n}{z_0^{n+1}}(z-z_0)^{n}\\
    \intertext{so,}
    f(z) &= \int f'(z)dz\\
         &= \int \sum_{n=0}^{\infty}\frac{(-1)^n}{z_0^{n+1}}(z-z_0)^n~dz\\
         &= \sum_{n=0}^{\infty}\frac{(-1)^{n}}{z_0^{n+1}}\int(z-z_0)^n~dz\\
         &= \sum_{n=0}^{\infty}\frac{(-1)^{n}}{z_0^{n+1}}\frac{(z-z_0)^{n+1}}{n+1} + C\\
         \intertext{Specifically, $C = \text{Log}(z_0)$. Thus,}
    f(z) &= \text{Log}(z_0) + \sum_{n=0}^{\infty}\frac{(-1)^{n}}{z_0^{n+1}}\frac{(z-z_0)^{n+1}}{n+1}.
  \end{align*}
  We have that the radius of convergence in $\C$ is equal to $\text{dist}_{(-\infty,0]}(z_0) = |\text{Im}(z_0)|$.\\

  For $f$ and $g$ with respective Taylor series, we can find their sum relatively easily (coefficient-wise addition), but for $fg$, we require convolution.
  \begin{align*}
    \left(a_0 + a_1z + a_2z^2 + \cdots\right)\left(b_0 + b_1z + b_2z^2 + \cdots\right) &= a_0b_0 + (a_0b_1 + a_1b_0)z + \cdots\\
                 &= \sum_{n=0}^{\infty}\left(\sum_{k=0}^{n}a_kb_{n-k}\right)z^n
  \end{align*}
  \section{Power Series}%
  Recall that the Taylor series for $f(z)$ about $z_0$ is
  \begin{align*}
    f(z) &= \sum_{j=0}^{\infty}\frac{f^{(j)}(z_0)}{j!}(z-z_0)^j\\
         &= \sum_{j=0}^{\infty}c_j(z-z_0)^j\\
    c_j &= \frac{f^{(j)}(z_0)}{j!}.
  \end{align*}
  Suppose instead that we start with the sequence $(c_n)_n$. For example, let $c_j = \frac{j^2 + 1 + i}{(2i)^j}$. We may ask if $\sum c_j(z-z_0)^j$ is convergent (and thus the Taylor series for some analytic function about $z_0$).\\

  If $\sum c_j(z-z_0)^j$ converges for some $z\neq z_0$, then it indeed is. A series of the form $f(z) = \sum c_j(z-z_0)^j$ is known as a power series. However, the function that the power series converges to may not be an elementary function.\\

  For every power series $\sum c_j(z-z_0)^j$, there exists a single value $R\in [0,\infty]$ such that the power series converges on $|z-z_0| < R$ and diverges on $|z-z_0| \geq R$. For every $r < R$, the power series converges \textit{uniformly} on $|z-z_0| < r$. If $R$ is finite, $|z-z_0| = R$ is called the circle of convergence. The power series may converge at some, all, or no points on $|z-z_0| = R$. $R$ is known as the radius of convergence.\\

  Let $c_k = \frac{k^2 + 1 + i}{(2i)^k}$, with $\sum c_k (z-5i)^k$ the series we must find the radius of convergence for. Using the ratio test, we find
  \begin{align*}
    \lim_{k\rightarrow\infty}\left|\frac{\frac{(k+1)^2 + 1 + i}{(2i)^{k+1}}(z-5i)^{k+1}}{\frac{k^2 + 1 + i}{(2i)^k}(z-5i)^k}\right| &= \lim_{k\rightarrow\infty}\left|(z-5i)\frac{(k+1)^2 + 1 + i}{(k^2 + 1 + i)(2i)}\right|\\
                              &= \lim_{k\rightarrow\infty}\left|\frac{(z-5i)}{2i} \right|\\
                              &= \frac{|z-5i|}{2}.
  \end{align*}
  If $\frac{|z-5i|}{2} < 1$, then the power series converges, so we have $R = 2$.\\

  We care about the uniform convergence of the power series since if $(f_n)_n$ is a sequence of continuous functions that converges uniformly to $f$ on $D$, then $f$ is continuous on $D$. If $(f_n)_n$ are analytic under the same condition, then $f$ is analytic.\\

  Notice that $f_n(z) = \sum_{j=0}^{n}c_j(z-z_0)^j$ is a polynomial. Since the $(f_n)_n$ are analytic, if it is the case that the power series converges uniformly on $D$, then $f(z)$ is analytic.
  \begin{enumerate}[(i)]
    \item Every power series with nonzero radius of convergence is an analytic function inside its circle of convergence.
    \item The Taylor series for the function $\sum_{j=0}^{\infty}c_j(z-z_0)^j$ is itself.
  \end{enumerate}
  For example, let $g(z) = \sum_{k=0}^{\infty}\frac{k^2 + 1 + i}{(2i)^k}(z-5i)^{k}$. Then, $g$ is analytic on $|z-5i| < 2$, and $g^{(9)}(5i) = \left(\frac{82 + i}{(2i)^{9}}\right)(9!)$\\

  To prove (ii), consider $\sum_{j=0}^{\infty}a_j(z-z_0)^j = \sum_{j=0}^{\infty}b_j(z-z_0)^j$. We then ask if $a_j = b_j$ for all $j$. The constant term of the $n$th-derivative of the left-hand side is $a_nn!$, and the constant term of the $n$th derivative of the right-hand side is $b_nn!$. Plugging in $z_0$ to the respective $n$th derivatives, we find that $a_n = b_n$.\\

  To prove that a sequence of continuous functions $(f_n)_n \rightarrow f$ uniformly to a continuous function $f$, we pick $z_0\in D$ to show that $f$ is continuous at $z_0$.\\

  Let $\varepsilon > 0$. We want to show that there exists $\delta > 0$ such that $|z-z_0| < \delta \Rightarrow |f(z) - f(z_0)| < \varepsilon$.\\

  Since $(f_n)_n \rightarrow f$ uniformly on $D$, there exists $M$ such that $\forall n \geq M$ and $\forall z\in D$, $|f_n(z) - f(z)| < \varepsilon$. Since $f_M$ is continuous, we have that $\exists \delta > 0$ such that $|f_M(z) - f_M(z_0)| < \varepsilon$ for $|z-z_0| < \delta$. Then,
  \begin{align*}
    |f(z) - f(z_0)| &= |f(z) - f_M(z) + f_M(z) - f_M(z_0) + f_M(z_0) - f(z_0)|\\
                    &\leq |f(z) - f_M(z)| + |f_M(z) - f_M(z_0)| + |f_M(z_0) - f(z_0)|\\
                    &< 3\varepsilon
  \end{align*}
  \subsection{Laurent Series}%
  Suppose $\sum_{j=1}^{\infty}a_j(z-z_0)^j$ converges on $|z-z_0| < R_1$. Then, $\sum_{j=1}^{\infty}a_jw^j$, where $w = z-z_0$ converges on $|w|< R_1$. Then, $\sum_{j=1}^{\infty}a_j \left(\frac{1}{z-z_0}\right)^{j}$ converges where $\left|\frac{1}{z-z_0}\right| < R_1$, so it converges with $|z-z_0| > \frac{1}{R_1}$.\\

  We write it as $\sum_{j=1}^{\infty}a_j(z-z_0)^{-j}$. Let $c_{-1} = a_1$, $c_{-2} = a_2$, etc.; then, we write the series as $\sum_{j=1}^{\infty}c_{-j}(z-z_0)^{-j}$. Suppose also that $\sum_{j=0}^{\infty}b_j(z-z_0)^j$ converges on $|z-z_0| < R_2$ such that $\frac{1}{R_1} < R_2$. Then, both series converge on the annulus defined by $ \frac{1}{R_1} < |z-z_0| < R_2$. Let $c_j = b_j$ for $j \geq 0$. Then,
  \begin{align*}
    \sum_{j=-\infty}^{\infty}c_j(z-z_0)^j &= \sum_{j=0}^{\infty}b_j(z-z_0)^j + \sum_{j=1}^{\infty}c_{-j}(z-z_0)^{-j}
  \end{align*}
  converges on the annulus.\\

  Suppose $f$ is analytic on the annulus $r < |z-z_0| < R$, with $r,R \in [0,\infty]$. Then, for all $z$ in the annulus, the series $= \sum_{j=-\infty}^{\infty}c_j(z-z_0)^j$ converges to $f(z)$, where $c_j$ is given by
  \begin{align*}
    c_j &= \frac{1}{2\pi i}\int_{\gamma}\frac{f(z)}{\left(z-z_0\right)^{j+1}}dz,
  \end{align*}
  where $\gamma$ is any counterclockwise simple closed curve in the annulus.\\

  When $j$ is positive, we have that
  \begin{align*}
    c_j &= \frac{1}{2\pi i}\int_{\gamma}\frac{f(z)}{(z-z_0)^{j+1}}\\
        &= \frac{1}{n!}f^{(j)}(z_0).
  \end{align*}
  To find the Laurent series for $f(z) = \frac{e^z}{z-i}$ in $\C\setminus \{i\}$, we do the following.\\

  We need $\sum_{j=-\infty}^{\infty}c_j(z-i)^j$. We can write $e^z = \sum_{j=0}^{\infty}\frac{z^j}{j!}$ about $0$.
  \begin{align*}
    e^z &= e^{z-i+i}\\
        &= e^ie^{z-i}\\
        &= e^{i}\sum_{j=0}^{\infty}\frac{(z-i)^j}{j!}.
  \end{align*}
  Thus,
  \begin{align*}
    \frac{e^z}{z-i} &= \frac{1}{z-i}e^i\sum_{j=0}^{\infty}\frac{(z-i)^j}{j!}\\
                    &= \sum_{j=0}^{\infty}\frac{e^i}{j!}(z-i)^{j-1},
  \end{align*}
  meaning it converges on $0 < |z-i| < \infty$.\\

  To try to find the Laurent series for $\frac{1}{z^2(z-i)}$, we may consider on different annuli. For $0 < |z| < 1$, we first have to find the Laurent series for $\frac{1}{z-i}$.
  \begin{align*}
    \frac{1}{z-i}\frac{i}{i} &= \frac{i}{1 - (iz)}\\
                             &= \sum_{j=0}^{\infty} i^{j+1}z^j.
  \end{align*}
  This Taylor series converges on $|z|<1$. Thus,
  \begin{align*}
    \frac{1}{z^2 (z-i)} &= \frac{1}{z^2}\sum_{j=0}^{\infty}i^{j+1} z^j\\
                        &= \sum_{j=0}^{\infty}i^{j+1}z^{j-2}.
  \end{align*}
  For $1 < |z| < \infty$, we can do
  \begin{align*}
    \frac{1}{z-i} &= \frac{1}{z\left(1-\frac{i}{z}\right)}\\
                  &= \frac{1}{z}\sum_{j=0}^{\infty}i^jz^{-j}\\
    \frac{1}{z^2(z-i)} &= \sum_{j=0}^{\infty}i^jz^{-j-3}.
  \end{align*}
  To find the Laurent series for $f(z) = \frac{1}{(z-2)(z-3)}$ on $|z| < 2$, we do the following.
  \begin{align*}
    \frac{1}{(z-2)(z-3)} &= \frac{1}{z-3} - \frac{1}{z-2}\\
           \frac{1}{z-3} &= \frac{1}{-3\left(1-(z/3)\right)}\\
                         &= -\frac{1}{3}\sum_{j=0}^{\infty}\left(\frac{z}{3}\right)^{j}\\
           \frac{1}{z-2} &= -\frac{1}{2}\sum_{j=0}^{\infty}\left(\frac{z}{2}\right)^j
  \end{align*}
  Both of these series converge on $|z|<2$, so
  \begin{align*}
    \frac{1}{z-3} - \frac{1}{z-2} &= \sum_{j=0}^{\infty}\frac{z^j}{2^{j+1}} - \frac{z^j}{3^{j+1}}\\
                                  &= \sum_{j=0}^{\infty} \frac{3^{j+1}-2^{j+1}}{6^{j+1}}z^j.
  \end{align*}
  \section{Cauchy Criterion and Convergence}%
  Let $(a_n)_n \in \C$ be such that $\forall \varepsilon > 0$, $\exists N$ large such that for $m,n\geq N$, $|a_m - a_n| < \varepsilon$. A sequence is convergent if and only if it is Cauchy.\\

  Let $(a_n)_n\rightarrow \ell\in \C$. Let $\varepsilon > 0$. Then, $\exists N$ such that for all $n\geq N$, $|a_n - L| < \varepsilon$. Let $m,n \geq N$. Then,
  \begin{align*}
    |a_n - a_m| &= |a_n - L + L - a_m|\\
                &\leq |a_n - L| + |a_m - L|\\
                &< 2\varepsilon
  \end{align*}
  The other direction requires the axiom of choice.\\

  Recall the comparison test: if $\sum b_j$ converges, and $|a_j| < b_j$ for all $j$, then $\sum a_j$ converges. To prove this, we require the Monotone Convergence Theorem --- every bounded monotone sequence of real numbers converges.\\

  Let $(a_j)_j$ be nondecreasing and bounded above by $B$. Since $(a_j)_j$ is bounded above, it has a least upper bound $L$. Let $\varepsilon > 0$. Since $L$ is the least upper bound, $L-\varepsilon$ is not an upper bound for $(a_j)_j$, meaning $\exists j$ such that $a_j > L - \varepsilon$. Thus, $L-\varepsilon < a_j \leq a_{j+1} \leq \cdots < L$. So, for all $k  > j$, $|a_k - L| < \varepsilon$.\\

  To show the comparison test, let $S_n = \sum_{j=0}^{n}|a_j|$. We have that $S_n$ is monotone increasing. Additionally, $S_n$ is bounded above since $S_n \leq \sum_{j=0}^{n} b_j \leq \sum b_j$, which converges. Let $T_n = \sum_{j=0}^{n}a_j$. Pick $m,n$ with $m < n$. Given $\varepsilon > 0$, we have that for $m,n \geq N$,
  \begin{align*}
    |T_m - T_n| &= \left|\sum_{j=m+1}^{n}a_j\right|\\
                &\leq \sum_{j=m+1}^{n}|a_j|\\
                &= S_n - S_m\\
                &< \varepsilon,
  \end{align*}
  so $T_n$ is Cauchy, and thus convergent.\\

  Let $A\subseteq \R$. Then, $\sup A$ is the least upper bound of $A$ --- if $A$ is not bounded above, then $\sup A = \infty$.\\

  For a sequence $(a_n)_n\in \R$, define $(x_n)_n$ as $x_n = \sup (a_j)_{j\geq n}$. Then, $x_n = \sup\{a_n,x_{n+1}\}$. Thus, we have $x_n \geq x_{n+1} \geq \cdots$, so $(x_n)_n$ may converge to some $L$, or $x_n \rightarrow \pm\infty$. We define $\lim_{n\rightarrow\infty}x_n$\\

  Let $(a_n)_n = (-1)^n$. Then, $\limsup a_n = 1$. However, $\limsup (-2)^n = \infty$.\\

  Given any power series $\sum_{j=0}^{\infty}a_j(z-z_0)^j$, $\exists R \in [0,\infty]$ such that the series converges uniformly for $|z-z_0| < R$. Let $\ell = \limsup \sqrt[n]{a_n}$. Then, $R = \frac{1}{\ell}$. If $\ell = 0$, then $R = \infty$, and if $\ell = \infty$, then $R = 0$.\\

  Let $z\in \C$ with $|z-z_0| < \frac{1}{\ell}$. Then, $\exists \ell'$ such that $|z-z_0| < \frac{1}{\ell'} < \frac{1}{\ell}$. Let $c = \ell' |z-z_0| < 1$. Let $x_n = \sup \{a_n,a_{n+1},\dots\}$. So, $\lim x_n = \ell$. 
\end{document}
