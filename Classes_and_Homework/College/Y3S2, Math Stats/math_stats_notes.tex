\documentclass[10pt]{extarticle}
\title{}
\author{}
\date{}
\usepackage[shortlabels]{enumitem}


%paper setup
\usepackage{geometry}
\geometry{letterpaper, portrait, margin=1in}
\usepackage{fancyhdr}
% sans serif font:
\usepackage{cmbright}
%symbols
\usepackage{amsmath}
\usepackage{bigints}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage[hidelinks]{hyperref}
\usepackage{gensymb}
\usepackage{multirow,array}
\usepackage{multicol}

\newtheorem*{remark}{Remark}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%chemistry stuff
%\usepackage[version=4]{mhchem}
%\usepackage{chemfig}

%plotting
\usepackage{pgfplots}
\usepackage{tikz}
\tikzset{middleweight/.style={pos = 0.5}}
%\tikzset{weight/.style={pos = 0.5, fill = white}}
%\tikzset{lateweight/.style={pos = 0.75, fill = white}}
%\tikzset{earlyweight/.style={pos = 0.25, fill=white}}

%\usepackage{natbib}

%graphics stuff
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[style=numeric, backend=biber]{biblatex} % Use the numeric style for Vancouver
\addbibresource{the_bibliography.bib}
%code stuff
%when using minted, make sure to add the -shell-escape flag
%you can use lstlisting if you don't want to use minted
%\usepackage{minted}
%\usemintedstyle{pastie}
%\newminted[javacode]{java}{frame=lines,framesep=2mm,linenos=true,fontsize=\footnotesize,tabsize=3,autogobble,}
%\newminted[cppcode]{cpp}{frame=lines,framesep=2mm,linenos=true,fontsize=\footnotesize,tabsize=3,autogobble,}

%\usepackage{listings}
%\usepackage{color}
%\definecolor{dkgreen}{rgb}{0,0.6,0}
%\definecolor{gray}{rgb}{0.5,0.5,0.5}
%\definecolor{mauve}{rgb}{0.58,0,0.82}
%
%\lstset{frame=tb,
%	language=Java,
%	aboveskip=3mm,
%	belowskip=3mm,
%	showstringspaces=false,
%	columns=flexible,
%	basicstyle={\small\ttfamily},
%	numbers=none,
%	numberstyle=\tiny\color{gray},
%	keywordstyle=\color{blue},
%	commentstyle=\color{dkgreen},
%	stringstyle=\color{mauve},
%	breaklines=true,
%	breakatwhitespace=true,
%	tabsize=3
%}
% text + color boxes
\renewcommand{\mathbf}[1]{\mathbbm{#1}}
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable}
\tcbuselibrary{skins}
\newtcolorbox{problem}[1]{colback=white,enhanced,title={\small #1},
          attach boxed title to top center=
{yshift=-\tcboxedtitleheight/2},
boxed title style={size=small,colback=black!60!white}, sharp corners, breakable}
%including PDFs
%\usepackage{pdfpages}
\setlength{\parindent}{0pt}
\usepackage{cancel}
\pagestyle{fancy}
\fancyhf{}
\rhead{Avinash Iyer}
\lhead{Mathematical Statistics: Class Notes}
\newcommand{\card}{\text{card}}
\newcommand{\ran}{\text{ran}}
\newcommand{\N}{\mathbbm{N}}
\newcommand{\Q}{\mathbbm{Q}}
\newcommand{\Z}{\mathbbm{Z}}
\newcommand{\R}{\mathbbm{R}}
\setcounter{secnumdepth}{0}
\begin{document}
\section{Distributions and Estimates}%
  The purpose of both of these distributions is to allow for inferences about $\mu$ and $\sigma$ in an unknown distribution. Both are quotients of known distributions.\\
\subsection{Preliminaries}%
  \begin{description}
    \item[Sample Mean:] Let $Y_1,\dots,Y_n$ be a random, independent sample from a distribution with mean $\mu$ and variance $\sigma^2$. Then,
      \begin{align*}
        \overline{Y} &= \frac{1}{n}\sum_{i=1}^{n}Y_i \tag*{Sample Mean}
      \end{align*}
      is a distribution with mean $\overline{\mu} = \mu$ and variance $\overline{\sigma}^2 = \frac{\sigma^2}{n}$. If the underlying distribution is a normal distribution, then $\frac{\overline{Y}-\mu}{\sigma/\sqrt{n}}$ is a \textit{standard} normal distribution.
    \item[Sample Variance:] The \textit{sample variance} is defined as
      \begin{align*}
        S^2 &= \frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\overline{Y})^2.\tag*{Sample Variance}
      \end{align*}
      It is important to note that the sample variance is found for samples drawn from a distribution; for population standard deviation/variance, we use $n$ instead of $n-1$ in the denominator.\\

      When $Y_i$ is a normal distribution, then $\frac{(n-1)S^2}{\sigma^2}$ is a $\chi^2$ distribution with $n-1$ df --- $S^2$ and $\overline{Y}$ are independent.
  \end{description}
\subsection{Definition of $T$ Distribution}%
  Let $Z$ be a standard normal distribution, $W$ be $\chi^2$ with $\nu$ df, and $Z$ and $W$ be independent. Then,
  \begin{align*}
    T &= \frac{Z}{\sqrt{W/\nu}}
  \end{align*}
  has a $T$ distribution with $\nu$ df.
  \begin{description}
    \item[Creating a $T$ Distribution:] Let $Y_i$ be sampled from a normal distribution with mean $\mu$ and standard deviation $\sigma$.\\
      
      Then, $Z = \frac{\overline{Y}-\mu}{\sigma/\sqrt{n}}$ is a standard normal distribution, and $W = \frac{(n-1)S^2}{\sigma^2}$ is $\chi^2$ with $n-1$ df.\\

      So,
      \begin{align*}
        T &= \frac{Z}{\sqrt{W/(n-1)}}\\
          &= \frac{(\overline{Y}-\mu)\sqrt{n}}{\sigma}\sqrt{\frac{(n-1)\sigma^2}{S^2}}\\
          &= \frac{(\overline{Y}-\mu)\sqrt{n}}{S}
      \end{align*}
      has a $T$ distribution with $n-1$ df.
    \item[$T$ Distribution:] Let $Y_1,\dots,Y_6$ be samples from a normal distribution with unknown $\mu$, $\sigma$. Estimate $P(|\overline{Y}-\mu|<(2S/\sqrt{n}))$.\\

      Thus, we have
      \begin{align*}
        P\left(|\overline{Y}-\mu| \leq \frac{2S}{\sqrt{n}}\right) &= P\left(-2\leq \frac{\sqrt{n}(\overline{Y}-\mu)}{S}\leq 2\right)\\
                                                                  &= P(-2 \leq T \leq 2)
      \end{align*}
      Thus, for $n=6$, we have that our random variable $T$ has 5 df. By looking at a $T$ distribution table, we can find that $P \approx 0.9$. We can also use R.
  \end{description}
\subsection{Definition of $F$ Distribution}%
  Let $W_1$ and $W_2$ be independent $\chi^2$ distributions with $\nu_1$ and $\nu_2$ df respectively. Then, the $F$ distribution with $\nu_1$ numerator df and $\nu_2$ denominator df is found as follows:
  \begin{align*}
    F &= \frac{W_1/\nu_1}{W_2/\nu_2}
  \end{align*}
  \begin{description}
    \item[Simplifying an $F$ Distribution:] Let $n_1$ samples be drawn from normal distribution with mean $\mu_1$ and variance $\sigma_1^2$, and $n_2$ samples be drawn from normal distribution with mean $\mu_2$ and variance $\sigma_2^2$. Both distributions are independent.\\

      From each of these samples, we find the sample variance, and create $\chi^2$ distributions with their respective df.
      \begin{align*}
        W_1 &= \frac{(n_1-1)S_1^2}{\sigma_1^2}\\
        W_2 &= \frac{(n_2-2)S_2^2}{\sigma_2^2}
      \end{align*}
      Therefore, we have
      \begin{align*}
        F &= \frac{W_1/(n_1-1)}{W_2/(n_2-1)}\\
          &= \frac{(n_1-1)S_1^2}{\sigma_1^2(n_1-1)} \frac{\sigma_2^2(n_2-1)}{(n_2-1)S_2^2}\\
          &= \frac{\sigma_2^2}{\sigma_1^2}\frac{S_1^2}{S_2^2}
      \end{align*}
      as an $F$ distribution with $n_1-1$ numerator df and $n_2-1$ denominator df.
    \item[Applying the $F$ Distribution:] Let $n_1=6$ and $n_2=10$ be two samples from independent normal distributions with the same $\sigma^2$. Find $b$ such that $P \left(\frac{S_1^2}{S_2^2} \leq b\right) = 0.95$.
      \begin{align*}
        \frac{S_1^2}{S_2^2} &= \frac{S_1^2/\sigma^2}{S_2^2/\sigma^2}\\
      \end{align*}
      The given $F$ distribution has $5$ numerator df and $9$ denominator df. Therefore, we want to find $0.95 = P(F_{5,9} < b)$, or find the $0.95$ quantile; in R, we find this with the \texttt{qt} function.
  \end{description}
  \subsection{Normal Approximation of Binomial}%
  Recall that a binomial distribution $Y$ with $n$ trials and $p$ probability of success has probabilities found below:
  \begin{align*}
    P(Y\leq \ell) &= \sum_{k=0}^{\ell} {n\choose k}p^k (1-p)^{n-k}.
  \end{align*}
  For very large $n$, this sum is hard to calculate. We could approximate with the Poisson distribution, but this still requires a lot of calculations and large factorial values. Instead, we will try the following:
    \begin{align*}
      X_i &= \begin{cases}
        1 & \text{$i$ trial success}\\
        0 & \text{$i$ trial failure}
      \end{cases}\\
        E(X_i) &= p\\
        E(X_i^2) &= p\\
        V(X_i) &= p(1-p)\\
        \overline{X} &= \frac{1}{n}\sum_{i=1}^{n}X_i = \frac{Y}{n}\\
        E(\overline{X}) &= p\\
        V(\overline{X}) &= \frac{p(1-p)}{n}
    \end{align*}
    By the Central Limit Theorem, we approximate $\overline{X}$ as a normal distribution with mean $p$ and standard deviation $\sqrt{\frac{p(1-p)}{n}}$.\\

    Alternatively, we can create, for large fixed $n$, $Y = n\overline{X}$ with mean $np$ and standard deviation $\sqrt{np(1-p)}$.\\

    For example, consider $p=0.5$, $n=100$, $Y = $ number of successes. To find $P\left(\frac{Y}{n} > 0.55\right)$. By the Central Limit Theorem, this is approximately a normal distribution with mean $0.5$ and standard deviation $0.05$.
    \begin{description}
      \item[Applying Central Limit Theorem:] Let $Y$ be a binomial distribution with $n=25$ and $p=0.4$. Then, $\mu = np = 10$, and standard deviation $\sigma = \sqrt{\frac{p(1-p)}{n}} = 5\sqrt{0.24}$.\\

        To find $P(Y\leq 8)$, we can potentially approximate with $P(X\leq 8.5)$ --- the reason we use $8.5$ instead of $8$ is due to the fact that $n$ may not be large enough, a process known as the continuity correction.\\

        Using standardization (or R), we find that this probability is approximately $0.269$.\\

        The actual probability $P(Y\leq 8)$ is found as below:
        \begin{align*}
          P(Y\leq 8) &= \sum_{k=0}^{8} {25\choose k}(0.4)^k(0.6)^{1-k}\\
                     &= 0.274
        \end{align*}
    \end{description}
    The normal approximation for the binomial is adequate when $p \pm 3\sqrt{\frac{p(1-p)}{n}}\in (0,1)$. Essentially, the binomial trial needs to have an adequate sample size such that the ``spread'' is small. This is equivalent to $n \geq 9\frac{\max(p,1-p)}{\min(p,1-p)}$.
  \section{Estimators}%
  Let $Y$ be a random variable with an \textit{unknown} distribution.
  \begin{description}
    \item[Parameter:] Feature of $Y$'s distribution that are not computable from samples.
    \item[Examples of Parameters:] $\mu$, $\sigma$, $m'_k$, interval $(a,b) \ni P(y\in I) = 0.95$.
    \item[Statistic:] Random variable that is computable from samples.
    \item[Examples of Statistics:] sample mean, $\overline{Y}$, sample variance, $S^2$, $Y_{(i)}$.
    \item[Estimator:] a statistic intended to approximate a parameter. A point estimator estimates a single value.
    \item[Examples of Estimators:] $\overline{Y}$ as an estimator for $\mu$, and $S^2$ as an estimator of $\sigma^2$.
  \end{description}
  \subsection{Bias and Mean Square Error of Estimators}%
  We want to find $\theta$, a constant parameter of the underlying distribution --- $\hat{\theta}$ is a random variable.\\

  If $E(\hat{\theta})$ is close to $\theta$, we can say that $\hat{\theta}$ is a good estimator --- more precisely, we define the bias $B(\hat{\theta}) = E(\hat{\theta})-\theta$, and if $B(\hat{\theta}) = 0$, then $\hat{\theta}$ is an unbiased estimator.\\

  In addition to minimizing bias, to see whether or not an estimator is good requires minimizing the variance of the estimator --- the mean squared estimator $\text{MSE}(\hat{\theta}) = V(\hat{\theta}) + B(\hat{\theta})^2$. Notice that for an \textit{unbiased} estimator, $\text{MSE}(\hat{\theta}) = V(\hat{\theta})$.
  \begin{description}
    \item[Exercise 8.12:] Let $\theta$ be the true voltage of some electronic device. The voltage test has results uniformly distributed over $[\theta,\theta + 1]$. There are $n$ tests, $Y_1,\dots,Y_n$. Evaluate $\overline{Y}$ as an estimator for $\theta$.
    \item[Solution:] Since the voltage is uniformly distributed over $[\theta,\theta + 1]$, we have that $Y_i$ is uniform on $[\theta,\theta + 1]$. Therefore, $E(Y_i) = \theta + 0.5$, and $V(Y_i) = \frac{1}{12}$.\\

      Therefore, $E(\overline{Y}) = \theta + 0.5$, and $V(\overline{Y}) = \frac{1}{12n}$, meaning $\text{MSE}(\hat{\theta}) = \frac{1}{12n} + \frac{1}{4}$.\\

      If we want an unbiased estimator for $\theta$, we take $\hat{\theta} = \overline{Y} - \frac{1}{2}$. Then,
      $E(\hat{\theta}) = E(\overline{Y}) - E(1/2) - \theta = 0$. By shifting this estimator, our new MSE is $\frac{1}{12n}$.
    \item[Example 8.1:] We will compare the two estimators of $\sigma^2$: sample variance and population variance.
      \begin{align*}
        S^2 &= \frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\overline{Y})^2\\
        {S'}^2 &= \frac{1}{n}\sum_{i=1}^{n}(Y_i - \overline{Y})^2
      \end{align*}
    \item[Solution:] Recall $V(X) = E(X^2) - (E(X))^2$. Therefore, $E(X^2) = V(X) + (E(X))^2$.
      \begin{align*}
        E(Y_i^2) &= V(Y_i) + \left(E(Y_i)\right)^2\\
                 &= \sigma^2 + \mu^2\\
        E(\overline{Y}^2) &= V(\overline{Y}) + \left(E(\overline{Y})\right)^2\\
                          &= \frac{\sigma^2}{n} + \mu^2\\
                          \shortintertext{Notice that}
        \sum (Y_i-\overline{Y})^2 &= \sum(Y_i^2 - 2Y_i\overline{Y} + \overline{Y}^2)\\
                                  &= \sum Y_i^2 - 2\overline{Y}\sum Y_i + \sum \overline{Y}^2\\
                                  &= \sum Y_i^2 -2n\overline{Y}^2+ n\overline{Y}^2\\
                                  &= \sum_{Y_i}^2 - n\overline{Y}^2\\
        E\left(\sum (Y_i-\overline{Y})^2\right) &= E(\sum Y_i^2) - nE(\overline{Y}^2)\\
                                                &= n(\sigma^2 + \mu^2) - n\left(\frac{\sigma^2}{n} + \mu^2\right)\\
                                                &= (n-1)\sigma^2\\
        B({S'}^2) &= \frac{1}{n}(n-1)\sigma^2 - \sigma^2\\
                  &= -\frac{1}{n}\sigma^2 \neq 0\\
        B(S^2) &= \frac{1}{n--1}(n-1)\sigma^2 - \sigma^2\\
               &= 0
      \end{align*}
      ${S'}^{2}$ is known as the \textit{biased sample variance}, while $S^2$ is the unbiased sample variance.
  \end{description}
  The standard error $\sigma_{\hat{\theta}} = \sqrt{V(\hat{\theta})}$. If $\hat{\theta}$ is unbiased, then $\sigma_{\hat{\theta}} = \sqrt{\text{MSE}(\hat\theta)}$
  \subsection{Errors and Confidence Intervals}%
  \begin{description}
    \item[Error of Estimation:] The error of estimation is $\varepsilon = |\hat{\theta} - \theta|$. Notice that while $\theta$ is a fixed value, $\varepsilon$ is a random variable.\\

      We say $\hat{\theta}$ is a ``good'' estimator if there is a high probability that $\varepsilon$ is small. Specifically, $\varepsilon$ being small often means $\exists b$ such that $\varepsilon < b$ --- alternatively, $|\hat{\theta} - \theta| < b$, meaning $\theta -b < \hat{\theta} < \theta + b$, so $\hat{\theta}\in (\theta-b,\theta + b)$.\\

      We often set $b$ to be $2\sigma_{\hat{\theta}}$, or $2\cdot\text{SE}(\hat{\theta})$.\\

      When $\hat{\theta}$ is unbiased, $\mu_{\hat{\theta}} = E(\hat{\theta}) = \theta$. So, the $2\sigma_{\hat{\theta}}$ interval about $\theta$ is the same as the $2\sigma_{\hat{\theta}}$ about $\hat{\theta}$.\\

      Finally, $\hat{\theta}$ often, but not always, has an approximate normal distribution. Therefore, the probability that $\hat{\theta}$ is within $2\sigma_{\hat{\theta}}$ of $\mu_{\hat{\theta}}$ is approximately 0.95. Specifically, $P(|\hat{\theta} - \mu_{\hat{\theta}}| < 2\sigma_{\hat{\theta}})$.\\

      Recall that Chebyshev's Theorem states that $P(|\hat{\theta} - -\mu_{\hat{\theta}}| < 2\sigma_{\hat{\theta}}) \geq 1-\frac{1}{2^2} = 0.75$.
    \item[Example 8.3:] Suppose there are two types of tire. $n_1 = n_2 = 100$ of each type, with $Y_1 = Y_2 = $ miles tire lasts. $\overline{Y}_{1} = 26400$ miles while $\overline{Y}_2 = 25100$ miles. $S^2_{1} = 144000000$ and $S_2^{2} = 196000000$.\\

      Let's try to estimate how much longer tire 1 lasts than tire 2. First, we will use an unbiased estimator for the mean (sample mean).
      \begin{align*}
        \mu_{Y_1-Y_2} &= \mu_{Y_1} - \mu_{Y_2}\\
                      &\approx \overline{Y}_1 - \overline{Y}_2\\
                      &= 1300\\
        \sigma_{\overline{Y}_1-\overline{Y}_2} &= \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}} \tag*{Table 8.1}\\
                                               &\approx \sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}} \tag*{$S^2$ an unbiased estimator for $\sigma^2$}\\
                                               &= 184.4
      \end{align*}
      Therefore, the difference in the life expectancy between the types is about $1300$ miles, and there is approximately probability 0.95 chance that the life expectancy is within 368.8 miles of 1300.\\

      The interval $[1300-368.8,1300+368.6]$ is called an interval estimator or confidence interval, expressed as $[\hat{\theta}_{L},\hat{\theta}_{H}]$.
      \begin{itemize}
        \item $\hat{\theta}_{L}$: lower confidence limit, a left endpoint estimator.
        \item $\hat{\theta}_{U}$: upper confidence limit, a right endpoint estimator.
      \end{itemize}
    \item[Example 8.4:] One sample, $Y$, from exponential distribution with PDF
      \begin{align*}
        f(y) &= \begin{cases}
          \frac{1}{\theta}e^{-y/\theta}& y\in[0,\infty)\\
          0 & y\in(-\infty,0)
        \end{cases}
      \end{align*}
      To estimate $\theta$, we would prefer a PDF without $\theta$. Let
      \begin{align*}
        U &= \frac{Y}{\theta}\tag*{pivotal quantity}\\
          F_{U}(u) &= P(U\leq u)\\
               &= P(Y/\theta \leq u)\\
               &= F_{Y}(u\theta)\\
               &= 1-e^{-u}\\
        f(u) &= \begin{cases}
          e^{-u}& u\in[0,\infty)\\
          0 & u\in(-\infty,0)
        \end{cases}\\
      \end{align*}
      We want $a,b$ such that $P(a\leq\theta\leq b) = 0.9$. Pick $c,d$ such that $P(c\leq U \leq d) = 0.9$.\\

      By integrating, we find $c = -\ln(0.95) = 0.051$, and $d = 2.996$. Now,
      \begin{align*}
        0.9 &= P(0.051 \leq U \leq 2.996)\\
            &= P(0.051 \leq Y/\theta \leq 2.996)\\
            &= P(0.051/Y \leq 1/\theta \leq 2.996/\leq)\\
            &= P(Y/2.996 \leq \theta \leq Y/0.051).
      \end{align*}
      meaning that for $Y=2$, there is a probability 0.9 that $\theta\in [0.668,39]$.
  \end{description}
\end{document}
